{"docstore/metadata": {"871d3b50-3fad-4381-abb5-2153027503ae": {"doc_hash": "6da3ffdc7befe75e1bd26449bc6a94b5da08f90b43f59edf6927edec31069913"}, "cb84743a-e549-4d56-8967-58adb8bab729": {"doc_hash": "c4938e028c4677b95c5e3889be16d840bf4686820ef4338c86a33050d8439f2a"}, "d5cc783b-84d4-4dd7-907a-0108da852566": {"doc_hash": "26b568c74f4734608d729747a03879f6a9425d7dc7bf085f8dcb6361f69e17fb"}, "2424edb5-ec5d-48f6-8c3c-926100170f55": {"doc_hash": "d194969d7b2ec875d5f3c0f2505cae29c6b8a4cc498dd7f2912e86ce1eb7d2d7"}, "300a9425-3af2-4d2f-9af2-6066ea8bc430": {"doc_hash": "efb7af2f14803818013e66cb6cdc6e56d9641fb64a148e1e2490c617d6dba4ae"}, "6db351c7-8d58-4b52-806d-e2551ce4a229": {"doc_hash": "f1b4d62986c2425edbe4a7d10e1ba43913d71efde38b7d4bfc1215878be62135"}, "104acf7e-8234-42c2-bb73-31dcf8af08e6": {"doc_hash": "2808ad5a02eeb90f26ccd1e4f3026cd4c8e30c27ce6df176e075eb41b403d094"}, "3062657e-8296-4104-9b5b-18480b32334e": {"doc_hash": "bac8716459ffea570c9c9737c16761c6ff1e5337cf3b441f9470970c650d7a46"}, "9d19b774-e908-41f8-8dc2-df1c57fab1a7": {"doc_hash": "59aec2ff20b9c7de021f18b1dc5bb7d5f2f77bbb193759778d4836e9dc141c53"}, "d5c48f9d-772b-4c36-918e-57eaaee753f5": {"doc_hash": "0a517fb1a430fb65c5fe88264c53e04d99afe85111d82f982a21b68ec38da579"}, "f93e563e-653e-4eac-83b4-ba661b2ab0af": {"doc_hash": "1d52bf486ebc067f24167d9bfa026689ae1878afd71aa15567f9429deea520dd"}, "69b9f622-130d-4485-bced-50eac3635847": {"doc_hash": "18a44aab8e6a17ca1ab2c2703ac86ea67258ff20b7adaa4b57c5f7463a691e05"}, "2f555e41-a011-482e-83fc-e94bb38542db": {"doc_hash": "f4487a96b75df6626ced0e7896df8dd7c9b220717e1a768580d7f561443e2a16"}, "16ce35c6-e42d-4179-b001-fb636d20e009": {"doc_hash": "c4ea8d630a1266b3db0012a3b5a697c7f5dceed9855d79aa279a6dcb9e2f7b8c"}, "acdc666f-7320-4a5c-8c86-f934a60c91d6": {"doc_hash": "35601155c023275491b8533bdd677d148934e7a61c05a2123740975cf05930e1"}, "134f22c4-cc56-4d68-b9e2-1d887190e6e2": {"doc_hash": "6a422522a60f6317478cb7b80cc42a295128f43922483160ca16c22f62d945cc"}, "53b146ee-0a8a-408c-aaef-5e154a22341a": {"doc_hash": "d4c8cb96531e18244d00b6b58a0c470c9eeae4310bbb463f7689068266bcc2c9"}, "ed43d80f-802f-4c59-9d8a-4bca2466b3e0": {"doc_hash": "54ec5cb87e9e1633ea010df1429d308741626008f98ee6bc4f9abce45f9648f4"}, "812c459f-49fa-4c5c-b325-ce568ab41da7": {"doc_hash": "1d52f46626b2c76018075b214c5e8a1e0d50b2b25c9621147df474d4b37a17bd"}, "0cf54341-c4ef-457d-9d4f-f0140373045d": {"doc_hash": "47638cad10325bb68df8cdb4ca03ba7808632094f27dc891e4efb09032002e20"}, "03178d60-60a0-4113-84c7-321aa5fe42b1": {"doc_hash": "1dde6075e38bdc49f68207e8a80f5f96835ba163abddf6f302359cac26801526"}, "2af5f18f-5bac-4a12-ac3a-ad60f4b55eb6": {"doc_hash": "81ea1e27ab72723144695ba348159c17589d00d016805aea013671a56e1f12e8"}, "f5d971c6-237b-4c71-bdab-95a9ac977abe": {"doc_hash": "7cbaef666215df1a941e6d750b2a17b213a4d86e5e401fc39b57f468c26f37c3"}, "a040f658-1183-4eec-b0d4-4b2dff3551f0": {"doc_hash": "1e3b160e23b4845e6a6fd89676e6f32f035ae502a604134a7246af9fcc53ecf2"}, "50eae1fd-7833-4d9a-9571-c7bc61f263f0": {"doc_hash": "5222fe6e45e84f0016ebd52f474e323184397e01adc4d99f09b9a16ce27a359a"}, "abc96849-154b-4b8f-a774-a69a0d414aa3": {"doc_hash": "81a8c6e461ec5283bc30caed427a9f34c77a81e3c0ad06b5a613ca4b65293945"}, "9f170370-8067-4165-b72f-f38b732b3837": {"doc_hash": "5b25a8c77dda4218598c5bf76b5e518d043123e65848558f67ca498c8e041589"}, "48a9d031-6c57-4b79-a2e4-672389855a5b": {"doc_hash": "ed350908e22cd5e252816fe3fe6c7f70b8819c24482cf4091cb21e02921b8d11"}, "96a3c2e2-6bc3-4e12-91a6-0ccc3f61ad12": {"doc_hash": "f210f8ec2b755d0e577362546bcff4b546cd978497402f9ac9c4f8a0576be6eb"}, "8b1afc1d-a331-4185-bb9b-6f7925d70f4f": {"doc_hash": "9bf57224e2197128e684ce56c25ed09c4cc672d106f0cbe58f13418abc1ce35d"}, "bede48f3-c4ea-414d-ba64-6988bf1ad565": {"doc_hash": "92f0455ac7c2774be3796354939df718683df7e0b2a4cf567431fe6d5f79a496"}, "e19bf402-40c5-4333-bc2c-642de947e6af": {"doc_hash": "15087bb44cd28b38d2bb9438df11668e0d8c13f9fd9767cbcdc31ad0d4cdc40c"}, "e6adb1bf-306f-4fa4-99e4-0e7758ab7dd7": {"doc_hash": "6c22f9d90773a7731fbd56c1067495e4f2f343be62b80981e4ff36bf4ac58fd6"}, "a54e9163-3e8b-4372-80cd-8d806574a92a": {"doc_hash": "a1781e42700ebc3969be2dded880fb2831b9e11e694607a99afbbc458cc24b85"}, "3d16e30f-f1d6-435a-a9d2-52cf15c240b3": {"doc_hash": "aebbcf240c8753f3a8ed01fb355f19cba1fbfc0889b8366ee346092c4b37819f"}, "2069422a-c464-4477-aea9-b9dc816eb2e1": {"doc_hash": "9f622f28c82b2f41327c0c23dd071b1768e913d12548edacfc32534339008448"}, "5a8b5ca4-5a57-4ab4-b76b-ab33597531df": {"doc_hash": "49137b69c4f477f121a0b3ab064bc240c34c2949449c4229427510cb2d291e6e"}, "7e6c13c0-f221-4a18-80c9-84f64cd19328": {"doc_hash": "ce8216630fbac416451227ccd8761a9b80dcd07f823ccf95fc9d7a85b2d63538"}, "ac2310dd-3a13-4576-acdb-adf1b2f0a1c3": {"doc_hash": "e49b05418c47a61f6cd962f281f7ba38175775adf95d6ea7b3d47192c863995d"}, "b3a28a36-39ba-4075-83b3-aa8eaa51307c": {"doc_hash": "4ebb00633dfef460257957af089bb0391d82da424535e494db1ac9e4444a29b6"}, "c421249a-126c-460d-8f11-25b05d0b6d24": {"doc_hash": "0ba9ee2dd66950eca461d0bd99aafb7908699293738f07c8ee4e0e8f0b07d36d"}, "ea312dac-7d27-4ec9-8736-50f1240376b1": {"doc_hash": "a47d4aa1f238f08b3fd41cc604d01193ff512abe139160179146622406bb554c"}, "dcb27f33-95a7-4c57-93cc-87259d92679f": {"doc_hash": "23a1b76744a707ba00c5b6d24495bdcbc6d8f6852ea13eb1f53e6ea5819b1d90"}, "5d737a46-0905-4864-9f3d-d9c0332eee07": {"doc_hash": "fc18506a13478828c884686bbda6a092620e8c8b20a3c101601f80da6193b46c"}, "fc0c55f3-477b-433a-8847-d1839c59f91f": {"doc_hash": "8c2ca31f692d8e95de2b7cc42df8fffb1135ab9336868bdfc30b651facf58d73"}, "69495d68-cd1b-4936-8294-7b5af706c19d": {"doc_hash": "f74652694549f88c0be89c406126a1003e2290207087103fee5a1bd1b95cb9b9"}, "fde63e85-8dec-4b3a-bfaf-b52c29429abc": {"doc_hash": "4978413618232124c4f9aaa8cf1b44e7ce0482f292213b615a7b8b867e18d696"}, "a2798db4-1ff4-4c0a-96e9-052e969fe8e3": {"doc_hash": "ed82bf83018582f6811c7c50a7b5f06b56ead221c8f37f73bb7d27faa21a2d6e"}, "93ef8a94-ca9c-4089-addc-ea989b7eea94": {"doc_hash": "a6148fa56f3c8f65ea0d90e04c055cd75c48c52b6c59e0857b75ced200e28fa5"}, "8e5deae7-bc71-4c67-bd61-8dccd1cb66ca": {"doc_hash": "5883772367ae1b913d78be98f5db67e83cadd8f95357b169b3380a125222dfd9"}, "d85eedae-284d-4c5e-9dcd-6f252d8558ea": {"doc_hash": "1a4072f4b9c80ad83bf232b4763770773c0f5a4324cfcfe4cf46ee6c33c48864"}, "0d9f4495-d3ea-47f7-8e6f-2f9615261d2a": {"doc_hash": "e559166d1772c1fd418db4e60edc7ebb8cb2b72ad45eca81df1b63dfb36e68ff"}, "55b7a8c8-7321-4fc5-b228-bcc3631d36b2": {"doc_hash": "bb08f9686dff768966d748d4d7f9da8999d295a1bba3840cd6d2d07ea5bbc215"}, "f186fa3c-0da7-471d-89d6-dbf797b7499d": {"doc_hash": "b12458143e8f2607d8347f7977ea81cedf7030dc9123446efaf35db49d784ca3"}, "5266a6f9-47f3-43c2-a34f-57b42c2a792f": {"doc_hash": "ece91ecd8ce32501030ae10def2166bc6bf9dc9909540fc0fce39dfb02181c3f"}, "21ebb549-b90e-438e-9f82-97eacb914bbd": {"doc_hash": "e37f6e77adf79bb437645bc869a727b1de3220049ae90a0ff915272d81cd7287"}, "1ef569a2-baa2-4856-a111-937b84abdfaa": {"doc_hash": "5f3a6ced2ca6c86efa9763fe9f9ec4eda0a9ff0a52e9dbf13a8decd2756dd2d3"}, "350d0493-624e-4f6c-9f54-6808d05ebd35": {"doc_hash": "08467b550b861b29a937a9ab1597371300accc1d7568ff99fbb163c02c45c6c1"}, "7ad78dd2-7941-40e2-896e-47ddb755595f": {"doc_hash": "bd12f8176606d89b67e7ba86c28a01ba3f940bfb8d836d0612e7cf764291407d"}, "18923536-bb2f-4fb3-914e-068b25b6dda3": {"doc_hash": "5c33395eb058aa39119093eed4daf24b276e42d3266229a7d8be44150ddd8ad7"}, "4b52b7f9-174d-491d-aea2-ed3beebfa479": {"doc_hash": "ed7b2007bcf87e9c8f7bffac224b0fd5b0760ee50f718cd5422a47f4abe573c1"}, "c2e99bde-8485-43c2-aa92-cd84ce2d8914": {"doc_hash": "ddcefd8837e9717b8aa6df10d3e26ce1195b7b2b879dc55fedcb68af56b73b5d"}, "9d4e7f11-5c56-464b-91f2-a501006a4b6f": {"doc_hash": "8a778d16baffc557ede6a1518f1e6495581db3a76914f0729df64e0690855fa6"}, "1aa6741f-a241-4f95-8228-ee90c90592a6": {"doc_hash": "da55f510bf6628a1f8d82874675d469581f003044d5f4c3c6b52a3567a4c7488"}, "fb2a73df-85c0-4bc1-a84c-fb159222f647": {"doc_hash": "8bcf183c68b403cb27531f65d7616c7158c055dee0799e33a35db08b3aaa8be1"}, "138d870d-4e3a-44e3-904b-063bcbd32f1b": {"doc_hash": "65b144a436426724183529fc56e7bd8cc3cad1bd1822c1184714a004d3802d95"}, "5a1ca5da-7871-42e1-8b9b-a29dcb68525d": {"doc_hash": "93e8f563c290e75c3e243d930c92da59d18a6b373106915e30169862c5834ed8"}, "1a152cef-294a-4783-b700-42724600b4dc": {"doc_hash": "50db13e1630a1eb67679e5576fc0e8a55c43a232b1952eef515ce8585578fb60"}, "9265669d-1acb-48b3-b487-874e8d9aaa2f": {"doc_hash": "caca1d7cd4a5a7362e40dee119351da8e26bdd41b7289ef18bbb84fda00e9228"}, "3583fedb-dab0-477e-9fca-59022505a5cd": {"doc_hash": "ad28bec93e13876ed45009077df53df72a2c26a5ba28a2fd2aba4dd8c183380e"}, "00273862-1688-4382-8529-60c704d17dee": {"doc_hash": "29cad760ba37ed454e393104a7b4c621fc51eeda79755b913c84529077011d5f"}, "56cd3672-aac2-4868-83dd-6722b2a3df2b": {"doc_hash": "2349dadf49e09066598808b0a88d641d435f7e1742abdb77f56dcaf4cecf1006"}, "c5cf1cfc-0589-4595-906d-0fcef5c5adaf": {"doc_hash": "cc5420142b45fbbfc8c7448d28e079daeda2fa22067c77525da4516cdf9a05dd"}, "57c5666f-caf3-4c0b-843d-b85ebdb2b8cf": {"doc_hash": "d3665afd249b25a728098b6cd58b408d7f08d9826a5fc54eeba052bee366fab9"}, "06f0db36-d782-41c3-b7be-70341a380d7f": {"doc_hash": "c7cef4ca8b226f4b0216b4aeee50f8b6813e7621b6654a5165ae0066b5cd348a"}, "5a2811a4-d662-425e-a17a-86d1f5165a31": {"doc_hash": "f4dcade5aa178b57d1fc7f570073840093034df0ed56bdf3107709fcc5d307a3"}, "9ef66d74-736b-450f-83a7-712c98bce50a": {"doc_hash": "593b11b18b9b92fe4458e439ee5302a7d80188997137bf0136c3439c4ba4d7a6"}, "5589ef5d-0866-4ef0-978a-d0a2c24dea81": {"doc_hash": "966c04dfa21e3f9503db05b031999bd3ec32960939dc5d7f26281463fc73cce3"}, "9d3afe5c-7588-4d15-9923-d9b0e822309b": {"doc_hash": "ff2ab30a5f44ebec384f98496bf360edfd123062f442a0be7919c852f7b4261f"}, "3805a50f-5dfa-41c8-8ba3-3f67bfb03f9c": {"doc_hash": "2cf7aa4fb56c717ac6497360bdeb36e8201de9b0f5ca9d74c67427bc144ec962"}, "b8ee1d32-493b-449a-beb8-d8520a37802f": {"doc_hash": "f18a1b4993ac031d66db1a9fdef5a88126960ebc27a9f6d8cda3b79778638d38"}, "1516baf2-cd5f-4208-93b0-a58a19e03362": {"doc_hash": "fddb2c8eda3054c25ecfc32a3894ef1578c3609e2cb8987fafa23c34f8bd1317"}, "5d063730-1e0d-44be-8ed4-cff4e5fdbb0b": {"doc_hash": "26d283ea1ce7a03344dfd6607b53971e17883b8edf0d8a1c0d4dd3875fdca613"}, "4cede6f6-d359-42ae-aa01-1c98fd837794": {"doc_hash": "491feb90d031eb6fb6bc044db72e269b7df381ec2c0fee2f454f3c19edb09ee4"}, "1a97aecc-de37-42eb-9ebb-67098c4102d2": {"doc_hash": "96a87545554223a9b906e2ced606a9bfa227e4e767fe81ef16897334e38f4995"}, "47c935ce-e1a6-461e-8b40-e6a4d223bfce": {"doc_hash": "741a591f4e90d26f742a0980634a76e513eeece856dd470bfe994b8956b6c77b"}, "0016cb3b-4268-4a70-8cc7-850b383f101d": {"doc_hash": "64c6128c6de413c78d5f3e951c45c3036edb4c84f9d94b4e0ce95f08a94a55e2"}, "355ab3b4-d78b-4eae-8fd2-045a52d16b70": {"doc_hash": "0a1d4214d93270117bfce3e81527ccfad81c0b5264bd4ebc0e185d1e32a5dbec"}, "dcd02c74-6e10-462d-b51f-bd0f856c25f0": {"doc_hash": "b7cb351d0e37dd79745da4798590223d989be2fe210c8107aa0761681ccf164c"}, "5514f934-fdae-4c8b-a25c-320b993e9a8a": {"doc_hash": "d1343fac12ffac15a8e359e5601d04006d69acf9701aedffd1bae76a42f02cfb"}, "f95150df-3418-4f10-8ba4-9ece4ce0426a": {"doc_hash": "3a0b5abc7c7c1792b136f7954a032a25bf6a66a8ed82360d5bbf5f0d6c71c63d"}, "0ef9c813-217f-49eb-9801-4173c367b7ec": {"doc_hash": "1fb5197ead45fa1603c8dd8be55da53349322bf33f5755139abe98fb2f76ca98"}, "96755489-7e42-41f8-a78b-1db7a7fb1a4a": {"doc_hash": "0d89f692e5881fbc08633187782645166a7037abb50e1a5f413175cf6779986f"}, "a029b7ea-41df-405e-a502-9fda07d4dc3b": {"doc_hash": "5ffba7f9b2516a21cb0a2329b14c6253ee8b5ded9235e8df96ca097bfd125612"}, "54930f9c-2fe6-4491-9cba-afe2a9ee3a2d": {"doc_hash": "e077d532b1cbacb901556033eacd1e1ae34c88d36fa01fb9305c6a22171f29e6"}, "358df029-e3d5-47f2-8a91-ecac427758d8": {"doc_hash": "5415f1d14d2378b54567b597527a92382502ef221164023738351526827072f3"}, "cf19eb7b-b6ce-44ea-bd71-23475b0672da": {"doc_hash": "4a33410fccd337e386069e9a145abda204591d010757535ff38bc83491397b15"}, "e6409ea2-8171-4a68-b5bf-d8d027eee305": {"doc_hash": "723f18e8a96179cff647d65b5579516f2c80f156e8c57bfb1f0daa029361679e"}, "ad94849d-7e39-4dc1-a546-fce839c90333": {"doc_hash": "be893f6672c95a94190bf87cc5d154d7ac01efffa566f381c57acc863cf41e7b"}, "884903e2-f29f-46ee-9696-f0f4e37e4492": {"doc_hash": "e15818f98f804f1a488a33dd361896d9d9027df3beb5e9501c94dd318bf48416"}, "54793541-16f0-4a99-b831-2924af4a25b0": {"doc_hash": "dd208f66e8a1755b714eb869f22049ba61200a2b1f16a962f9db608be744255b"}, "61e70219-c049-46c2-988e-61bc2808fd9c": {"doc_hash": "3997b9208c36a0ea90017aa56d926c186696b2dcd4433b34d2f392125f72e6da"}, "77f35ad4-56f4-432f-9bc7-a50bfc4a59bf": {"doc_hash": "aacd1e25ee9704277a0af13f6841bb3583fae734e38bc46130b43658899ca3ae"}, "0ed60197-76aa-483a-bb67-ab9461d80071": {"doc_hash": "005c3cb636f60817b04490bc925739c42f52c7886ebfb277f648aad08d911412"}, "bcba8925-4287-486d-addb-7fc0dc7b084e": {"doc_hash": "b06ce7efdaafef020239bd71cdb99de0159d5278690a6184cc676df372649d45"}, "89b40abf-2d85-444f-a58c-81b9a7260493": {"doc_hash": "b15c8e02b32f3d65d025cffc22847285aaaf0d38e2da8b7a936bd45746377038"}, "1cfcc608-1e8e-4aa8-a300-93d781fd7bde": {"doc_hash": "62c50f8d96cd8cbda21db4a29e1ccaf4fa147ac42877190ad74a9bd9f205791a"}, "668c827f-783f-41ab-a6d7-2aecba18a25d": {"doc_hash": "dbc2504b14d7e69d2e678329a2ab569c4b3d57eeb26245772a14df8c0e94934c"}, "d7f8cc98-ee5a-4c4c-8a5c-00616c06b870": {"doc_hash": "806c09db8c07fddc14933851454d9ac2c326b6821100fddf2f07eb4ccc0d5b72"}, "cb216f3d-d657-4f40-8dfd-e34ab75d0140": {"doc_hash": "af456ffa3d290f1c9d58e70b528fbd6b037ece6097ab3b655f5e9e2fd9cf2011"}, "4f14005d-bd5d-44a3-adcb-558597007bf9": {"doc_hash": "cfcae8222434e832c16c5f68eedce62b0e5dce22ae854da70ccc742855b354dd"}, "e8bf6b39-eb18-43a8-a2f9-7cf39e0399d2": {"doc_hash": "ab807aa75cc55d27308d96227e4b609f73102c23890af1081c8a1980a6ca16ae"}, "996b257d-0970-40e6-968e-5bad1dc7afe4": {"doc_hash": "601b1aafd7d1f3bf5e8b17186eb166cde51162902c8b70144dacaa84b7d77efe"}, "acedce96-7968-4f7b-a093-4b09eec7239a": {"doc_hash": "0cde6323e14c070519fb00e50a966484f3f8b2797ea58eec7c50cb0a56efc564"}, "6a96ca53-f468-45de-a1bf-09a3cd088779": {"doc_hash": "fca0794a25d4c41254f2e7b6a3579f0a3b771c5a384932c68ffc0a32853da8e6"}, "07813d15-c94f-426a-ac8a-4cab81a6c1b1": {"doc_hash": "b6c642544b033fecd6422d1571fcb25d25fb9eb2952b10a3067117ed4df8f662"}, "21e0cea0-063c-4048-a322-fa7e5ce07b56": {"doc_hash": "e6286ea0ffd457f4c1df58b4168898c2bac13ccd7dcbf2ad11c91b73eebeffa1"}, "277cc3f0-056e-4379-ad8f-3c2701309caa": {"doc_hash": "eb9f0e6c45d207a799cace9a5af08844a1803753ff0f952c35657099fc18d81b"}, "017a6232-70bf-4b59-9d7a-e6982c90c219": {"doc_hash": "c7160d38be86c75a18fbeda6ff99aeb4c811dcc4ddad791c60911af0a1b353e8"}, "0d1deb84-4c47-45c6-b2c1-12f65f88351c": {"doc_hash": "45f8b6cc490df61c0918f7ab9fbaa47d31b59ca6252b2f5d66b775bd5a2e0215"}, "2a9ecc0a-3ce6-468c-9016-88149b9fa0d6": {"doc_hash": "e4cc838c039f1f9ae2ad6e92f103a759d0d12d158e88f4b988432291a47b3493"}, "60bfa312-8993-4d4f-95cc-621ceca115a0": {"doc_hash": "114566b758da7d5da8605c5188d807cfe3c17a82d2168076739de92c68b1f8e2"}, "cb73713a-2980-4eb6-a636-e270920adf39": {"doc_hash": "1ea7f40f9793f3cabf3e3c912601132a1d748e254ebc50e00ad363a3bfb87195"}, "0fa51dd4-b29e-441f-b240-e99776adbecf": {"doc_hash": "20313cc894b3061f3513546e7fcbde52a2d7f2acf5e1b280833c129b4ebc5614"}, "450677bb-6f0d-4014-afc2-67e5ea845ec3": {"doc_hash": "eeb573a6a9becf41757ba57889654dedaf9f8b9c38feb18f071ce428ace6bbe3"}, "6cd36fb3-e29d-4da2-956c-db40c3ee8441": {"doc_hash": "04ab0330f1be467a724445066dcd09a68327dfc94acb0020e9df03b94336f66e"}, "56b59297-9e5d-4630-a73d-fc2a6fd9397a": {"doc_hash": "cee1e222eaeada62dc9e79a8d53dd6596e170d6c56b6e63e77581695638f270c"}, "aec39ccd-c1a3-4439-b0ac-cef5088016bf": {"doc_hash": "d4a61404be60c49c72e901401e7a5fc0b925e8d64975af18bef8c164c71386a8"}, "03a6a44f-8504-4045-8836-b5aa2191535c": {"doc_hash": "2510a92bc383f19fb99c28306f9732b9057bf556cdadf9702f654b73f9f8e168"}, "f0f7e263-498f-4d45-9593-b72cd7a12d17": {"doc_hash": "827abc7bdabc97d3cb00cddeef201386c32165dbc4741b5a21401d292d780b1d"}, "05a6c422-7294-4fc6-94f1-bd7d97663266": {"doc_hash": "784b4a2120437793c386ff77393b0b74710631dd731a7865357e8473be832a46"}, "e36c3ba6-f844-49ef-b910-fb37c1b8c050": {"doc_hash": "d0ed1c2330b2aacaf8574ed9e1bbedcd0a430dac063eff624d8b6dd7838bf7dc"}, "dfd37746-5555-40ca-b73f-c47988f3c3ec": {"doc_hash": "5df2c89eb7c61b86d9e73be91721cfd9ee0f64f2bc7dc68e2606df3e098ceff7"}, "f9d64bbc-270f-4fd1-90de-95935d9bb0b5": {"doc_hash": "6847c07266edc5e28b07e760445249bd2b3779acf3d779f56a8ffd3ef38ca841"}, "35c739ee-5096-49db-a454-1dc5daaf17c3": {"doc_hash": "90c4f368db4a12baf866c505ad8bba7721c8b5bbc2ccc2a505a12ab2b27bed76"}, "cb7d46fc-f4de-4ec7-822b-2d63104ae8f9": {"doc_hash": "dc209b60024c282da08ea6b61f6ed2b128deeb33e93c9bed522f6310d2646258"}, "fbc31c6c-d94c-4d57-bb66-71bc64863237": {"doc_hash": "925dc1859adde1489f3bf57c93002d830ae180e2fbf2301eef2431dcf4652953"}, "ae3ecee6-09eb-4a74-9c8b-c3dabbe70915": {"doc_hash": "cac1701151ffab641f6663088955287ea0cd7a7ecd66d661bf52851a02566508"}, "489d0952-c652-4455-8647-bdff613d627d": {"doc_hash": "9ec251e82662f31b78bc0a0b927e00b5205aa4758cab32b5fb7388fae3b3b702"}, "6bdd7f73-78a1-4fa1-a1ba-46c94495e6cb": {"doc_hash": "5662bee0e0563acde090bef25410287e02da36d556faee885986caa6b68c6929"}, "745912f2-28f4-4e90-a136-d37204c8e3fe": {"doc_hash": "b9433804affe4b05f52b433d8986808815d6692fba24dc835d0c4b912983c688"}, "26b8738f-df9e-4842-9c76-5feba40d521e": {"doc_hash": "97369fa0c5088ca9e14fd1c6b78ffbc0adc873c24d79e5f48fde09a6d5eae9e5"}, "11161654-d461-4b4c-b381-d5599d0d625c": {"doc_hash": "a2202f2c3629aeef7d249459aeb025a24c343b4acc5f280a66b196ae5cb1b8e4"}, "c3eafbcc-4e46-46a2-90ff-6f8631632732": {"doc_hash": "8210124f0e5ddfba2d89ad6262b4de575c5f5c637db34616da5391be2867a2bd"}, "d8164c4b-ed1e-4fa4-9989-7b98213ec7c2": {"doc_hash": "f27dc957b59271406979cf2cc4d05ec70ac23f15484c27a1744ab9075ca5efa6"}, "203bcb74-47cf-4eab-b80b-71bfce10048a": {"doc_hash": "bffdb8264a30aa8d1e4658e45a5199b85bbc95478191e0c8897a2bd4e55936bc"}, "c04c1f25-324a-4953-a92c-ec6c3f58a2f6": {"doc_hash": "023932fc213dea4ffa1a91748420b0b87856ba97f0c3e3c73acca8b9c326c47a"}, "34edbdef-4dd0-41e4-a94c-19dcc56901ff": {"doc_hash": "5d7631723e8a1c034e9dd2f93cad40d7c5019226d3d9997a80a0a18555ad7590"}, "1cb66b78-ae2b-471a-b99d-530d55815367": {"doc_hash": "ad684589fa5990881329944f6070a0162cec09bec9f9adf3ec55205ad9b5f5c5"}, "597c10b2-8c1e-4ed9-89f9-22c8c80d204c": {"doc_hash": "78b82f66c184bce03a49f6f97219f2f2598f40aaf645584753cf1639faba4456"}, "ae61f657-b28c-41f2-89fe-64edbaf3e5ea": {"doc_hash": "ce6495654105438065a79a9244baa9a3fc49195f0e27d015a4bbfeb0d592d651"}, "826608c1-1289-49fb-a3d2-f6977d4b2638": {"doc_hash": "d03be3d253fb6f4445bf9ef95479180a17efb1800272c8b2800f24ddaddf9d61"}, "b411a8c0-4773-423f-bfdc-21fc7e6f1fe4": {"doc_hash": "d38809ffbba4d7749a9598c40f8a10a7e5e71ffb971b41cd01ddcf9989f16bba"}, "7fb6f579-d8be-474c-a49d-e7b55b5cb250": {"doc_hash": "12054373ea30697e6845293c89a6e50ea880d992b987821f052adb7c887725d5"}, "3baff753-dc51-4c1c-a86f-75a54efe7495": {"doc_hash": "82a92fec13622c2f09f7ea46d8355f4d47cccf0b98748059f95d5f02457d93e5"}, "b72f0c78-328c-4807-967f-b455dcbe931f": {"doc_hash": "456fa9d2dc006b43c6dc42ceb844089258c601d460ccfd43af1ecc016cdafc22"}, "7c896a4b-43b0-41e8-8b13-0e3ba062f704": {"doc_hash": "fd01ab0a6b5cb9fb395534eb723ff2021525a8bdaa0498db9352617aa563bae3"}, "b9225d5f-864f-4cd9-bbe3-74d6fbefdd1c": {"doc_hash": "67206ffbf086da359b01f04db4896fdd6fc2bbe5f24328d7d03f6ca0e1c77260"}, "1c822044-c953-4ee9-be1e-3c7baba319ed": {"doc_hash": "47de3ba1c03c2e2fd5a71fb1a8ca48074c9418c528e55821a3950490da59108e"}, "cf728e8b-a8c5-447f-8e63-f1bfd1d9fd4e": {"doc_hash": "3ab4b0f3ae617802b6aad7612008215a510c4bdeb9dd9684125a637ea35a8de3"}, "2c177e7c-1cf1-4858-9600-663fa6bbdff0": {"doc_hash": "3f844f044038cd5b992380d1ef162802ce47d8f4f0910b7499b6a8295ffbf1f7"}, "920576d9-c386-4eaa-99c0-44cd2014733b": {"doc_hash": "5aa096169b26e5c58be82441b0bda03522f657ecf84d7a19b12860ed31ed786e"}, "985bd4f2-a183-474f-8f34-65c813549cc8": {"doc_hash": "f11232c7c0298a695b1ae1e7af393466d1cf8e5d8e88791cbb55ebd380534972"}, "f784056a-3adf-4cda-94ee-cd31a7703a4c": {"doc_hash": "d1b86d60fe85f03f410e14cbe4755c46bbc84ed133507378b73c1d883b1e5b7d"}, "66cbdbc6-cbb8-4ef5-b411-13ba5de8f7fe": {"doc_hash": "8bcc6fca74969d14eaed44f26515696b4fc6a499e8320fa2b710a27baa8811f3"}, "15411be5-ffd6-4fb5-ac14-808eb20a6a03": {"doc_hash": "571fd514970dd2952f66178bebba605143f228699f195236d840d569c9dc2a5c"}, "e5fb2c82-dceb-4684-bbb5-e5bf5159701d": {"doc_hash": "05403281ebda471c8a079da69c5149f2d6108403593b5ecb44a52cd37f726bfe"}, "1ce8130f-d2a9-412d-adb8-e6c237655759": {"doc_hash": "d16d42f363184feb9d5c40a9c9c8acc7e444acd539af8bb43723de8c6fff50d6"}, "eecd3f6b-2c03-4de7-9462-b5e58fb8fe71": {"doc_hash": "14fd997d6cbc7ade8b5a7b13caeedc1fbedc0d75039deb3019ca562666ad7ba8"}, "2168fb38-6bad-467f-b6d8-389795bedd16": {"doc_hash": "59587b4feef4ec0d7711b7ef5eeaa72d6faaf0af2063584ad51dbc4ef97e23c2"}, "3d7e2eea-2036-49de-95fa-80ac7924a7e7": {"doc_hash": "273cbe979e1b47deb4ce1ddc901641dda65b763dd6a055d9e01f7d460e7c42f8"}, "b4ddf1cf-ca01-47ec-8798-2a81ea6b6f68": {"doc_hash": "3e4fd549f662c9216f03f03795e107c9ba282906836b19bf3ad9b45260d0a9cc"}, "8e22705b-9910-44ab-b73a-2b49e21e8939": {"doc_hash": "507bae35c8ca7e7010bd176c4ab60aa9b43fc091271ccda7871eca1abf315a48"}, "930cad80-7f97-44da-b225-3f130a88d03d": {"doc_hash": "ef5f8674337f4d63f13f6279a9a155d2e83d33c86a08479ff96201711a61d0ad"}, "74b58bb4-f1ce-4947-b097-94d35c579e35": {"doc_hash": "2548dec5e8a45bab37b77080491bb5be7864dd75dc259670ef53243fec448929"}, "3e08fb0b-7d95-4209-a3ae-fd4ef4e8a028": {"doc_hash": "c07f42e9efa5fc470108c9e57d97f293b15495fd052d23b85db5fb2ea19c613f"}, "eaf857e7-049d-4741-aefa-a3d00ab93ab0": {"doc_hash": "8939e77b2c1d6ee94af8b76d956f94028af5fdb0a630573be0240f404d1116e8"}, "3f92ec4b-ca8b-4441-aeb5-81e7e790b2b9": {"doc_hash": "c039124ab910714f72fbaf9a0fe1ca499bcde1a1f2764fc360c827873850c5ac"}, "a6cb6435-905f-4067-9322-cbb82796d1a8": {"doc_hash": "4a1708160a37fd11027a4d1bb4d4ccda5a96793efd851cd7f79d6b420b2191f5"}, "d441f992-cbc7-422b-a40f-5bd54074a78d": {"doc_hash": "9b6ad0ac1c7a2b52f16603fa067887983dab239714c38b2d2099e14ec5ee7b4b"}, "8297ed42-65a4-499a-91a6-3ac0d3101021": {"doc_hash": "eaeddb901535b7769b47ed17091e3e668572756712155de19aa4cc300a4acdd3"}, "df7454f7-4d6f-4d97-8b57-d81a9226e184": {"doc_hash": "ed770b4a938c6c9691b706db7c612f9a14f8e142b865f011f19ea7173c620680"}, "328a6eae-0eab-4dbb-962c-bd7a6071a93a": {"doc_hash": "37b8aaa8b5fb95969207d35ad35758dc89a6226622e62b3fece3bfb92adcc341"}, "8195a1ca-b64b-4309-9aa6-74f36fba0e6c": {"doc_hash": "9e2efdbcc8cf904fb82db96f5dc5d7ab02be483a4f1fa8e7f71f6b965d1d9a29"}, "cba9e15d-ae54-4a82-bd96-21e56409f321": {"doc_hash": "4b0530e2a1348fe9d0196e021d30d852ebc9e035750b1b2fe2d6f0690d871915"}, "434c8543-9767-4ed9-a65e-da72f5e65914": {"doc_hash": "d02b3e1af8008ab62e0bada3fb616c53ed285ca76ea94081282303f2f3c422f7"}, "ad43fe79-f688-4952-883a-1fdb0a5b8239": {"doc_hash": "274152a0951f0767ecb00354815f71f9315c5c48370b8f881311a72e7302d4fd"}, "950944e9-a7eb-4870-9e73-33d3f0013bc9": {"doc_hash": "627730e1465766e0b9a7777a7c264c73eea901e113028f7bc6c7f080b2f07c49"}, "f45c243e-1497-466a-8108-8e3a6031f9fe": {"doc_hash": "265589e753708eb1c80d3b6116f49c07ab383d457cf7e5d000f2533dbc93d4f4"}, "1ca1cd9e-d7b1-4c94-9496-902c99c1d6fe": {"doc_hash": "ab6d48c482f5e33df8d3f76ba5096666f7aa56366684c776c3f836332bb12162"}, "331d4155-85dd-40df-855f-abdfa676fac9": {"doc_hash": "d4fd94ab3f65e656abd4fd32b0bb148430320dc5f8789dcef15c31f6f07a425c"}, "e24ede6d-cf78-437b-9498-aea357fa47aa": {"doc_hash": "42fd4dba269da2dc64fb65f4ba44a422b35a12298a97bd16d6fae9afaa3a8a30"}, "8c93c0be-7796-498c-aaf4-e7cd511b4447": {"doc_hash": "8b31ffa71bb1ec8b01f4623da372629face6f316cbe30fb6f4d63207178f53b7"}, "0a6964f2-bb1b-487b-b22b-9bd062c566d0": {"doc_hash": "e7d8d78d6b5efd6811e96b68c9b9d6962e22e2d3cd9e7a856ffb8c9d4e852dea"}, "f2735c18-bfc7-4488-9391-ea29ff474f54": {"doc_hash": "aff005b2a3d751952364adc96fbbeb80bc033e1ef45d4d67c58cb54e3cd0ac55"}, "3311c3c9-b112-4dc8-9661-4d62dfc6742e": {"doc_hash": "4f7ccaf0b561d1a7c1c5910c7a312797a24608c64af56b85593ebd20bf2ee23f"}, "38b6021c-b637-4717-9d84-e49a38da29ee": {"doc_hash": "fa9f5cfa9ee0947838b82e3bc657679fbf4a1013df4dffad62b016158b500246"}, "73d71f49-50a3-49f3-b746-61ca9dfb8ab3": {"doc_hash": "7f2376b41fc0cb94ee323b623a53cec72a896ee0aeb280b5a6da78ca69e40c95"}, "7ae1dcf6-b460-4d4a-acbf-5c423d68efda": {"doc_hash": "16b43285306a013344bbadef22cf6ed718396e19c00afc098106fae580a12316"}, "94c7b847-31c9-4f66-90c3-353a35d9828c": {"doc_hash": "317535d88ce8874348aef013a7b121bc343cf203ec34f54d79b4be8fa9d3103a"}, "9887d925-189c-4753-ab8c-54f476e29da3": {"doc_hash": "5f149e9d5a6878cf52efbae3dc56f835bc6e8f40a03f7d69abcd48812d971607"}, "2ae9ed2d-ba21-4988-b015-b4f022a1404b": {"doc_hash": "a496c57ac96a10d758213424956e8efa39e4960b13cee022f6b0138d6611bd79"}, "82d0704b-adaf-45e5-9108-c7f7b555ca0a": {"doc_hash": "14758698796dc100031f9289dd07234f8c878d7f94bcb5a93673777281431063"}, "fa3d94b7-1315-40bb-a269-37b1bb943b79": {"doc_hash": "4c8be2b31d47e72952211186a98816bd51314ce06598a45c126fcbda0aa85660"}, "9ec77d71-1d72-4623-959b-4649866c15e4": {"doc_hash": "e64632fb43b5035c9a5fc49d54ecbdf0cc4db372538d3c77570174135597474b"}, "1227d61e-6885-49f5-8ed7-2ed4533d3631": {"doc_hash": "2bd636ab8337d4e4e525d0f184875f94170fb55e4bdc82191a424744967a7f18"}, "abb402c9-f17d-46b8-b6af-05498460f07a": {"doc_hash": "1a935b1d42151414478812e3556d432640c1dd77035b0c8aefd37a3fc31cf168"}, "27345450-8ee9-4cc4-91b9-9b0890494381": {"doc_hash": "16d8340cf0c418db86a3c0664727f4cf460e6815577b9bb4cd9a4c984bb3cd3e"}, "0a5a6a44-63c2-4161-8a78-3e47b255f0a2": {"doc_hash": "5f4b58af7ad0095aa302463dea4c49c9b61d5f48bcd5114aceac55426788885b"}, "5b42a20a-7a10-49aa-90a3-45c05d397e2f": {"doc_hash": "a44d20788d9654906b07b6ff7f31f6b7fd927a301fff6adfb9df285fce9b54c5"}, "2290e198-9fb4-4b7a-9355-8d32c6ceb6c0": {"doc_hash": "61c2c78e1c57daab06eb3ceaa7d4ea8634490d4e3d714aa085e5cda8e6abeb22"}, "7f5ffab3-4e83-4e60-bb03-52dee15f52a3": {"doc_hash": "a60ab734ab66d3bc158139e4fc277810cc4c30325ee3d43fe45bfa79c64b1726"}, "877e08cf-023b-48a9-a38c-4e9dd2fe855f": {"doc_hash": "ba0dcac86b850c37b59e74441d0bfca8da4cc64b06882625f57f29cde9e775f3"}, "ca80b4df-50a5-4571-818c-46b04778d6a9": {"doc_hash": "d3179b5f5f4771087995e462441ba086591444055977572d86b22ccf83df8b10"}, "741d0bb6-6711-46b2-8469-ee30d206318b": {"doc_hash": "be1b11477f20e929d576133a3a1f01f79f1b755ba25c7f65b17304d7d33230a5"}, "93e549c0-5e21-4536-9ef6-470c45da509b": {"doc_hash": "ff27cd269f1d88c000988d444cc7d8124bb89e577d3655164872bacf76ff2ae8"}, "35144928-0a0c-4ea1-971b-abe3e790e50b": {"doc_hash": "f0019bf83a0508f923baa91d888e4138ab55164647afccea848208732ba3049d"}, "5f1fa700-e632-4d02-9ec2-3d21b18bc11d": {"doc_hash": "eb46d0b595c3193c4eea1262bfca7ca2d60af7bf7921663995a9038059b116e5"}, "c6894a5c-68f8-464f-96d4-563b110a7233": {"doc_hash": "aaad91b0fd8ec8b4dbec50e6fffeba887601026ca6e93a45ba1e88026aa6a886"}, "a2f3d8f2-ebfd-48f3-b735-534d1f6fd42a": {"doc_hash": "9f17ef519e0f42c60f7246a65f4d199ffce9c28f3539f54e9d95ed298a2fc44e"}, "3e7f5741-bf18-48a4-a5bd-130265d966a9": {"doc_hash": "91be65ef6b414e9123037d96b4d092ac8029ce6be2ea474a138ba024893499ae"}, "03c25c38-5f6c-40ef-8671-78e8ec47cbf4": {"doc_hash": "b3b9ce96525038f02d6f99354624019c64a2dbbe6dbca3603130f5d512cce043"}, "a3a151f9-6353-434d-ae9e-7b40bd691597": {"doc_hash": "e2ddf77631fa141022a97e2aecb938996fdd7c352082a8f8895b1179a2d04ae7"}, "a40a8b7e-8f72-4482-b7f1-a7f8a008c607": {"doc_hash": "60e9feeb65f6c95f58c2ee2152c2d394dd213bb7b1c3e4d110be37f589e51cb0"}, "f82214ba-a8fd-4347-b32c-db1e927b153a": {"doc_hash": "0b44981168493a42afecb1423bbe4ce150bdf34879aa82659f23b90bface8d21"}, "a5b6b26f-87e2-41f8-abac-d549d49f61b4": {"doc_hash": "769c76576986eff2fabe9dd915be3c46741edf5e0d4b260b9ddbb099ddcb0fd2"}, "08625520-6815-491b-a696-514ec14bb470": {"doc_hash": "58e4cb05667fd93e15b0a5288da06bb9b8b60fa969a6687dadf487fc79a21807"}, "0cdbb62a-5cc7-424e-9890-a5e28b69e03c": {"doc_hash": "840b726bb44d5f624eac2651ade5766d10aa7172ad548fc5a0bb6a4a198eba23"}, "1e26ae6c-6131-4bd7-974e-d26805e86455": {"doc_hash": "61163f4ff397dc2e146607722312b8e2dce269eeca94484d2cab69ce3732a0e1"}, "e28e61ed-b96f-4fa4-8c75-94607aaf9381": {"doc_hash": "fb4ffd4173360dcf62a4529a53a331a601374655dafb11523925cadc7ccd3bdc"}, "d3d21100-97ce-405b-9675-ead70847721c": {"doc_hash": "2257006632ee628493d611d681f560eaccd3f2aa28b6ea3d06b96780743eb183"}, "130e407d-a072-4509-9554-e299f1841c0f": {"doc_hash": "006478635991cd71e7e4de588d35e3ef827d7bef071b0648d68bf35851955eb5"}, "d53b67ee-f0e2-49b6-aa20-955b0ca96b2c": {"doc_hash": "27071a24d83aa0a0899ece3860c5a6029119b171a07c7373703e960a1c63d452"}, "15b6aeb1-fbeb-437b-b807-f4aebe2ee102": {"doc_hash": "39772efdad79faa25e2408f77630b5101b5c1f397f739eca5545cb14c4f3c40a"}, "9fe57de2-5dee-4eca-83b0-daa46a76283f": {"doc_hash": "73f122be7782ac17b6b1ffc8191fcbe0aaae7fb71f142d0736c9ede1ae790827"}, "144ba5ce-e885-41f9-a124-3c0061eeafcb": {"doc_hash": "e4f7e3f436eac5afb9a2bd3e2cebfd5ba85654f28e4ded7bc8fe39492be413c0"}, "e3f5c41f-9d4f-4465-964f-a03f80ba13cd": {"doc_hash": "33d857d81e47216ad43089e8f28caa26dd1e4f58f22cda7cd7654478b0ba6deb"}, "58383262-46e9-4fd9-9f9c-e07d86eed030": {"doc_hash": "9c4a2c9ec8f8e45c53f9e2f06ca52e639f583f7da91ab4aa1f9e87d951ddf454"}, "cf01ef30-db47-4095-96e4-846b421d6994": {"doc_hash": "12b4e307b1ae076e6f0e45383668d630425e0957189973910897b2328cd91ea8"}, "68f3c2f7-cb24-4c9f-9011-a09f2578a70c": {"doc_hash": "e48a5aadb5864b36fe0b8349dc82ff3b2d5e50ff6b8f9f955d1db1273f43f5aa"}, "fde55601-f138-436d-89f2-4d814eb5d60a": {"doc_hash": "537457ef1bc1a1c4c37d7c6ad12884bcb42201f36e4a7c3cf4faec65279dbb61"}, "8df6acac-b47d-4c83-a66e-9ce5415d1354": {"doc_hash": "f1337ec14c359f615d69ffcf23ccbed73c9ac0763a8122f4df6b8fb54ae3bc42"}, "49b9fad6-8e87-4b79-9bb5-e59356e9cabc": {"doc_hash": "0da1f762271437e806e06e8c97456d6172967710ca755898ebf7a05bb5795910"}, "5c28eb19-4c81-479e-ae1e-58d3c7c7484f": {"doc_hash": "2573b5c0d1f9f5ac13b76f2dd6b329b3d79c789e93a71ece92c0a6fe10a7111c"}, "45e4abbd-648d-4c90-915e-e93affc4596d": {"doc_hash": "8a83bf7df19c79903024be30f3d0dee0e46de792ab6389883bb3aad57b55ce64"}, "3fcf8f87-4641-4a96-adc0-67b7a6b17f0f": {"doc_hash": "469a174ee0d02209bc67e439a9c12f5b7eb3b6ca51cee547774837eff1f8c4ea"}, "34b7f21f-a181-419e-9039-20176af27117": {"doc_hash": "8f0320401415e630135e819972c856c843b287281391b4c23ef125033e1a1424"}, "501d2b30-d7ce-4183-b3ec-c8363f53eb21": {"doc_hash": "f1c5d6ba193cdf10be4cbfcd8ad3489c4ba42d6e41a61714b9aee51863451ebb"}, "2c8ccbc7-ab94-4837-9bb6-d2c1a949fc1c": {"doc_hash": "221d88c87ef1804504c42a4d834e6275a2ac8068689681b9120ebfc83bdebc58"}, "b807df28-b842-452a-908d-12415dab3b5f": {"doc_hash": "23c6ff992ae319c857446fa4bdcb0a20a2b794ceffb8237abe20d832a3cf2c58"}, "ee695332-dfce-4236-bbfb-9d4472e63323": {"doc_hash": "0f80a6a4af7567de213d180e496210a9e532d3a0f31be92a3cb62b858c41c69a"}, "2aac5abf-978b-4787-99c1-95386b55bfd7": {"doc_hash": "b22326d31ff8e654c8f645f13d64b1ffc1c6d1b29a6f490bcfcb38b2da5e3cad"}, "9505c2f8-555e-46cb-b118-f216f191810b": {"doc_hash": "62753c438c15ce378323a23ca1978b103fb7551f8581b5e3d52cab464b7d43ce"}, "2b8cfdd1-78d7-479b-8abd-a7333ac3682f": {"doc_hash": "5d691e82703ec4d672ac7a005a3afc1314b9969bf0d45e8c773137b0698ba16f"}, "f68ddd63-d84f-41a2-9c3d-9882f5d33575": {"doc_hash": "a4b345c80e7f2521eb24fee900b5bd35cc75b033aca1964e907100065e88c0ef"}, "8a5ff00a-9e8f-450c-b442-27013f5a9f66": {"doc_hash": "33f92de32953e4708a7b24143da3873a201d1e1c69eec2b8b48fbc36233bdb0b"}, "97992d48-0f29-4e1c-896b-9b135936d969": {"doc_hash": "8a0d81c477f98b877d2085fe123f0d3fc2d1bd05e3857fa7b101eb73f0a9fa2e"}, "63bdab34-408b-4f57-a3b2-80e23070634e": {"doc_hash": "c9d0e91099b3a5612ed72e4af40ad165fbc60fd4956c3223377d96839375a3d0"}, "28f7d8e4-7af8-400a-a33d-ad8703f6d564": {"doc_hash": "0d632572af8f128f1cc7fa1144522179762f6328970758bc3d3fc4a0f096e1b1"}, "a7b58e55-9e55-4714-8337-acc55ffc2c20": {"doc_hash": "2d38b71f7c3bcd40c3ac702298f202c0b519808a89c4385e66f9124dbe0da410"}, "656fe6a6-f9a1-44af-a953-1d9a2a3d9b8d": {"doc_hash": "240dedba578edd7ff98b8d0b69b3a8b0a593f2e41a0f5be952867598e1e79c22"}, "54f8fd33-4d34-44e1-b2b5-d3a246fa6ca0": {"doc_hash": "5f85744ffc289d55baa3afc460979400b5d28808385a1bf6a41ace461d4d7557"}, "2541c835-8ddc-42aa-83c2-2ed338203a96": {"doc_hash": "b3108337f630e2d1363fec155d89aa6deb553b9606ff28a7038ed9c73bc37803"}, "e7732b8d-f617-4069-9e3a-db7f24ca8381": {"doc_hash": "e44e12f0feafef68b3f54f145bd37db66a9799342974cf6b474eb471478f73fc"}, "9a28a8e8-b4b1-4767-856f-f579579bab7a": {"doc_hash": "70c8fbe813d1cb1b643100c3bacc6526098d227edd60506433c443d59be54270"}, "81b75af8-dbab-4f15-a7d3-fd2cc0ca81fe": {"doc_hash": "ae7e49ba11f82206bf5f0607500f3cc372d63b3f73bad1f4d16cf236557be3e7"}, "485d2b29-db88-412c-92d5-a863a54758e9": {"doc_hash": "925c052fa43e2620e1b4ce095af0120bdc303ea9fa73046db018131853442ee0"}, "dd456587-ab8e-41dc-bfc6-03cf9489796a": {"doc_hash": "5266bc24ddbcf5f3a096af46656f0c86f56145ccedcabde9ed30d025331d8c5b"}, "e439006d-71c5-4ca6-a7f0-7f97c7b2973a": {"doc_hash": "92b69ccba6f97ca2a3407e39d4cd3dd932feb9a5d19e6335b31ab5a8c296dd5c"}, "ef844671-48f6-427c-82be-112a825ae0fd": {"doc_hash": "4d8bd774df4da8bb53373c76222f37a9827fe12071c51172787c7e2e16f0da97"}, "32a546d9-9e1a-4aab-a42b-68c284303937": {"doc_hash": "e1ee58967561d583b8839134d7829d79005cea08a087d11893659245d73b9812"}, "47b5abf6-5d3e-440f-98c4-b582c4ad3b5c": {"doc_hash": "249a12294f3d40bb2f09492ce6a3a89340f4db74459d277e515e316d17811e01"}, "348ba703-76fa-45a2-8662-02b9a83eaf04": {"doc_hash": "6acb9e81af061d635549ab4523b9aa6ab5db1a58eaec0cb06ff88977e1c696e1"}, "f1de361f-3c94-41f0-970f-80376e5ab2cb": {"doc_hash": "681c9d262458516c48b7590af3fdeecec0031c153ad19ce42b65e313786a244a"}, "71e89d55-b7ba-4159-89fd-ce13d910c59b": {"doc_hash": "2714ab6c3827eb2dd5f94252f2b13a50b3a6b14dba13f74c83d19fc6d118991c"}, "fb07b749-5058-41b4-9783-8efb372e3875": {"doc_hash": "0d7e08a712cdc0e00bf9eca82485efad75d74b8b67c9d79e5d27850e1a0f4bfe"}, "6dd4659e-1c48-465c-9777-9e40d8a61e5a": {"doc_hash": "9b5da6498365302ae5300c63f04b7d1fe2ad582d9088d80e68f5f27d8ecb19be"}, "918eed3a-9b25-4319-b1c1-c4e6e6c8cc5a": {"doc_hash": "5cddd8e69069281eba7394b82719ba1bfa462f9d8965267a815508f33af7e74d"}, "5eb7ec54-f407-4a8d-8cd5-7d7e61875761": {"doc_hash": "85f406e3f25512879b16a5bf98b6587320fea565baaca70832d716570c5378ec"}, "6864d9a2-ea1c-4c00-9fda-d0c2c569a10f": {"doc_hash": "0373d842a5f683ba6538cbee9e15eb013dd390272b6b0c8920f5065f140935be"}, "ed1674a3-7b76-4ba7-96d3-4943926b0067": {"doc_hash": "5b12cbfa8b25656b7a1400a7ab577c71edd9f345cc73660023073aa1cb6d6d83"}, "8088188c-e2f4-4462-96be-28001cc6b1e6": {"doc_hash": "8a500391e438daf3936ec6723981c86c367060a370b4e753ca4f6b846324fb02"}, "85c1a2f4-789f-4dbb-bf74-0c854e9e8ba7": {"doc_hash": "cdfc108fd74bd03fddbf898b588aebd1e3cf9307e96fd71309de44bdd9193a0e"}, "767dc615-f128-4822-baa2-148da39bdec7": {"doc_hash": "5c89222e0580a32979c5f99d5b3aa7937e4d0e67f2b516103ce7c6bfc04bfeb5"}, "05eb2266-4f55-409d-925b-852e70b1957d": {"doc_hash": "809d9f5651d9a9dd1641b9f7ccd14993cc8bfdfe71854c11544caa58ba4bde5b"}, "80745ad6-f891-4343-942b-c0d3cd084bce": {"doc_hash": "b7f66150edd3a6832768f2acc11c018e62a8d3becac263efff7b4e56cf1d0d7b"}, "074dc83c-7eae-4dab-b791-9d3f62e711f0": {"doc_hash": "97443f2c903af30d1b53632ddbc6cd2b896d9da67191c2a3e50b87f9bd6923d9"}, "78d2f2fd-a2e2-4a68-a050-e2c58546ccad": {"doc_hash": "8b595d2227e6cdddb9035a9a9afd6389265c801405491b6ee1cc7c2d2aa47eee"}, "e6398d84-5f39-4238-840a-789a1a048ad5": {"doc_hash": "d2ff79b642a39cd625d7c213913ef228fb676fd960a5a7f2ec21b897c6d05181"}, "17507475-1ef5-4c7e-9395-47be57ee1518": {"doc_hash": "5e74a7c373cb7c1e6b35df42f0f5a152241563ee60488249924db4138c12201b"}, "1107e400-7e51-4c61-8db0-368e08d9f33f": {"doc_hash": "384346c4ba46c4e4a595a8cf7542d2ccb13b301577bd5570aeca7ec58d1214ff"}, "3445637f-4df4-4c4f-8348-0afd5ab37c62": {"doc_hash": "0e96ed2cd67629efa9042f391d03a16c1952a9ff5932452d8035491af2d981ee"}, "475fae35-c113-43c1-8b60-1fc2deb86b8d": {"doc_hash": "1cd6732cc5cc6ed821cb45a0ff3f64f56c8e61c54b52f6f684a2e0b81b03ae61"}, "c118a6a2-a1ed-486b-8724-2e24838c4183": {"doc_hash": "ae2b2b56cf7db0f402e052c983f277284f2f7b665d8e7ea18e8d42592687df48"}, "897cfaf4-5179-468b-b906-ad124f4073c5": {"doc_hash": "98c17ec53b93d232808f8854b2ab90cddec83e6c22aa8a286ad1db66df4c6719"}, "4cbdf18f-c70e-43a2-b532-e19081e12f06": {"doc_hash": "b56af9d14b035cdeaad60c95565d53ad4e4049428d986206708668a677d13820"}, "6d39224e-a14f-4783-9923-acd54e6eecb7": {"doc_hash": "f524832a3fbe93a7f7adb295cd7f2ba51dc0ba19b1c82e05fd2878881ed6c227"}, "e2d6f93b-9d81-411d-8946-2cbb79b1b7e9": {"doc_hash": "b896af581879f397350143f740851d034af8390de34fc5c8702d89c9f8df3b4c"}, "612f6380-13da-42da-8535-5a4dc85d6bef": {"doc_hash": "38d4a5283110e9df2251219de2479ec6f08bdbf4502515d7ffeb1875cf8469b2"}, "fbea1bda-3be4-4926-9c16-b5a07d36a28c": {"doc_hash": "337a052fb72fd6ebcaaed357417d9e4c36ff9524eed3625cf126f0fe66e13be8"}, "02ce576e-1e4a-48a8-8a23-107629626f49": {"doc_hash": "22d834a6d6c48ad1ef69bd1b5eafd749fa93a3335c288fb7339baf89eb977271"}, "e847a9fc-90cd-4e85-8116-94d9e58f5ed5": {"doc_hash": "4c3c88c7626b538e626f9107cdc33f0ec355f8100927ea3e4adc881b408c766d"}, "079a11e7-a00a-4cad-b315-fb8378bdfdbd": {"doc_hash": "55b9657bb893f21a5db8a01c4f2e6405e03d61edee53ed3b4adb59a2c209839d"}, "723cf673-bbb1-4500-89f8-9f5cc94b0d84": {"doc_hash": "4224f6dd41d8102b3553ac54e4ff8db85b715914e6790cd8b6d3eb4f2b908fae"}, "7e194999-39ca-4e04-ad0e-49e420d1f639": {"doc_hash": "64dced136b6dbf00e9276c96380f46bdccd9e7211d28377e875d9b52d85a2056"}, "98733934-b954-4b81-8f36-0fb8a13309bf": {"doc_hash": "b441f951d7a530602b88f59893bccbde04755b2fde7094549a91efad4a1bdabb"}, "7a5fbb5f-1eaa-4a68-848a-ed32a0d6f786": {"doc_hash": "e3658366e07dff9616bd940e443a3a9cf782068cd0f23d1c4db3481e3c8e5487"}, "e3677d79-1637-4d2f-a1f9-5928d23e128b": {"doc_hash": "c203f2716797c13dfee22047b6ccea0e6cc5141fd4fed7747c49c8218b571618"}, "02b66e0a-a81a-4c91-b359-606a35f8d02b": {"doc_hash": "919f61d29af5306921540705b3de8bdf318be0d345bedd71faf8e9331772ba2a"}, "07bad7aa-81d9-4679-adca-01fda8b47c2b": {"doc_hash": "34f4730bc8d5c4241c6fa2663d9398fd8e5a81382220d7d24fd011c47a677690"}, "384591f0-576f-4134-963e-41fc4e1708f8": {"doc_hash": "8e85ed3822214921d0d8dfbdfb5f7d971243720e7e81ddbae7dd40154e1eb459"}, "8ae95869-9773-43c1-bff8-c77f8050c212": {"doc_hash": "bc7ac5bb94303abd661542dc224037d77f5d49ed1dbd4fc756ac2198ad807f7f"}, "9b144f3a-8a56-441e-867e-726919db8395": {"doc_hash": "82e13a88ad34edd5050f29aded729fccd3de8f4f5b8b23502ce60e6d70a03d38"}, "e130b2e3-3448-46f3-8867-0e14961e5f37": {"doc_hash": "643c06f72f56d8cd71163d19b68d3af9caa121b4acd8ab28cc5377fba930d371"}, "7315a732-fa2f-4c94-8bd6-f71016f863b0": {"doc_hash": "db312330b1598926166a5f06803e9eacfd376ac7c9b67f842c5871d94264f6fe"}, "28718f79-23f3-4ded-920b-9ae7fe26b2c4": {"doc_hash": "0389817c1e3a9805332e0139bb162df42c53cd24dc9f8ec5a8c75de10e510ebc"}, "190b24b0-06f5-4576-b0c8-8271823ee729": {"doc_hash": "99c56ec91649fec7edb34a0e021a03eca323a15ea507588d2b55ee44c1053059"}, "8260f2e2-9c56-45ea-97a4-13ff33644195": {"doc_hash": "c83cec59c82ef98f940c10a50d6b64590be521009b37f92eeeb6068889c0cde8"}, "6158f7f3-48d1-4ed7-84a9-41a036bf900d": {"doc_hash": "898f60db06e877683eccd5d14d856ba6d5f55ad3f42e4baa8cbd53555c687a6a"}, "4d9bbf3b-3f9c-45fe-8bcc-1f4d1fd48351": {"doc_hash": "d30408256bb88e2a97b206738ff90575f26691d128c71d5afa31fc9f112a6312"}, "2903ce6d-ebaa-4bf1-87dc-631a4901423f": {"doc_hash": "a41f53f7d75c62546d6de8c43fba3e8b8979c873920936580bcd867f59554f9d"}, "3334bdb5-95a7-42e5-a9c8-7e4cd5cf6d26": {"doc_hash": "41c78a0b6da9e15efd13e537626c5a8c894c649c62fbe035783f3ca8b5facca7"}, "3c1e5c8b-c7f8-4091-9e74-1b2cd192ba60": {"doc_hash": "c90a6db343be8790939a82af2e84cb68654822e59a818e2ecb79aaef6a6e4f0d"}, "67363930-4f81-4774-9c81-17a3b7bd24f4": {"doc_hash": "1ac2a9e306819fb6424c49103762fec3a93753de830ebf2d553aca54cf33116a"}, "2be66d28-7232-43e1-b066-919831ad00f3": {"doc_hash": "966da7289b3fe86163f6591f5ae747d067cd811e8fc17b109e03454452b4e299"}, "6f8de890-a4ca-4fee-b097-e857639b4a9d": {"doc_hash": "d8d490bad5eab2a607f7550da42b66c007f787d01b54530149e34a6ebe962e2f"}, "7822f6be-a0d7-4be0-9782-6a98a20da6d5": {"doc_hash": "eabca36af27cb4d824faf0681565e31b01c42564b9a3b31cadd4180359e6f80a"}, "5f6ee1e9-4865-486a-8365-6c6313b35b17": {"doc_hash": "06bd3aad5666c5e44d38380f8592855397bdd2227e04dc8344a5994141715a96"}, "4a9586ad-f865-4eb9-bcff-b7899383dc53": {"doc_hash": "f820c3cfe1f58582bca57aaddf32b87d41d60d23164277b678750bd7c045b6b8"}, "0376df55-a1ec-4c8d-b5cb-04064c986da9": {"doc_hash": "50df139747f37c41e1cd9b602f085f6d710719fc01b0ce5eefe7eb99ddd732f6"}, "feee9d8a-da92-4f4e-8f26-8290be3d6aef": {"doc_hash": "6417bbb5e3febf45ff70a37cd0f3650f2c5a3a3e8c2c4e3e0cb6dec68006363c"}, "f4c8710f-7080-499f-af04-cdc9b423762a": {"doc_hash": "40c25c938a16d7d829e9c88ad2823d5e5161accc07bc345d46b51c6b41dcbaa7"}, "a02d6212-2cbf-46f6-97a0-bee556591b4c": {"doc_hash": "dbeb83354f15d437ed2277214a8f0839533f4d01a6d5fe8d73c16d8cdd6d8a90"}, "97540b97-83ce-434f-a195-7130b7e47e2f": {"doc_hash": "862a23559a486c72d9ad86a95294e4b5fb62a53c54df3ad884b22fb537b76a2e"}, "d4718bf2-3848-4913-8e63-d15c9eafb2c5": {"doc_hash": "42b88e26e6b862511e0d464b32e7040ba95414c5d6c4441c232bc1a52424ecc5"}, "efacd29a-de74-4bd6-abf5-26d7bef9a05c": {"doc_hash": "7fa26fee952a6d6651b05c503975caf20212c6fa27fa7c00603ecb01f8da5558"}, "cf6f6a6d-288c-46f3-888f-da68f10551b7": {"doc_hash": "b349fb6bda41a2b07d2de71fa44540be47de19beeb245971f2db152121f3a0b4"}, "ffdc80bd-1dd3-451b-8b1b-71945852fb82": {"doc_hash": "c1247d00e9a6172814b1d58661e115b3d97647bab939257b0271c1f0e8356bd9"}, "6092ed07-ef7c-46c1-a5ff-e8e3e092ef07": {"doc_hash": "1d44b420142e9af0aed993aa7a15af7b134b87d6fe4893894e57cfaa493432a2"}, "8e788139-6a2d-445e-a29d-059f0c55c583": {"doc_hash": "f22fc620e8c14864a5d2fc3fb90b047d5801deb5e7cfb7a5e793363c66d2e486"}, "b8de9fbc-f754-46db-8d6b-36f61a335b39": {"doc_hash": "0e054e1b15126065bc79d2f5f87c62b3ec57b07a2ad6274b966202cdcd288b59"}, "6bbe41af-9966-41e5-b6b1-fc7eefb7e8fd": {"doc_hash": "c877ef7359285b5493eaa9a5a91c4aecd1290749a777dd2cffde52b61d93f1dc"}, "8a5464cb-d78a-4982-a225-e2818fa56c5c": {"doc_hash": "c34dd7c78d2d87d3931f417c4208e3b285a282fae3389eccff444afebb8b7d48"}, "5d8ae302-8b4d-4778-bf93-9bea81aabf55": {"doc_hash": "777a03c574661e044ddd59dcee1a05bf745e005a9b352f6a65fc7d230d48ba6a"}, "4876e045-a56f-443d-a39b-b4ca4adbcf06": {"doc_hash": "6ef386b46c7a3dfe68ea9f48f337d4f04bd7a760fdaf2b25b274c7bfecede414"}, "665a6a5a-bf7a-440f-90de-53809621086a": {"doc_hash": "afa85d705e42502cb42bf104bf4702a4933a577e44c77b022e9a8791f81d1f19"}, "b828656c-52d1-40ce-8cb5-848988876d1e": {"doc_hash": "ed948133480bd2704f3d1900848ebbdc699a539dccaf803db8e33c93991b12bb"}, "9d42f92d-ac94-41e0-a4a7-3c801a389ab2": {"doc_hash": "c7320411352faa8650166f07a6114d3ea5ef79f2a919eeaf64c1595d8006ee6f"}, "63d885cc-8e6a-4c49-92f4-f1e2579cd19f": {"doc_hash": "0807f5fc60c72e60f31865ac47565b3b9284b4a977fc3a45004c7a3c7b37c314"}, "d04a3e50-4c30-407d-9f0a-cf8448f9e909": {"doc_hash": "e8fee7c581e4f4fc7f102a6a1b422fdb20b18d12e4474b655f9268490456dd00"}, "65e0aea6-37c8-40fc-b6bd-5ddab6a1a7ee": {"doc_hash": "57238fbd8192f4f68bee6904f51ca5c209b3994785f187872cc11ae48eb82a08"}, "97e0c188-4028-44d0-93cb-5c154f2da1aa": {"doc_hash": "6bc1f9a692b86bc78982466b4e4f297986455e1e368de00a53b5f653b41ba3b6"}, "223bad54-02b3-4c7c-b72a-8c111e419a17": {"doc_hash": "db463361dcff0164bd16f6e0ce07c89949b967d37b8c24e9e10c00e3b198304a"}, "f59a8808-7afd-446a-b2c7-054837a8a10f": {"doc_hash": "022917ec98d98a99db0cbbd179981d9ccb76c825a00f429f353fed3b3de21563"}, "90fb0068-e5f0-4b09-b605-39a3b2b3b4f8": {"doc_hash": "70a43352547679b96d87152b2dec65d32be5c4cfbd6394220802e85822b771ab"}, "94bb8087-9ea1-491a-9566-b9e696746325": {"doc_hash": "b69d9ebca7ba603a7b0dedcfb1910ebc1cc840e1216cbc6f62b238c880a2c6f8"}, "b27a658b-17ed-42c9-8328-dd95fd1a554d": {"doc_hash": "777adaa5f2abe547e2ee798236861e7b6bfec260790092cbe33bb1a7b59fbe29"}, "9420320a-5485-47af-b3b3-9d2394e0fb28": {"doc_hash": "0c6f3fe8bfdb3d46e93027df1d94ddf0611944beb62bc118b80321214578fe44"}, "10c270f7-c6a4-426c-99cb-62497edfa3ec": {"doc_hash": "201adf089abcbc48eaaa4c0c2538580c42d5b38bf0e7bf13429d72c4ba594edc"}, "58a28981-4c63-4f88-8fc9-dedb222a2be4": {"doc_hash": "360460bc80dc80af7cdebc0541efd0c00364886dba7db13e9c680a2e941dd044"}, "95fc0a79-4dea-4ec5-aed3-a15d7baa996b": {"doc_hash": "36688b3f882baba7333c1010f4e6fd632028b0953ff75d9c8c379243ae5bde01"}, "28be53d0-40c1-49d3-82c4-015ffd58e553": {"doc_hash": "54fbdc687ff034af39ee945de0ae03f3a96938dc78294c9aeac2d70ddb9cb365"}, "a7269508-7340-4022-ac78-7a0d8a4c3d6f": {"doc_hash": "2c7f0362e3402a8fdb6dc0f21fd8f3139d58bb42c0c60b27b19dbd6152434d5c"}, "24d2310b-1630-49c0-bca7-8a6073a9977d": {"doc_hash": "04f953bddb30012ab6979b21f227c249ef5488c4d1cf863c1862cdd4c1f0e570"}, "651fab2c-916f-43b6-b194-4e1352c1f7d7": {"doc_hash": "3c5965b4225a7797c15bf2e19d7813e43ad8f49bbaf3cc88c74d7c1981aa3565"}, "88463729-bd29-40ff-9152-f053362d28bb": {"doc_hash": "936b627d98998e40946440a3f12fa1efe4d5a346ff677b66861729b50872b16c"}, "8a2f2222-114b-422b-b98c-bc8573c6bea8": {"doc_hash": "114191b3591357a274873fba18f34805b97e0088820040fcda5ca8195d0a3bfa"}, "52f119fb-66f2-4af5-bcc7-db9266c7ba07": {"doc_hash": "fac7bfd74af8f5f39f3817f329c4740278fadab957a7897ccf7f4b79a50de203"}, "a3cb16e7-7c58-4084-867f-1970821bfb25": {"doc_hash": "5a5a94cc1e6a33008bea7b7363850cd4ea551e729bd2de407ad1f0db03d517aa"}, "69c35237-3d27-4b3b-bf9e-33473e1d29f9": {"doc_hash": "36c5c0de938ab703a99f4c0ae3d0b0af287f1a3e8e1d83faa006156470b8546c"}, "a4dc758c-ff84-4542-898b-853228f99617": {"doc_hash": "1aca51367087dc03ef35b600076270f6ccedab4daea608fa270c84fa8e67c6b9"}, "10a0ac3d-9c0d-4a07-b474-42ad22f888e4": {"doc_hash": "b7a14a54ce0ee5fbcdd72675d19d21c630088250b223cca2f1be8645409a0957"}, "3cd5081b-a795-4657-812e-3c96aec80c97": {"doc_hash": "0688a6caba14e18df750aec7cc811d606fd8c82fc5bc99c563409bc68f19b51c"}, "3ec3efef-9b4d-4bc9-8294-3b8e66604083": {"doc_hash": "b20e908e748c013ba7ecd1202be50b7e32ffc6d5356e3e789224a3c5ff146827"}, "56c50e83-778c-4a8d-a6c9-82b1dddf9c61": {"doc_hash": "60c3c982dbf461df850b563df241f8854aada6fb74bc2cb99b919ecd743cef67"}, "dc69d50e-6515-4bbe-a837-98585e516100": {"doc_hash": "883a747f435853df011242c56717937d49525a16a52c9d939982faf5f005ac41"}, "558109a6-7026-4c8f-8778-2cdf16c09cfa": {"doc_hash": "ba3404a89c5e0ea40f711dbf3ed519f23fb8b5febbda676cd1b68a3832d3a2a1"}, "c9fc9f92-cb2d-4421-b846-55e0057af71c": {"doc_hash": "e393aaba7725e56be62316cee64d6085d4221d0f591aa23970930f805b74ffca"}, "2c0837de-2cb3-47e7-b0f6-d0e2038dad0c": {"doc_hash": "4ef82305b1716459328fbac3bc344612767a63dd462ac7cfde9caaf4badeacbd"}, "60437352-f29f-457f-ac7e-aaee006ab40e": {"doc_hash": "dea2f904000e59d49708f976e89e2cd6cce70ebd1a50c95f2d6561c5e223553b"}, "f8acfccb-67d4-49bf-9c56-e98f49ad0663": {"doc_hash": "e25c32d73fbc83d54b207c6466c74eb1e072205a13666ccb6c0f0a84e08074a6"}, "392950b5-8248-4e1d-88f1-fe62343fa59c": {"doc_hash": "44cd4ea2719ee57a27908b49acdf78375640f8f24ed98ad0df8338ac5de78483"}, "57c9f308-cb02-44b2-92e6-caaadec73cf6": {"doc_hash": "f492d059395c2576398a4288d7995ec2c50ba5499afc0f720f8b9374428eec05"}, "753d1ba4-8a8c-45c3-8de2-fa459870eed6": {"doc_hash": "8d8bf65b0fc2fd4cbb6bdb461b2c956b49e05a47fda0eeeb160fb0703a351bc5"}, "9ba97870-6d0a-4de2-96c0-ea5eaddfcf49": {"doc_hash": "2d11a7552ca6024bf0c8950df8cc539136aac02d898e235c5385641354a3c567"}, "9a4bd243-7f99-4ccd-8d9d-9d92c425910f": {"doc_hash": "fbfb82a432d5539cc1c1ba10e2d9480ac8747499b19bf565b6850844ad0bbcbb"}, "2f21ded7-7be9-4082-a2c6-361aaa664f8d": {"doc_hash": "e52ed5b18009a643f1859a22852070f89c89f1adb6311d9af8b1368d1cb1427b"}, "287dca0e-1408-46b6-8928-04f9df06a57a": {"doc_hash": "830144fc110fa124210f5f4670c603085666b5186f77682ed14c7e52c218a8ab"}, "981affea-7639-4462-973d-498b13d93eb5": {"doc_hash": "9a0a9ace7270ada23c99ba89cf7d7505671e3d835efb6ab328a16928d0bd2d8b"}, "260e993e-e5f4-4110-8e2e-18b4003fc8d6": {"doc_hash": "ae5a5b2a11cbc51aed9519d424c87f0c7dac26c798a431b33cddfbe552d1f0ed"}, "596e2186-01bd-468c-bf08-e946c1da24ee": {"doc_hash": "9c53853840a0bd41a400d1245fe7b8d2155f57b404459790eeb796bf2f152468"}, "0728fa30-975f-4a4e-b383-1aa7d7ee4924": {"doc_hash": "4d110dba88d609c077968ce8107739897f7445bbdf78bbc51c34cbac38e6a4ed"}, "ca1a8d00-af59-48dc-8ff8-a29bd8a2b2f7": {"doc_hash": "5839e2b0edfe92347c0fa348cc9cac1b2d2193e01b1ec015d3430fbf5c994494"}, "32fd8de8-d400-4e85-b96a-6897b0dff592": {"doc_hash": "4d972b85570068be3325b1737bb272780c4e5aa5f0ab32724541fa1a0a719004"}, "b3e3209e-36f4-4604-ad05-89bf87475424": {"doc_hash": "e77d5810bfbe10f2ed8d4c7614e486cd33b5de981ac7a670e9d4e20c89b3196b"}, "bfbc89a4-4b63-41bf-aa13-f46cd01554af": {"doc_hash": "8f06b8c0eb6a4a79f4fd6c8d2a32c2bc62d44e6300ab311ce84f1fddf714302c"}, "a375a15d-8da4-4e5a-a22e-e5adfc613f71": {"doc_hash": "d51fa4d1360b141c0c2a8bca62621efa83444d9013462b7916826763e419d8c2"}, "d49e7643-39c7-4754-9ea2-315fdc4efcfe": {"doc_hash": "49811da734a778a962cb6f9a28c7fcdd1e8039829a3448cae4adc3844c371807"}, "86a40205-2f59-4ab9-a6d5-4b434ea7ba79": {"doc_hash": "15d0f3a941e5c53a004035d65d7e486b0fdf0622228cbbbab270506a84d0157c"}, "a9a40071-6002-475a-a7c5-c224caa05099": {"doc_hash": "c7a9803367723b778330fbc8b336a0e10000b20ef59a6d0b2bd69ccc4e478e02"}, "e1f5b31c-339e-4eb1-9f89-dafe5eec57ea": {"doc_hash": "1297cc65159a1b303bcd132a8832d78d73ee7eec0d5e07b339e0569b15809109"}, "49f557cd-b836-4eba-8b4d-6e8b44ee7f5e": {"doc_hash": "ca18f9d166988d5f12422564cc24ae3421dedabf0e0dccee133ad04e3deedae0"}, "716ba413-4a07-465c-930f-bd06482d55aa": {"doc_hash": "4526c06d5d9d1d1b18ccb2a558e98529dcfb60e6e146d610b8d37380ea121130"}, "9711e810-b9dc-4513-9073-b88ed22273f4": {"doc_hash": "660f0ea7de86f8dcb19844453be95fcb62f6f72edfd5ebf31543889ec26f69df"}, "65ad5212-ed11-40f2-af99-0b576dbdc2b8": {"doc_hash": "6884e0e9016c5bfe7495bd87eb1d9a3c689c8b0f069ff2d81762776bc4bc2095"}, "acaa7a76-8b5f-436a-a6fc-2cfda15d17ea": {"doc_hash": "adb1aa5061ed0bb41819d238659005460ec70ad966569f89439558fe76db4af5"}, "39e4e540-2e7c-43eb-b2b2-c5ae930d8130": {"doc_hash": "15034f0b0fbd656d38e7b82231532d88d4ffa5f312d9397461039ad0c59e9943"}, "a8fae5e3-8c57-4db3-9a5a-f51a523527f2": {"doc_hash": "c86d0205823884a058009cf58a3854b32c77ef73a02111caf0c4724ca3bfca55"}, "3e2a41d5-0551-4bb1-be07-32866cfcf759": {"doc_hash": "fa96f7723b6d181121de28a6b2a4ca3da74c647c63840f46b73e110883fad152"}, "8c728b39-1f32-4e93-92ed-c267e62a9580": {"doc_hash": "a6387833f80c12ac315e7da6266a607ccd6e9ae369acdab187d982a96ddc1222"}, "276de109-608d-497f-a10f-d9958ab15824": {"doc_hash": "df2311040adfee8cc93e356b0f14ddea8718d011de52b943015a927a6900a545"}, "b3015948-f2a2-47ca-b4ba-f915965c323d": {"doc_hash": "67b1b32a63d6ba5fcf1e80c09e231ca92c625bf0eecede53eb45e1f6118412e0"}, "e81df23c-18ec-4c19-83b4-aa0054488511": {"doc_hash": "edb1045ed49951e33f069dd2f2f6864ea222d72d4c6ff689985af211b01cd015"}, "4ad116fd-94cb-4739-a52b-09ddde0c7ec0": {"doc_hash": "ca9638e96914c02420d583c899de482cbc7e1b95f68872a52b45d4a5750dcefd"}, "567ebb76-e3ca-4574-b4c6-b185fb5fdcd9": {"doc_hash": "3e954d7d20ab2a339bb4bda8d79c84bd950e9bab518a90ca3d93fc5b6892dfba"}, "116e8b02-7922-4d09-bad6-58f68db3ac02": {"doc_hash": "c03a0ed2a7b9b9646a400fa5cdc222affae6119e462f763d6dfc9494c169d7b6"}, "d493e40e-d3e1-403d-b569-c423bb4380f2": {"doc_hash": "fe316dcbf362c36e2cc6a9c85c8fd2c51f2598e75759a443026bd9ea74da167a"}, "be7b177e-46d7-4369-ba4e-99e9de3f6e37": {"doc_hash": "de5a3f2dddfd903099ad10a28fdd7cd1b29945293958541e8a22fcb7fe0e3ddd"}, "f844c86e-ee8c-4b3b-aaf4-0c9c289086c5": {"doc_hash": "f00d41d40b1ae494be9322ec3e12d970303ed88dc3ef56b598fb7b9729828bd2"}, "25dca542-9b08-46c3-82e3-35f308dc2994": {"doc_hash": "e14207cb5388733f6f81a3e1754e404d3cccb0c8d6170e93d181a95b6cd236f5"}, "e82226ce-d75c-4cde-960d-11ce4a21c944": {"doc_hash": "64038843427ea61781572138bc07dc1e690dab5617bebdc1a6527a952abcd2eb"}, "29237465-ef73-4ed8-9907-6b4c04e43b9e": {"doc_hash": "2437393f560625fe773c760867f0f0d4c24d3db1549fc14d49676ecb5eeeb223"}, "a32107d5-5a9d-41ec-84e1-8c2b418ca93a": {"doc_hash": "4f7f69b360e7795ef0517f82e936397805dc88aac34314a0fe310a974dc77f6a"}, "48bff303-73dd-4ee5-8ceb-7cc7523fa8b5": {"doc_hash": "fcdf9a614ce8e938fa33d27f43a02ea821dc015f16e507d4373b3a0a33bdbd7b"}, "db4936b0-33d4-4d46-999c-613255922792": {"doc_hash": "01f88189de0ad0b2745b2d0d4b9f25e0fc6e9a94f95262c614b73469efb82a66"}, "0d62c003-d614-436f-9552-c3f9934ff5ef": {"doc_hash": "28a60317951a17545cbe973c14509e04642a502dab1b5751c140ea280848b473"}, "48b10adc-552a-470e-8364-7dfdc0c110fe": {"doc_hash": "080732a9ff18ae611fd0c494393cb2f4fcb53deb218b6f9c8b63c3da5b5440de"}, "04a50d25-cc33-42ba-a91b-b5f8471a413c": {"doc_hash": "18adb0f60238b6289dda7842f149e67e09a53c6029d07d8bb28377ef48b20437"}, "a228ab89-3a5d-48a4-8a62-3696e9c1a2c6": {"doc_hash": "13e01c9007be7eb20e003cc62927f01302707848591ac1a433651b8c71f8df5d"}, "cdef0c6c-3e87-4201-896e-670ae3dff634": {"doc_hash": "c85d43e5032bed9e9628dacca7d0876171ca324cb688f54c730dc4e20ee95e31"}, "234999d6-7dd5-41af-8767-1e1dfd63f0c5": {"doc_hash": "13377817025df7b4c4befd80307c159a7510728fd2407da166075f734482c0ef"}, "2e7ae1e2-3b5f-4ac1-a5ff-d3d9dfe587bf": {"doc_hash": "3332f3a2678ef7d56b98dbaaab23f9c9c3d459e7b81de0b41f52d4ed35b329ff"}, "0fe82c31-64f0-424f-866f-c3dd2e4acfbf": {"doc_hash": "d6707d7cd8218924f469ec96e5f377db5ad7e342d6ece105e14467345b7b57f0"}, "c3db5b8c-d519-4543-b88e-78269d2a1add": {"doc_hash": "e755be219706f2b855e73b56aef448ce4d25f241beddd1954b1e26081e743a6f"}, "b296c0bc-15d8-4e8d-9094-83e37a8dec57": {"doc_hash": "035f11172ba3b0d333273af296d74fcefd6d06c9b06a2e7f563c1a4759c873c6"}, "ef8b209e-d15b-493b-abfe-857f5a6e3417": {"doc_hash": "04652e543bd12728340d7cff83cf633dbabcb8b98478f1c0e6db25ef5384318e"}, "208daeff-7428-42df-95ae-4ff36e07868a": {"doc_hash": "8ab3055a2dd3ed88697e95bc04d5c418bee5225cee8e4736241dfaafac2629ed"}, "762f3d1a-04ee-477e-a629-a1c6769bbab6": {"doc_hash": "9b2e265303602b3937931666fb14172de654d5e4e08e70f698f095f75f2f00c7"}, "8cb8a8e5-25f0-42d7-87ed-c3663bf7d5ee": {"doc_hash": "7a7455d1fa4975722e9c52b6cb0e4b312f34883ce9ec5c678996a48d28cfa9e0"}, "e8939af1-e442-451e-a761-b9061a868b86": {"doc_hash": "7ad90d0060325f9e9cd2278ca4a81ae154fcbb4f12b075002b32bf1a354f969c"}, "3515e404-84b9-4929-b3de-4284ef0a87df": {"doc_hash": "d2490d7e17c1fa8fdc4e51b827c53a9ec28b700ee71217c5338b0b00dad716ce"}, "8d8431af-bc1b-452a-8e06-3e5a3e345ad7": {"doc_hash": "e1c412cbde3727bb3fe21f8867a575e6dabf056e361625f9dd776718ff19d899"}, "f102d4a0-01a3-4448-920c-f5d4596d1c39": {"doc_hash": "2e8c58865b5bab6c6eebb00c4aa0c85ddba888a9bb4e71743d61ad41605ad786"}, "6cab64a2-7131-45cf-99f9-b4b42dd32f96": {"doc_hash": "d3110852743593568241c5f29bb31685ea63b0a7e01208d2a2497768889eb524"}, "a325fd42-232b-44a0-bf14-53c527fab714": {"doc_hash": "e4cfaae1c68c3cc0f62abe65ffb04683fc37a8220c6316c2bdf2309b3cbf5f73"}, "f88a87a0-a62c-4b61-8e2a-981bdef3012f": {"doc_hash": "68d2963853c53315b330fd3a96ae01e98bfab62fd2874a599fbb3ec356b1face"}, "03f73cd0-c882-40d9-a6a2-7e725ad5baf0": {"doc_hash": "fe82140a0a4da082a1658e314d0950f581fbd2c5407beb7d5499bb16686f9b1b"}, "8997d91b-9461-4613-8f1e-7cfad75c3dd6": {"doc_hash": "9174bf9c3f00e7ac06ae06ede5ddaae9a27cac6539edd1fdb99c6821140ce2c4"}, "e267c7ee-ad0c-4fd9-b430-65e2513aaa0e": {"doc_hash": "157e2613ec68c1c2d7af130cb4bd0a84db26aac9f908709411201a8362b1932e"}, "29232692-2ecd-4ca2-86de-53a41d5e7b21": {"doc_hash": "ed3ad4602f182d2995ae8bc8131e5bbb42811a53f045fe31accd450a1b098dc5"}, "617d6efb-3459-4844-b5cf-41c894dbc945": {"doc_hash": "1b62d9631b6fd37fbb21b6008385f23881d8f205008d1615923cb469c176204e"}, "463e9172-94ff-4eca-a179-9447806f5407": {"doc_hash": "d39b0923f90983b8ad6357b0c52efdfffe8e2d6cbdf271170f7f284fd6c0ff9f"}, "ac21383f-f7ff-4093-b0ed-1d511681136b": {"doc_hash": "75b6a9bde8a901034b072cef76f2a4ea2c4c35df273456a9258336738888199b"}, "669c5a06-a208-4579-abf6-39fe1d563d2e": {"doc_hash": "b7626d69243d19701ee7043120dc4f2940d64161a548effc89e061dc01b747ff"}, "3d4652a7-6e62-4fff-ab3c-7be155cf8eac": {"doc_hash": "eb66736adcdd03e8b67368ccd6cade60a5f022bb6652d06eea03037e5cf7f6bc"}, "f1e04342-19f3-46d6-9531-9b2ab49aa145": {"doc_hash": "4a83424da2c00d5fe151c281d1ab43207316afbd5e0b181ffe667e028be25367"}, "fb8c4189-ee99-4e03-bdd2-23a0bda2cc76": {"doc_hash": "1bce3064a4ad4d86ee333ab698634ec1f59b2e23ccfb43830f9384040db28b44"}, "2a34daaf-f887-4d1c-b2a1-519bc874b642": {"doc_hash": "b473762957bb4ddc21ec9af7cf92b02c2c474367ec1c6349ca197f7a0b684fee"}, "b02f7298-f2b7-4eb6-932b-e8b8dbe913e1": {"doc_hash": "07440754a125c37e6963823083af9e12577b2a86342ff0549ff468612dbf32ce"}, "faed50ec-03e0-42a4-a96b-75080474835c": {"doc_hash": "967feffc76d230c60460703d7a6a9b6649cd55954507fe1dcb76b391f5c025a3"}, "ee834d83-ccca-426c-a7d1-70bde5dc3cac": {"doc_hash": "b131a246bbfcd24a890f2fb9a974b86886d05edfb1750a7a6c6657a10a52a40b"}, "23d5fc6d-5572-4622-a226-cb511aaf1f20": {"doc_hash": "2a96764fb368d0093c4e8c885ba39a4358c716d885355025f39af6cc5c4dfb36"}, "c0dff789-15af-4a97-93d4-fbca5385752e": {"doc_hash": "99b31d51392a7867403196677e64d1bf5a5681e7f66a017b4c759cd31969d87b"}, "ea1005ed-503b-44c1-8df9-f561f494b4e3": {"doc_hash": "bc3043c130be1856523f7c19ffba1261712aeeaaa25c4af568d56291bd939df1"}, "b05a4bb3-8bb4-46d0-8e8a-e301900c8088": {"doc_hash": "da67b18609b0fb6ce46300c3e5c08d866cb72dea19daf60b3e79393db16c40bb"}, "d52c0555-15da-47c4-9298-ccffe77d2ca7": {"doc_hash": "71b3a8d915edc5a853278b3962e273d1065335308a096562812a37c1da228c01"}, "3e1b813f-97fd-4309-947d-c1e78493ed22": {"doc_hash": "6719eff684487a46c2c1f693fb757c625209e3483654dfc67757fbd0301b14c2"}, "1fa7f339-a178-4a49-b5a5-63fe8985e293": {"doc_hash": "f5fcd403d69d1bb87985958e7ae97f1c8a63cc49c7ff4802fa938db6c124cf0a"}, "900c711d-cb41-42ae-8395-07ec2a4765ed": {"doc_hash": "23d70e6819961c0f91c227eedddd2aa4376e53fb1a80c10bd16258b5a1652c59"}, "171d2d18-69c4-47af-9e93-9c6ae5c0d055": {"doc_hash": "8192b6cb080f610a57d5aa879c27a4d35c50da5d8d73955666f06c42071f0a6c"}, "0418c5d6-d9e3-46c6-8fc9-dd0be3a6dda2": {"doc_hash": "7b05c8d78b89fd9436a615978c16ac123563221ae7cd62a99cd0c7042618fc0d"}, "ea90e577-361c-48de-b0b9-3d168655d1a4": {"doc_hash": "b33e0d460126366dbc5bb2e0bba3c27e683e851b532fe0f44761e7db06338ef7"}, "69cd95da-af06-49cc-a1c5-9fb5a8404bfe": {"doc_hash": "9f3bf6c05f074fe879fc6496db836e916fb02050b3a43c7ffd1645970e090a43"}, "df4379a1-2ab5-4e37-a18c-26b16996ab7f": {"doc_hash": "65035fa58e3275e1fcf1a4042ba879903f0c35e429457acc39a2131d19dfda70"}, "03e2429d-7fdd-454a-a561-efe7c38a6dce": {"doc_hash": "7a4ac30e04900b5d3c02cfadabd365a4987e079b82f5ec7ddea57ee8cdde8211"}, "3cce9c40-4cc8-4963-a5ca-a11d32fe5b4b": {"doc_hash": "fc6e6b56ced58bff0719d1167bdbd5a472fd85f6d23bfd278742fbc783ac90c3"}, "738cefda-8b6a-4215-96cb-454154292b8b": {"doc_hash": "10d67cfbab86077f7d74ac2f56d882993999237032571878903c3231eff384c2"}, "043da2e4-f3fe-42c5-ae21-3069c40ecec0": {"doc_hash": "a4f83c70aebafd8a14394143262258a47bf9792d655b6ff54e76a441b0c84cec"}, "0a70a199-afd7-4524-b3f2-4f677155b355": {"doc_hash": "661d83fd2c38f6bc5d344e5cb2982c89022cf63ca7cb9aeb69ec3672f3f88faf"}, "93312774-8a01-4736-8f9d-d1b0c3fb013b": {"doc_hash": "464547cb076dbad527a207050f1628678e234dc0926f373991cdb3991dbb7e56"}, "3843aa89-ab0c-42a9-80fb-4b531e8d2868": {"doc_hash": "38e8286c7b54720bfd7b4cab3cfa557cd48dbfab2bcf896f09f791c7c6504064"}, "198b11ef-d048-4c97-9c8b-ec61394940d4": {"doc_hash": "9f587e26b490b469dfb3626eda6346ed51c3d5b2b9e55bd2c10541dfdc48c002"}, "32f6b2fd-c9dd-4e3f-b53b-bae5ed7b2f05": {"doc_hash": "024b082a093d7d7ff2e3e772d68c105551096079dd0eb85b1c3bb920f226578c"}, "62df075e-1639-49fb-bbca-e3e3465ec132": {"doc_hash": "60e7d8010fb4debe506c3eb86ddd5827bdc16dc3053e20fd70893d3015bbb55d"}, "6f845ed0-c9c2-4243-8211-a25fb2ee4977": {"doc_hash": "cdff8b5db70e45b2ba5678be05e981db5bf3bb0149dc6e0d967b2fdf3f0ce97b"}, "4e1262d7-aec6-4254-943b-67598d88b6f9": {"doc_hash": "0298dff07a19d29e91a1e9a97237bcf30a0d63323cf22fd8f7f03f47c88097c6"}, "311050d6-4a8d-4889-9d1c-c56f0a44a873": {"doc_hash": "3101ff2ac743f987c6cd0890c77f9da44317af464ac272cf7cfe181d5581218b"}, "7c2e5da7-0380-40e9-abdb-4a04e47c8f25": {"doc_hash": "9ab481bd6c9d5225c500c3f7427d5ae28c92dba1c99b101830712cca71585c07"}, "c3565267-9c7d-4510-85c8-30ca726e1d3b": {"doc_hash": "c789e0561a4b89c007ccfa4ce658e5a83d547a11b5e08e7d71f231985f570709"}, "5fc4c8c9-d1cd-49a9-92e0-8a5975243395": {"doc_hash": "5bfc01268f8a04bb1035237a4675602b5687fc9379f41715b52f5df7c1b421aa"}, "c694d345-37f9-4c9c-bab5-c3305e0cd38d": {"doc_hash": "ae41c7309b2e36eb3cc391d6ec5fe1e4d55e2a87a7752c67d8ee3175fb7c6626"}, "de556126-45e4-45f9-a592-a2513214bdb7": {"doc_hash": "03a5e0441b9be09f321e8619273f25cf43e3c595ca83b0cae71f931881c7b7a6"}, "abdbe149-b32e-4dfa-8dc1-10a00d706056": {"doc_hash": "0371f79c371cb01fe3a7820eda4c6c7b01448ba5b70202ef5b64baff76d9413a"}, "8f548b23-6a59-45e7-be95-2720f60d61a0": {"doc_hash": "13e68fa4581009fb62d4db9dd1537acb2cae5c05a6042412587a61687d761cac"}, "77de79d1-9153-4bc9-a9ea-6854b733d0f3": {"doc_hash": "2633cbad88daa0a00a8a55998454585c139f4f065eb991fdacc613601d35357f"}, "5f777021-d6fd-4d6e-a2de-cb69085b4c50": {"doc_hash": "220cdce9274faffee647109f94852b00de98ebc78b18e0da4a34c4e88273b825"}, "07b3b64a-c1f6-4438-a478-4d55877d5a7b": {"doc_hash": "a12393ecdf776257b496e31bbe4e7f6d46317777ea95c603ff24322b16cdcfa0"}, "af409767-88a6-4b20-a962-879e18aac71b": {"doc_hash": "6a06945c613cae67cd64f5edc1599c80905baea26659bcc1784002b3914e2e93"}, "31039fce-7644-4847-a69a-02570d44570a": {"doc_hash": "993b37c99bc93f7a0f05b0bf26a3e5557ab05acfbe67a87c1bbfd683f11a5dbc"}, "87149753-e730-4e86-994b-b295e9325087": {"doc_hash": "ab3eafe1c9b779383b6fe29496550697a1018fea94fee8fc19f7f565987473e7"}, "f85a56e9-675a-40dc-92d1-5b125e7c48e3": {"doc_hash": "2da6cadf156b5cfe690c76ff0bede08ace9bc213defecad3650bcc78f30aa42a"}, "d6ddda2d-6a67-4eb7-9cd3-809f218b9603": {"doc_hash": "3b0cb3326c73040ac3ba2deea0052d4d46297cbb9e4cfe2978841d0237304b7f"}, "8f776bac-a0dd-4fcd-bd14-526ed9fc4232": {"doc_hash": "1df81feb1a286fbb42d97e438ea2d5133fd78db1348e44eb612b0627022a15cd"}, "8a1294c6-9dfa-4d6d-815c-fd20ad0779dd": {"doc_hash": "736da2d2e41d55e589a1e0a547eccbbf4be74382c3a79e25bb5bf37a612706ab"}, "45209a62-ebaa-4ff8-a260-0601f8ae3319": {"doc_hash": "82290278cb30ed93d9dceda1ca78175e1c2a9f4308c640eb555e05184d2cc637"}, "96560e92-36c8-400a-a092-eed4df49ee87": {"doc_hash": "44b532fa239834d4f1b6287dca21877e5d2e64c8178c1793cb01c219e94323a5"}, "8ae87073-1048-4ed0-9d4a-aca16181b333": {"doc_hash": "f2f7ca5cd39b3287ce505e0ef1fa1d128387cbbe3754e21c4d35146c711aa0e5"}, "3a34f740-9a5f-45a5-8c19-c5136b2d32d1": {"doc_hash": "095b39922d13a8d5b1946cabb058614da38266f9c7fd206733f3ca52ebfabc24"}, "9a0ba7ac-2c71-4194-844d-42d7c8451ae6": {"doc_hash": "19bfc456135d2ab4692d6c2b843d3449dfd83aa2dd436e568dac7a49971d469f"}, "30c18d8e-b3f9-4c65-ae87-4eb72f473992": {"doc_hash": "599af4089b6ffc2ae8506ed32c5dec37fc85aa57f51c79b7e6bf687c52bed7a8"}, "e6161d5b-cc97-4d3a-9704-fc39164f2bb1": {"doc_hash": "521433e1569aa5c7a4544907ac6d02a9b558dc19794b7515f62999dafa21d5ba"}, "3fbfd5f0-9a44-4bb1-829d-938665f99e29": {"doc_hash": "808d794336e9fa6b1ffecd09f064a26fbca81845354ca62f12b0da29d0e60773"}, "6bbd6237-ddee-46ea-bfeb-f7795cbaf879": {"doc_hash": "80cbeb60936e852035464093a1071877b8645ecae992f152e79ce7448e870a75"}, "262fc2b6-ba2a-4445-9e91-2fbe16e3cca1": {"doc_hash": "465b94e34b1f5e6f29c1d8f521b5ffbb391ac07f427ce8745dae0924a02e8799"}, "d62faf18-e471-4ad4-8b67-402ab5a5ffa4": {"doc_hash": "c9e18167a2bf29b14befd535584db285189685014191ac19e42e6e446e976e34"}, "348603bf-cdb9-4d77-b4d8-eae412c8893a": {"doc_hash": "ec293322c65b75689dd8c93c1b8207d6f9e9c0b45df71eff7b710ebde923a381"}, "2808eb57-eb6d-4b12-be90-2624cc70490c": {"doc_hash": "c783c4819a4ddf69d8e70145f6d9b06875c846c5b314324bc6bb81b039b83405"}, "809b0f70-d4d3-4e2f-96a1-4f773e32295f": {"doc_hash": "179a096afae16dfd6a1f916f44053073423a11bfd81616bbb301c42c9ffeeb01"}, "34b96a9e-4970-41ef-890d-b183e76a05ff": {"doc_hash": "1b57b6966229f61e016b035bfb1d5b892db8cae73f27b6a36cb2504f0abef7fa"}, "41867f00-2376-4b0e-a591-7def0d7d3c09": {"doc_hash": "fabf1362a6dadfa844aff0c8898e3171ffc13fb82bbddcd8c853535bc267d254"}, "49efbcbf-4c37-4da7-8827-e95241536672": {"doc_hash": "9035a095a87781b0001bd07afb22ba0e4ad331479689abde680b44b80b8f091d"}, "21e316a5-c81b-4345-971f-5ce15d6bf70e": {"doc_hash": "35562fcaea27aba07ba0bcfb10e796f8d0d19da9e18e9bbb2f8e473cd69b27af"}, "e60d61f9-abeb-4124-bd0b-42a1097fb14e": {"doc_hash": "ad3d80da9802efb95f667b0115b1a8349701eb5e5a224b6898e278699252c7f6"}, "bdbede43-3230-4ac5-9499-11a1f20b7d05": {"doc_hash": "56377344496126f51ce4ade6844ea329ab7fbb8c92ce9aa4ea313f9962bb621e"}, "72b57050-2e8b-4d80-9d7e-5dbbc063e8f3": {"doc_hash": "0d3e0b10f5f47bdaa08a505c7aeebd006df970cbe139a0402e3e6335d97e6d0e"}, "ba29908c-54bc-4099-97b1-ac74e21d61d3": {"doc_hash": "f5bf9ccaf106b7316aecf12e66e6d0e18e37e2f76f0cd349f076b536920d1051"}, "0c335f71-d316-4c2f-aee1-fe5b7a6fb2ab": {"doc_hash": "8b3514db14c29cd89edd537fb93ffc7249780617e30e180c5b665a2bd5be654e"}, "c201a44f-f30a-41e1-a53c-f34d554325e1": {"doc_hash": "b19fa763fc70594814559bdc7e9adc4981115a2ba68d59330d669367c53e63d5"}, "94f6fa57-baf6-4e75-bed9-0b8366928b53": {"doc_hash": "46459b1df322340cbf0f5ec76a6f124a6e2f5d8a48a0609a2807f7c003b711b5"}, "a2ea11ef-2755-400a-82b2-a854888619e8": {"doc_hash": "a43c181e51bf5a70b36ecfcb49631739e0209d8cd64cd625fafa6146d01d38f8"}, "17544ba6-9c95-47f6-92fd-4b6166ef83d0": {"doc_hash": "f22699cc7c50668f1a68b9eeb1ebba5478bfbc5c5d4f39602fb4e76f67856835"}, "ae9a98b2-7504-400c-9157-bb1192b49b1d": {"doc_hash": "f0c22d385a615c24ca95fac759cbd40e947849fedba57aea258ae399d8b00eed"}, "0e06d3f1-d395-4699-8c19-36c378470454": {"doc_hash": "d8e0dd1e912bb3357995a7fbc7ba70042e8a708d152e6bc4f4c41922b0750a4c"}, "dad43b8e-a4dc-4009-8798-fa9e0d4057ae": {"doc_hash": "f9aeef4124d7dbd8d13561ab7d2facc05ae166c7d6d8cbe2a11b8df34e4a8a2d"}, "7cb635d6-10c4-4d8d-9c5d-eeb4d7eabfd9": {"doc_hash": "8b990a631b160b0e69021394b108a8b7ed1a5d4f4fb7372331dc26ca237523f3"}, "37742fcb-397c-4ee5-ad23-748ce7269d1b": {"doc_hash": "f8f811d2e75b32bd96b937714ab9ca35142109ff1e8980f8b159c718c8f56eb1"}, "99ed26ed-fada-4b22-b013-4d9aabb48ffd": {"doc_hash": "fb20b39cebf4e07add1f3d4745f8b407b156ed93e2bafe76f2a82d78b4b0f645"}, "a3e8b29a-e193-40eb-9b19-8c7579e693c4": {"doc_hash": "dfdabec3c8db739df120a93163d6fc2ed687bcc275e5eb04096d8f5e450a4b3d"}, "a111b34c-ecd5-4f0e-aff0-53da9d7524a2": {"doc_hash": "38dfbbe617face1b60ac9518d50287be37efec199e33e641483b6a9a1e56c8ef"}, "ccc63e36-4354-462d-ae45-0ed197abf110": {"doc_hash": "1ec9fa9f10706c4a5a13a4e276093a1d99b4650de68310a0fe59384f7cd6dfe9"}, "42ef1e82-640c-448d-a900-4289c3e461ed": {"doc_hash": "cd56486363d255bccf204bc2335c138c516f1acd30bf113881884380593c07fe"}, "5f695748-cda2-4393-a7b4-922c9e6f6e23": {"doc_hash": "a6e2309b4f4e4da69a69357b7bb1a81adda296d2047d15a2bb11f00ffcadb5c4"}, "22a5e191-b22b-41ec-9af7-6292a9eb065c": {"doc_hash": "7c8f20f7827c6eb2f432ccfab97e73db18832a18df297544bc738bf46e94efd4"}, "bbd9cc4d-f591-4b67-b46f-1ab6dad20d14": {"doc_hash": "1ee31900a1b0378642ba32f8b60a0270bfbe6d8500f8fd140c601bdc8eb78184"}, "5ea466ed-e2b6-4f6d-9ff5-5ec9506cf7e5": {"doc_hash": "7cd9f054e25d885a82e89fed79725d374ec78286af1e64af03170cb4efd2edec"}, "3c5ecec6-3dc9-42ba-b7ac-2d10ee6680a8": {"doc_hash": "a6e719d38bba329e87ca37f07ce6fd066037c75048d676c649e43012974859fb"}, "7ca43289-2b64-46d8-a61d-2728fdafe7d9": {"doc_hash": "736e90521029e1f040412468a3e0ef91bd5efa5d4b3a8f8f3a1cf97e10b0ca69"}, "af776e96-2951-426f-9326-ff9d3f913c0f": {"doc_hash": "7fdca10a20338ed49b068d6d8b260c7be662be05013a390e3592ce59253b30bc"}, "023921a1-6487-43e7-92df-c16539ea6b52": {"doc_hash": "b21f119c187515be37e0ccf5eaf1cdb8698e15ea91a9bc65b7861fa7b7720d74"}, "55a5666a-fb0b-423f-a9ac-8d5d01b57d8d": {"doc_hash": "195f3b78fe2706345f4f202fa3946d08767a609dcf01271f9c05e2bc91068b7a"}, "116d8cac-52de-4cc5-97e9-20787801d8ed": {"doc_hash": "fa78c99519ae466b5cc9d2c0d1eaf3ad7eaaaad53f3ca5264f60094933c1e92c"}, "1de960fd-008d-4088-b15a-f7deec3530f3": {"doc_hash": "e9eaab059bea04bfbc5241525140535675f6dc338e4a62466b52338e37b217c7"}, "ed686fd1-3a09-45dd-8c93-3c57462283c5": {"doc_hash": "83bd3e3d93bc1a842687b5e33c496b42ef8e0a494ad91fee396b165755bdd4ee"}, "5d6a0469-beae-4d32-9eb5-d2cc571e1d05": {"doc_hash": "88f31ebe6ae8ee239acac6f95efbf5ca0907a4b06c7d3f56d6cd152c56ad19a5"}, "091ea130-063e-4de2-9288-9b2e25921200": {"doc_hash": "861806648e275d0c513789327ffa9f621754a1772aacaf6c3953f84c43d4f3ce"}, "3b88bf01-d417-4c41-96bd-49045ffe9251": {"doc_hash": "f5fddcefa263cd66fc6922cff09f5383720b58fbad63e6da72922c3ec3487723"}, "6d695841-de87-4cec-b8c6-65e0cd5e96c6": {"doc_hash": "fffea5fca19104ee00d7bd09a8fbf9a7a00d94d0f41efd06b960287c131b0024"}, "d314d3c1-39f1-43de-b6f3-ca383750ac29": {"doc_hash": "dfeaef6f347715568a2790a58b77cab986a3cf062b0f638ddcf0a6a37e6273ce"}, "d4411b26-079d-4bf9-abc3-c56395a383a9": {"doc_hash": "898b29474884891f121783cf85fe4759f1e014549cef47430525c4ac3eb5d88c"}, "02c131b6-5c9b-4c6b-a959-1362ff2609c4": {"doc_hash": "fffe4e957102dfed9cd89eea97f543199ea03b2c302afecd86c25fb5d4ec79c1"}, "bd0bc731-7071-4602-b03e-34201e2f0966": {"doc_hash": "5ef19c7469375cf4557f5cef20b7f22d4e57e97e0e5d617dd400e0a2ed6410e5"}, "9321c92a-90bb-4025-99f9-c0d6fe3bf5e6": {"doc_hash": "b2ac3c08a354c17fac75485e0b4a8eb053ae9b96fa895090c82f525470ffdb43"}, "292cea4b-7e08-496a-a2d1-ec32285b2624": {"doc_hash": "ee5a8294a1f5393f495b8579a18282139c66fa89498fcd823e682fa6221db906"}, "32730335-a5f7-4ddf-b6f6-3b699482765e": {"doc_hash": "0053e099c4d6724313b54306d5f121a38fbbc88d1bb7bd48b7eac3d71767de42"}, "250210c9-e39b-4aac-a165-a8ce358a344a": {"doc_hash": "f523a367a6ae2911ef00a6d983122c1fb0009a9bccbcc5466e81ab11801dc505"}, "5e22dadf-7d0d-48d9-83c7-78f282b305b1": {"doc_hash": "6b0a3428f6fa7e768c5274fc6752d9972836cf503b794f20c7dd7bc1c02c1ce2"}, "632c0c58-7140-44ca-a752-4d03377793c2": {"doc_hash": "7a02ec039d6c68411cbf60258fc74488d2a278ce225220de427dd04a5ff0aee1"}, "6538ca1c-6389-475d-95e3-0dda8f2aae70": {"doc_hash": "d963ed92e60425d03aee4da17f304150bfdde2e1e772bcfaa9624a6eff021bfb"}, "5d7b809c-3664-4da3-8e01-1570fafc43d0": {"doc_hash": "163ff58d77803b0aa18f757a83b0f9a6de2d6b18746464f8e3f00d1ecd203a10"}, "8d8c0b13-0579-4305-985e-35d71852e463": {"doc_hash": "a2b55e8da24fbc2f348651dab359e5545b863a0d1776d7e66089477db2cca220"}, "66a4aac8-c161-4900-a806-732aabe3645c": {"doc_hash": "65cea2848bb1db8b1591f2700925062c7ca8b05968a40d754d4314b8743a1183"}, "a8958aca-2dc3-4d83-943b-21a3ff0ef2c2": {"doc_hash": "7cea8f1c7e7d9b92519ebb3a85d171983b81fb01f9b12a73b72e05ae4efd2d3a"}, "fcee70a3-c249-4d25-b081-a7d124db4bb9": {"doc_hash": "6520c7dd57d2f26698aba15466d8cc107b1e943a9d87b277625450465619ccff"}, "dae754a0-1a1a-4e6f-9641-86af3214d9d6": {"doc_hash": "4c1efc717e06e6c4d9e6a9b26bd325d4d3668ea77b43694b914474093510b55f"}, "861c3b80-aa6f-43d5-8b49-66f5744a746d": {"doc_hash": "c87b5612d72efb8f8914af6b204d535865bba120a522d7672b025005c021e8d0"}, "425ec74a-64f8-4a7c-b713-487cf73c3b9b": {"doc_hash": "c87a93432b1a49563ed2c4dd88fa27c27f5c4606755eb304928b02b8ef236625"}, "04079a44-a0c5-40c1-a670-090ab6eb22bc": {"doc_hash": "7cc0231168e7d03507c3e91fb9158b9fce0990d70f80236171dbad3fe07aa68d"}, "c185ca05-fbcb-4496-94c8-1d49eac6315a": {"doc_hash": "14dbf345bcb49e32cc08d50322cabe5ac55291c7cca760ea57d1e55bd8a88e6d"}, "7ed6bd9b-9036-48fa-956a-3be468c826e7": {"doc_hash": "694f7b8c5ca464dbac4d86249f99f262c0ec512d86950859d48ecccdfcda5f38"}, "9e416e6c-3310-4159-800e-87f59f678829": {"doc_hash": "1d32bdb1260ec86f768b7b83731814c826f974bec8f416578d78bf632d6267b1"}, "d9876c2f-258c-4025-956e-ef15b2291d87": {"doc_hash": "057023e3eb03eb6771dd54208fd3bad211a930d1d211bea43ce5f9ccfe589487"}, "be7782c5-393d-43f2-9642-c4dffa73c03f": {"doc_hash": "08982a305139379da277d374187b4aa8514cf159b883435c3cacb5e065a704cf"}, "6278ed83-2ce8-4bd7-a340-2a749944d25c": {"doc_hash": "7754b8141c5638d29d8bbd02a1ffd5d463b8b63fcc475f6f68e18154ab95c6ba"}, "5d57a5e6-a217-46ac-b645-b9e986fe01e9": {"doc_hash": "e11cb811583f14dae59af05e0ca49414ded449936682776c6efd82e31535b8fa"}, "cecf4042-47c1-4f30-bf01-f576f615225b": {"doc_hash": "5c71a311ba857ea04eb0a484854d3a53976ef0d44e563c6864a4c56d714cf0d8"}, "2903b270-59d1-4707-9c33-c8ee7b3729d4": {"doc_hash": "933cf25bac233ef75d1364b56fca43601cfb59299c9b76f022cea5dfd5991512"}, "061194be-e1a6-4d02-826e-5a9867278024": {"doc_hash": "c96a41749e2eb453774f2e5306657c9111bb6d6805705a1f700c98e6af6e5beb"}, "35ee9614-a94b-43b1-b64d-65aea654a9f8": {"doc_hash": "920110184864e758708f722888a04de7f4717b2b69a9434e1a694c870710a51e"}, "414b5133-b882-4d85-9428-2e4a97caedfb": {"doc_hash": "a07c01d712a924aea0b677ba66bcbd66200245bd54c1ce115634df06702226e3"}, "060507c6-6e1a-45ee-b716-7b42b3befdbe": {"doc_hash": "e6caa6c8fe3f4d5c5dbfd86e13bab202521ce751f0c9952bd09be6408b151990"}, "e4d778e6-e49e-4369-a886-5b12797a5575": {"doc_hash": "593a38ca846145ea4559afb208343ff88ab85d210095e9ff327c86260e74448c"}, "09ce386f-c4bb-4427-b762-a5eef372d82b": {"doc_hash": "d5144254065f2666e4dad317bfab3869b7813dc40bef914519f65878fd30ec60"}, "84969d9f-7bb2-47b6-b789-514eb4aa6506": {"doc_hash": "be5daee588b0713a322293a7b321f73d68f6c5434b435b571606300c7b634321"}, "c7620063-db41-4e45-b45f-56075a76b263": {"doc_hash": "02a8e8d1f3cf94fc47e4b92ddac604a82b06a564fb66c5772a740539bd72de62"}, "09fc725d-4ec7-49f9-b5d0-64869840f38b": {"doc_hash": "f205f910bd26eaaed1325616ff730cc41e36d6aaec98d5a719e7ff27f0ec76c8"}, "a334b79d-bc58-47b6-a005-720688223122": {"doc_hash": "8806dd197fb855793da8fbbaf0298346e50038d4a3d09aa22d49e5a5ff8aab64"}, "8b7ceb80-aa2e-4e48-8e2f-ba3a7d47c1f5": {"doc_hash": "822688acb0ad3080ebc42cf6f22cc4dba9895a3c262c6033031e53bfc8af6432"}, "f6d21bf0-a8da-42b5-bd12-ae224c4b9f51": {"doc_hash": "59d058492e3bcf600fa147e93d75714e55b3eb160ce3c0a9cc9b8b1af80fc5e4"}, "44d4d1f9-7517-492b-8911-c8bd2739d6a0": {"doc_hash": "fca801ea2bc11782df9b82d9d8c5939b75e5a522c2600a758e75330c169aa528"}, "cc20e891-73fb-4374-b9d0-f6a6a7ed1baa": {"doc_hash": "8c43ac020f8f4b8f1213e64188bcc9c58ba6499f9929417d2f698fbe8ae6831e"}, "47176e8c-572a-40c6-968e-4bc1161f6818": {"doc_hash": "b76ccbabd41594c1c534f605d6b84b2143531fec3c6174a0b5ac75c7057e069e"}, "6d28232c-9ab6-46eb-8be9-6e5237a94df8": {"doc_hash": "d47078ecbaa7072c17769d58210824e7edddf59856e9a6518a9cc6a80e453120"}, "83b70094-efd0-46b8-999f-3da3c52e6e09": {"doc_hash": "41ee9b482f9eb02bdd2ce4e7b0deec861c4570fe669e278c4b19fc43e7c26217"}, "51a95c28-2547-4734-9da8-eae059c51383": {"doc_hash": "be2db629a58ed7a55cf3c9a777854c1f4257f25f0c307da3b750fbcd7fdf787d"}, "666cc462-52c3-4e94-b3e7-7984029798c8": {"doc_hash": "748662114950ca70150447cbf310f110008a74003e92cd7a0a8a7e9ff6bef9ba"}, "4c72a3e2-88fc-4705-a399-ef2052cb34e8": {"doc_hash": "a59542d22805d3cbf826bf7fb1108fbf5d2770c261ba022e9e661e7294db4b5b"}, "48aec26e-3e79-4b06-a7ea-7ed7d5c3939d": {"doc_hash": "b678fb6c9f87cf8cc0c1c4995de7f4b1ac4c09b47df197f11b2e5110d76f42d9"}, "ac580ae6-71f3-4a58-82af-fa9e9e8b0914": {"doc_hash": "dcedc8c2d0d16975f4efa41e18ceac4e248293d9df93bbbabe598624a6aa2472"}, "991893b7-43c4-478d-b71c-ee8de6a1e887": {"doc_hash": "b4d9cfc6c4aef0add54a275377ea21d875d216bfa62a8704d48f6d9a845d89c0"}, "e75e3fbc-d8c0-4bc6-98af-08e3bf6ae3bb": {"doc_hash": "6d3ad3aa2db8d646ffd7d6184d23ade0bd4bc30f4c909fbbb064c47359faf480"}, "4271e01f-18dc-4d56-8c12-719847a7811f": {"doc_hash": "3061754fece2c1b209ac0bf16ddeff073c42553601bab02b86af1cf7432d2c5f"}, "6201432e-157d-4ebb-bdc9-b87c4b23e19f": {"doc_hash": "834242f3692aac825ba5f372a7b82fb8fbc5e7caf51abfecb42bba0059fdb8e3"}, "84da2bac-e9f7-4709-9729-def7a50e786a": {"doc_hash": "ec42c38392ffec6647d35376fd523ae275570235a1c2d9f9198e3fa7bda448ba"}, "fe13f16d-6976-49bf-81db-f726614a2a62": {"doc_hash": "d45641609754118e2c65c4c9112251decd8ca4341864153f73d759f63919438d"}, "da4a8ed3-10c5-405a-aba5-060ee9c4c2e0": {"doc_hash": "2b6e532aa8b64ab40d3762af83cd367f3759ae784b5d4735beea6526d8ce7948"}, "f8e1bf56-6206-4ff4-a24a-0ead616c7779": {"doc_hash": "763571df3a8ed3a81119043ebb94cbf771ac1c40036a87a90122ad929ca2da58"}, "81fae6ac-d851-4449-b510-dbfdca52c67e": {"doc_hash": "2e25715d88fbb66c6df1e99d2dbc62faff37ca87c890b2201240ec7f21779bf9"}, "55e028f3-f449-4969-b7ca-a01e8a419b8e": {"doc_hash": "f794b508e81fbc47e830a6453f8278d357932db78eb95e48bc0f70740c6fd50d"}, "b76d07f2-f954-41ba-ab13-8f2c14be6282": {"doc_hash": "d7f789221a316a76be43444d6bf5380bc73c6827694ceae5cbe79b25018042b9"}, "cf5f7ca1-8d66-4d74-b631-804a2f74506c": {"doc_hash": "e705a2299006a49b3c53235a9bb795cf13b9aa7e0f8d7ddf7ce956805d076494"}, "de8e8d6e-bf9a-4337-9827-a1061c84fc24": {"doc_hash": "f740f049203aa4414a1998102a70a12bc7e05cb22ce3cb15358859351ceebf0b"}, "51640bf9-6692-4269-82d2-0e8aff4fdb3d": {"doc_hash": "ebd1b14304d019b1edd30a972bad71d86e3d77d47968acf9a3264023ed6c6185"}, "0ddfac00-ff5e-4702-b4fa-332efd549f8e": {"doc_hash": "57cbfe3f21e51fbf3d50c227cf4ffa660bc8504633ef0c6381138c158c5ea55e"}, "b3e1d7d6-9d2f-4a62-a632-67e4f4c594e2": {"doc_hash": "d80d236fcd81ee6c0715ca5351ba13ff03ac19a43b30ceeb19b1e6bac14342cd"}, "bed81d72-afe1-4be2-896c-2a67fd8162d9": {"doc_hash": "bc2a3cc981f6b278e20d3306a425d0d1bc91a5a146531c851c009ccebf11c8e2"}, "f5d3fdaf-2368-4a06-81b7-dbd8103fbaad": {"doc_hash": "7fe678bace5ebedc56b275bafe6a2061772d59047b208a39682236733a90e80e"}, "835a55d7-23e7-4bac-9b9f-4c280b2a1d47": {"doc_hash": "55a12b5f042a577170ac32606e9c6e1c0ccc7e30eb35b56dcefbe78a06b75c89"}, "5c870abf-bde8-4072-8732-c7d4041bc885": {"doc_hash": "ee70aa8e7b93c79049cfbe94f031c38a33699b5bf5d1a569fe71ce712e3cf6f6"}, "db8af660-28c8-4400-a466-ae2a1f784609": {"doc_hash": "effec8635328ce069f9c24f96b4c6813af2c03c3fa53dbed28108c532b45df0a"}, "23dd34fb-9491-4907-98a3-60cd0980d95d": {"doc_hash": "236ea226a8032b863c5b9731274d522cf538cd8f12ff2854e70df8e0914f4c6e"}, "9c964db4-c302-4aa9-ba47-2a2bbb589766": {"doc_hash": "849b5abe0d61dc39af97b0eae9d6dc3652bf7aaa99b0dc3ff6740883964c8bcb"}, "f5fe9527-e8aa-412e-be89-97291aa15f45": {"doc_hash": "081b7ab40fb06bbcb18a4740d5a1a10f4af204db2028738d716ca48ee2e17af5"}, "755aabfb-a502-44c4-b4bc-8a48eb0ad053": {"doc_hash": "8e5d4e5daaa387c5063b513bddc6836ae7b08f88c98ea07a6ed93109fc15e6b0"}, "fbd6f09e-29a1-4af6-a774-f0f01e64e4f9": {"doc_hash": "d830edfbf32843851dce7fde76ed4329dae038cd424f0cac22d3f4789b5e1f4c"}, "0e218f96-dca5-49ca-9795-a02d51658d16": {"doc_hash": "bf6804d737d5dff77746aabde052345c6e63df91feb4ab15bd359b54de038a6e"}, "39ff2375-5d64-4a92-bac1-72c0dda2921d": {"doc_hash": "01f82b34379547159b5d8ec7bb978c84370c61708ea5021a10415a8a39d894af"}, "f9f19ec5-3f91-4818-8a2a-ac5e7821c805": {"doc_hash": "46e641037b3db33d264fac82b14e085910a52cd06fbf68e06c620124126ebede"}, "d503dc02-9a9b-46d0-a04a-93b0d914879a": {"doc_hash": "b9c3e0d281066762d1497589b00325e81b2dda7dc2aee8d022af5e5e7b8792c1"}, "3e764b53-33d8-491f-b48b-7cacf8263cc3": {"doc_hash": "8c679ac18d8f6afa466e22ac7828dc86f331c57afcf9d8e0e09b5dfc3d2c518e"}, "910c6062-ae22-43ea-bf2d-b156d3d0924d": {"doc_hash": "77d9a48f15926de03489ee4baa617a35afca203b1e42b99bb66323d4db8f6649"}, "3b985f5f-2318-4e08-81db-38d5de3de25b": {"doc_hash": "134de9d6716b0d27554c54948a1d74445aee8b8b69d1ec7c19ca703ce8c878c3"}, "e37ff5b0-2466-4ff3-83ac-48e9408d92a5": {"doc_hash": "cbb14fff789c61be261f758f39c3d967d31930e55a2d9b456a5ecd7c62155ed7"}, "08adeb84-66ad-4a69-81a0-3b3838d61071": {"doc_hash": "66fd36aea41ad942bf808453fe2ba804f15d7b4555f23576c66e14ff8128e9a2"}, "27bb71a8-79a5-4e9c-926f-110559e863be": {"doc_hash": "518512d6f8a329aaa3b24ce7bd752d15582812523cadcb14210e73a8678e0dc0"}, "73f863c6-904c-4d13-ba81-8687ad836722": {"doc_hash": "aba8a940fbda47d42fac98a09f76da5067b610d1ab78ee96f12c5d0212e0c8ac"}, "69487513-8247-4c52-8dd2-69b0056d84e1": {"doc_hash": "9fafcdad616b5ab1e2df000cd674c5f75846114d9e9024c610fe465f40f5e97d"}, "37ea7c09-7bdd-405d-843c-a176029fa909": {"doc_hash": "fa699e57e6ebf9367f8eed8a39b477a4a7549f0962a275240c4ec200e812b9fe"}, "2b4e9b0a-9e67-4b4b-be54-a7a5f0cfed39": {"doc_hash": "34e7e0662aa6298bc277e1b49ff397647d66aa9c45ad2982231ef099f3bd922d"}, "9e7378f9-53e5-47dc-81a5-e0cc06cb78c3": {"doc_hash": "3810c239c36a56aa2ac8cb722bcbbf92a1712d9a52c1e9ec677d2a13cc90bde2"}, "5827db1f-ab9d-4a48-8b97-d1b96fd81e59": {"doc_hash": "444c60c897011ba29511afac0098a52fd631f186e9d215976c92d1c68faf5489"}, "7eb64382-983a-4aa5-ad2c-1db74912024e": {"doc_hash": "087c25730c4c4ccd848c8dcc6b073f1eb1693dc48b9e5ec80e1a87a3ad5ce056"}, "1a8d7753-df0a-4de6-b540-eb7c703bc8ed": {"doc_hash": "8db77d3691e312122df05c0bbed42deddf95c9860f81cb6375150f5bfd1e998d"}, "82e33b3a-0203-4e9a-8a58-f107b8f328dd": {"doc_hash": "e5ca83400be3958bd4bdff78863cbb16e2c42b0120364b75ca332c75f89dad48"}, "25634cd6-5273-42b1-a2b1-00da0db4754e": {"doc_hash": "e0a1bdc224a365e9d412a3e5a4d9a2d7e11b4c432463853952ae02347102fda3"}, "82a91afa-a974-4adc-96bb-f05e71d7ac83": {"doc_hash": "3395f3cbca4eea1bff5c46b5bd9f83488f2ebe282bb3d78d5ea2dff2f39f6f11"}, "263940b7-26b5-42e9-b0e2-988360a4524e": {"doc_hash": "bed2ffa013133b032a8502a43b2f1a36b3bbb57d72e37a8ebe5b4761b995a227"}, "64924814-4dca-4b66-9b0e-8a65a5598cd9": {"doc_hash": "5da4d88deba2fb1f1907ee34e64a8927cbaf04a6f6924576d9eb1b5be25801e6"}, "6b758866-7a1c-414a-9e1d-81ca3c7452fa": {"doc_hash": "561db7486eb834d2097eed1b3eb7242d1f52090f9ad20f02a48707b4f84db974"}, "826606ee-124b-48c5-98cd-a0eed855df04": {"doc_hash": "592a88261b8a7197146fddf3da827828dac41a15d655bb0f3e0055b94104d40e"}, "a3cf7615-d6db-4a16-9115-6b94a573de65": {"doc_hash": "29386cce54aad87207e89f3ae734f395d8f6aaff6f075835f0ba337663fb9db2"}, "ff76814b-288c-4a74-af5b-9c0e292e16ee": {"doc_hash": "739c30ac907d69541404a2f1cc8844f1ef642267b0518d16c51e97b2fc612d85"}, "8a052764-3b55-4a26-b35d-94bf04549b35": {"doc_hash": "a8a714e2ea1f9129bf2e494ae40d030b4811315ffdcb1c2675f36bb61a5ae2cb"}, "ab99e35d-f964-4225-aa34-6c86154eb4c9": {"doc_hash": "b532ec51f09a9f4903a03bec2137f6fc9a36a175bc27dd8ef538a5fbc438bd24"}, "e2686c30-65db-48e5-930a-2836cbbdc129": {"doc_hash": "dc8451144824908505ee1b3a4e79d735468b3534e865f4f40973933b025dd6d9"}, "8b8d12c5-09d2-493a-baba-5fe97928fc6b": {"doc_hash": "814fbc5719c0a382cfc6c7129414248551a350924cf0b9f914e4227ddc04890e"}, "037f7d86-a40f-43ae-94c8-5f5743fe94d6": {"doc_hash": "4301349adcbe12e9114bef7d3f789e6c61d2d4425712d5b6def4f35a1810ac56"}, "a461839d-bc7f-4b45-b277-9dfb5c29708b": {"doc_hash": "af797481f864d7ae839c9d5e091b7b4c207028ac10e2aed290b188f4048b4b58"}, "30c49d18-8015-459f-9536-498dd5fa452d": {"doc_hash": "542b179a1f5453ab80e51f1d05c2cc0a7e2a43bf178afe88bbad7dc6ae1351ac"}, "c56210c6-3939-4bff-abeb-31847e6e38d5": {"doc_hash": "eadb0d787be05c0aef6b39b4e5aee81b3fcc2a8654bdfc689214ba1b36e38e5d"}, "8e21687b-08b3-4e71-bb77-5c5a41ff7da5": {"doc_hash": "30cf226bef15128d49e68ec3d458ecf90bd8f30a1edeb5306876196ae8fd0818"}, "fdf55cc8-d1d4-4b99-b459-841b5bc33b81": {"doc_hash": "c72ed78b703295bb2353206114d73a0ae29ac7905e651e28ab728bc3d72d7e51"}, "df85b1d9-6271-4885-afd9-7c04c5bde21f": {"doc_hash": "0c8d95ca0c5d10d78fadcb3a816a33af92998d0fbf7551f6b54383e89de9645d"}, "3f5827d6-36e5-4069-b915-36c000972820": {"doc_hash": "d8a9c376f3118322f52d43d9f5a89ab94950929e1bd6f96d996031e45d1cc5e5"}, "06e0d21c-96d1-4ff9-a3db-4c497e2b6d3d": {"doc_hash": "6a5cb1f53f83768a1d872162d01705d4e3dc1a150cc1b913a3f119e429e24198"}, "d5914db6-6075-46e9-a658-cefb2f47cb4c": {"doc_hash": "d8b0a9df4f9580fd13b9eb778630a4cac95deda22a521107a267fc5493866036"}, "bc90069c-86b0-48fb-9d7b-34a5530bec3f": {"doc_hash": "2929da4bc274dd69307f6814cb534b00e2e42ca092510c40ef096f4fcd38a773"}, "5ea9fce6-3c7e-4439-bf0e-df1bfd52dcdf": {"doc_hash": "6c1d22abc57e5ee565ee05d8737e0369b91056d859f054e72287419fb5f554c7"}, "9c68c128-815b-4593-a691-a8cccb744925": {"doc_hash": "e57f124d3e631c282769889439c1babec0cfaf37b6bfb08e8e5ddc27c9d79d2b"}, "b4031009-0fd8-4291-b4e9-c3763e75817d": {"doc_hash": "978bd09ecf6a181c5b975cfc7dbcbe519f6a33182d5014cd0d895e8efd73a8bc"}, "46676383-9b39-4406-94c2-f41198863fb8": {"doc_hash": "d1e86e3bf8f9e8e55ee65499617e2c958a2a3863fd31b84047ad1c1a2146cf74"}, "968c8d7c-468b-4bfc-aaaa-0c7b877806b9": {"doc_hash": "f43c4491093070605ad2f91703e3c96442ba3a85641bacde3b9f2b6e1e55f698"}, "36a82600-daf3-498d-bb8d-a57f5a743221": {"doc_hash": "77c4c8bed8b9dc73cd0ffeb19a55b6b9ed5731a92dad785fcd48f32eb8fd9890"}, "ac5dabf2-cd55-4a9d-9126-baf8ccd82ad5": {"doc_hash": "7b53113d426362543a0c60666f905982f092ddefcbf480b29cfca5bdf89331df"}, "c29e5705-7c84-449e-b712-e3e0cf7f7d30": {"doc_hash": "f7b0bd40951a2ca2ce4655ad59d405cb85dd764c1e03fc4424990df393b24c42"}, "8f6e8d3b-77ff-481a-9c2e-f07de5bd7f5f": {"doc_hash": "ec3ebbe527c28e75fd599765c5fd088f25938967988fe862979acbead280c45a"}, "e2d50b56-6c66-479f-8bb8-43afe8c946b4": {"doc_hash": "6cd60263a61b9dbacaa8bf7b1e743983a71da9189c2d837b7b8c134c13149c7c"}, "3370ad48-11e5-491f-a2a4-998124489f56": {"doc_hash": "b7c932c706a623c85b9f9f649b5fbc14b9ed055be65a276e2aab1b2470123aba"}, "6712f101-8220-4e6a-a6dd-df52d3c8178b": {"doc_hash": "a889ad53dd08e98a29184865b3659f111f96a918a0ec3c6e38346d2ee6fda70f"}, "0808bf88-d232-4e29-ae4d-fdf81cd8497e": {"doc_hash": "3a3b6f5a118a3c1426c6d05cebea094536a52fb054de80323fe2cd87d930d974"}, "c1bc2a4c-01ce-46d7-ad94-5adbb3faedce": {"doc_hash": "55cd84b773d259247d6b19ea644d6cecebac749c29ac5220707ba13e497a25a4"}, "f7e65ac7-d522-407c-a838-7807804831ca": {"doc_hash": "fa210a567b57d404251743fa6cb5774ce0859ee15783d169643a0e0140796081"}, "5b4d3233-e376-462a-b0f0-0c4763563f66": {"doc_hash": "d316506f2e6fac3beb7b78716b3fef367e02b99e20b0aedc91a9af87002ee348"}, "f4bc6c1d-5d37-45ed-8735-ca10e2fe0744": {"doc_hash": "c02326de04a18d68f81b579b82d9e90ea605f1bcda2e830a954d3b8073370fd1"}, "58431ef2-58ed-4313-9ed1-32cbc99c9894": {"doc_hash": "e16e884cd6413a89799824cbcefd251889ce344ae6cb7f25a4d02cb07c7bb4ff"}, "a1c99f96-93f0-4dd4-a7e2-4e75fb35bfb9": {"doc_hash": "91bf569f2fb19d153cf837c5c51e64b001c48b397f31ade189d1a65716cb9bd6"}, "44c390c0-a1fe-41bd-9001-4924bf1d239d": {"doc_hash": "8f84d4067504c83ff459a59c50a2daf671c32d37d04d64176c77c8ce57e61041"}, "72bfda1e-80e5-4cee-b696-58707b8c5744": {"doc_hash": "94df5afdec0ee98a584f6f5ab8a6a86635f033ef261ad7ef989cadb14e8b4f8a"}, "58ae74ba-0330-4317-be2f-c8fa23eff7e2": {"doc_hash": "f81f2a0ccb695eda1a7bd51b424271ea20500dc38a4f391b39ccaba44de2c346"}, "46e4f9bc-9f32-4847-b066-0427be7867e2": {"doc_hash": "1373898a98f31ea81e5c02b916d8934393ac674be7af66aaa4a39f3fb3c4f72b"}, "df147f34-70d8-4747-a4fe-ccf00e0b76dd": {"doc_hash": "e69d4cba8ee0938c96aaacff1f33486ed08e10788cbd73ebe983abfb320f8a2d"}, "5e85fb98-cf78-4f6f-934c-9f17b7b382cb": {"doc_hash": "7e343f14af646c3627c8477a826a2b3d89276bda4c1aa56c1b1a336869c9c424"}, "73e961b6-276d-4211-b841-b9a91e280495": {"doc_hash": "879c6b97a58045e65db308be18c51f7930f32f92e9e3233fd9c93edde09deb0c"}, "c04efdd7-d0dc-4e52-903f-f629f9e77a6f": {"doc_hash": "4b101b55527c429a35e5a8393876f34977b152d40e367cdf660897059e681560"}, "c0920de5-d7f8-452a-b122-6cde6ef71f63": {"doc_hash": "8c2edce22c25f41f0fd11d920331db75d2eca0fee9db72599f08ce54ba3a0898"}, "8662682a-3132-4979-95f0-827158a584b0": {"doc_hash": "381ec53cd7843f0ecba062c334c5f926aea16476abce88a7907ebbaa3cc3e8b3"}, "40757c83-eb10-42e4-83ef-eb26fe81a08b": {"doc_hash": "945d99cdfaf6793f07abe291e83e571f454b5bccb886d33c5e0166f86eda9666"}, "9ed28ec0-8bb1-40fd-b83a-4580f0fb7752": {"doc_hash": "f9ddb3f7a5a46d632806972b4b1fdcd1a0c8b92ee5abc9a8a988d469737ac7b6"}, "c0329a00-e90f-4603-84c9-4c89636a1848": {"doc_hash": "6c2014d2d898cf9f324a349d78b30e0bed3d4d1d5b854d22589f1d6c82a8cbdf"}, "cabfa5ea-17e4-4623-9a5a-dca794aaef43": {"doc_hash": "22dcd309d5d501d2a8c1b84d1e548638fb8388de3f0f51176a345e5b01749367"}, "eeb45073-d83d-46eb-976f-2c83063aedd5": {"doc_hash": "f3c30346c11f34f2142055b4eac5165e50ed10e393cafa1d7759f811ce5f8a3f"}, "3388caeb-0857-41e6-a79f-b98db694a970": {"doc_hash": "a8fefbc869334fec8434a857a4aa93c50079747da3d9cca871411a965ce59c4f"}, "66169e07-5e32-436e-a6b8-aa1222217db8": {"doc_hash": "4f716d717ad6db132b5677a285087005beff6351ebf35124da04afa482811c03"}, "b29d9b6c-f381-4f76-985b-21019f3c8d2f": {"doc_hash": "f623d8ca1d2d1407918ad521eebc148b6c263504b3fae4235c81c3579ff6e860"}, "f226a730-e040-4155-b549-597868e2e230": {"doc_hash": "18668065ab647198a40bb158b14d90d4c3a50951df1d194083707a77b1972443"}, "2be39aa6-17c4-47fb-89b4-4a5ef8848189": {"doc_hash": "8253187fc775262707fa0ee2312792aae67092b8361f03ebfcd2fc510c2c4192"}, "1f22a396-8cc2-49eb-9668-5ea06a3dc014": {"doc_hash": "d93f58c46989dcb3412e338f5cfd2f4cb52f9634a03a25f8bb23a032171035f9"}, "cdda3407-53a0-495a-aaf7-ce576a106b8c": {"doc_hash": "9b70743abbded66665c81641ec1f1bc441fb3dfc5bb76d4c91a04d5d5b1a7fc3"}, "e52f0d76-156d-41bb-8441-b7634e00b072": {"doc_hash": "0f6393b932b7f142e71b94d29dd0e30f70579797675c2c567b6493dc59fe92d3"}, "5cfdde5a-2109-4317-bf65-aed75f379543": {"doc_hash": "7e207594e5f18dc2c57c775a1cbb6fdfc82d184826a8687380199871e8cb5d27"}, "84b17ff6-c036-4958-b269-f3c0ea6d9069": {"doc_hash": "d46225746a564a266b693eb53b9f07a8ce28e0421a879ee64edbff90edb87536"}, "a0cb80fc-1895-4b6e-b85d-0e1952dad5ff": {"doc_hash": "47f28e289d8870e6b7da837f3de726439314a5bf59cdc07ee355947cb912c14f"}, "3112b129-b348-447b-a39d-4f80d933e111": {"doc_hash": "1770b2659924dabd3433361bf01c534845fcb0379a0d784ea3fa5584e4545765"}, "6bcaede3-7b9b-421d-b5a8-a8e91a6bf345": {"doc_hash": "92e8c17b28a978fe2b0b6814c26dd5a22ff86b534729b046e96b39ece16946f9"}, "c4eaf983-c17b-40dd-88a6-b6459ce896d5": {"doc_hash": "cf2efa3587ad9176cf31c0d26031ceb2279f933cf5f3dfe5b72ed9943804c9ac"}, "81444144-fe76-46e7-a689-f5a194530428": {"doc_hash": "5d635926dfae1dfef5b01a866b89bae44d2f9c1a69fbed2e78048fe0dd67598d"}, "a87cd04d-2952-44d0-9dd1-6aed64886bf8": {"doc_hash": "192d4ade94d81aaab5510d702bb5a69676aa7ad7dbcbcd48220aa106f4b72cd3"}, "320d8ab1-fc93-41f5-a823-7ac83da0740c": {"doc_hash": "b30f80404ece01e9a964ccd351d8bb94ab8abbeedee1bcc513131315b30515b8"}, "50c3a265-0522-4066-beec-9f7f8254362b": {"doc_hash": "e9126e2fd3e5b5aaab04be015ddf5b0c547fd3778a03818d780665f8a5d411c6"}, "7227b1ac-6680-4309-9435-d3e8b50e6e09": {"doc_hash": "53825be63cb248297b5217edbfc171fcca81939d35bedb788b13f5ddf5193d80"}, "ccbfbbaa-39e7-40b0-aea3-4dad7b8195cd": {"doc_hash": "9709d268a26ad3d465d62640efd86dd38920e1a63ce97a50dab8a5d7ae0e1149"}, "53113cc1-1a1d-4eb2-8ec9-a70a8b6b60c8": {"doc_hash": "347b3e40b9a7536a7f038a819d692ab24178cdb79a0fca24267d3c6124381862"}, "058b01c2-178c-4b85-9daf-fdd7524fbcaa": {"doc_hash": "b9cb94707fd2d6d3771c57724b3d011bc221c2b17106c6c25bc08d2def781fc5"}, "4eaf5818-351b-43b1-b1b6-1715d2c72f2a": {"doc_hash": "8659ddf6e015d5cd3065b9e7fe96b010571b09de7c81bdf525708c0c46db9979"}, "1f3198cd-1fbb-4ef2-9c15-ff1aee802f95": {"doc_hash": "0f24492dc2065309c2844ecbecc0bd4d8c1ec04844400592d09789ed036a1a41"}, "073d156a-3d7d-41e2-8dfc-73264e2a45db": {"doc_hash": "0b9eeca9113af384dcd0085718bae9a644bac81abe62fd8b71b86cf204eb9a38"}, "71705436-a974-4326-98b8-1213641a307c": {"doc_hash": "6e07b6f06173167df03fd5e90d8e16a8ede1fd5a2e8c794a37bf737d02bed2ed"}, "5456cb88-ade6-42d5-8765-f083caad9a9e": {"doc_hash": "ef4d2a742c10c1c2a74630c6393b65b5f3c8662a6e3c2fb41288c97bfea357b4"}, "c78ccfcd-5053-4382-8a39-513af65bbd21": {"doc_hash": "c91ada2409c29132d889058252a98c309000b2767203760554bbebae7669a68a"}, "8d4a9a93-7aac-4b68-9463-014212852a77": {"doc_hash": "7c9def7c3a2a4abbf137ac1a0ce954bb0a9bc4f8b26aba5e884948561371716a"}, "e79cc13d-2702-4ba7-8dd1-b031e073fda8": {"doc_hash": "02621e6dfef21e1a4184fb3deb757c6205f668d956e983b601ae428400e8a079"}, "0b7563ba-0043-45d9-b15b-5be0c1bfa86c": {"doc_hash": "49dbf30270029e01aaac2aa1817c3d231ab044b547c63f341576625556faa479"}, "88c00642-f60b-438c-811c-46c9f533cd70": {"doc_hash": "4c580b96afa8d35a5f39c247ee080232435954488a3f4ef5c7ac78f8b9faace0"}, "d9621506-6dc6-4a04-861e-b46e1fe2f7c8": {"doc_hash": "0e5291be9ab45b67c615249f8ce7bceee8c1bc833e34b427781d68c491127a44"}, "4f3f4d84-c6db-427d-aef4-087e66a726f1": {"doc_hash": "429319697b1fcd65cfebdf5dd98ea85d8e8ee96dab2e4f4e6940d5793986b8ed"}, "bec38c44-ad6a-4061-840c-8ca4bbe10c03": {"doc_hash": "6e86d9087ddd333eda42b85e182d09f6f5a2df4467b8431f1acb4e8d7f6157d3"}, "74fe005c-7f2e-45e1-990a-15d8216e671f": {"doc_hash": "fd1d794a821d31bad9dcf91492b8b4505ad3e1542ec57c8ef6ee0249cf477a3a"}, "5181b5d7-1a27-4264-8ba9-1704de9a992d": {"doc_hash": "2065cf59e4e77508d37da23853d18a3e42f3e3b6043a89171660e19b72296e91"}, "3ff91c1b-2b43-4ff5-b51d-2f6eafd677ce": {"doc_hash": "fb23af3df55d9cb8c1030d040e49d36120723a2b8323aa29c26b8ddc022670f2"}, "2bc61a05-4028-424a-93f0-4c6b75fdd57b": {"doc_hash": "b32eb74b3bd95dfd789d17141bfa24b706ded2816c93be812e75188199387568"}, "a69052d8-9daf-4c5f-849c-a99ce8025601": {"doc_hash": "b3e9531b5cbac60b0361ca53ff160cbb08d06e7c0488fbb346e334ca5baa8542"}, "f00f2e14-6cb3-420f-961c-2df9e75cb6bc": {"doc_hash": "0c5d5a925b8d85f323be0b2744ee35e0057e0ec8f91b1e8fc2d3058610f2b8dc"}, "8243ba08-275d-4a4a-a884-51ac4b5c976d": {"doc_hash": "8a69862bf48f6aaba84f6904747e6c81279231a252ff88cbc74c9d6f8b42a6d6"}, "8d3047e2-a2ec-4e61-9317-6a311e064ceb": {"doc_hash": "082eaabf35048ae437dc2afe9f0f073f4b4fe452a1e8edc0bd556e2bee94761c"}, "690f3c58-ab5c-4201-af74-9c4fbf67803e": {"doc_hash": "5f2de04ed1a10556355c5a3b37fa94dd302c300941a453b5903bd0347e939c8e"}, "018d0dc1-b2a3-4435-8070-c5660bb58715": {"doc_hash": "a35e94f80b0832ce9806ce056ac3d1e620b3bedabb2188ac91a36eacab41774a"}, "9828697a-c93b-4564-8a63-391bccb19e6b": {"doc_hash": "3ae1680d06c86f20bedaadccfa5b427d20e809864639402c426984b6ebdae090"}, "b37c10b6-67b5-42b7-899b-44d1e2b5cca0": {"doc_hash": "e8a0feb74c66a28d9a85ef887405afd8729ad9e617b98edbdc773bf2c0c226a3"}, "48a6eb14-09a8-4c76-a824-fa0771c18750": {"doc_hash": "babbffb1de8c2b3e205650a7dd23a28a268657756d61bd6ef7011794dfed4877"}, "71b9257c-0170-4539-9f1f-e7097a703a83": {"doc_hash": "7b6968b52e19e2aeea432c3725490e889c992f19cfbf13242ce5706176600491"}, "54743af6-3252-472f-bb4a-dafd97c382dc": {"doc_hash": "b90ef179872471bbff6ad2096491763c586d308a807ce40b885a61a817e26dff"}, "5b0a7cf3-0fc2-43fd-bb9f-4d1c949783b2": {"doc_hash": "52b9c1fe8d7c9e8949e85f6fb03026edee8a42c7f2480b8820ea63eab7d7053a"}, "d882f4dd-9f35-45a5-9185-af523271fa08": {"doc_hash": "b9ca951fcea342541bfbfdcbda2d1dfd1d052abfcbf596175819226bfc302d08"}, "ff2feb26-984d-4077-9374-e836773b7a3c": {"doc_hash": "9e97683b14f720ec549a882fd6844b00d3866e4f3c2d1982f27936e87e434aea"}, "1f941c83-e9fa-4225-9f8d-a06cc6b82dce": {"doc_hash": "98471443c46b8ee8b251e363d6cd66250b72b28d81a645d276bc0400798eb21d"}, "c1c73548-4c98-4d36-b9ac-7b6ae716b7bb": {"doc_hash": "e4c244a6f4e8fc0807e513376fa735f1054054c5bcc9ede36478bfcc80a50f4b"}, "9ee64aa1-85cb-4170-85ae-2c7ea5ac75bd": {"doc_hash": "34ebb86e95d349bb7dd28b4b2438e1783379212ea9d991a38aa2e81f81f0ba32"}, "5eb06312-6bd7-4bce-aeff-5caf7e23669c": {"doc_hash": "f7731b7a8c4549d7527333006b15e17404e8ff9704606036d497004c66f5e473"}, "0dc73dbc-03dd-456f-b5e5-e1870d927417": {"doc_hash": "c5d1d5395f6a6385adad3a557bc1d7316a6e0fcd9cd2c3ed19cc1116c26a65f3"}, "bbb43f78-1408-4f83-9507-bd2db8d70495": {"doc_hash": "1ba83d9ad023f81fdfabcaa0dc1cba1fdff7edec901e1b3fe63187472dd4031d"}, "59ebc0dc-7177-40a9-908e-8fdd1486de7d": {"doc_hash": "23174a6d5fff36dcdd807c4897f3caa22ccbec074a3605a0f413a279eb079724"}, "291a51d1-e632-4c42-b3b2-b585dfcfc46f": {"doc_hash": "34b3553546bf2c7ccea0b8f09dffbedaaf0ff8682a01143e676a4d1aeaa6f6c2"}, "9cfa6308-42c9-4fd6-8a39-f54308fa805e": {"doc_hash": "c79f120fe606fae5b11df89262a6327ef72a0603db1efaf9791859d9efe6f32e"}, "ed38f2f6-5c10-402c-b262-4898afa7646c": {"doc_hash": "726c1fc62dba50d855fe98b406cc51b0a89438aa21129b79b2e23655c3cd9a7e"}, "bd4f891f-93ef-45e5-a51f-252737118a3f": {"doc_hash": "0635f595a7edb6aa77d14b6e899c9418407030971294d7d0ee7607f7486c7f0e"}, "2bac844c-4e04-4cf5-a6b2-651cc769ad41": {"doc_hash": "4dd986a62fa0727f1a5884f3cad29457e8cdacefa5cddd5d534d5c701f4cc043"}, "c247e2f5-25aa-40af-a916-0a14fe0c753f": {"doc_hash": "f12f0265c58e3d7368113007a71fc2bfa4d5673d74099cb67c9c696f1ea414bd"}, "4e4265c5-f394-4a0a-b5bd-e586df4fb49e": {"doc_hash": "88758c4828a88e0d7a2c40691c2232b45a4e6efe04f97b1d688c6dc256ac2b47"}, "1fc60c7e-a2d0-4db8-9078-945435990f04": {"doc_hash": "06ee0b0ffd3190bf090f98af7a234265fa27b77ec175b8cb407c52f90ede6cad"}, "eb1ffb7e-a5b8-4b32-9ab9-ce1d5a533a4b": {"doc_hash": "be4b6cd6ba2eb8b1000523f5bd71942f7b132856b98402fca3093c1f22582785"}, "a82f9a55-5b05-4236-9670-551baf777de0": {"doc_hash": "101c415bc828e6d35dd5637944b413bdfc86146be7d9dd479a5b67bc959280fb"}, "61386123-97f3-4adf-ac07-6239c5911327": {"doc_hash": "2431924a9d093d37eba73aca560e4eb3b26131b2db9592b60f392c9370ea2e7a"}, "42aec36b-53df-48cd-bfbe-0ac7ec3ad6bf": {"doc_hash": "8b96aa6f277d8d18a3c47936deb15a9efa183f22ea622028884e3ea4d6cbde09"}, "ad84d79b-79cd-4d0e-ac03-11e199ebc793": {"doc_hash": "0520d1ed5fd044c823534fb679afa8c6abdf1e832d775b797d203328848adea7"}, "b6e7d5a8-4972-44a0-b8d1-f8d842f58e2c": {"doc_hash": "1088cf901a043685bcffc29601ab7e48d8467d1f88b94c5500ab62c4816c4025"}, "6f7b4745-ecae-45ee-af2f-f89005795b53": {"doc_hash": "3a24f3b18b344870e492391477e0f60435c3ac854336f1661109abea94d6cf62"}, "9adde165-deb7-4ab5-a831-5bc53e249169": {"doc_hash": "8aa8d68ab206a84c84d7c758b849f879cab2a9568fa3445044fca964b4a9f135"}, "60ced58c-33e8-4cc9-8bbb-6943f9843f58": {"doc_hash": "9fbad1d83ee530b39d8573c1c57d142fdcbe775332a2907d8257ccc6edc8c2ee"}, "ce824a6f-b860-4546-b49f-c6a1e27109aa": {"doc_hash": "874248f1dec4bf9c5cb119737aa6daeab8fe3345c63bd253ed1cef2e42684b83"}, "c8ea66a0-94fc-40d6-b69d-62d27ed1e316": {"doc_hash": "e61a3910e56277a88942d6e510c8e62b06dac235e7ce72637e03219c923c62c2"}, "1b85c16f-ae28-480d-9c1c-17e41c24710e": {"doc_hash": "f24e86475d7a16d3e7132de0729b1efa7bf7ab6aa10c3acdb1b1f940abe0d555"}, "afd9d681-b877-45f6-acf5-1f7eb85f9d3d": {"doc_hash": "22a7baa5cb0c61b4efb860269534fa9d965ebfa6b6aed71d4a18b9e20de04655"}, "42fcc3eb-64af-4bfd-a9e5-b7fea6b8a1a6": {"doc_hash": "9eea9a7ad74f55bab72efb83ef386afda14b8280a382b5cd30374ab1d03a6a7d"}, "4107a2ef-27ba-4c9c-b321-d06259a7f1fd": {"doc_hash": "339eb2dca8dfeee445be2018eee08f59feb4ee091e1ddc25c3d69197aae94e9f"}, "ef8ab771-cab1-4488-99ef-ff6a203071c4": {"doc_hash": "43fa550ffbc4e0e1d865724f8b23a9c0fb0607f62dfa41c8830b8ec00589b477"}, "ad8d73b8-ac7b-424b-a452-e9316bdb9526": {"doc_hash": "dc08c6c6c5f5ab3fec6426385b532956f9baf7be7a0501162c4eae7774a615f2"}, "e74ef894-b85d-4c23-bbdf-f69cf80657c1": {"doc_hash": "088ee2403ec4ed32ba70e93374879470aa9376e9d1a815c2d06b2de189284c14"}, "486ff700-2fc7-479d-92d5-ebf1c40fa3de": {"doc_hash": "d43be03d686db5420502beba6e5e7762d1a034997cc4453b8264686a0f228f0f"}, "cae98049-27b5-4d02-a06a-60d31f8e8b3e": {"doc_hash": "e3839fea2c12b177968da181ce1a51bdc7a82a290fe46c8f109b03f27d7fc0b0"}, "fb3a9ef7-2d6e-49d9-8f30-579292ad7fe4": {"doc_hash": "a9a8a711f6a4093b00e7e79567fc8bb0b2ab94e00d554229bd61f83c224997e9"}, "b3f1ae8e-8bb5-4ade-94f0-09ce9cfb2a94": {"doc_hash": "2e0b36056542477049aaff586383528c47d397348137c83b75442a33f6b7dc60"}, "dd51fa6b-d105-4b97-aab0-1b593b7399e0": {"doc_hash": "0c0a0727b57b54dc7b45255a4333c7cf2ef9911466ea6316734e7a1f83e2d434"}, "ba171e4e-91c8-4790-a4de-f55b34980ff6": {"doc_hash": "129a0c5db640fdb3376c2307ca57d2e15e8964bd07f6d5f0e7a68f6da9a48fc4"}, "2a55fc4b-2a9b-4ba8-9ddc-e57984155527": {"doc_hash": "3ed34c1ec8fe2dc794b20762d830eb8bf1135d5df8357e87cc0ff5f2b8e43a6d"}, "f94941f6-02fe-4e7f-96a2-ca9fba1a8258": {"doc_hash": "d83ee28dd23f8f2ea92381064bdb688ada520e201112797a8a5063f89d32fbac"}, "33d11e6f-f405-4ec7-a187-5c636f06fd1c": {"doc_hash": "a6ac3d769741e618f97e836e152fe14d708022ef909e385d3132d89175152d73"}, "ba3ad208-d7e5-4540-aae0-f5683f114fcd": {"doc_hash": "8727652e2876633016200990500e7b147f4818eb6f3cef8b43d65c9b96ca978f"}, "85b733f7-a307-4ebb-859d-24db7a04601a": {"doc_hash": "38f98c7fe5ffaf6be0bd21c1c6e841fc00b2d555531910f38e972e86e93b49c8"}, "074bfe58-c21c-4f17-88c1-6349f60f03bf": {"doc_hash": "4135e2265be65d25dfe542b808a5fd916d941c808220fc1ae6db4479adf22fa9"}, "fc0d252e-78ee-41f4-825e-846d83f7aad1": {"doc_hash": "9f9b8b8132c13b18cb4fd8b737de6d141fc6f629d0620a38eb3c126becd0ae99"}, "58630ecd-040a-4d41-adc7-c96f5ed68f2e": {"doc_hash": "24079c542eceb696d1d81dd6ded7a99914f80dd258ea0b3b1a1ba2a9fa93273d"}, "080e157d-637b-404b-b3ba-47366fd7685e": {"doc_hash": "746a09ef1d172e6a67b9c73c48988173e2ae7b0d6a88f84ecb83b1246d115caf"}, "e78a2689-7a37-4bda-96ac-21156028839d": {"doc_hash": "57e866f2b73ee5f2dab2a6b936262fa03fe05b4f0280ffd6f4d48daa965aa532"}, "9c31644c-c95c-4af8-bba8-eab570ba39ac": {"doc_hash": "38bcb579c81307b9ce27ac5c19d69c39a861e842edcdea6b7041273dd2102de8"}, "1d6fe272-839a-4911-987c-591eb5115285": {"doc_hash": "8316b660cafbdbd13ec3f390a0d46158970ab7aac9a7dc21a96957b92e0faf7f"}, "33a0f625-afad-4be8-bfbc-ce8d58403e3f": {"doc_hash": "d6f558718f4c42537ae06e0381d94d97dd774118ebe76af0e3f17f3673124205"}, "3b3c19bf-26f1-4254-beb3-451061b2a996": {"doc_hash": "a6feeaba919d1438c459699acb03e055427f0d9ed0cb49467b6545bbf66d2842"}, "ae2ad7f5-2068-4364-ab54-0618e47818fe": {"doc_hash": "eb81b92d955256d643c6c89fed11c6f4dae440fa943a5c0817ecac1ebd719119"}, "63284a19-1d31-407d-b0ce-7b1b415a32c6": {"doc_hash": "b21ef1a3cd9643a8d4df275e485fbba7981f17585d35fb96b0c6eca2197ce032"}, "3978f398-50cd-491f-9cba-6b15b8a8f65b": {"doc_hash": "36fe684cf0dfb4537a2e9c5301afd6255fbb41352897c688c9eb34ab3c6bffc2"}, "7968ab6c-6f53-4414-b6ec-729069db691d": {"doc_hash": "5925316470c4820bc6d96d5f7106ed17e674bd1c55da5174b0e09747ca400418"}, "d63f215a-75f6-4e75-8dce-faef2aa5b85d": {"doc_hash": "21313da02719121afbbd853cb3b30da211f380d8d49bc38e671f611cf137dc38"}, "056924d6-62a6-4400-b933-4fe735623cb5": {"doc_hash": "e8ad3ab81d82a971dc5c3c2c8b2140c74aacddc88c48f45e437bf91d59435c7b"}, "ebaaee77-9fbb-4978-87f7-cf823c83c9b8": {"doc_hash": "3372cd14d40ebf9fd5f300c312f4c3062312ca7cb274dbd62c636c151947d82f"}, "60102272-c71e-4199-aea1-0e27c36ab3f3": {"doc_hash": "c847f8f2f112bd1af7fbb5afc0cce63b5485f37da5e4084d459faabd467ac79f"}, "d01a482b-133d-4e6c-bbf6-71a18168969c": {"doc_hash": "03267c4280436bf72f915d4621b2bfdfeabb66420153d77203287a0925443196"}, "cfff0532-1c8e-4178-8963-035fd8cb465f": {"doc_hash": "c22e9bbbf60da2a65f80f9e3de7369669785e66c4b8ddd6728fd15c6fbb52f9c"}, "acb6f6d4-af6e-4e55-a556-cc2717d06dde": {"doc_hash": "a4f9894b9b06d8ae113c3455c0a16a2531a1be69ab158d77ad6821d8dacbe6af"}, "7f4e2d94-bad2-44e8-a5ab-a126cc399ce9": {"doc_hash": "7bca2c060666f41bf1519e375a2e1a21905ec4ad9cf30ee2c71cc2da2def00a6"}, "1b22b670-8831-430b-bbe8-49f9a8a106d1": {"doc_hash": "d43baaa6dbef0e5861cf62100635ee0466f2c49a6984b32c570d73c14f9749bb"}, "351805d9-2e03-4a58-9755-da2ad73f11f1": {"doc_hash": "5ed7d30dac224f1a2ae4af6e9f84ef7e2b876e7ef418963613507b63cf5bf28a"}, "e3be30e8-b347-4c37-bc51-2a377a769ee4": {"doc_hash": "618bd2ed9269dbd97e9920d0ffa9cd2c775e25d5d649a694a192e7ffe5ad0d49"}, "f774a08c-4ae5-47fe-9c1a-727fabcfc736": {"doc_hash": "3f8de638611eff261443e34baeeabafe812fe299ea1e68362ad9fb122194fab3"}, "53133d62-848d-4b88-9429-c37d77543eea": {"doc_hash": "c4cf18befc529d7b57d98434b7f6a9d3f83663ed8454a0f600226cf1be7b8497"}, "0a96f709-b94e-41d7-a687-c9ea5482513c": {"doc_hash": "61dd21206d050782367c8abb7f1350c314f50ddf182be3c55091a8b3930f27ee"}, "492f8551-600e-4b52-a503-23313c4aebdd": {"doc_hash": "ef6ef39b7a0606f81a3de78d062805636986c30fbffb6826c16eee300547a2b6"}, "1a198fcf-7dd6-4ee1-b771-3898d8b9021a": {"doc_hash": "3bce43d689f7058d17e03a317748c51787c3f86e7b0bfa9131c381bfd3e925b8"}, "1e49beb2-333b-4906-835b-446f18db5808": {"doc_hash": "3f7eb8d15d769cd29ec713c96d0267cfc28e3b2d540f8b786a3b8b16e428eee4"}, "bad2f220-7e63-4c79-971d-d93654aaa53c": {"doc_hash": "cbc9a704d1e0611910ab8dc40668921082b95d2e82990130365357e8eaa45a76"}, "1ec05fb4-76e3-423e-ac96-a7d17cb6a723": {"doc_hash": "86f3e5428c54d15c2518559095be901c2a138dfd6c3c798a5b6e24bd9be47220"}, "3030d109-7725-4fbf-9737-c34cd67a2b34": {"doc_hash": "75e11898657fd839856e26894f9bdf8224bd9f991e02883d0c582e082f895bbc"}, "1733a2d8-ef91-49df-acdf-4839e87167f4": {"doc_hash": "4c1c9457144a636deea3f8b97aaedb39df4fe64fe5dad713100817afa9f3fa68"}, "eb3c3009-e404-4f59-8621-91b642414133": {"doc_hash": "e9fdc61d75b06e4a035668895015e296943675a08fe72e3a6d0d644a900f375f"}, "282b8c5c-4319-492e-9fea-05e9474bd324": {"doc_hash": "a8b6b47e5c2e82775159d529aa82f3df85afc458660bbbbbd1c119b206c20996"}, "b9f27d7a-3d04-411e-ae44-0ac9df35f190": {"doc_hash": "1bf5ec2d677da8dc9317d6f224d50b0ca2e4f5edbc9c7a296aad19c3536554b4"}, "f754821e-66ef-457d-90c6-88e38e6360b2": {"doc_hash": "f7c9f77500ff70402ec956292a409b11d821cbc7a9b83901c0fe46501f359974"}, "40b62148-e2d7-4ce5-bd23-f2968b93fd3a": {"doc_hash": "8bcd2b8395a4edd965b7948c7650807afe774363a170b2041cc40b3eee5f83f0"}, "f046e795-dab7-407d-9746-278978aee8f0": {"doc_hash": "0d219011d16ebf9d81930c0a476d9be0f7f8e8acdd40544127f94ed6e7ffb23f"}, "69ca6a43-8a48-4b4e-9f7e-78e09c660a74": {"doc_hash": "98d77f4e0cad85bba7a53df45518b2ec5fb8bf7534ce50e7f6b8699ca0a8e7d7"}, "d2d5a33b-ce41-4e72-876d-a0c7f6a53142": {"doc_hash": "8954cdb2d832191307553e9e1e0b3939452ea103031f1f11b76e7356bff6fdfd"}, "f852869a-b39f-4b0f-93cf-55fe035bdfc4": {"doc_hash": "45697f764496f3753a3b15971f365b502471ef0c8557692d9146a59341296fef"}, "28e149df-fb73-45da-ba6a-72b7b99c5a1d": {"doc_hash": "f809854eb4f9118e18c1599ede9934940523d366afc6dd6458134fe66fbdac54"}, "5714f914-a773-4296-aa73-ef4081008cb3": {"doc_hash": "2c5cd17ab88ace533b56cc6950bba982de9bfc3f5bee25492d14d26ac249a5fb"}, "409d599e-1f2f-4e4d-b2a7-a47f2368d0b3": {"doc_hash": "cce7b6bbbab16084102ba5970573882c44920bcdd1ada3ec7ad20adc621e62b2"}, "747f701b-a2f9-416c-85e5-c84bfc22a6f6": {"doc_hash": "d21d25e83010d919712d6d37ce07fa0b17d50f255b01ffd2cb0fd625a761c991"}, "8a000c91-cc57-4d04-9553-361d15e61505": {"doc_hash": "8c0fe1d63bb9f56f27af632faf3462fca26598ad7b5845d32842bd3931e8a743"}, "4e5cbca3-ad32-4ddf-a881-efb68765f161": {"doc_hash": "5051bfdfc199b7a0da79ff6d3fcd970bb2a85500a24ec85cc53309494b5fa9ce"}, "2f1f09a5-6fda-48a1-b668-4e342160f2c1": {"doc_hash": "c2f8272b9e0641a2f487b9d04f6930128b7523dd3f5a404966e5fd98463dfb53"}, "6a60bc5c-2abb-4bce-8371-9fc55cf252be": {"doc_hash": "064c3d887460b2832601c002c55acbed9e49b9816f1a78f4014097efa654a0c1"}, "3ba03a1c-3fb6-404c-b2a1-5a88a1c5d8cf": {"doc_hash": "94d01cd4441b79fa618b6e675a7bd59fe4b43fac69d99cd68317115928384ce2"}, "2494c10d-01aa-4916-a318-7ff596aa1c12": {"doc_hash": "9d77189d02a27c450269f6336e7f1b7fbe59ce6bbd00d93889c248fc9445d96f"}, "7b5ca763-96ef-49bc-bf85-f920654dcdcf": {"doc_hash": "d578beee15f2fbe2e5ba147dc3ea0cc9728e8bdc332f18b520f326ece8f60429"}, "b8e1839e-61fc-4cc6-8966-671282874ca9": {"doc_hash": "7d7d2d3809abdffd332a7ed30d14f96ff700a3e0697c9c99a8715e088075d4d1"}, "9722d968-fbd0-44b9-a02c-655a19a028a0": {"doc_hash": "ef5f994ac32a7f7005ae8a13cf971023af80a3868dc6f6a2345d973ac06675f3"}, "667906e4-5af2-4607-afde-796423524863": {"doc_hash": "cd81b0cbbe41262e2399b9980491041591a2c689dab765114e5c88d281e366a1"}, "0f927881-92f7-43e8-a227-765004823485": {"doc_hash": "4165872fb1ab6545d976d7410bdedc7459b6bc2ee231e36901b329eefd6c576d"}, "95e21963-a06a-49ef-9ffb-683fc5e93a0c": {"doc_hash": "82ffe52a584e5b44209daadfeeaff57e11c863102b6c7c0477cc1403ec7f8fbb"}}, "docstore/data": {"d5cc783b-84d4-4dd7-907a-0108da852566": {"__data__": {"text": "EDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n\nThe Art of Multiprocessor Programming\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nThe Art of Multiprocessor\nProgramming\nRevised First Edition\nMaurice Herlihy\nNir Shavit\nAMSTERDAM \u000fBOSTON \u000fHEIDELBERG \u000fLONDON\nNEW YORK \u000fOXFORD \u000fPARIS \u000fSAN DIEGO\nSAN FRANCISCO \u000fSINGAPORE \u000fSYDNEY \u000fTOKYO\nMorgan Kaufmann Publishers is an imprint of Elsevier\n\nAcquiring Editor T odd Green\nDevelopment Editor Robyn Day\nProject Manager Paul Gottehrer\nDesigner Joanne Blank\nMorgan Kaufmann is an imprint of Elsevier\n225 Wyman Street, Waltham, MA 02451, USA\n\u00a9 2012 Elsevier, Inc. All rights reserved.\nNo part of this publication may be reproduced or transmitted in any form or by any means, electronic or\nmechanical, including photocopying, recording, or any information storage and retrieval system, without\npermission in writing from the publisher. Details on how to seek permission, further information about the\nPublisher\u2019s permissions policies and our arrangements with organizations such as the Copyright Clearance\nCenter and the Copyright Licensing Agency, can be found at our website: www.elsevier.com/permissions.\nThis book and the individual contributions contained in it are protected under copyright by the Publisher (other\nthan as may be noted herein).\nNotices\nKnowledge and best practice in this \ufb01eld are constantly changing. As new research and experience broaden our\nunderstanding, changes in research methods or professional practices, may become necessary. Practitioners and\nresearchers must always rely on their own experience and knowledge in evaluating and using any information or\nmethods described herein. In using such information or methods they should be mindful of their own safety and\nthe safety of others, including parties for whom they have a professional responsibility.\nT o the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any liability\nfor any injury and/or damage to persons or property as a matter of products liability, negligence or otherwise, or\nfrom any use or operation of any methods, products, instructions, or ideas contained in the material herein.\nLibrary of Congress Cataloging-in-Publication Data\nApplication submitted.\nBritish Library Cataloguing-in-Publication Data\nA catalogue record for this book is available from the British Library.\nISBN: 978-0-12-397337-5\nFor information on all MK publications\nvisit our website at http://store.elsevier.com\nPrinted in the United States of America\n12 13 14 15 16 10 9 8 7 6 5 4 3 2 1\n\nFor my parents, David and Patricia Herlihy, and for Liuba, David, and Anna.\nFor my parents, Noun and Aliza, my beautiful wife Sha\ufb01, and my kids,\nYonadav and Lior, for their love and their patience, their incredible,\nunbelievable, and unwavering patience, throughout the writing of this book.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nContents\nAcknowledgments .............................................................. xvii\nPreface ................................................................................. xxi\nSuggested Ways to T each the Art of Multiprocessor\nProgramming ....................................................................... xxiii\n1 Introduction ...................................................................... 1\n1.1 Shared Objects and Synchronization ......................... 3\n1.2 A Fable ......................................................................... 6\n1.2.1 Properties of Mutual Exclusion .......................... 8\n1.2.2 The Moral ........................................................... 9\n1.3 The Producer\u2013Consumer Problem ........................... 10\n1.4 The Readers\u2013Writers Problem ................................. 12\n1.5 The Harsh Realities of Parallelization ........................ 13\n1.6 Parallel Programming .................................................. 15\n1.7 Chapter Notes", "doc_id": "d5cc783b-84d4-4dd7-907a-0108da852566", "embedding": null, "doc_hash": "26b568c74f4734608d729747a03879f6a9425d7dc7bf085f8dcb6361f69e17fb", "extra_info": null, "node_info": {"start": 0, "end": 4072}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "3": "2424edb5-ec5d-48f6-8c3c-926100170f55"}}, "__type__": "1"}, "2424edb5-ec5d-48f6-8c3c-926100170f55": {"__data__": {"text": "19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nContents\nAcknowledgments .............................................................. xvii\nPreface ................................................................................. xxi\nSuggested Ways to T each the Art of Multiprocessor\nProgramming ....................................................................... xxiii\n1 Introduction ...................................................................... 1\n1.1 Shared Objects and Synchronization ......................... 3\n1.2 A Fable ......................................................................... 6\n1.2.1 Properties of Mutual Exclusion .......................... 8\n1.2.2 The Moral ........................................................... 9\n1.3 The Producer\u2013Consumer Problem ........................... 10\n1.4 The Readers\u2013Writers Problem ................................. 12\n1.5 The Harsh Realities of Parallelization ........................ 13\n1.6 Parallel Programming .................................................. 15\n1.7 Chapter Notes ............................................................ 15\n1.8 Exercises ...................................................................... 16\nIPRINCIPLES ............................................................................... 19\n2 Mutual Exclusion ............................................................ 21\n2.1 Time ........................................................................... 21\nvii\nviii Contents\n2.2 Critical Sections ........................................................ 22\n2.3 2-Thread Solutions .................................................... 24\n2.3.1 The LockOne Class .......................................... 25\n2.3.2 The LockTwo Class .......................................... 26\n2.3.3 The Peterson Lock ........................................... 27\n2.4 The Filter Lock .......................................................... 28\n2.5 Fairness ...................................................................... 31\n2.6 Lamport\u2019s Bakery Algorithm .................................... 31\n2.7 Bounded Timestamps ............................................... 33\n2.8 Lower Bounds on the Number of Locations .......... 37\n2.9 Chapter Notes .......................................................... 40\n2.10 Exercises .................................................................... 41\n3 Concurrent Objects ..................................................... 45\n3.1 Concurrency and Correctness ................................ 45\n3.2 Sequential Objects .................................................... 48\n3.3 Quiescent Consistency ............................................. 49\n3.3.1 Remarks ........................................................... 51\n3.4 Sequential Consistency ............................................. 51\n3.4.1 Remarks ........................................................... 52\n3.5 Linearizability ............................................................. 54\n3.5.1 Linearization Points .......................................... 55\n3.5.2 Remarks ........................................................... 55\n3.6 Formal De\ufb01nitions .................................................... 55\n3.6.1 Linearizability .................................................... 57\n3.6.2 Compositional Linearizability ........................... 57\n3.6.3 The Nonblocking Property .............................. 58\n3.7 Progress Conditions ................................................. 59\n3.7.1 Dependent Progress Conditions ..................... 60\n3.8 The Java Memory Model .......................................... 61\n3.8.1 Locks and Synchronized Blocks ....................... 62\n3.8.2 Volatile Fields .................................................... 63\n3.8.3 Final Fields ........................................................ 63\nContents ix\n3.9 Remarks ..................................................................... 64\n3.10 Chapter Notes .......................................................... 65\n3.11 Exercises .................................................................... 66\n4 Foundations of Shared Memory ........................... 71\n4.1 The Space of Registers ............................................. 72\n4.2 Register Constructions ............................................. 77\n4.2.1 MRSW Safe Registers ...................................... 78\n4.2.2 A Regular Boolean MRSW Register ................ 78\n4.2.3 A Regular M-Valued MRSW Register ............. 79\n4.2.4 An Atomic SRSW Register .............................. 81\n4.2.5 An Atomic MRSW Register ............................ 82\n4.2.6 An Atomic MRMW Register ........................... 85\n4.3 Atomic Snapshots ..................................................... 87\n4.3.1 An Obstruction-Free Snapshot ....................... 87\n4.3.2 A Wait-Free Snapshot ..................................... 88\n4.3.3 Correctness Arguments .................................. 90\n4.4 Chapter Notes .......................................................... 93\n4.5 Exercises .................................................................... 94\n5 The Relative Power of Primitive\nSynchronization Operations ................................... 99\n5.1 Consensus Numbers ................................................ 100\n5.1.1 States and Valence ........................................... 101\n5.2 Atomic Registers ....................................................... 103\n5.3 Consensus Protocols ................................................ 106\n5.4 FIFO Queues ............................................................. 106\n5.5 Multiple Assignment Objects ................................... 110\n5.6 Read\u2013Modify\u2013Write Operations", "doc_id": "2424edb5-ec5d-48f6-8c3c-926100170f55", "embedding": null, "doc_hash": "d194969d7b2ec875d5f3c0f2505cae29c6b8a4cc498dd7f2912e86ce1eb7d2d7", "extra_info": null, "node_info": {"start": 3033, "end": 8853}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d5cc783b-84d4-4dd7-907a-0108da852566", "3": "300a9425-3af2-4d2f-9af2-6066ea8bc430"}}, "__type__": "1"}, "300a9425-3af2-4d2f-9af2-6066ea8bc430": {"__data__": {"text": "..................................................... 87\n4.3.1 An Obstruction-Free Snapshot ....................... 87\n4.3.2 A Wait-Free Snapshot ..................................... 88\n4.3.3 Correctness Arguments .................................. 90\n4.4 Chapter Notes .......................................................... 93\n4.5 Exercises .................................................................... 94\n5 The Relative Power of Primitive\nSynchronization Operations ................................... 99\n5.1 Consensus Numbers ................................................ 100\n5.1.1 States and Valence ........................................... 101\n5.2 Atomic Registers ....................................................... 103\n5.3 Consensus Protocols ................................................ 106\n5.4 FIFO Queues ............................................................. 106\n5.5 Multiple Assignment Objects ................................... 110\n5.6 Read\u2013Modify\u2013Write Operations ............................. 112\n5.7 Common2 RMW Operations .................................. 114\n5.8 The compareAndSet() Operation .......................... 116\nx Contents\n5.9 Chapter Notes .......................................................... 117\n5.10 Exercises .................................................................... 118\n6 Universality of Consensus ........................................ 125\n6.1 Introduction ............................................................... 125\n6.2 Universality ................................................................ 126\n6.3 A Lock-Free Universal Construction ...................... 126\n6.4 A Wait-Free Universal Construction ...................... 130\n6.5 Chapter Notes .......................................................... 136\n6.6 Exercises .................................................................... 137\nII PRACTICE ................................................................................ 139\n7 Spin Locks and Contention ...................................... 141\n7.1 Welcome to the Real World .................................... 141\n7.2 T est-And-Set Locks ................................................... 144\n7.3 TAS-Based Spin Locks Revisited .............................. 146\n7.4 Exponential Backoff ................................................... 147\n7.5 Queue Locks ............................................................. 149\n7.5.1 Array-Based Locks ........................................... 150\n7.5.2 The CLH Queue Lock ..................................... 151\n7.5.3 The MCS Queue Lock ..................................... 154\n7.6 A Queue Lock with Timeouts ................................. 157\n7.7 A Composite Lock .................................................... 159\n7.7.1 A Fast-Path Composite Lock ........................... 165\n7.8 Hierarchical Locks ..................................................... 167\n7.8.1 A Hierarchical Backoff Lock ............................. 167\n7.8.2 A Hierarchical CLH Queue Lock .................... 168\n7.9 One Lock T o Rule Them All .................................... 173\n7.10 Chapter Notes .......................................................... 173\n7.11 Exercises .................................................................... 174\nContents xi\n8 Monitors and Blocking Synchronization .......... 177\n8.1 Introduction ............................................................... 177\n8.2 Monitor Locks and Conditions ................................ 178\n8.2.1 Conditions ........................................................ 179\n8.2.2 The Lost-Wakeup Problem ............................. 181\n8.3 Readers\u2013Writers Locks ............................................ 183\n8.3.1 Simple Readers\u2013Writers Lock ......................... 184\n8.3.2 Fair Readers\u2013Writers Lock .............................. 185\n8.4 Our Own Reentrant Lock ........................................ 187\n8.5 Semaphores ............................................................... 189\n8.6 Chapter Notes .......................................................... 189\n8.7 Exercises .................................................................... 190\n9 Linked Lists: The Role of Locking ........................ 195\n9.1 Introduction ............................................................... 195\n9.2 List-Based Sets ........................................................... 196\n9.3 Concurrent Reasoning .............................................. 198\n9.4 Coarse-Grained Synchronization ............................. 200\n9.5 Fine-Grained Synchronization .................................. 201\n9.6 Optimistic Synchronization ...................................... 205\n9.7 Lazy Synchronization ................................................ 208\n9.8 Non-Blocking Synchronization ................................. 213\n9.9 Discussion .................................................................. 218\n9.10 Chapter Notes .......................................................... 219\n9.11 Exercises .................................................................... 219\n10 Concurrent Queues and the ABA Problem 223\n10.1 Introduction ............................................................. 223\n10.2 Queues .................................................................... 224\n10.3 A Bounded Partial Queue ...................................... 225\n10.4 An Unbounded T otal Queue .................................. 229\n10.5 An Unbounded Lock-Free Queue ......................... 230\nxii Contents\n10.6 Memory Reclamation and the ABA Problem ....... 233\n10.6.1 A Na \u00a8\u0131ve Synchronous Queue ...................... 237\n10.7 Dual Data Structures", "doc_id": "300a9425-3af2-4d2f-9af2-6066ea8bc430", "embedding": null, "doc_hash": "efb7af2f14803818013e66cb6cdc6e56d9641fb64a148e1e2490c617d6dba4ae", "extra_info": null, "node_info": {"start": 8941, "end": 14662}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2424edb5-ec5d-48f6-8c3c-926100170f55", "3": "6db351c7-8d58-4b52-806d-e2551ce4a229"}}, "__type__": "1"}, "6db351c7-8d58-4b52-806d-e2551ce4a229": {"__data__": {"text": "Synchronization ................................. 213\n9.9 Discussion .................................................................. 218\n9.10 Chapter Notes .......................................................... 219\n9.11 Exercises .................................................................... 219\n10 Concurrent Queues and the ABA Problem 223\n10.1 Introduction ............................................................. 223\n10.2 Queues .................................................................... 224\n10.3 A Bounded Partial Queue ...................................... 225\n10.4 An Unbounded T otal Queue .................................. 229\n10.5 An Unbounded Lock-Free Queue ......................... 230\nxii Contents\n10.6 Memory Reclamation and the ABA Problem ....... 233\n10.6.1 A Na \u00a8\u0131ve Synchronous Queue ...................... 237\n10.7 Dual Data Structures .............................................. 238\n10.8 Chapter Notes ........................................................ 241\n10.9 Exercises .................................................................. 241\n11 Concurrent Stacks and Elimination ................. 245\n11.1 Introduction ............................................................. 245\n11.2 An Unbounded Lock-Free Stack ............................ 245\n11.3 Elimination ............................................................... 248\n11.4 The Elimination Backoff Stack ................................ 249\n11.4.1 A Lock-Free Exchanger ................................ 249\n11.4.2 The Elimination Array .................................. 251\n11.5 Chapter Notes ........................................................ 254\n11.6 Exercises .................................................................. 255\n12 Counting, Sorting, and Distributed\nCoordination ................................................................. 259\n12.1 Introduction ............................................................. 259\n12.2 Shared Counting ..................................................... 259\n12.3 Software Combining ............................................... 260\n12.3.1 Overview ..................................................... 261\n12.3.2 An Extended Example ................................. 267\n12.3.3 Performance and Robustness ...................... 269\n12.4 Quiescently Consistent Pools and Counters ........ 269\n12.5 Counting Networks ................................................ 270\n12.5.1 Networks That Count ................................. 270\n12.5.2 The Bitonic Counting Network ................... 273\n12.5.3 Performance and Pipelining .......................... 280\n12.6 Diffracting Trees ...................................................... 282\n12.7 Parallel Sorting ........................................................ 286\n12.8 Sorting Networks ................................................... 286\n12.8.1 Designing a Sorting Network ...................... 287\n12.9 Sample Sorting ......................................................... 290\n12.10 Distributed Coordination ....................................... 291\nContents xiii\n12.11 Chapter Notes ........................................................ 292\n12.12 Exercises .................................................................. 293\n13 Concurrent Hashing and Natural\nParallelism ...................................................................... 299\n13.1 Introduction ............................................................. 299\n13.2 Closed-Address Hash Sets ..................................... 300\n13.2.1 A Coarse-Grained Hash Set ........................ 302\n13.2.2 A Striped Hash Set ...................................... 303\n13.2.3 A Re\ufb01nable Hash Set ................................... 305\n13.3 A Lock-Free Hash Set ............................................. 309\n13.3.1 Recursive Split-Ordering .............................. 309\n13.3.2 The BucketList Class ................................ 312\n13.3.3 The LockFreeHashSet<T> Class ............... 313\n13.4 An Open-Addressed Hash Set ............................... 316\n13.4.1 Cuckoo Hashing ........................................... 316\n13.4.2 Concurrent Cuckoo Hashing ....................... 318\n13.4.3 Striped Concurrent Cuckoo Hashing .......... 322\n13.4.4 A Re\ufb01nable Concurrent Cuckoo Hash Set . 324\n13.5 Chapter Notes ........................................................ 325\n13.6 Exercises .................................................................. 326\n14 Skiplists and Balanced Search ............................. 329\n14.1 Introduction ............................................................. 329\n14.2 Sequential Skiplists .................................................. 329\n14.3 A Lock-Based Concurrent Skiplist ........................ 331\n14.3.1 A Bird\u2019s-Eye View ......................................... 331\n14.3.2 The Algorithm .............................................. 333\n14.4 A Lock-Free Concurrent Skiplist ........................... 339\n14.4.1 A Bird\u2019s-Eye View ......................................... 339\n14.4.2 The Algorithm in Detail ............................... 341\n14.5 Concurrent Skiplists ............................................... 348\n14.6 Chapter Notes ........................................................ 348\n14.7 Exercises .................................................................. 349\nxiv Contents\n15 Priority Queues ........................................................... 351\n15.1 Introduction .............................................................", "doc_id": "6db351c7-8d58-4b52-806d-e2551ce4a229", "embedding": null, "doc_hash": "f1b4d62986c2425edbe4a7d10e1ba43913d71efde38b7d4bfc1215878be62135", "extra_info": null, "node_info": {"start": 14793, "end": 20373}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "300a9425-3af2-4d2f-9af2-6066ea8bc430", "3": "104acf7e-8234-42c2-bb73-31dcf8af08e6"}}, "__type__": "1"}, "104acf7e-8234-42c2-bb73-31dcf8af08e6": {"__data__": {"text": "A Lock-Based Concurrent Skiplist ........................ 331\n14.3.1 A Bird\u2019s-Eye View ......................................... 331\n14.3.2 The Algorithm .............................................. 333\n14.4 A Lock-Free Concurrent Skiplist ........................... 339\n14.4.1 A Bird\u2019s-Eye View ......................................... 339\n14.4.2 The Algorithm in Detail ............................... 341\n14.5 Concurrent Skiplists ............................................... 348\n14.6 Chapter Notes ........................................................ 348\n14.7 Exercises .................................................................. 349\nxiv Contents\n15 Priority Queues ........................................................... 351\n15.1 Introduction ............................................................. 351\n15.1.1 Concurrent Priority Queues ....................... 351\n15.2 An Array-Based Bounded Priority Queue ............ 352\n15.3 A Tree-Based Bounded Priority Queue ................ 353\n15.4 An Unbounded Heap-Based Priority Queue ........ 355\n15.4.1 A Sequential Heap ....................................... 356\n15.4.2 A Concurrent Heap ..................................... 357\n15.5 A Skiplist-Based Unbounded Priority Queue ....... 363\n15.6 Chapter Notes ........................................................ 366\n15.7 Exercises .................................................................. 366\n16 Futures, Scheduling, and Work\nDistribution .................................................................... 369\n16.1 Introduction ............................................................. 369\n16.2 Analyzing Parallelism ............................................... 375\n16.3 Realistic Multiprocessor Scheduling ...................... 378\n16.4 Work Distribution .................................................. 380\n16.4.1 Work Stealing ............................................... 380\n16.4.2 Yielding and Multiprogramming ................... 381\n16.5 Work-Stealing Dequeues ........................................ 382\n16.5.1 A Bounded Work-Stealing Dequeue .......... 382\n16.5.2 An Unbounded Work-Stealing DEQueue .. 386\n16.5.3 Work Balancing ............................................ 389\n16.6 Chapter Notes ........................................................ 391\n16.7 Exercises .................................................................. 392\n17 Barriers ............................................................................. 397\n17.1 Introduction ............................................................. 397\n17.2 Barrier Implementations ......................................... 398\n17.3 Sense-Reversing Barrier ......................................... 399\n17.4 Combining Tree Barrier .......................................... 400\n17.5 Static Tree Barrier ................................................... 402\nContents xv\n17.6 T ermination Detecting Barriers ............................. 404\n17.7 Chapter Notes ........................................................ 408\n17.8 Exercises .................................................................. 409\n18 Transactional Memory ............................................. 417\n18.1 Introduction ............................................................. 417\n18.1.1 What is Wrong with Locking? ..................... 417\n18.1.2 What is Wrong with compareAndSet() ? .. 418\n18.1.3 What is Wrong with Compositionality? ...... 420\n18.1.4 What can We Do about It? ......................... 421\n18.2 Transactions and Atomicity .................................... 421\n18.3 Software Transactional Memory ............................ 424\n18.3.1 Transactions and Transactional Threads ...... 427\n18.3.2 Zombies and Consistency ........................... 428\n18.3.3 Atomic Objects ............................................ 429\n18.3.4 Dependent or Independent Progress? ........ 431\n18.3.5 Contention Managers .................................. 431\n18.3.6 Implementing Atomic Objects ..................... 433\n18.3.7 An Obstruction-Free Atomic Object .......... 434\n18.3.8 A Lock-Based Atomic Object ...................... 438\n18.4 Hardware Transactional Memory .......................... 445\n18.4.1 Cache Coherence ........................................ 446\n18.4.2 Transactional Cache Coherence .................. 447\n18.4.3 Enhancements .............................................. 447\n18.5 Chapter Notes ........................................................ 448\n18.6 Exercises .................................................................. 449\nIII APPENDIX ........................................................................... 451\nA Software Basics .............................................................. 453\nA.1 Introduction .............................................................. 453\nA.2 Java ............................................................................. 453\nA.2.1 Threads ........................................................... 453\nA.2.2 Monitors .......................................................... 455\nA.2.3 Yielding and Sleeping ...................................... 458\nA.2.4 Thread-Local Objects ..................................... 458\nxvi Contents\nA.3 C# .............................................................................. 460\nA.3.1 Threads", "doc_id": "104acf7e-8234-42c2-bb73-31dcf8af08e6", "embedding": null, "doc_hash": "2808ad5a02eeb90f26ccd1e4f3026cd4c8e30c27ce6df176e075eb41b403d094", "extra_info": null, "node_info": {"start": 20415, "end": 25775}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6db351c7-8d58-4b52-806d-e2551ce4a229", "3": "3062657e-8296-4104-9b5b-18480b32334e"}}, "__type__": "1"}, "3062657e-8296-4104-9b5b-18480b32334e": {"__data__": {"text": "Cache Coherence .................. 447\n18.4.3 Enhancements .............................................. 447\n18.5 Chapter Notes ........................................................ 448\n18.6 Exercises .................................................................. 449\nIII APPENDIX ........................................................................... 451\nA Software Basics .............................................................. 453\nA.1 Introduction .............................................................. 453\nA.2 Java ............................................................................. 453\nA.2.1 Threads ........................................................... 453\nA.2.2 Monitors .......................................................... 455\nA.2.3 Yielding and Sleeping ...................................... 458\nA.2.4 Thread-Local Objects ..................................... 458\nxvi Contents\nA.3 C# .............................................................................. 460\nA.3.1 Threads ........................................................... 460\nA.3.2 Monitors .......................................................... 461\nA.3.3 Thread-Local Objects ..................................... 462\nA.4 Pthreads .................................................................... 464\nA.4.1 Thread-Local Storage ..................................... 465\nA.5 Chapter Notes ......................................................... 466\nB Hardware Basics ........................................................... 469\nB.1 Introduction (and a Puzzle) ..................................... 469\nB.2 Processors and Threads ........................................... 472\nB.3 Interconnect .............................................................. 472\nB.4 Memory ..................................................................... 473\nB.5 Caches ....................................................................... 473\nB.5.1 Coherence ...................................................... 474\nB.5.2 Spinning ........................................................... 476\nB.6 Cache-Conscious Programming, or the Puzzle\nSolved ........................................................................ 476\nB.7 Multi-Core and Multi-Threaded Architectures ...... 477\nB.7.1 Relaxed Memory Consistency ........................ 478\nB.8 Hardware Synchronization Instructions ................. 479\nB.9 Chapter Notes ......................................................... 481\nB.10 Exercises .................................................................... 481\nBibliography ......................................................................... 483\nIndex ......................................................................................... 495\nAcknowledgments\nWe would like to thank Doug Lea, Michael Scott, Ron Rivest, T om Corman, Radia\nPerlman, George Varghese and Michael Sipser for their help in \ufb01nding the right\npublication venue for our book.\nWe thank all the students, colleagues, and friends who read our draft chapters\nand sent us endless lists of comments and ideas: Y ehuda Afek, Shai Ber, Mar-\ntin Buchholz, Vladimir Budovsky, Christian Cachin, Cliff Click, Y oav Cohen,\nDave Dice, Alexandra Fedorova, Pascal Felber, Christof Fetzer, Sha\ufb01 Goldwasser,\nRachid Guerraoui, Tim Harris, Danny Hendler, Maor Hizkiev, Eric Koskinen,\nChristos Kozyrakis, Edya Ladan, Doug Lea, Oren Lederman, Pierre Leone, Y ossi\nLev, Wei Lu, Victor Luchangco, Virendra Marathe, Kevin Marth, John Mellor-\nCrummey, Mark Moir, Dan Nussbaum, Kiran Pamnany, Ben Pere, T orvald\nRiegel, Vijay Saraswat, Bill Scherer, Warren Schudy, Michael Scott, Ori Shalev,\nMarc Shapiro, Y otam Soen, Ralf Suckow, Seth Syberg, Alex Weiss, and Zhenyuan\nZhao. We apologize for any names inadvertently omitted.\nWe thank Mark Moir, Steve Heller, and our colleagues in the Scalable Syn-\nchronization group at Sun Microsystems for their incredible support during the\nwriting of the book.\nThanks to all who have sent us errata to improve this book, including:\nRajeev Alur, Matthew Allen, Karolos Antoniadis, Cristina Basescu, Liran Bar-\nsisa, Igor Berman, Konstantin Boudnik, Bjoern Brandenburg, Martin Buch-\nholz, Kyle Cackett, Mario Calha, Michael Champigny, Neill Clift, Eran Cohen,\nDaniel B. Curtis, Gil Danziger, Venkat Dhinakaran, David Dice, Wan Fokkink,\nDavid Fort, Robert P . Goddard, Brian Goetz, Bart Golsteijn, K. Gopinath, Enes\nGoktas, Jason", "doc_id": "3062657e-8296-4104-9b5b-18480b32334e", "embedding": null, "doc_hash": "bac8716459ffea570c9c9737c16761c6ff1e5337cf3b441f9470970c650d7a46", "extra_info": null, "node_info": {"start": 25563, "end": 30035}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "104acf7e-8234-42c2-bb73-31dcf8af08e6", "3": "9d19b774-e908-41f8-8dc2-df1c57fab1a7"}}, "__type__": "1"}, "9d19b774-e908-41f8-8dc2-df1c57fab1a7": {"__data__": {"text": "Alur, Matthew Allen, Karolos Antoniadis, Cristina Basescu, Liran Bar-\nsisa, Igor Berman, Konstantin Boudnik, Bjoern Brandenburg, Martin Buch-\nholz, Kyle Cackett, Mario Calha, Michael Champigny, Neill Clift, Eran Cohen,\nDaniel B. Curtis, Gil Danziger, Venkat Dhinakaran, David Dice, Wan Fokkink,\nDavid Fort, Robert P . Goddard, Brian Goetz, Bart Golsteijn, K. Gopinath, Enes\nGoktas, Jason T. Greene, Dan Grossman, Tim Halloran, Muhammad Amber\nHassaan, Matt Hayes, Francis Hools, Ben Horowitz, Barak Itkin, Paulo Jan-\notti, Kyungho Jeon, Ahmed Khademzadeh, Irena Karlinsky, Habib Khan, Omar\nxvii\nxviii Acknowledgments\nKhan, Namhyung Kim, Guy Korland, Sergey Kotov, Doug Lea, Y ossi Lev, Adam\nMacBeth, Kevin Marth, Adam Morrison, Adam Weinstock, Mike Maloney, Tim\nMcIver, Sergejs Melderis, Bartosz Milewski, Mark Moir, Adam Morrison, Vic-\ntor Luchangco, Jose Pedro Oliveira, Dale Parson, Jonathan Perry, Amir Pnueli,\nPat Quillen, Binoy Ravindran, Roei Raviv, Sudarshan Raghunathan, Jean-Paul\nRigault, Michael Rueppel, Mohamed M. Saad, Assaf Schuster, Marc Shapiro,\nNathar Shah, Huang-Ti Shih, Joseph P . Skudlarek, James Stout, Mark Sum-\nmer\ufb01eld, Deqing Sun, Seth Syberg, Fuad Tabba, Binil Thomas, John A Trono,\nThomas Weibel, Adam Weinstock, Jaeheon Yi, Zhenyuan Zhao, Ruiwen Zuo,\nChong Xing.\nThis book offers complete code for all the examples, as well as\nslides, updates, and other useful tools on its companion web page\nat:http://store.elsevier.com/product.jsp?isbn=9780123973375\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nPreface\nThis book is intended to serve both as a textbook for a senior-level undergraduate\ncourse, and as a reference for practitioners.\nReaders should know enough discrete mathematics to understand \u201cbig-O\u201d\nnotation, and what it means for a problem to be NP-complete. It is helpful to\nbe familiar with elementary systems constructs such as processors, threads, and\ncaches. A basic understanding of Java is needed to follow the examples. (We\nexplain advanced language features before using them.) Two appendixes summa-\nrizewhatthereaderneedstoknow:AppendixAcoversprogramminglanguage\nconstructs,andAppendixBcoversmultiprocessorhardwarearchitectures.\nThe \ufb01rst third covers the principles of concurrent programming, showing how\ntothink like a concurrent programmer. Like many other skills such as driving a\ncar, cooking a meal, or appreciating caviar, thinking concurrently requires cul-\ntivation, but it can be learned with moderate effort. Readers who want to start\nprogramming right away may skip most of this section, but should still read\nChapters 2 and 3which cover the basic ideas necessary to understand the rest\nof the book.\nWe \ufb01rst look at the classic mutual exclusion problem ( Chapter 2 ). This chap-\nter is essential for understanding why concurrent programming is a challenge. It\ncovers basic concepts such as fairness and deadlock. We then ask what it", "doc_id": "9d19b774-e908-41f8-8dc2-df1c57fab1a7", "embedding": null, "doc_hash": "59aec2ff20b9c7de021f18b1dc5bb7d5f2f77bbb193759778d4836e9dc141c53", "extra_info": null, "node_info": {"start": 30693, "end": 33638}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3062657e-8296-4104-9b5b-18480b32334e", "3": "d5c48f9d-772b-4c36-918e-57eaaee753f5"}}, "__type__": "1"}, "d5c48f9d-772b-4c36-918e-57eaaee753f5": {"__data__": {"text": "skills such as driving a\ncar, cooking a meal, or appreciating caviar, thinking concurrently requires cul-\ntivation, but it can be learned with moderate effort. Readers who want to start\nprogramming right away may skip most of this section, but should still read\nChapters 2 and 3which cover the basic ideas necessary to understand the rest\nof the book.\nWe \ufb01rst look at the classic mutual exclusion problem ( Chapter 2 ). This chap-\nter is essential for understanding why concurrent programming is a challenge. It\ncovers basic concepts such as fairness and deadlock. We then ask what it means\nfor a concurrent program to be correct ( Chapter 3 ). We consider several alter-\nnative conditions, and the circumstances one might want to use each one. We\nexamine the properties of shared memory essential to concurrent computation\n(Chapter 4 ), and we look at the kinds of synchronization primitives needed to\nimplement highly concurrent data structures ( Chapters 5 and 6).\nWe think it is essential that anyone who wants to become truly skilled in the\nart of multiprocessor programming spend time solving the problems presented\nin the \ufb01rst part of this book. Although these problems are idealized, they distill\nxxi\nxxii Preface\nthe kind of thinking necessary to write effective multiprocessor programs. Most\nimportant, they distill the style of thinking necessary to avoid the common mis-\ntakes committed by nearly all novice programmers when they \ufb01rst encounter\nconcurrency.\nThe next two-thirds describe the practice of concurrent programming. Each\nchapter has a secondary theme, illustrating either a particular programming pat-\ntern or algorithmic technique. At the level of systems and languages, Chapter 7\ncovers spin locks and contention. This chapter introduces the importance of\nthe underlying architecture, since spin lock performance cannot be understood\nwithout understanding the multiprocessor memory hierarchy. Chapter 8 covers\nmonitor locks and waiting, a common synchronization idiom, especially in Java.\nChapter 16 covers work-stealing and parallelism, and Chapter 17 describes bar-\nriers, all of which are useful for structuring concurrent applications.\nOther chapters cover concurrent data structures. All these chapters depend\non Chapter 9, and the reader should read this chapter before reading the others.\nLinked lists illustrate different kinds of synchronization patterns, ranging from\ncoarse-grained locking, to \ufb01ne-grained locking, to lock-free structures (Chap-\nter 9). The FIFO queues illustrate the ABA synchronization hazard that arises\nwhen using atomic synchronization primitives (Chapter 10), Stacks illustrate an\nimportant synchronization pattern called elimination (Chapter 11), Hash maps\nshow how an algorithm can exploit natural parallelism (Chapter 13), Skip lists\nillustrate ef\ufb01cient parallel search (Chapter 14), and priority queues illustrate\nhow one can sometimes weaken correctness guarantees to enhance performance\n(Chapter 15).\nFinally, Chapter 18 describes the emerging transactional approach to concur-\nrency, which we believe will become increasingly important in the near future.\nThe importance of concurrency has not always been acknowledged. Here is\na quote from a 1989 New York Times article on new operating systems for the\nIBM PC:\nReal concurrency\u2013in which one program actually continues to function while\nyou call up and use another\u2013is more amazing but of small use to the average\nperson. How many programs do you have that take more than a few seconds to\nperform any task?\nRead this book, and decide for yourself.\nSuggested Ways to T each\nthe Art of Multiprocessor\nProgramming\nPreface\nThe following are three possible tracks to teaching a multiprocessor program-\nming course using the material in the book.\nThe \ufb01rst track is a short course for practitioners who are interested in tech-\nniques that can be", "doc_id": "d5c48f9d-772b-4c36-918e-57eaaee753f5", "embedding": null, "doc_hash": "0a517fb1a430fb65c5fe88264c53e04d99afe85111d82f982a21b68ec38da579", "extra_info": null, "node_info": {"start": 33483, "end": 37336}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9d19b774-e908-41f8-8dc2-df1c57fab1a7", "3": "f93e563e-653e-4eac-83b4-ba661b2ab0af"}}, "__type__": "1"}, "f93e563e-653e-4eac-83b4-ba661b2ab0af": {"__data__": {"text": "concurrency\u2013in which one program actually continues to function while\nyou call up and use another\u2013is more amazing but of small use to the average\nperson. How many programs do you have that take more than a few seconds to\nperform any task?\nRead this book, and decide for yourself.\nSuggested Ways to T each\nthe Art of Multiprocessor\nProgramming\nPreface\nThe following are three possible tracks to teaching a multiprocessor program-\nming course using the material in the book.\nThe \ufb01rst track is a short course for practitioners who are interested in tech-\nniques that can be applied directly to problems at hand.\nThe second track is a longer course for students who are not Computer Science\nmajors , but who are interested in learning the basics of multiprocessor program-\nming, as well as techniques likely to be useful in their own areas.\nThe third track is a semester-long course for Computer Science majors , either\nupper-level undergraduates or graduate students.\nPractitioner Track\nCover Chapter 1, emphasizing Amdahl\u2019s law and its implications. In Chapter 2,\ncover sections 2.1, 2.2, 2.3, and 2.6. Mention the implications of the impossibility\nproofs in Section 2.8. In Chapter 3, skip Sections 3.3 and 3.6.\nCover Chapter 7, except for Sections 7.7 and 7.8. Chapter 8, which deals mon-\nitors and reentrant locks, may be familiar to some practitioners. Skip Section 8.5\non Semaphores.\nCover Chapters 9, 10, except for 10.7, and Sections 11.1 and 11.2. Skip the\nmaterial in Sections 11.3 and onwards. Skip Chapter 12.\nCover Chapters 13 and 14. Skip Chapter 15. Cover Chapter 16, except for Sec-\ntion 16.5. In Chapter 17, teach sections 17.1\u201317.3.\nxxiii\nxxiv Suggested Ways to T each\nNon-CS Major Track\nCover Chapter 1, emphasizing Amdahl\u2019s law and its implications. In Chapter 2,\ncover sections 2.1, 2.2, 2.3, 2.5, and 2.6. Mention the implications of the impossi-\nbility proofs in Section 2.8. In Chapter 3 skip Section 3.6.\nCover the material in Sections 4.1 and 4.2, and Chapter 5. Mention the uni-\nversality of consensus, but skip Chapter 6.\nCover Chapter 7, except for Sections 7.7 and 7.8. Cover Chapter 8.\nCover Chapters 9, 10, except for 10.7, and Chapter 11. Skip Chapter 12.\nCover Chapters 13 and 14. Skip Chapter 15. Cover Chapter 16. In Chapter 17,\nteach sections 17.1\u201317.3. Cover Chapter 18.\nCS Major Track\nThe slides on the companion page were developed for a semester-long course\nCover Chapters 1 and 2 (Section 2.7 is optional) and 3. (Section 3.6 is\noptional). Cover Chapters 4, 5, and 6. Before starting Chapter 7, it may be useful\nto review basic multiprocessor architecture (Appendix B).\nCover Chapter 7 (Sections 7.7 and 7.8 are optional). Cover Chapter 8 if\nyour students are unfamiliar with Java monitors. Cover Chapters 9 and 10\n(Section 10.7 optional). Cover Chapters 11, 12 (Sections 12.7, 12.8, and 12.9\nare optional), 13, and 14. Chapter 15 is optional. Cover Chapters 16 and 17.\nChapter 18 is optional.\n1Introduction\nThe computer industry is undergoing, if not another revolution, certainly a vig-\norous shaking-up. The major chip manufacturers have, for the time being at\nleast, given up trying to make processors run faster. Moore\u2019s Law has not been\nrepealed: each year, more and more transistors \ufb01t into the same space, but their\nclock speed cannot be increased without", "doc_id": "f93e563e-653e-4eac-83b4-ba661b2ab0af", "embedding": null, "doc_hash": "1d52bf486ebc067f24167d9bfa026689ae1878afd71aa15567f9429deea520dd", "extra_info": null, "node_info": {"start": 37345, "end": 40647}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d5c48f9d-772b-4c36-918e-57eaaee753f5", "3": "69b9f622-130d-4485-bced-50eac3635847"}}, "__type__": "1"}, "69b9f622-130d-4485-bced-50eac3635847": {"__data__": {"text": "optional). Cover Chapters 11, 12 (Sections 12.7, 12.8, and 12.9\nare optional), 13, and 14. Chapter 15 is optional. Cover Chapters 16 and 17.\nChapter 18 is optional.\n1Introduction\nThe computer industry is undergoing, if not another revolution, certainly a vig-\norous shaking-up. The major chip manufacturers have, for the time being at\nleast, given up trying to make processors run faster. Moore\u2019s Law has not been\nrepealed: each year, more and more transistors \ufb01t into the same space, but their\nclock speed cannot be increased without overheating. Instead, manufacturers are\nturning to \u201cmulticore\u201d architectures, in which multiple processors (cores) com-\nmunicate directly through shared hardware caches. Multiprocessor chips make\ncomputing more effective by exploiting parallelism : harnessing multiple proces-\nsors to work on a single task.\nThe spread of multiprocessor architectures will have a pervasive effect on how\nwe develop software. Until recently, advances in technology meant advances in\nclock speed, so software would effectively \u201cspeed up\u201d by itself over time. Now,\nhowever, this free ride is over. Advances in technology will mean increased par-\nallelism and not increased clock speed, and exploiting such parallelism is one of\nthe outstanding challenges of modern Computer Science.\nThis book focuses on how to program multiprocessors that communicate via\na shared memory. Such systems are often called shared-memory multiprocessors\nor, more recently, multicores . Programming challenges arise at all scales of mul-\ntiprocessor systems\u2014at a very small scale, processors within a single chip need\nto coordinate access to a shared memory location, and on a large scale, proces-\nsors in a supercomputer need to coordinate the routing of data. Multiprocessor\nprogramming is challenging because modern computer systems are inherently\nasynchronous : activities can be halted or delayed without warning by interrupts,\npreemption, cache misses, failures, and other events. These delays are inherently\nunpredictable, and can vary enormously in scale: a cache miss might delay a pro-\ncessor for fewer than ten instructions, a page fault for a few million instructions,\nand operating system preemption for hundreds of millions of instructions.\nWe approach multiprocessor programming from two complementary direc-\ntions: principles and practice. In the principles part of this book, we focus on\ncomputability : \ufb01guring out what can be computed in an asynchronous concur-\nrent environment. We use an idealized model of computation in which multiple\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00001-0\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.1\n2 Chapter 1 Introduction\nconcurrent threads manipulate a set of shared objects . The sequence of the thread\noperations on the objects is called the concurrent program orconcurrent algo-\nrithm . This model is essentially the model presented by the JavaTM, C#, or C++\nthread packages.\nSurprisingly, there are easy-to-specify shared objects that cannot be imple-\nmented by any concurrent algorithm. It is therefore important to understand\nwhat not to try, before proceeding to write multiprocessor programs. Many of the\nissues that will land multiprocessor programmers in trouble are consequences of\nfundamental limitations of the computational model, so we view the acquisition\nof a basic understanding of concurrent shared-memory computability as a nec-\nessary step. The chapters dealing with principles take the reader through a quick\ntour of asynchronous computability, attempting to expose various computabil-\nity issues, and how they are addressed through", "doc_id": "69b9f622-130d-4485-bced-50eac3635847", "embedding": null, "doc_hash": "18a44aab8e6a17ca1ab2c2703ac86ea67258ff20b7adaa4b57c5f7463a691e05", "extra_info": null, "node_info": {"start": 40678, "end": 44322}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f93e563e-653e-4eac-83b4-ba661b2ab0af", "3": "2f555e41-a011-482e-83fc-e94bb38542db"}}, "__type__": "1"}, "2f555e41-a011-482e-83fc-e94bb38542db": {"__data__": {"text": "by any concurrent algorithm. It is therefore important to understand\nwhat not to try, before proceeding to write multiprocessor programs. Many of the\nissues that will land multiprocessor programmers in trouble are consequences of\nfundamental limitations of the computational model, so we view the acquisition\nof a basic understanding of concurrent shared-memory computability as a nec-\nessary step. The chapters dealing with principles take the reader through a quick\ntour of asynchronous computability, attempting to expose various computabil-\nity issues, and how they are addressed through the use of hardware and software\nmechanisms.\nAn important step in the understanding of computability is the speci\ufb01cation\nand veri\ufb01cation of what a given program actually does. This is perhaps best\ndescribed as program correctness . The correctness of multiprocessor programs,\nby their very nature, is more complex than that of their sequential counterparts,\nand requires a different set of tools, even for the purpose of \u201cinformal reasoning\u201d\n(which, of course, is what most programmers actually do). Sequential correct-\nness is mostly concerned with safety properties. A safety property states that some\n\u201cbad thing\u201d never happens. For example, a traf\ufb01c light never displays green in all\ndirections, even if the power fails. Naturally, concurrent correctness is also con-\ncerned with safety, but the problem is much, much harder, because safety must be\nensured despite the vast number of ways that the steps of concurrent threads can\nbe interleaved. Equally important, concurrent correctness encompasses a variety\nofliveness properties that have no counterparts in the sequential world. A live-\nness property states that a particular good thing will happen. For example, a red\ntraf\ufb01c light will eventually turn green. A \ufb01nal goal of the part of the book dealing\nwith principles is to introduce a variety of metrologies and approaches for rea-\nsoning about concurrent programs, which will later serve us when discussing the\ncorrectness of real-world objects and programs.\nThe second part of the book deals with the practice of multiprocessor pro-\ngramming, and focuses on performance. Analyzing the performance of multi-\nprocessor algorithms is also different in \ufb02avor from analyzing the performance\nof sequential programs. Sequential programming is based on a collection of well-\nestablished and well-understood abstractions. When we write a sequential pro-\ngram, we usually do not need to be aware that underneath it all, pages are being\nswapped from disk to memory, and smaller units of memory are being moved\nin and out of a hierarchy of processor caches. This complex memory hierarchy is\nessentially invisible, hiding behind a simple programming abstraction.\nIn the multiprocessor context, this abstraction breaks down, at least from\na performance perspective. T o achieve adequate performance, the programmer\nmust sometimes \u201coutwit\u201d the underlying memory system, writing programs that\nwould seem bizarre to someone unfamiliar with multiprocessor architectures.\n1.1 Shared Objects and Synchronization 3\nSomeday perhaps, concurrent architectures will provide the same degree of ef\ufb01-\ncient abstraction now provided by sequential architectures, but in the meantime,\nprogrammers should beware.\nThe principles part of the book presents a progressive collection of shared\nobjects and programming tools. Every object and tool is interesting in its own\nright, and we use each one to expose the reader to higher-level issues: spin-locks\nillustrate contention, linked lists illustrate the role of locking in data structure\ndesign, and so on. Each of these issues has important consequences for program\nperformance. The hope is that the reader will understand the issue in a way that\nwill later allow him or her to apply the lessons learned to speci\ufb01c", "doc_id": "2f555e41-a011-482e-83fc-e94bb38542db", "embedding": null, "doc_hash": "f4487a96b75df6626ced0e7896df8dd7c9b220717e1a768580d7f561443e2a16", "extra_info": null, "node_info": {"start": 44266, "end": 48101}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "69b9f622-130d-4485-bced-50eac3635847", "3": "16ce35c6-e42d-4179-b001-fb636d20e009"}}, "__type__": "1"}, "16ce35c6-e42d-4179-b001-fb636d20e009": {"__data__": {"text": "but in the meantime,\nprogrammers should beware.\nThe principles part of the book presents a progressive collection of shared\nobjects and programming tools. Every object and tool is interesting in its own\nright, and we use each one to expose the reader to higher-level issues: spin-locks\nillustrate contention, linked lists illustrate the role of locking in data structure\ndesign, and so on. Each of these issues has important consequences for program\nperformance. The hope is that the reader will understand the issue in a way that\nwill later allow him or her to apply the lessons learned to speci\ufb01c multiprocessor\nsystems. We culminate with a discussion of state-of-the-art technologies such as\ntransactional memory .\nWe would like to include a few words about style. The book uses the Java pro-\ngramming language. There are, of course, other suitable languages which readers\nwould have found equally appealing. We have a long list of reasons for our spe-\nci\ufb01c choice, but perhaps it is more suitable to discuss them over a cup of coffee!\nIn the appendix we explain how the concepts expressed in Java are expressed in\nother popular languages or libraries. We also provide a primer on multiprocessor\nhardware. Throughout the book, we avoid presenting speci\ufb01c performance num-\nbers for programs and algorithms, and stick to general trends. There is a good\nreason for this: multiprocessors vary greatly, and unfortunate though it may be,\nat this point in time, what works well on one machine may be signi\ufb01cantly less\nimpressive on another. Sticking to general trends is our way of guaranteeing that\nthe validity of our assertions will be sustained over time.\nWe provide references at the end of each chapter. The reader will \ufb01nd a bibli-\nographical survey of the material covered, with suggestions for further reading.\nEach chapter also includes a collection of exercises which readers can use to gauge\ntheir comprehension or entertain themselves on Sunday mornings.\n1.1 Shared Objects and Synchronization\nOn the \ufb01rst day of your new job, your boss asks you to \ufb01nd all primes between\n1 and 1010(never mind why), using a parallel machine that supports ten con-\ncurrent threads. This machine is rented by the minute, so the longer your pro-\ngram takes, the more it costs. Y ou want to make a good impression. What do\nyou do?\nAs a \ufb01rst attempt, you might consider giving each thread an equal share of the\ninput domain. Each thread might check 109numbers, as shown in Fig. 1.1. This\napproach fails, for an elementary, but important reason. Equal ranges of inputs\ndo not necessarily produce equal amounts of work. Primes do not occur uni-\nformly: there are more primes between 1 and 109than between 9\u0001109and 1010.\nT o make matters worse, the computation time per prime is not the same in all\nranges: it usually takes longer to test whether a large number is prime than a\n4 Chapter 1 Introduction\n1void primePrint {\n2 int i = ThreadID.get(); // thread IDs are in {0..9}\n3 int block = power(10, 9);\n4 for (int j = (i *block) + 1; j <= (i + 1) *block; j++) {\n5 if(isPrime(j))\n6 print(j);\n7 }\n8}\nFigure 1.1 Balancing load by dividing up the input domain. Each thread in f0..9ggets an equal\nsubset of the range.\n1Counter counter = new Counter(1); // shared by all threads\n2void primePrint {\n3 long i = 0;\n4 long limit = power(10, 10);\n5 while (i < limit) { // loop until all numbers taken\n6 i = counter.getAndIncrement(); // take next untaken", "doc_id": "16ce35c6-e42d-4179-b001-fb636d20e009", "embedding": null, "doc_hash": "c4ea8d630a1266b3db0012a3b5a697c7f5dceed9855d79aa279a6dcb9e2f7b8c", "extra_info": null, "node_info": {"start": 48107, "end": 51536}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2f555e41-a011-482e-83fc-e94bb38542db", "3": "acdc666f-7320-4a5c-8c86-f934a60c91d6"}}, "__type__": "1"}, "acdc666f-7320-4a5c-8c86-f934a60c91d6": {"__data__": {"text": "*block) + 1; j <= (i + 1) *block; j++) {\n5 if(isPrime(j))\n6 print(j);\n7 }\n8}\nFigure 1.1 Balancing load by dividing up the input domain. Each thread in f0..9ggets an equal\nsubset of the range.\n1Counter counter = new Counter(1); // shared by all threads\n2void primePrint {\n3 long i = 0;\n4 long limit = power(10, 10);\n5 while (i < limit) { // loop until all numbers taken\n6 i = counter.getAndIncrement(); // take next untaken number\n7 if(isPrime(i))\n8 print(i);\n9 }\n10 }\nFigure 1.2 Balancing the work load using a shared counter. Each thread gets a dynamically\ndetermined number of numbers to test.\nsmall number. In short, there is no reason to believe that the work will be divided\nequally among the threads, and it is not clear even which threads will have the\nmost work.\nA more promising way to split the work among the threads is to assign each\nthread one integer at a time (Fig. 1.2). When a thread is \ufb01nished with testing\nan integer, it asks for another. T o this end, we introduce a shared counter , an\nobject that encapsulates an integer value, and that provides a getAndIncrement ()\nmethod that increments its value, and returns the counter\u2019s prior value to the\ncaller.\nFig. 1.3 shows a na \u00a8\u0131ve implementation of Counter in Java. This counter imple-\nmentation works well when used by a single thread, but it fails when shared by\nmultiple threads. The problem is that the expression\nreturn value++;\nis actually an abbreviation of the following, more complex code:\nlong temp = value;\nvalue = temp + 1;\nreturn temp;\nIn this code fragment, value is a \ufb01eld of the Counter object, and is shared among\nall the threads. Each thread, however, has its own local copy of temp , which is a\nlocal variable to each thread.\n1.1 Shared Objects and Synchronization 5\n1public class Counter {\n2 private long value; // counter starts at one\n3 public Counter( int i) { // constructor initializes counter\n4 value = i;\n5 }\n6 public long getAndIncrement() { // increment, returning prior value\n7 return value++;\n8 }\n9}\nFigure 1.3 An implementation of the shared counter.\nNow imagine that threads AandBboth call the counter\u2019s getAndIncrement ()\nmethod at about the same time. They might simultaneously read 1 from value ,\nset their local temp variables to 1, value to 2, and both return 1. This behavior\nis not what we intended: concurrent calls to the counter\u2019s getAndIncrement ()\nreturn the same value, but we expect them to return distinct values. In fact, it\ncould get even worse. One thread might read 1 from value , but before it sets\nvalue to 2, another thread would go through the increment loop several times,\nreading 1 and setting to 2, reading 2 and setting to 3. When the \ufb01rst thread \ufb01nally\ncompletes its operation and sets value to 2, it will actually be setting the counter\nback from 3 to 2.\nThe heart of the problem is that incrementing the counter\u2019s value requires\ntwo distinct operations on the shared variable: reading the value \ufb01eld into a\ntemporary variable and writing it back to the Counter object.\nSomething similar happens when you try to pass someone approaching you\nhead-on in a corridor. Y ou may \ufb01nd yourself veering right, then left several times\nto avoid the other person doing exactly the same thing. Sometimes you manage\nto avoid bumping into them and sometimes you do not, and in fact, as we see\nin the later chapters, such collisions are provably unavoidable.1On an intuitive\nlevel, what is going on is that each of you is performing two distinct steps: looking\nat", "doc_id": "acdc666f-7320-4a5c-8c86-f934a60c91d6", "embedding": null, "doc_hash": "35601155c023275491b8533bdd677d148934e7a61c05a2123740975cf05930e1", "extra_info": null, "node_info": {"start": 51692, "end": 55171}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "16ce35c6-e42d-4179-b001-fb636d20e009", "3": "134f22c4-cc56-4d68-b9e2-1d887190e6e2"}}, "__type__": "1"}, "134f22c4-cc56-4d68-b9e2-1d887190e6e2": {"__data__": {"text": "the shared variable: reading the value \ufb01eld into a\ntemporary variable and writing it back to the Counter object.\nSomething similar happens when you try to pass someone approaching you\nhead-on in a corridor. Y ou may \ufb01nd yourself veering right, then left several times\nto avoid the other person doing exactly the same thing. Sometimes you manage\nto avoid bumping into them and sometimes you do not, and in fact, as we see\nin the later chapters, such collisions are provably unavoidable.1On an intuitive\nlevel, what is going on is that each of you is performing two distinct steps: looking\nat (\u201creading\u201d) the other\u2019s current position, and moving (\u201cwriting\u201d) to one side or\nthe other. The problem is, when you read the other\u2019s position, you have no way\nof knowing whether they have decided to stay or move. In the same way that you\nand the annoying stranger must decide who passes on the left and who on the\nright, threads accessing a shared Counter must decide who goes \ufb01rst and who\ngoes second.\nAs we will see in Chapter 5, modern multiprocessor hardware provides special\nread-modify-write instructions that allow threads to read, modify, and write a\nvalue to memory in one atomic (i.e., indivisible) hardware step. For the Counter\nobject, we can use such hardware to increment the counter atomically.\n1A preventive approach such as \u201calways sidestep to the right\u201d does not work because the approach-\ning person may be British.\n6 Chapter 1 Introduction\nWe can also provide such atomic behavior by guaranteeing in software (using\nonly read and write instructions) that only one thread executes the read-and-\nwrite sequence at a time. The problem of making sure that only one thread at a\ntime can execute a particular block of code is called the mutual exclusion problem,\nand is one of the classic coordination problems in multiprocessor programming.\nAs a practical matter, you are unlikely ever to \ufb01nd yourself having to design\nyour own mutual exclusion algorithm (instead, you would probably call on a\nlibrary). Nevertheless, understanding how to implement mutual exclusion from\nthe basics is an essential condition for understanding concurrent computation\nin general. There is no more effective way to learn how to reason about essential\nand ubiquitous issues such as mutual exclusion, deadlock, bounded fairness, and\nblocking versus nonblocking synchronization.\n1.2 A Fable\nInstead of treating coordination problems (such as mutual exclusion) as pro-\ngramming exercises, we prefer to think of concurrent coordination problems as\nif they were physics problems. We now present a sequence of fables, illustrating\nsome of the basic problems. Like most authors of fables, we retell stories mostly\ninvented by others (see the Chapter Notes at the end of this chapter).\nAlice and Bob are neighbors, and they share a yard. Alice owns a cat and Bob\nowns a dog. Both pets like to run around in the yard, but (naturally) they do\nnot get along. After some unfortunate experiences, Alice and Bob agree that they\nshould coordinate to make sure that both pets are never in the yard at the same\ntime. Of course, we rule out trivial solutions that do not allow any animals into\nan empty yard.\nHow should they do it? Alice and Bob need to agree on mutually compatible\nprocedures for deciding what to do. We call such an agreement a coordination\nprotocol (or just a protocol , for short).\nThe yard is large, so Alice cannot simply look out of the window to check\nwhether Bob\u2019s dog is present. She could perhaps walk over to Bob\u2019s house and\nknock on the door, but that takes a long time, and what if it rains? Alice might\nlean out the window and shout \u201cHey Bob! Can I let the cat out?\u201d The problem\nis that Bob might not hear", "doc_id": "134f22c4-cc56-4d68-b9e2-1d887190e6e2", "embedding": null, "doc_hash": "6a422522a60f6317478cb7b80cc42a295128f43922483160ca16c22f62d945cc", "extra_info": null, "node_info": {"start": 55028, "end": 58732}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "acdc666f-7320-4a5c-8c86-f934a60c91d6", "3": "53b146ee-0a8a-408c-aaef-5e154a22341a"}}, "__type__": "1"}, "53b146ee-0a8a-408c-aaef-5e154a22341a": {"__data__": {"text": "yard.\nHow should they do it? Alice and Bob need to agree on mutually compatible\nprocedures for deciding what to do. We call such an agreement a coordination\nprotocol (or just a protocol , for short).\nThe yard is large, so Alice cannot simply look out of the window to check\nwhether Bob\u2019s dog is present. She could perhaps walk over to Bob\u2019s house and\nknock on the door, but that takes a long time, and what if it rains? Alice might\nlean out the window and shout \u201cHey Bob! Can I let the cat out?\u201d The problem\nis that Bob might not hear her. He could be watching TV , visiting his girlfriend,\nor out shopping for dog food. They could try to coordinate by cell phone, but\nthe same dif\ufb01culties arise if Bob is in the shower, driving through a tunnel, or\nrecharging his phone\u2019s batteries.\nAlice has a clever idea. She sets up one or more empty beer cans on Bob\u2019s\nwindowsill (Fig. 1.4), ties a string around each one, and runs the string back\nto her house. Bob does the same. When she wants to send a signal to Bob, she\nyanks the string to knock over one of the cans. When Bob notices a can has been\nknocked over, he resets the can.\n1.2 A Fable 7\nFigure 1.4 Communicating with cans.\nUp-ending beer cans by remote control may seem like a creative solution, but\nit is still deeply \ufb02awed. The problem is that Alice can place only a limited number\nof cans on Bob\u2019s windowsill, and sooner or later, she is going to run out of cans to\nknock over. Granted, Bob resets a can as soon as he notices it has been knocked\nover, but what if he goes to Canc \u00b4un for Spring Break? As long as Alice relies on\nBob to reset the beer cans, sooner or later, she might run out.\nSo Alice and Bob try a different approach. Each one sets up a \ufb02ag pole, easily\nvisible to the other. When Alice wants to release her cat, she does the following:\n1.She raises her \ufb02ag.\n2.When Bob\u2019s \ufb02ag is lowered, she unleashes her cat.\n3.When her cat comes back, she lowers her \ufb02ag.\nBob\u2019s behavior is a little more complicated.\n1.He raises his \ufb02ag.\n2.While Alice\u2019s \ufb02ag is raised\na)Bob lowers his \ufb02ag\nb)Bob waits until Alice\u2019s \ufb02ag is lowered\nc)Bob raises his \ufb02ag\n3.As soon as his \ufb02ag is raised and hers is down, he unleashes his dog.\n4.When his dog comes back, he lowers his \ufb02ag.\nThis protocol rewards further study as a solution to Alice and Bob\u2019s problem.\nOn an intuitive level, it works because of the following \ufb02ag principle . If Alice and\nBob each\n1.raises his or her own \ufb02ag, and then\n2.looks at the other\u2019s \ufb02ag,\n8 Chapter 1 Introduction\nthen at least one will see the other\u2019s \ufb02ag raised (clearly, the last one to look will\nsee the other\u2019s \ufb02ag raised) and will not let his or her pet enter the yard. However,\nthis observation does not prove that the pets will never be in the yard together.\nWhat if, for example, Alice lets her cat in and out of the yard several times while\nBob is looking?\nT o prove that the pets will never be in the yard together, assume by way of con-\ntradiction that there is a way the pets could end up in the yard together. Consider\nthe last time Alice and Bob each raised their \ufb02ag and looked at the other\u2019s \ufb02ag\nbefore sending the pet into the yard. When", "doc_id": "53b146ee-0a8a-408c-aaef-5e154a22341a", "embedding": null, "doc_hash": "d4c8cb96531e18244d00b6b58a0c470c9eeae4310bbb463f7689068266bcc2c9", "extra_info": null, "node_info": {"start": 58791, "end": 61925}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "134f22c4-cc56-4d68-b9e2-1d887190e6e2", "3": "ed43d80f-802f-4c59-9d8a-4bca2466b3e0"}}, "__type__": "1"}, "ed43d80f-802f-4c59-9d8a-4bca2466b3e0": {"__data__": {"text": "raised) and will not let his or her pet enter the yard. However,\nthis observation does not prove that the pets will never be in the yard together.\nWhat if, for example, Alice lets her cat in and out of the yard several times while\nBob is looking?\nT o prove that the pets will never be in the yard together, assume by way of con-\ntradiction that there is a way the pets could end up in the yard together. Consider\nthe last time Alice and Bob each raised their \ufb02ag and looked at the other\u2019s \ufb02ag\nbefore sending the pet into the yard. When Alice last looked, her \ufb02ag was already\nfully raised. She must have not seen Bob\u2019s \ufb02ag, or she would not have released the\ncat, so Bob must have not completed raising his \ufb02ag before Alice started looking.\nIt follows that when Bob looked for the last time, after raising his \ufb02ag, it must\nhave been after Alice started looking, so he must have seen Alice\u2019s \ufb02ag raised and\nwould not have released his dog, a contradiction.\nThis kind of argument by contradiction shows up over and over again, and it is\nworthwhile spending some time convincing ourselves why this claim is true. It is\nimportant to note that we never assumed that \u201craising my \ufb02ag\u201d or the \u201clooking at\nyour \ufb02ag\u201d happens instantaneously, nor did we make any assumptions about how\nlong such activities take. All we care about is when these activities start or end.\n1.2.1 Properties of Mutual Exclusion\nT o show that the \ufb02ag protocol is a correct solution to Alice and Bob\u2019s problem,\nwe must understand what properties are required of a solution, and then show\nthat they are met by the protocol.\nFirst, we proved that the pets are excluded from being in the yard at the same\ntime, a property we call mutual exclusion .\nMutual exclusion is only one of several properties of interest. After all, as we\nnoted earlier, a protocol in which Alice and Bob never release a pet satis\ufb01es the\nmutual exclusion property, but it is unlikely to satisfy their pets. Here is another\nproperty of central importance. First, if one pet wants to enter the yard, then it\neventually succeeds. Second, if both pets want to enter the yard, then eventually\nat least one of them succeeds. We consider this deadlock-freedom property to be\nessential.\nWe claim that Alice and Bob\u2019s protocol is deadlock-free. Suppose both pets\nwant to use the yard. Alice and Bob each raise their \ufb02ags. Bob eventually notices\nthat Alice\u2019s \ufb02ag is raised, and defers to her by lowering his \ufb02ag, allowing her cat\ninto the yard.\nAnother property of compelling interest is starvation-freedom (sometimes\ncalled lockout-freedom ): if a pet wants to enter the yard, will it eventually suc-\nceed? Here, Alice and Bob\u2019s protocol performs poorly. Whenever Alice and Bob\nare in con\ufb02ict, Bob defers to Alice, so it is possible that Alice\u2019s cat can use the yard\n1.2 A Fable 9\nover and over again, while Bob\u2019s dog becomes increasingly uncomfortable. Later\non, we will see how to make protocols prevent starvation.\nThe last property of interest concerns waiting . Imagine that Alice raises her\n\ufb02ag, and is then suddenly stricken with appendicitis. She (and the cat) are taken\nto the hospital, and after a successful operation, she spends the next week under\nobservation at the hospital. Although Bob is relieved that Alice is well, his dog\ncannot use the yard for an entire week until Alice returns. The problem is that\nthe protocol states that Bob (and his dog) must wait for Alice to lower her \ufb02ag.\nIf Alice is delayed (even for a good reason), then Bob is also", "doc_id": "ed43d80f-802f-4c59-9d8a-4bca2466b3e0", "embedding": null, "doc_hash": "54ec5cb87e9e1633ea010df1429d308741626008f98ee6bc4f9abce45f9648f4", "extra_info": null, "node_info": {"start": 61928, "end": 65420}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "53b146ee-0a8a-408c-aaef-5e154a22341a", "3": "812c459f-49fa-4c5c-b325-ce568ab41da7"}}, "__type__": "1"}, "812c459f-49fa-4c5c-b325-ce568ab41da7": {"__data__": {"text": "prevent starvation.\nThe last property of interest concerns waiting . Imagine that Alice raises her\n\ufb02ag, and is then suddenly stricken with appendicitis. She (and the cat) are taken\nto the hospital, and after a successful operation, she spends the next week under\nobservation at the hospital. Although Bob is relieved that Alice is well, his dog\ncannot use the yard for an entire week until Alice returns. The problem is that\nthe protocol states that Bob (and his dog) must wait for Alice to lower her \ufb02ag.\nIf Alice is delayed (even for a good reason), then Bob is also delayed (for no\napparent good reason).\nThe question of waiting is important as an example of fault-tolerance . Nor-\nmally, we expect Alice and Bob to respond to each other in a reasonable amount\nof time, but what if they do not do so? The mutual exclusion problem, by its very\nessence, requires waiting: no mutual exclusion protocol avoids it, no matter how\nclever. Nevertheless, we see that many other coordination problems can be solved\nwithout waiting, sometimes in unexpected ways.\n1.2.2 The Moral\nHaving reviewed both the strengths and weaknesses of Bob and Alice\u2019s protocols,\nwe now turn our attention back to Computer Science.\nFirst, we examine why shouting across the yard and placing cell phone calls did\nnot work. Two kinds of communication occur naturally in concurrent systems:\n\u0004Transient communication requires both parties to participate at the same\ntime. Shouting, gestures, or cell phone calls are examples of transient commun-\nication.\n\u0004Persistent communication allows the sender and receiver to participate at dif-\nferent times. Posting letters, sending email, or leaving notes under rocks are\nall examples of persistent communication.\nMutual exclusion requires persistent communication. The problem with shout-\ning across the yard or placing cell phone calls is that it may or may not be okay\nfor Bob to unleash his dog, but if Alice is not able to respond to messages, he will\nnever know.\nThe can-and-string protocol might seem somewhat contrived, but it corre-\nsponds accurately to a common communication protocol in concurrent systems:\ninterrupts . In modern operating systems, one common way for one thread to get\nthe attention of another is to send it an interrupt. More precisely, thread Ainter-\nrupts thread Bby setting a bit at a location periodically checked by B. Sooner or\nlater,Bnotices the bit has been set and reacts. After reacting, Btypically resets\nthe bit (Acannot reset the bit). Even though interrupts cannot solve the mutual\nexclusion problem, they can still be very useful. For example, interrupt commu-\nnication is the basis of the Java language\u2019s wait () and notifyAll () calls.\n10 Chapter 1 Introduction\nOn a more positive note, the fable shows that mutual exclusion between two\nthreads can be solved (however imperfectly) using only two one-bit variables,\neach of which can be written by one thread and read by the other.\n1.3 The Producer\u2013Consumer Problem\nMutual exclusion is not the only problem worth investigating. Eventually, Alice\nand Bob fall in love and marry. Eventually, they divorce. (What were they think-\ning?) The judge gives Alice custody of the pets, and tells Bob to feed them. The\npets now get along with one another, but they side with Alice, and attack Bob\nwhenever they see him. As a result, Alice and Bob need to devise a protocol\nfor Bob to deliver food to the pets without Bob and the pets being in the yard\ntogether. Moreover, the protocol should not waste anyone\u2019s time: Alice does not\nwant to release her pets into the yard unless there is food there, and Bob does not\nwant to enter the yard unless the pets have consumed all the food. This problem\nis known as the", "doc_id": "812c459f-49fa-4c5c-b325-ce568ab41da7", "embedding": null, "doc_hash": "1d52f46626b2c76018075b214c5e8a1e0d50b2b25c9621147df474d4b37a17bd", "extra_info": null, "node_info": {"start": 65383, "end": 69087}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ed43d80f-802f-4c59-9d8a-4bca2466b3e0", "3": "0cf54341-c4ef-457d-9d4f-f0140373045d"}}, "__type__": "1"}, "0cf54341-c4ef-457d-9d4f-f0140373045d": {"__data__": {"text": "judge gives Alice custody of the pets, and tells Bob to feed them. The\npets now get along with one another, but they side with Alice, and attack Bob\nwhenever they see him. As a result, Alice and Bob need to devise a protocol\nfor Bob to deliver food to the pets without Bob and the pets being in the yard\ntogether. Moreover, the protocol should not waste anyone\u2019s time: Alice does not\nwant to release her pets into the yard unless there is food there, and Bob does not\nwant to enter the yard unless the pets have consumed all the food. This problem\nis known as the producer\u2013consumer problem.\nSurprisingly perhaps, the cans-and-string protocol we rejected for mutual\nexclusion does exactly what we need for the producer\u2013consumer problem. Bob\nplaces a can standing up on Alice\u2019s windowsill, ties one end of his string around\nthe can, and puts the other end of the string in his living room. He then puts food\nin the yard and knocks the can down. From now on, when Alice wants to release\nthe pets, she does the following:\n1.She waits until the can is down.\n2.She releases the pets.\n3.When the pets return, Alice checks whether they \ufb01nished the food. If so, she\nresets the can.\nBob does the following:\n1.He waits until the can is up.\n2.He puts food in the yard.\n3.He pulls the string and knocks the can down.\nThe state of the can thus re\ufb02ects the state of the yard. If the can is down, it means\nthere is food and the pets can eat, and if the can is up, it means the food is gone\nand Bob can put some more out. We check the following three properties:\n\u0004Mutual Exclusion : Bob and the pets are never in the yard together.\n\u0004Starvation-freedom : If Bob is always willing to feed, and the pets are always\nfamished, then the pets will eat in\ufb01nitely often.\n\u0004Producer\u2013Consumer : The pets will not enter the yard unless there is food, and\nBob will never provide more food if there is unconsumed food.\n1.3 The Producer\u2013Consumer Problem 11\nThis producer\u2013consumer protocol and the mutual exclusion protocol consid-\nered in the last section both ensure that Alice and Bob are never in the yard at the\nsame time. Nevertheless, Alice and Bob cannot use this producer\u2013consumer pro-\ntocol for mutual exclusion, and it is important to understand why. Mutual exclu-\nsion requires deadlock-freedom: anyone must be able to enter the yard in\ufb01nitely\noften on their own, even if the other is not there. By contrast, the producer\u2013\nconsumer protocol\u2019s starvation-freedom property assumes continuous coopera-\ntion from both parties.\nHere is how we reason about this protocol:\n\u0004Mutual Exclusion : We use a slightly different proof style than that used in our\nearlier mutual exclusion proof: a \u201cstate machine\u201d-based proof rather than one\nby contradiction. We think of the stringed can as a state machine. The can has\ntwo states, up and down, and it repeatedly transitions between these states.\nWe argue that mutual exclusion holds since it holds initially and continues to\nhold when transitioning from any state of the can to the other.\nInitially the can is either up or down. Let us say it was down. Then only the\npets can go in, and mutual exclusion holds. In order for the can to be raised\nby Alice, the pets must \ufb01rst leave, so when the can is raised, the pets are not\nin the yard and mutual exclusion is maintained since they will not enter again\nuntil it is knocked over. In order for the can to be knocked over, Bob must\nhave left the yard, and will not enter until it is raised again, so mutual exclu-\nsion is maintained once the can is", "doc_id": "0cf54341-c4ef-457d-9d4f-f0140373045d", "embedding": null, "doc_hash": "47638cad10325bb68df8cdb4ca03ba7808632094f27dc891e4efb09032002e20", "extra_info": null, "node_info": {"start": 69101, "end": 72611}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "812c459f-49fa-4c5c-b325-ce568ab41da7", "3": "03178d60-60a0-4113-84c7-321aa5fe42b1"}}, "__type__": "1"}, "03178d60-60a0-4113-84c7-321aa5fe42b1": {"__data__": {"text": "can to the other.\nInitially the can is either up or down. Let us say it was down. Then only the\npets can go in, and mutual exclusion holds. In order for the can to be raised\nby Alice, the pets must \ufb01rst leave, so when the can is raised, the pets are not\nin the yard and mutual exclusion is maintained since they will not enter again\nuntil it is knocked over. In order for the can to be knocked over, Bob must\nhave left the yard, and will not enter until it is raised again, so mutual exclu-\nsion is maintained once the can is knocked over. There are no other possible\ntransitions, and so our claim holds.\n\u0004Starvation-freedom : suppose the claim does not hold: It must be the case that\nin\ufb01nitely often Alice\u2019s pets are hungry, there is no food, and Bob is trying to\nprovide food but does not succeed. The can cannot be up, as then Bob will\nprovide food and knock over the can, allowing the pets to eat. So it must be\nthat the can is down, and since the pets are hungry, Alice will eventually raise\nthe can, bringing us back to the former case.\n\u0004Producer\u2013Consumer : The mutual exclusion property implies that the pets and\nBob will never be in the yard together. Bob will not enter the yard until Alice\nraises the can, which she will do only if there is no more food. Similarly, the\npets will not enter the yard until Bob lowers the can, which he will do only\nafter placing the food.\nLike the mutual exclusion protocol we have already described, this protocol\nexhibits waiting . If Bob deposits food in the yard, and immediately goes on vaca-\ntion without remembering to reset the can, then the pets may starve, despite the\npresence of food.\nTurning our attention back to Computer Science, the producer\u2013consumer\nproblem appears in almost all parallel and distributed systems. It is the way in\nwhich processors place data in communication buffers to be read or transmitted\nacross a network interconnect or shared bus.\n12 Chapter 1 Introduction\n1.4 The Readers\u2013Writers Problem\nBob and Alice eventually decide they love their pets so much they need to com-\nmunicate simple messages about them. Bob puts up a billboard in front of his\nhouse. The billboard holds a sequence of large tiles, each tile holding a single let-\nter. Bob, at his leisure, posts a message on the bulletin board by lifting one tile at\na time. Alice, at her leisure, reads the message by looking at the billboard through\na telescope, one tile at a time.\nThis may sound like a workable system, but it is not. Imagine that Bob posts\nthe message:\nsell the cat\nAlice, looking through her telescope, transcribes the message\nsell the\nAt this point Bob takes down the tiles and writes out a new message\nwash the dog\nAlice, continuing to scan across the billboard transcribes the message\nsell the dog\nY ou can imagine the rest.\nThere are some straightforward ways to solve the readers\u2013writers problem.\n\u0004Alice and Bob can use the mutual exclusion protocol to make sure that Alice\nreads only complete sentences. She might still miss a sentence, however.\n\u0004They can use the can-and-string protocol, where Bob produces sentences and\nAlice consumes them.\nIf this problem is so easy to solve, then why do we bring it up? Both the mutual\nexclusion and producer\u2013consumer protocols require waiting : if one participant\nis subjected to an unexpected delay, so is the other. In the context of shared\nmultiprocessor memory, a solution to the readers\u2013writers problem is a way of\nallowing a thread to capture an instantaneous view of several memory locations.\nCapturing such a view without waiting, that is, without preventing other threads\nfrom modifying these locations while they are being read, is a powerful tool", "doc_id": "03178d60-60a0-4113-84c7-321aa5fe42b1", "embedding": null, "doc_hash": "1dde6075e38bdc49f68207e8a80f5f96835ba163abddf6f302359cac26801526", "extra_info": null, "node_info": {"start": 72648, "end": 76307}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0cf54341-c4ef-457d-9d4f-f0140373045d", "3": "2af5f18f-5bac-4a12-ac3a-ad60f4b55eb6"}}, "__type__": "1"}, "2af5f18f-5bac-4a12-ac3a-ad60f4b55eb6": {"__data__": {"text": "and\nAlice consumes them.\nIf this problem is so easy to solve, then why do we bring it up? Both the mutual\nexclusion and producer\u2013consumer protocols require waiting : if one participant\nis subjected to an unexpected delay, so is the other. In the context of shared\nmultiprocessor memory, a solution to the readers\u2013writers problem is a way of\nallowing a thread to capture an instantaneous view of several memory locations.\nCapturing such a view without waiting, that is, without preventing other threads\nfrom modifying these locations while they are being read, is a powerful tool that\ncan be used for backups, debugging, and in many other situations. Surprisingly,\nthe readers\u2013writers problem does have solutions that do notrequire waiting. We\nexamine several such solutions later on.\n1.5 The Harsh Realities of Parallelization 13\n1.5 The Harsh Realities of Parallelization\nHere is why multiprocessor programming is so much fun. In an ideal world,\nupgrading from a uniprocessor to an n-way multiprocessor should provide about\nann-fold increase in computational power. In practice, sadly, this never happens.\nThe primary reason for this is that most real-world computational problems can-\nnot be effectively parallelized without incurring the costs of inter-processor com-\nmunication and coordination.\nConsider \ufb01ve friends who decide to paint a \ufb01ve-room house. If all the rooms\nare the same size, then it makes sense to assign each friend to paint one room, and\nas long as everyone paints at about the same rate, we would get a \ufb01ve-fold speed-\nup over the single-painter case. The task becomes more complicated if the rooms\nare of different sizes. For example, if one room is twice the size of the others,\nthen the \ufb01ve painters will not achieve a \ufb01ve-fold speedup because the overall\ncompletion time is dominated by the one room that takes the longest to paint.\nThis kind of analysis is very important for concurrent computation. The for-\nmula we need is called Amdahl\u2019s Law . It captures the notion that the extent to\nwhich we can speed up any complex job (not just painting) is limited by how\nmuch of the job must be executed sequentially.\nDe\ufb01ne the speedupSof a job to be the ratio between the time it takes one pro-\ncessor to complete the job (as measured by a wall clock) versus the time it takes\nnconcurrent processors to complete the same job. Amdahl\u2019s Law characterizes\nthe maximum speedup Sthat can be achieved by nprocessors collaborating on\nan application, where pis the fraction of the job that can be executed in parallel.\nAssume, for simplicity, that it takes (normalized) time 1 for a single processor to\ncomplete the job. With nconcurrent processors, the parallel part takes time p=n\nand the sequential part takes time 1 \u0000p. Overall, the parallelized computation\ntakes time:\n1\u0000p+p\nn\nAmdahl\u2019s Law says that the speedup, that is, the ratio between the sequential\n(single-processor) time and the parallel time, is:\nS=1\n1\u0000p+p\nn\nT o illustrate the implications of Amdahl\u2019s Law, consider our room-painting\nexample. Assume that each small room is one unit, and the single large room\nis two units. Assigning one painter (processor) per room means that \ufb01ve of six\nunits can be painted in parallel, implying that p= 5=6, and 1\u0000p= 1=6. Amdahl\u2019s\nLaw states that the resulting speedup is:\nS=1\n1\u0000p+p\nn=1\n1=6 + 1=6= 3\n14 Chapter 1 Introduction\nAlarmingly,", "doc_id": "2af5f18f-5bac-4a12-ac3a-ad60f4b55eb6", "embedding": null, "doc_hash": "81ea1e27ab72723144695ba348159c17589d00d016805aea013671a56e1f12e8", "extra_info": null, "node_info": {"start": 76243, "end": 79597}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "03178d60-60a0-4113-84c7-321aa5fe42b1", "3": "f5d971c6-237b-4c71-bdab-95a9ac977abe"}}, "__type__": "1"}, "f5d971c6-237b-4c71-bdab-95a9ac977abe": {"__data__": {"text": "o illustrate the implications of Amdahl\u2019s Law, consider our room-painting\nexample. Assume that each small room is one unit, and the single large room\nis two units. Assigning one painter (processor) per room means that \ufb01ve of six\nunits can be painted in parallel, implying that p= 5=6, and 1\u0000p= 1=6. Amdahl\u2019s\nLaw states that the resulting speedup is:\nS=1\n1\u0000p+p\nn=1\n1=6 + 1=6= 3\n14 Chapter 1 Introduction\nAlarmingly, \ufb01ve painters working on \ufb01ve rooms where one room is twice the size\nof the others yields only a three-fold speedup.\nIt can get worse. Imagine we have ten rooms and ten painters, where each\npainter is assigned to a room, but one room (out of ten) is twice the size of the\nothers. Here is the resulting speedup:\nS=1\n1=11 + 1=11= 5:5\nWith even a small imbalance, applying ten painters to a job yields only a \ufb01ve-fold\nspeedup, roughly half of what one might na \u00a8\u0131vely expect.\nThe solution therefore, as with our earlier prime printing problem, seems to\nbe that as soon as one painter\u2019s work on a room is done, he/she helps others to\npaint the remaining room. The issue of course is that this shared painting of the\nroom will require coordination among painters, but can we afford to avoid it?\nHere is what Amdahl\u2019s Law tells us about the utilization of multiprocessor\nmachines. Some computational problems are \u201cembarrassingly parallel\u201d: they can\neasily be divided into components that can be executed concurrently. Such prob-\nlems sometimes arise in scienti\ufb01c computing or in graphics, but rarely in sys-\ntems. In general, however, for a given problem and a ten-processor machine,\nAmdahl\u2019s Law says that even if we manage to parallelize 90% of the solution,\nbut not the remaining 10%, then we end up with a \ufb01ve-fold speedup, but not a\nten-fold speedup. In other words, the remaining 10% that we did not parallelize\ncut our utilization of the machine in half. It seems worthwhile to invest an effort\nto derive as much parallelism from the remaining 10% as possible, even if it is\ndif\ufb01cult. Typically, it is hard because these additional parallel parts involve sub-\nstantial communication and coordination. Here is a major focus of this book:\nunderstanding the tools and techniques that allow programmers to effectively\nprogram the parts of the code that require coordination and synchronization,\nbecause the gains made on these parts may have a profound impact on perfor-\nmance.\nReturning to the prime number printing program of Fig. 1.2, let us revisit the\nthree main lines of code:\ni = counter.getAndIncrement(); // take next untaken number\nif(isPrime(i))\nprint(i);\nIt would have been simpler to have threads perform these three lines atom-\nically, that is, in a single mutually-exclusive block. Instead, only the call to\ngetAndIncrement () is atomic. This approach makes sense when we consider\nthe implications of Amdahl\u2019s Law: it is important to minimize the granularity\nof sequential code; in this case, the code accessed using mutual exclusion. More-\nover, it is important to implement mutual exclusion in an effective way, since the\ncommunication and coordination around the mutually exclusive shared counter\ncan substantially affect the performance of our program as a whole.\n1.7 Chapter Notes 15\n1.6 Parallel Programming\nFor many of the applications you may wish to parallelize, you will \ufb01nd that\nthere are signi\ufb01cant parts that can", "doc_id": "f5d971c6-237b-4c71-bdab-95a9ac977abe", "embedding": null, "doc_hash": "7cbaef666215df1a941e6d750b2a17b213a4d86e5e401fc39b57f468c26f37c3", "extra_info": null, "node_info": {"start": 79735, "end": 83085}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2af5f18f-5bac-4a12-ac3a-ad60f4b55eb6", "3": "a040f658-1183-4eec-b0d4-4b2dff3551f0"}}, "__type__": "1"}, "a040f658-1183-4eec-b0d4-4b2dff3551f0": {"__data__": {"text": "sense when we consider\nthe implications of Amdahl\u2019s Law: it is important to minimize the granularity\nof sequential code; in this case, the code accessed using mutual exclusion. More-\nover, it is important to implement mutual exclusion in an effective way, since the\ncommunication and coordination around the mutually exclusive shared counter\ncan substantially affect the performance of our program as a whole.\n1.7 Chapter Notes 15\n1.6 Parallel Programming\nFor many of the applications you may wish to parallelize, you will \ufb01nd that\nthere are signi\ufb01cant parts that can easily be determined as executable in paral-\nlel because they do not require any form of coordination or communication.\nHowever, at the time this book is being written, there is no cookbook recipe for\nidentifying these parts. This is where the application designer needs to use his\nor her accumulated understanding of the algorithm being parallelized. Luckily,\nin many cases it is obvious how to \ufb01nd such parts. The more substantial prob-\nlem, the one which this book addresses, is how to deal with the remaining parts\nof the program. As noted earlier, these are the parts that cannot be easily par-\nallelized because the program must access shared data and requires interprocess\ncoordination and communication in an essential way.\nThe goal of this text is to expose the reader to core ideas behind modern coor-\ndination paradigms and concurrent data structures. We present the reader with a\nuni\ufb01ed, comprehensive picture of the elements that are key to effective multipro-\ncessor programming, ranging from basic principles to best-practice engineering\ntechniques.\nMultiprocessor programming poses many challenges, ranging from grand\nintellectual issues to subtle engineering tricks. We tackle these challenges using\nsuccessive re\ufb01nement, starting with an idealized model in which mathematical\nconcerns are paramount, and gradually moving on to more pragmatic models,\nwhere we increasingly focus on basic engineering principles.\nFor example, the \ufb01rst problem we consider is mutual exclusion, the oldest and\nstill one of the most basic problems in the \ufb01eld. We begin with a mathematical\nperspective, analyzing the computability and correctness properties of various\nalgorithms on an idealized architecture. The algorithms themselves, while clas-\nsical, are not practical for modern architectures. Nevertheless, learning how to\nreason about such idealized algorithms is a necessary step toward learning how\nto reason about more realistic (and more complex) algorithms. It is particularly\nimportant to learn how to reason about subtle liveness issues such as starvation\nand deadlock.\nOnce we understand how to reason about such algorithms in general, we\nturn our attention to more realistic contexts. We explore a variety of algorithms\nand data structures using different multiprocessor architectures with the goal of\nunderstanding which are effective, and why.\n1.7 Chapter Notes\nMost of the parable of Alice and Bob is adapted from Leslie Lamport\u2019s invited\naddress to the 1984 ACM Symposium on Principles of Distributed Computing\n[92]. The readers\u2013writers problem is a classical synchronization problem that has\n16 Chapter 1 Introduction\nreceived attention in numerous papers over the past twenty years. Amdahl\u2019s Law\nis due to Gene Amdahl, a parallel processing pioneer [9].\n1.8 Exercises\nExercise 1. The dining philosophers problem was invented by E. W. Dijkstra, a\nconcurrency pioneer, to clarify the notions of deadlock and starvation freedom.\nImagine \ufb01ve philosophers who spend their lives just thinking and feasting. They\nsit around a circular table with \ufb01ve chairs. The table has a big plate of rice. How-\never, there are only \ufb01ve chopsticks (in the original formulation forks)", "doc_id": "a040f658-1183-4eec-b0d4-4b2dff3551f0", "embedding": null, "doc_hash": "1e3b160e23b4845e6a6fd89676e6f32f035ae502a604134a7246af9fcc53ecf2", "extra_info": null, "node_info": {"start": 82953, "end": 86702}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f5d971c6-237b-4c71-bdab-95a9ac977abe", "3": "50eae1fd-7833-4d9a-9571-c7bc61f263f0"}}, "__type__": "1"}, "50eae1fd-7833-4d9a-9571-c7bc61f263f0": {"__data__": {"text": "Law\nis due to Gene Amdahl, a parallel processing pioneer [9].\n1.8 Exercises\nExercise 1. The dining philosophers problem was invented by E. W. Dijkstra, a\nconcurrency pioneer, to clarify the notions of deadlock and starvation freedom.\nImagine \ufb01ve philosophers who spend their lives just thinking and feasting. They\nsit around a circular table with \ufb01ve chairs. The table has a big plate of rice. How-\never, there are only \ufb01ve chopsticks (in the original formulation forks) available,\nas shown in Fig. 1.5. Each philosopher thinks. When he gets hungry, he sits down\nand picks up the two chopsticks that are closest to him. If a philosopher can pick\nup both chopsticks, he can eat for a while. After a philosopher \ufb01nishes eating, he\nputs down the chopsticks and again starts to think.\n1.Write a program to simulate the behavior of the philosophers, where each\nphilosopher is a thread and the chopsticks are shared objects. Notice that you\nmust prevent a situation where two philosophers hold the same chopstick at\nthe same time.\n2.Amend your program so that it never reaches a state where philosophers are\ndeadlocked, that is, it is never the case that each philosopher holds one chop-\nstick and is stuck waiting for another to get the second chopstick.\n3.Amend your program so that no philosopher ever starves.\n4.Write a program to provide a starvation-free solution for any number of\nphilosophers n.\nExercise 2. For each of the following, state whether it is a safety or liveness prop-\nerty. Identify the bad or good thing of interest.\n1.Patrons are served in the order they arrive.\n2.What goes up must come down.\nFigure 1.5 Traditional dining table arrangement according to Dijkstra.\n1.8 Exercises 17\n3.If two or more processes are waiting to enter their critical sections, at least one\nsucceeds.\n4.If an interrupt occurs, then a message is printed within one second.\n5.If an interrupt occurs, then a message is printed.\n6.The cost of living never decreases.\n7.Two things are certain: death and taxes.\n8.Y ou can always tell a Harvard man.\nExercise 3. In the producer\u2013consumer fable, we assumed that Bob can see whether\nthe can on Alice\u2019s windowsill is up or down. Design a producer\u2013consumer pro-\ntocol using cans and strings that works even if Bob cannot see the state of Alice\u2019s\ncan (this is how real-world interrupt bits work).\nExercise 4. Y ou are one of Precently arrested prisoners. The warden, a deranged\ncomputer scientist, makes the following announcement:\nY ou may meet together today and plan a strategy, but after today you will be in\nisolated cells and have no communication with one another.\nI have set up a \u201cswitch room\u201d which contains a light switch, which is either onor\noff. The switch is not connected to anything.\nEvery now and then, I will select one prisoner at random to enter the \u201cswitch\nroom.\u201d This prisoner may throw the switch (from ontooff, or vice-versa), or may\nleave the switch unchanged. Nobody else will ever enter this room.\nEach prisoner will visit the switch room arbitrarily often. More precisely, for any\nN, eventually each of you will visit the switch room at least Ntimes.\nAt any time, any of you may declare: \u201cwe have all visited the switch room at least\nonce.\u201d If the claim is correct, I will set you free. If the claim is incorrect, I will feed\nall of you to the crocodiles. Choose wisely!\n\u0004Devise a winning strategy when you know that the initial state of the", "doc_id": "50eae1fd-7833-4d9a-9571-c7bc61f263f0", "embedding": null, "doc_hash": "5222fe6e45e84f0016ebd52f474e323184397e01adc4d99f09b9a16ce27a359a", "extra_info": null, "node_info": {"start": 86789, "end": 90191}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a040f658-1183-4eec-b0d4-4b2dff3551f0", "3": "abc96849-154b-4b8f-a774-a69a0d414aa3"}}, "__type__": "1"}, "abc96849-154b-4b8f-a774-a69a0d414aa3": {"__data__": {"text": "or vice-versa), or may\nleave the switch unchanged. Nobody else will ever enter this room.\nEach prisoner will visit the switch room arbitrarily often. More precisely, for any\nN, eventually each of you will visit the switch room at least Ntimes.\nAt any time, any of you may declare: \u201cwe have all visited the switch room at least\nonce.\u201d If the claim is correct, I will set you free. If the claim is incorrect, I will feed\nall of you to the crocodiles. Choose wisely!\n\u0004Devise a winning strategy when you know that the initial state of the switch\nisoff.\n\u0004Devise a winning strategy when you do not know whether the initial state of\nthe switch is onoroff.\nHint: not all prisoners need to do the same thing.\nExercise 5. The same warden has a different idea. He orders the prisoners to stand\nin line, and places red and blue hats on each of their heads. No prisoner knows\nthe color of his own hat, or the color of any hat behind him, but he can see the\nhats of the prisoners in front. The warden starts at the back of the line and asks\neach prisoner to guess the color of his own hat. The prisoner can answer only\n\u201cred\u201d or \u201cblue.\u201d If he gives the wrong answer, he is fed to the crocodiles. If he\nanswers correctly, he is freed. Each prisoner can hear the answer of the prisoners\nbehind him, but cannot tell whether that prisoner was correct.\n18 Chapter 1 Introduction\nThe prisoners are allowed to consult and agree on a strategy beforehand (while\nthe warden listens in) but after being lined up, they cannot communicate any\nother way besides their answer of \u201cred\u201d or \u201cblue.\u201d\nDevise a strategy that allows at least P\u00001 ofPprisoners to be freed.\nExercise 6. Use Amdahl\u2019s law to answer the following questions:\n\u0004Suppose the sequential part of a program accounts for 40% of the program\u2019s\nexecution time on a single processor. Find a limit for the overall speedup that\ncan be achieved by running the program on a multiprocessor machine.\n\u0004Now suppose the sequential part accounts for 30% of the program\u2019s compu-\ntation time. Let snbe the program\u2019s speedup on nprocesses, assuming the rest\nof the program is perfectly parallelizable. Y our boss tells you to double this\nspeedup: the revised program should have speedup s0\nn>sn=2. Y ou advertise\nfor a programmer to replace the sequential part with an improved version that\nrunsktimes faster. What value of kshould you require?\n\u0004Suppose the sequential part can be sped up three-fold, and when we do so,\nthe modi\ufb01ed program takes half the time of the original on nprocessors.\nWhat fraction of the overall execution time did the sequential part account\nfor? Express your answer as a function of n.\nExercise 7. Running your application on two processors yields a speedup of S2.\nUse Amdahl\u2019s Law to derive a formula for Sn, the speedup on nprocessors, in\nterms ofnandS2.\nExercise 8. Y ou have a choice between buying one uniprocessor that executes\n\ufb01ve zillion instructions per second, or a ten-processor multiprocessor where\neach processor executes one zillion instructions per second. Using Amdahl\u2019s Law,\nexplain how you would decide which to buy for a particular application.\nIPrinciples\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n2Mutual Exclusion\nMutual exclusion is perhaps the most prevalent form of coordination in multi-\nprocessor programming. This chapter covers classical mutual exclusion\nalgorithms that work by reading", "doc_id": "abc96849-154b-4b8f-a774-a69a0d414aa3", "embedding": null, "doc_hash": "81a8c6e461ec5283bc30caed427a9f34c77a81e3c0ad06b5a613ca4b65293945", "extra_info": null, "node_info": {"start": 90149, "end": 93567}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "50eae1fd-7833-4d9a-9571-c7bc61f263f0", "3": "9f170370-8067-4165-b72f-f38b732b3837"}}, "__type__": "1"}, "9f170370-8067-4165-b72f-f38b732b3837": {"__data__": {"text": "zillion instructions per second, or a ten-processor multiprocessor where\neach processor executes one zillion instructions per second. Using Amdahl\u2019s Law,\nexplain how you would decide which to buy for a particular application.\nIPrinciples\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n2Mutual Exclusion\nMutual exclusion is perhaps the most prevalent form of coordination in multi-\nprocessor programming. This chapter covers classical mutual exclusion\nalgorithms that work by reading and writing shared memory. Although these\nalgorithms are not used in practice, we study them because they provide an ideal\nintroduction to the kinds of algorithmic and correctness issues that arise in every\narea of synchronization. The chapter also provides an impossibility proof. This\nproof teaches us the limitations of solutions to mutual exclusion that work by\nreading and writing shared memory, and will help to motivate the real-world\nmutual exclusion algorithms that appear in later chapters. This chapter is one of\nthe few that contains proofs of algorithms. Though the reader should feel free\nto skip these proofs, it is very helpful to understand the kind of reasoning they\npresent, because we can use the same approach to reason about the practical\nalgorithms considered in later chapters.\n2.1 Time\nReasoning about concurrent computation is mostly reasoning about time. Some-\ntimes we want things to happen simultaneously, and sometimes we want them\nto happen at different times. We need to reason about complicated conditions\ninvolving how multiple time intervals can overlap, or, sometimes, how they can-\nnot. We need a simple but unambiguous language to talk about events and dura-\ntions in time. Everyday English is too ambiguous and imprecise. Instead, we\nintroduce a simple vocabulary and notation to describe how concurrent threads\nbehave in time.\nIn 1689, Isaac Newton stated \u201cabsolute, true and mathematical time, of itself\nand from its own nature, \ufb02ows equably without relation to anything external.\u201d\nWe endorse his notion of time, if not his prose style. Threads share a common\ntime (though not necessarily a common clock). A thread is a state machine , and\nits state transitions are called events .\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00002-2\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.21\n22 Chapter 2 Mutual Exclusion\nEvents are instantaneous : they occur at a single instant of time. It is conve-\nnient to require that events are never simultaneous: distinct events occur at dis-\ntinct times. (As a practical matter, if we are unsure about the order of two events\nthat happen very close in time, then any order will do.) A thread Aproduces a\nsequence of events a0,a1,:::Threads typically contain loops, so a single program\nstatement can produce many events. We denote the j-th occurrence of an event\naibyaj\ni. One eventaprecedes another event b, writtena!b, ifaoccurs at an\nearlier time. The precedence relation \u201c!\u201d is a total order on events.\nLeta0anda1be events such that a0!a1. An interval (a0,a1) is the duration\nbetweena0anda1. IntervalIA= (a0,a1)precedesIB= (b0,b1), writtenIA!IB,\nifa1!b0(that is, if the \ufb01nal event of IAprecedes the starting event of IB).\nMore succinctly, the !relation is a partial order on intervals. Intervals that are\nunrelated by!are said to be concurrent . By analogy with events, we denote the\nj-th execution of interval IAbyIj\nA.\n2.2", "doc_id": "9f170370-8067-4165-b72f-f38b732b3837", "embedding": null, "doc_hash": "5b25a8c77dda4218598c5bf76b5e518d043123e65848558f67ca498c8e041589", "extra_info": null, "node_info": {"start": 93537, "end": 97012}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "abc96849-154b-4b8f-a774-a69a0d414aa3", "3": "48a9d031-6c57-4b79-a2e4-672389855a5b"}}, "__type__": "1"}, "48a9d031-6c57-4b79-a2e4-672389855a5b": {"__data__": {"text": "events such that a0!a1. An interval (a0,a1) is the duration\nbetweena0anda1. IntervalIA= (a0,a1)precedesIB= (b0,b1), writtenIA!IB,\nifa1!b0(that is, if the \ufb01nal event of IAprecedes the starting event of IB).\nMore succinctly, the !relation is a partial order on intervals. Intervals that are\nunrelated by!are said to be concurrent . By analogy with events, we denote the\nj-th execution of interval IAbyIj\nA.\n2.2 Critical Sections\nIn an earlier chapter, we discussed the Counter class implementation shown in\nFig. 2.1. We observed that this implementation is correct in a single-thread sys-\ntem, but misbehaves when used by two or more threads. The problem occurs if\nboth threads read the value \ufb01eld at the line marked \u201cstart of danger zone,\u201d and\nthen both update that \ufb01eld at the line marked \u201cend of danger zone.\u201d\nWe can avoid this problem if we transform these two lines into a critical section :\na block of code that can be executed by only one thread at a time. We call this\nproperty mutual exclusion . The standard way to approach mutual exclusion is\nthrough a Lock object satisfying the interface shown in Fig. 2.2.\nFor brevity, we say a thread acquires (alternately locks ) a lock when it executes\nalock () method call, and releases (alternately unlocks ) the lock when it executes\n1class Counter {\n2 private int value;\n3 public Counter( int c) { // constructor\n4 value = c;\n5 }\n6 // increment and return prior value\n7 public int getAndIncrement() {\n8 int temp = value; // start of danger zone\n9 value = temp + 1; // end of danger zone\n10 return temp;\n11 }\n12 }\nFigure 2.1 TheCounter class.\n2.2 Critical Sections 23\n1public interface Lock {\n2 public void lock(); // before entering critical section\n3 public void unlock(); // before leaving critical section\n4}\nFigure 2.2 TheLock interface.\n1public class Counter {\n2 private long value;\n3 private Lock lock; // to protect critical section\n4\n5 public long getAndIncrement() {\n6 lock.lock(); // enter critical section\n7 try {\n8 long temp = value; // in critical section\n9 value = temp + 1; // in critical section\n10 return temp;\n11 }finally {\n12 lock.unlock(); // leave critical section\n13 }\n14 }\n15 }\nFigure 2.3 Using a lock object.\nanunlock () method call. Fig. 2.3 shows how to use a Lock \ufb01eld to add mutual\nexclusion to a shared counter implementation. Threads using the lock () and\nunlock () methods must follow a speci\ufb01c format. A thread is well formed if:\n1.each critical section is associated with a unique Lock object,\n2.the thread calls that object\u2019s lock () method when it is trying to enter the crit-\nical section, and\n3.the thread calls the unlock () method when it leaves the critical section.\nPragma 2.2.1. In Java, these methods should be used in the following struc-\ntured way.\n1mutex.lock();\n2try {\n3 ... // body\n4 }finally {\n5 mutex.unlock();\n6 }\nThis idiom ensures that the lock is acquired before entering the try block,\nand that the lock is released when control leaves the block, even if some state-\nment in the block throws an unexpected exception.\n24 Chapter 2 Mutual Exclusion\nWe now formalize the properties that a good Lock algorithm should satisfy.\nLetCSj\nAbe the interval during which Aexecutes the critical section for the\nj-th time. Let us assume, for simplicity, that each thread repeatedly acquires and\nreleases the lock, with other work taking place", "doc_id": "48a9d031-6c57-4b79-a2e4-672389855a5b", "embedding": null, "doc_hash": "ed350908e22cd5e252816fe3fe6c7f70b8819c24482cf4091cb21e02921b8d11", "extra_info": null, "node_info": {"start": 97134, "end": 100469}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9f170370-8067-4165-b72f-f38b732b3837", "3": "96a3c2e2-6bc3-4e12-91a6-0ccc3f61ad12"}}, "__type__": "1"}, "96a3c2e2-6bc3-4e12-91a6-0ccc3f61ad12": {"__data__": {"text": "}finally {\n5 mutex.unlock();\n6 }\nThis idiom ensures that the lock is acquired before entering the try block,\nand that the lock is released when control leaves the block, even if some state-\nment in the block throws an unexpected exception.\n24 Chapter 2 Mutual Exclusion\nWe now formalize the properties that a good Lock algorithm should satisfy.\nLetCSj\nAbe the interval during which Aexecutes the critical section for the\nj-th time. Let us assume, for simplicity, that each thread repeatedly acquires and\nreleases the lock, with other work taking place in the meantime.\nMutual Exclusion Critical sections of different threads do not overlap. For\nthreadsAandB, and integers jandk, either CSk\nA!CSj\nBorCSj\nB!CSk\nA.\nFreedom from Deadlock If some thread attempts to acquire the lock, then some\nthread will succeed in acquiring the lock. If thread Acalls lock () but never\nacquires the lock, then other threads must be completing an in\ufb01nite number\nof critical sections.\nFreedom from Starvation Every thread that attempts to acquire the lock even-\ntually succeeds. Every call to lock () eventually returns. This property is some-\ntimes called lockout freedom .\nNote that starvation freedom implies deadlock freedom.\nThe mutual exclusion property is clearly essential. Without this property, we\ncannot guarantee that a computation\u2019s results are correct. In the terminology of\nChapter 1, mutual exclusion is a safety property. The deadlock-freedom prop-\nerty is important. It implies that the system never \u201cfreezes.\u201d Individual threads\nmay be stuck forever (called starvation ), but some thread makes progress. In the\nterminology of Chapter 1, deadlock-freedom is a liveness property. Note that a\nprogram can still deadlock even if each of the locks it uses satis\ufb01es the deadlock-\nfreedom property. For example, consider threads AandBthat share locks `0\nand`1. First,Aacquires`0andBacquires`1. Next,Atries to acquire `1andB\ntries to acquire `0. The threads deadlock because each one waits for the other to\nrelease its lock.\nThe starvation-freedom property, while clearly desirable, is the least com-\npelling of the three. Later on, we will see practical mutual exclusion algorithms\nthat fail to be starvation-free. These algorithms are typically deployed in cir-\ncumstances where starvation is a theoretical possibility, but is unlikely to occur\nin practice. Nevertheless, the ability to reason about starvation is essential for\nunderstanding whether it is a realistic threat.\nThe starvation-freedom property is also weak in the sense that there is no\nguarantee for how long a thread waits before it enters the critical section. Later\non, we will look at algorithms that place bounds on how long a thread can wait.\n2.3 2-Thread Solutions\nWe begin with two inadequate but interesting Lock algorithms.\n2.3 2-Thread Solutions 25\n2.3.1 The LockOne Class\nFig. 2.4 shows the LockOne algorithm. Our 2-thread lock algorithms follow the\nfollowing conventions: the threads have ids 0 and 1, the calling thread has i, and\nthe otherj= 1\u0000i. Each thread acquires its index by calling ThreadID.get() .\nPragma 2.3.1. In practice, the Boolean flag variables in Fig. 2.4 , as well\nas the victim andlabel variables in later algorithms must all be declared\nvolatile to work properly. We explain the reasons in Chapter 1 and\nAppendix B . We will begin declaring the appropriate variables as volatile in\nChapter 7 .\nWe use write A(x=v) to denote the event in which Aassigns value vto \ufb01eld\nx, and read A(x==v) to denote the event", "doc_id": "96a3c2e2-6bc3-4e12-91a6-0ccc3f61ad12", "embedding": null, "doc_hash": "f210f8ec2b755d0e577362546bcff4b546cd978497402f9ac9c4f8a0576be6eb", "extra_info": null, "node_info": {"start": 100353, "end": 103850}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "48a9d031-6c57-4b79-a2e4-672389855a5b", "3": "8b1afc1d-a331-4185-bb9b-6f7925d70f4f"}}, "__type__": "1"}, "8b1afc1d-a331-4185-bb9b-6f7925d70f4f": {"__data__": {"text": "Each thread acquires its index by calling ThreadID.get() .\nPragma 2.3.1. In practice, the Boolean flag variables in Fig. 2.4 , as well\nas the victim andlabel variables in later algorithms must all be declared\nvolatile to work properly. We explain the reasons in Chapter 1 and\nAppendix B . We will begin declaring the appropriate variables as volatile in\nChapter 7 .\nWe use write A(x=v) to denote the event in which Aassigns value vto \ufb01eld\nx, and read A(x==v) to denote the event in which Areadsvfrom \ufb01eldx. Some-\ntimes we omit vwhen the value is unimportant. For example, in Fig. 2.4 the event\nwriteA(flag [i] =true) is caused by Line 7of the lock () method.\nLemma 2.3.1. The LockOne algorithm satis\ufb01es mutual exclusion.\nProof: Suppose not. Then there exist integers jandksuch that CSj\nA6!CSk\nBand\nCSk\nB6!CSj\nA. Consider each thread\u2019s last execution of the lock () method before\nentering itsk-th (j-th) critical section.\nInspecting the code, we see that\nwriteA(flag [A] =true)!readA(flag [B] == false)!CSA (2.3.1)\nwriteB(flag [B] =true)!readB(flag [A] == false)!CSB (2.3.2)\nreadA(flag [B] == false)!writeB(flag [B] =true)(2.3.3)\nNote that once flag [B] is set to true it remains true. It follows that Eq. 2.3.3\nholds, since otherwise thread Acould not have read flag [B] as false .Eq. 2.3.4\nfollows from Eqs. 2.3.1 \u20132.3.3 , and the transitivity of the precedence order.\n1class LockOne implements Lock {\n2 private boolean [] flag = new boolean [2];\n3 // thread-local index, 0 or 1\n4 public void lock() {\n5 int i = ThreadID.get();\n6 int j = 1 - i;\n7 flag[i] = true ;\n8 while (flag[j]) {} // wait\n9 }\n10 public void unlock() {\n11 int i = ThreadID.get();\n12 flag[i] = false ;\n13 }\n14 }\nFigure 2.4 TheLockOne algorithm.\n26 Chapter 2 Mutual Exclusion\nwriteA(flag [A] =true)!readA(flag [B] == false)! (2.3.4)\nwriteB(flag [B] =true)!readB(flag [A] == false)\nIt follows that write A(flag [A] =true)!readB(flag [A] == false) without an\nintervening write to the flag [] array, a contradiction. 2\nTheLockOne algorithm is inadequate because it deadlocks if thread executions\nare interleaved. If write A(flag [A] =true) and write B(flag [B] =true) events\noccur before read A(flag [B]) and read B(flag [A]) events, then both threads wait\nforever. Nevertheless, LockOne has an interesting property: if one thread runs\nbefore the other, no deadlock occurs, and all is well.\n2.3.2 The LockTwo Class\nFig. 2.5 shows an alternative lock algorithm, the LockTwo class.\nLemma 2.3.2. The LockTwo algorithm satis\ufb01es mutual exclusion.\nProof: Suppose not. Then there exist integers jandksuch that CSj\nA6!CSk\nBand\nCSk\nB6!CSj\nA. Consider as before each thread\u2019s last execution of the lock () method\nbefore entering its k-th (j-th) critical section.\nInspecting the code, we see that\nwriteA(victim", "doc_id": "8b1afc1d-a331-4185-bb9b-6f7925d70f4f", "embedding": null, "doc_hash": "9bf57224e2197128e684ce56c25ed09c4cc672d106f0cbe58f13418abc1ce35d", "extra_info": null, "node_info": {"start": 103921, "end": 106686}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "96a3c2e2-6bc3-4e12-91a6-0ccc3f61ad12", "3": "bede48f3-c4ea-414d-ba64-6988bf1ad565"}}, "__type__": "1"}, "bede48f3-c4ea-414d-ba64-6988bf1ad565": {"__data__": {"text": "and all is well.\n2.3.2 The LockTwo Class\nFig. 2.5 shows an alternative lock algorithm, the LockTwo class.\nLemma 2.3.2. The LockTwo algorithm satis\ufb01es mutual exclusion.\nProof: Suppose not. Then there exist integers jandksuch that CSj\nA6!CSk\nBand\nCSk\nB6!CSj\nA. Consider as before each thread\u2019s last execution of the lock () method\nbefore entering its k-th (j-th) critical section.\nInspecting the code, we see that\nwriteA(victim =A)!readA(victim ==B)!CSA (2.3.5)\nwriteB(victim =B)!readB(victim ==A)!CSB (2.3.6)\nThreadBmust assignBto the victim \ufb01eld between events write A(victim =A)\nand read A(victim =B) (see Eq. 2.3.5). Since this assignment is the last, we have\nwriteA(victim =A)!writeB(victim =B)!readA(victim ==B): (2.3.7)\nOnce the victim \ufb01eld is set to B, it does not change, so any subsequent read\nreturnsB, contradicting Eq. 2.3.6. 2\n1class LockTwo implements Lock {\n2 private int victim;\n3 public void lock() {\n4 int i = ThreadID.get();\n5 victim = i; // let the other go first\n6 while (victim == i) {} // wait\n7 }\n8 public void unlock() {}\n9}\nFigure 2.5 TheLockTwo algorithm.\n2.3 2-Thread Solutions 27\nThe LockTwo class is inadequate because it deadlocks if one thread runs\ncompletely ahead of the other. Nevertheless, LockTwo has an interesting prop-\nerty: if the threads run concurrently, the lock () method succeeds. The LockOne\nandLockTwo classes complement one another: each succeeds under conditions\nthat cause the other to deadlock.\n2.3.3 The Peterson Lock\nWe now combine the LockOne andLockTwo algorithms to construct a starvation-\nfree Lock algorithm, shown in Fig. 2.6. This algorithm is arguably the most\nsuccinct and elegant two-thread mutual exclusion algorithm. It is known as\n\u201cPeterson\u2019s Algorithm,\u201d after its inventor.\nLemma 2.3.3. The Peterson lock algorithm satis\ufb01es mutual exclusion.\nProof: Suppose not. As before, consider the last executions of the lock () method\nby threadsAandB. Inspecting the code, we see that\nwriteA(flag [A] =true)! (2.3.8)\nwriteA(victim =A)!readA(flag [B])!readA(victim )!CSA\nwriteB(flag [B] =true)! (2.3.9)\nwriteB(victim =B)!readB(flag [A])!readB(victim )!CSB\nAssume, without loss of generality, that Awas the last thread to write to the\nvictim \ufb01eld.\nwriteB(victim =B)!writeA(victim =A) (2.3.10)\n1class Peterson implements Lock {\n2 // thread-local index, 0 or 1\n3 private boolean [] flag = new boolean [2];\n4 private int victim;\n5 public void lock() {\n6 int i = ThreadID.get();\n7 int j = 1 - i;\n8 flag[i] = true ; // I\u2019m interested\n9 victim = i; // you go first\n10 while (flag[j] && victim == i) {}; // wait\n11 }\n12 public void unlock() {\n13 int i = ThreadID.get();\n14 flag[i] = false ; // I\u2019m not interested\n15 }\n16 }\nFigure 2.6 ThePeterson lock algorithm.\n28 Chapter 2", "doc_id": "bede48f3-c4ea-414d-ba64-6988bf1ad565", "embedding": null, "doc_hash": "92f0455ac7c2774be3796354939df718683df7e0b2a4cf567431fe6d5f79a496", "extra_info": null, "node_info": {"start": 106717, "end": 109438}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8b1afc1d-a331-4185-bb9b-6f7925d70f4f", "3": "e19bf402-40c5-4333-bc2c-642de947e6af"}}, "__type__": "1"}, "e19bf402-40c5-4333-bc2c-642de947e6af": {"__data__": {"text": "private boolean [] flag = new boolean [2];\n4 private int victim;\n5 public void lock() {\n6 int i = ThreadID.get();\n7 int j = 1 - i;\n8 flag[i] = true ; // I\u2019m interested\n9 victim = i; // you go first\n10 while (flag[j] && victim == i) {}; // wait\n11 }\n12 public void unlock() {\n13 int i = ThreadID.get();\n14 flag[i] = false ; // I\u2019m not interested\n15 }\n16 }\nFigure 2.6 ThePeterson lock algorithm.\n28 Chapter 2 Mutual Exclusion\nEquation 2.3.10 implies that Aobserved victim to beAin Eq. 2.3.8. Since A\nnevertheless entered its critical section, it must have observed flag [B] to be false ,\nso we have\nwriteA(victim =A)!readA(flag [B] == false) (2.3.11)\nEqs. 2.3.9\u20132.3.11, together with the transitivity of !, imply Eq. 2.3.12.\nwriteB(flag [B] =true)!writeB(victim =B)!\nwriteA(victim =A)!readA(flag [B] == false) (2.3.12)\nIt follows that write B(flag [B] =true)!readA(flag [B] == false). This obser-\nvation yields a contradiction because no other write to flag [B] was performed\nbefore the critical section executions. 2\nLemma 2.3.4. The Peterson lock algorithm is starvation-free.\nProof: Suppose not. Suppose (without loss of generality) that Aruns forever\nin the lock () method. It must be executing the while statement, waiting until\neither flag [B] becomes false orvictim is set toB.\nWhat isBdoing whileAfails to make progress? Perhaps Bis repeatedly enter-\ning and leaving its critical section. If so, however, then Bsetsvictim toBas soon\nas it reenters the critical section. Once victim is set toB, it does not change, and\nAmust eventually return from the lock () method, a contradiction.\nSo it must be that Bis also stuck in its lock () method call, waiting until either\nflag [A] becomes false orvictim is set toA. But victim cannot be both Aand\nB, a contradiction. 2\nCorollary 2.3.1. ThePeterson lock algorithm is deadlock-free.\n2.4 The Filter Lock\nWe now consider two mutual exclusion protocols that work for nthreads, where\nnis greater than 2. The \ufb01rst solution, the Filter lock, is a direct generalization\nof the Peterson lock to multiple threads. The second solution, the Bakery lock,\nis perhaps the simplest and best known n-thread solution.\nTheFilter lock, shown in Fig. 2.7, creates n\u00001 \u201cwaiting rooms,\u201d called levels ,\nthat a thread must traverse before acquiring the lock. The levels are depicted in\nFig. 2.8. Levels satisfy two important properties:\n\u0004At least one thread trying to enter level `succeeds.\n\u0004If more than one thread is trying to enter level `, then at least one is blocked\n(i.e., continues to wait at that level).\n2.4 The Filter Lock 29\n1class Filter implements Lock {\n2 int[] level;\n3 int[] victim;\n4 public Filter( int n) {\n5 level = new int [n];\n6 victim = new int [n]; // use 1..n-1\n7 for (int i = 0; i < n; i++) {\n8 level[i] = 0;\n9 }\n10 }\n11 public void lock() {\n12 int me = ThreadID.get();\n13 for (int i = 1; i < n; i++) { // attempt level i\n14 level[me] = i;\n15 victim[i] =", "doc_id": "e19bf402-40c5-4333-bc2c-642de947e6af", "embedding": null, "doc_hash": "15087bb44cd28b38d2bb9438df11668e0d8c13f9fd9767cbcdc31ad0d4cdc40c", "extra_info": null, "node_info": {"start": 109476, "end": 112382}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "bede48f3-c4ea-414d-ba64-6988bf1ad565", "3": "e6adb1bf-306f-4fa4-99e4-0e7758ab7dd7"}}, "__type__": "1"}, "e6adb1bf-306f-4fa4-99e4-0e7758ab7dd7": {"__data__": {"text": "The Filter Lock 29\n1class Filter implements Lock {\n2 int[] level;\n3 int[] victim;\n4 public Filter( int n) {\n5 level = new int [n];\n6 victim = new int [n]; // use 1..n-1\n7 for (int i = 0; i < n; i++) {\n8 level[i] = 0;\n9 }\n10 }\n11 public void lock() {\n12 int me = ThreadID.get();\n13 for (int i = 1; i < n; i++) { // attempt level i\n14 level[me] = i;\n15 victim[i] = me;\n16 // spin while conflicts exist\n17 while ((9k != me) (level[k] >= i && victim[i] == me)) {};\n18 }\n19 }\n20 public void unlock() {\n21 int me = ThreadID.get();\n22 level[me] = 0;\n23 }\n24 }\nFigure 2.7 TheFilter lock algorithm.\nCSl 5 0\nl 5 1non-CS with n threads \nl 5 n22\nl 5 n21n21 threads\nn22 threads\n2 threadsl 5 2\nFigure 2.8 There are n\u00001 levels threads pass through, the last of which is the critical section.\nThere are at most nthreads that pass concurrently into level 0, n\u00001 into level 1 (a thread in\nlevel 1 is already in level 0), n\u00002 into level 2 and so on, so that only one enters the critical\nsection at level n\u00001.\nThe Peterson lock uses a two-element boolean flag array to indicate\nwhether a thread is trying to enter the critical section. The Filter lock gener-\nalizes this notion with an n-element integer level [] array, where the value of\nlevel [A] indicates the highest level that thread Ais trying to enter. Each thread\nmust pass through n\u00001 levels of \u201cexclusion\u201d to enter its critical section. Each\nlevel`has a distinct victim [`] \ufb01eld used to \u201c\ufb01lter out\u201d one thread, excluding it\nfrom the next level.\n30 Chapter 2 Mutual Exclusion\nInitially a thread Ais at level 0. We say that Ais at leveljforj>0, when it last\ncompletes the waiting loop in Line 17with level [A]>j. (So a thread at level j\nis also at level j\u00001, and so on.)\nLemma 2.4.1. Forjbetween 0 and n\u00001, there are at most n\u0000jthreads at\nlevelj.\nProof: By induction on j. The base case, where j= 0, is trivial. For the induction\nstep, the induction hypothesis implies that there are at most n\u0000j+ 1 threads at\nlevelj\u00001. T o show that at least one thread cannot progress to level j, we argue\nby contradiction: assume there are n\u0000j+ 1 threads at level j.\nLetAbe the last thread at level jto write to victim [j]. BecauseAis last, for\nany otherBat levelj:\nwriteB(victim [j])!writeA(victim [j]):\nInspecting the code, we see that Bwrites level [B] before it writes to victim [j], so\nwriteB(level [B] =j)!writeB(victim [j])!writeA(victim [j]):\nInspecting the code, we see that Areads level [B] after it writes to victim [j], so\nwriteB(level [B] =j)!writeB(victim [j])!writeA(victim [j])!readA(level [B]):\nBecauseBis at levelj, every time Areads level [B], it observes a value greater\nthan or equal to j, implying that Acould not have completed its waiting loop in\nLine 17, a contradiction. 2\nEntering the critical section is equivalent to entering level n\u00001.\nCorollary 2.4.1. TheFilter lock algorithm satis\ufb01es mutual", "doc_id": "e6adb1bf-306f-4fa4-99e4-0e7758ab7dd7", "embedding": null, "doc_hash": "6c22f9d90773a7731fbd56c1067495e4f2f343be62b80981e4ff36bf4ac58fd6", "extra_info": null, "node_info": {"start": 112426, "end": 115273}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e19bf402-40c5-4333-bc2c-642de947e6af", "3": "a54e9163-3e8b-4372-80cd-8d806574a92a"}}, "__type__": "1"}, "a54e9163-3e8b-4372-80cd-8d806574a92a": {"__data__": {"text": "level [B] after it writes to victim [j], so\nwriteB(level [B] =j)!writeB(victim [j])!writeA(victim [j])!readA(level [B]):\nBecauseBis at levelj, every time Areads level [B], it observes a value greater\nthan or equal to j, implying that Acould not have completed its waiting loop in\nLine 17, a contradiction. 2\nEntering the critical section is equivalent to entering level n\u00001.\nCorollary 2.4.1. TheFilter lock algorithm satis\ufb01es mutual exclusion.\nLemma 2.4.2. The Filter lock algorithm is starvation-free.\nProof: We argue by reverse induction on the levels. The base case, level n\u00001, is\ntrivial, because it contains at the most one thread. For the induction hypothesis,\nassume that every thread that reaches level j+ 1 or higher, eventually enters (and\nleaves) its critical section.\nSupposeAis stuck at level j. Eventually, by the induction hypothesis, there\nare no threads at higher levels. Once Asetslevel [A] toj, then any thread at\nlevelj\u00001 that subsequently reads level [A] is prevented from entering level j.\nEventually, no more threads enter level jfrom lower levels. All threads stuck at\nleveljare in the waiting loop at Line 17, and the values of the victim andlevel\n\ufb01elds no longer change.\nWe now argue by induction on the number of threads stuck at level j. For the\nbase case, ifAis the only thread at level jor higher, then clearly it will enter level\nj+ 1. For the induction hypothesis, we assume that fewer than kthreads cannot\nbe stuck at level j. Suppose threads AandBare stuck at level j.Ais stuck as\n2.6 Lamport\u2019s Bakery Algorithm 31\nlong as it reads victim [j] =A, andBis stuck as long as it reads victim [j] =B.\nThevictim \ufb01eld is unchanging, and it cannot be equal to both AandB, so one\nof the two threads will enter level j+ 1, reducing the number of stuck threads to\nk\u00001, contradicting the induction hypothesis. 2\nCorollary 2.4.2. TheFilter lock algorithm is deadlock-free.\n2.5 Fairness\nThe starvation-freedom property guarantees that every thread that calls lock ()\neventually enters the critical section, but it makes no guarantees about how long\nthis may take. Ideally (and very informally) if Acalls lock () beforeB, thenA\nshould enter the critical section before B. Unfortunately, using the tools at hand\nwe cannot determine which thread called lock () \ufb01rst. Instead, we split the lock ()\nmethod into two sections of code (with corresponding execution intervals):\n1.Adoorway section, whose execution interval DAconsists of a bounded num-\nber of steps, and\n2.awaiting section, whose execution interval WAmay take an unbounded num-\nber of steps.\nThe requirement that the doorway section always \ufb01nish in a bounded number of\nsteps is a strong requirement. We will call this requirement the bounded wait-free\nprogress property. We discuss systematic ways of providing this property in later\nchapters.\nHere is how we de\ufb01ne fairness.\nDe\ufb01nition 2.5.1. A lock is \ufb01rst-come-\ufb01rst-served if, whenever, thread A\ufb01nishes\nits doorway before thread Bstarts its doorway, then Acannot be overtaken by B:\nIfDj\nA!Dk\nB, then CSj\nA!CSk\nB.\nfor threadsAandBand integers jandk.\n2.6 Lamport\u2019s Bakery", "doc_id": "a54e9163-3e8b-4372-80cd-8d806574a92a", "embedding": null, "doc_hash": "a1781e42700ebc3969be2dded880fb2831b9e11e694607a99afbbc458cc24b85", "extra_info": null, "node_info": {"start": 115191, "end": 118287}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e6adb1bf-306f-4fa4-99e4-0e7758ab7dd7", "3": "3d16e30f-f1d6-435a-a9d2-52cf15c240b3"}}, "__type__": "1"}, "3d16e30f-f1d6-435a-a9d2-52cf15c240b3": {"__data__": {"text": "property. We discuss systematic ways of providing this property in later\nchapters.\nHere is how we de\ufb01ne fairness.\nDe\ufb01nition 2.5.1. A lock is \ufb01rst-come-\ufb01rst-served if, whenever, thread A\ufb01nishes\nits doorway before thread Bstarts its doorway, then Acannot be overtaken by B:\nIfDj\nA!Dk\nB, then CSj\nA!CSk\nB.\nfor threadsAandBand integers jandk.\n2.6 Lamport\u2019s Bakery Algorithm\nThe Bakery lock algorithm appears in Fig. 2.9. It maintains the \ufb01rst-come-\n\ufb01rst-served property by using a distributed version of the number-dispensing\nmachines often found in bakeries: each thread takes a number in the doorway,\nand then waits until no thread with an earlier number is trying to enter it.\nIn the Bakery lock, flag [A] is a Boolean \ufb02ag indicating whether Awants to\nenter the critical section, and label [A] is an integer that indicates the thread\u2019s\nrelative order when entering the bakery, for each thread A.\n32 Chapter 2 Mutual Exclusion\n1class Bakery implements Lock {\n2 boolean [] flag;\n3 Label[] label;\n4 public Bakery ( int n) {\n5 flag = new boolean [n];\n6 label = new Label[n];\n7 for (int i = 0; i < n; i++) {\n8 flag[i] = false ; label[i] = 0;\n9 }\n10 }\n11 public void lock() {\n12 int i = ThreadID.get();\n13 flag[i] = true ;\n14 label[i] = max(label[0], ...,label[n-1]) + 1;\n15 while ((9k != i)(flag[k] && (label[k],k) << (label[i],i))) {};\n16 }\n17 public void unlock() {\n18 flag[ThreadID.get()] = false ;\n19 }\n20 }\nFigure 2.9 The Bakery lock algorithm.\nEach time a thread acquires a lock, it generates a new label [] in two steps.\nFirst, it reads all the other threads\u2019 labels in any order. Second, it reads all the\nother threads\u2019 labels one after the other (this can be done in some arbitrary order)\nand generates a label greater by one than the maximal label it read. We call the\ncode from the raising of the \ufb02ag (Line 13) to the writing of the new label []\n(Line 14) the doorway . It establishes that thread\u2019s order with respect to the other\nthreads trying to acquire the lock. If two threads execute their doorways concur-\nrently, they may read the same maximal label and pick the same new label. T o\nbreak this symmetry, the algorithm uses a lexicographical ordering <<on pairs\noflabel [] and thread ids:\n(label [i],i)<<(label [j],j))\nif and only if (2.6.13)\nlabel [i]<label [j] or label [i] =label [j] andi<j:\nIn the waiting part of the Bakery algorithm (Line 15), a thread repeatedly rereads\nthe labels one after the other in some arbitrary order until it determines that no\nthread with a raised \ufb02ag has a lexicographically smaller label/id pair.\nSince releasing a lock does not reset the label [], it is easy to see that each\nthread\u2019s labels are strictly increasing. Interestingly, in both the doorway and wait-\ning sections, threads read the labels asynchronously and in an arbitrary order, so\nthat the set of labels seen prior to picking a new one may have never existed in\nmemory at the same time. Nevertheless, the algorithm works.\nLemma 2.6.1. The Bakery lock algorithm is", "doc_id": "3d16e30f-f1d6-435a-a9d2-52cf15c240b3", "embedding": null, "doc_hash": "aebbcf240c8753f3a8ed01fb355f19cba1fbfc0889b8366ee346092c4b37819f", "extra_info": null, "node_info": {"start": 118346, "end": 121322}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a54e9163-3e8b-4372-80cd-8d806574a92a", "3": "2069422a-c464-4477-aea9-b9dc816eb2e1"}}, "__type__": "1"}, "2069422a-c464-4477-aea9-b9dc816eb2e1": {"__data__": {"text": "arbitrary order until it determines that no\nthread with a raised \ufb02ag has a lexicographically smaller label/id pair.\nSince releasing a lock does not reset the label [], it is easy to see that each\nthread\u2019s labels are strictly increasing. Interestingly, in both the doorway and wait-\ning sections, threads read the labels asynchronously and in an arbitrary order, so\nthat the set of labels seen prior to picking a new one may have never existed in\nmemory at the same time. Nevertheless, the algorithm works.\nLemma 2.6.1. The Bakery lock algorithm is deadlock-free.\n2.7 Bounded Timestamps 33\nProof: Some waiting thread Ahas the unique least ( label [A],A) pair, and that\nthread never waits for another thread. 2\nLemma 2.6.2. The Bakery lock algorithm is \ufb01rst-come-\ufb01rst-served.\nProof: IfA\u2019s doorway precedes B\u2019s,DA!DB, thenA\u2019s label is smaller since\nwriteA(label [A])!readB(label [A])!writeB(label [B])!readB(flag [A]),\nsoBis locked out while flag [A] istrue. 2\nNote that any algorithm that is both deadlock-free and \ufb01rst-come-\ufb01rst-served\nis also starvation-free.\nLemma 2.6.3. The Bakery algorithm satis\ufb01es mutual exclusion.\nProof: Suppose not. Let AandBbe two threads concurrently in the critical sec-\ntion. Let labelingAand labelingBbe the last respective sequences of acquiring\nnew labels prior to entering the critical section. Suppose that ( label [A],A)<<\n(label [B],B). WhenBsuccessfully completed the test in its waiting section, it\nmust have read that flag [A] was false or that ( label [B],B)<<(label [A],A).\nHowever, for a given thread, its id is \ufb01xed and its label [] values are strictly\nincreasing, so Bmust have seen that flag [A] was false . It follows that\nlabelingB!readB(\ufb02ag[A])!writeA(\ufb02ag[A])!labelingA\nwhich contradicts the assumption that ( label [A],A)<<(label [B],B).2\n2.7 Bounded Timestamps\nNotice that the labels of the Bakery lock grow without bound, so in a long-lived\nsystem we may have to worry about over\ufb02ow. If a thread\u2019s label \ufb01eld silently rolls\nover from a large number to zero, then the \ufb01rst-come-\ufb01rst-served property no\nlonger holds.\nLater on, we will see constructions where counters are used to order threads,\nor even to produce unique identi\ufb01ers. How important is the over\ufb02ow problem in\nthe real world? It is dif\ufb01cult to generalize. Sometimes it matters a great deal. The\ncelebrated \u201cY2K\u201d bug that captivated the media in the last years of the twentieth\ncentury is an example of a genuine over\ufb02ow problem, even if the consequences\nwere not as dire as predicted. On January 18, 2038, the Unix time_t data struc-\nture will over\ufb02ow when the number of seconds since January 1, 1970 exceeds 232.\nNo one knows whether it will matter. Sometimes, of course, counter over\ufb02ow is\na nonissue. Most applications that use, say, a 64-bit counter are unlikely to last\nlong enough for roll-over to occur. (Let the grandchildren worry!)\n34 Chapter 2 Mutual Exclusion\nIn the Bakery lock, labels act as timestamps : they establish an order among\nthe contending threads. Informally, we need to ensure that if one thread takes a\nlabel after another, then", "doc_id": "2069422a-c464-4477-aea9-b9dc816eb2e1", "embedding": null, "doc_hash": "9f622f28c82b2f41327c0c23dd071b1768e913d12548edacfc32534339008448", "extra_info": null, "node_info": {"start": 121177, "end": 124244}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3d16e30f-f1d6-435a-a9d2-52cf15c240b3", "3": "5a8b5ca4-5a57-4ab4-b76b-ab33597531df"}}, "__type__": "1"}, "5a8b5ca4-5a57-4ab4-b76b-ab33597531df": {"__data__": {"text": "struc-\nture will over\ufb02ow when the number of seconds since January 1, 1970 exceeds 232.\nNo one knows whether it will matter. Sometimes, of course, counter over\ufb02ow is\na nonissue. Most applications that use, say, a 64-bit counter are unlikely to last\nlong enough for roll-over to occur. (Let the grandchildren worry!)\n34 Chapter 2 Mutual Exclusion\nIn the Bakery lock, labels act as timestamps : they establish an order among\nthe contending threads. Informally, we need to ensure that if one thread takes a\nlabel after another, then the latter has the larger label. Inspecting the code for the\nBakery lock, we see that a thread needs two abilities:\n\u0004to read the other threads\u2019 timestamps ( scan), and\n\u0004to assign itself a later timestamp ( label ).\nA Java interface to such a timestamping system appears in Fig. 2.10. Since our\nprincipal application for a bounded timestamping system is to implement the\ndoorway section of the Lock class, the timestamping system must be wait-free.\nIt is possible to construct such a wait-free concurrent timestamping system (see\nthe chapter notes), but the construction is long and rather technical. Instead, we\nfocus on a simpler problem, interesting in its own right: constructing a sequen-\ntialtimestamping system, in which threads perform scan-and- label operations\none completely after the other, that is, as if each were performed using mutual\nexclusion. In other words, consider only executions in which a thread can per-\nform a scan of the other threads\u2019 labels, or a scan, and then an assignment of a\nnew label, where each such sequence is a single atomic step. The principles under-\nlying a concurrent and sequential timestamping systems are essentially the same,\nbut they differ substantially in detail.\nThink of the range of possible timestamps as nodes of a directed graph (called\naprecedence graph ). An edge from node ato nodebmeans thatais a later time-\nstamp thanb. The timestamp order is irre\ufb02exive : there is no edge from any node\nato itself. The order is also antisymmetric : if there is an edge from atob, then\nthere is no edge from btoa. Notice that we do notrequire that the order be\ntransitive : there can be an edge from atoband frombtoc, without necessarily\nimplying there is an edge from atoc.\nThink of assigning a timestamp to a thread as placing that thread\u2019s token on\nthat timestamp\u2019s node. A thread performs a scan by locating the other threads\u2019\ntokens, and it assigns itself a new timestamp by moving its own token to a node\nasuch that there is an edge from ato every other thread\u2019s node.\nPragmatically, we would implement such a system as an array of single-writer\nmulti-reader \ufb01elds, where array element Arepresents the graph node where\nthreadAmost recently placed its token. The scan () method takes a \u201csnapshot\u201d of\nthe array, and the label () method for thread Aupdates theA-th array element.\n1 public interface Timestamp {\n2 boolean compare(Timestamp);\n3 }\n4 public interface TimestampSystem {\n5 public Timestamp[] scan();\n6 public void label(Timestamp timestamp, int i);\n7 }\nFigure 2.10 A timestamping system interface.\n2.7 Bounded Timestamps 35\n0123\nFigure 2.11 The precedence graph for an unbounded timestamping system. The nodes rep-\nresent the set of all natural numbers and the edges represent the total order among them.\nFig. 2.11 illustrates the precedence graph for the unbounded timestamp sys-\ntem used in the Bakery lock. Not surprisingly, the graph is in\ufb01nite: there is one\nnode for each natural number, with a directed edge from node ato nodebwhen-\nevera>b", "doc_id": "5a8b5ca4-5a57-4ab4-b76b-ab33597531df", "embedding": null, "doc_hash": "49137b69c4f477f121a0b3ab064bc240c34c2949449c4229427510cb2d291e6e", "extra_info": null, "node_info": {"start": 124259, "end": 127794}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2069422a-c464-4477-aea9-b9dc816eb2e1", "3": "7e6c13c0-f221-4a18-80c9-84f64cd19328"}}, "__type__": "1"}, "7e6c13c0-f221-4a18-80c9-84f64cd19328": {"__data__": {"text": "int i);\n7 }\nFigure 2.10 A timestamping system interface.\n2.7 Bounded Timestamps 35\n0123\nFigure 2.11 The precedence graph for an unbounded timestamping system. The nodes rep-\nresent the set of all natural numbers and the edges represent the total order among them.\nFig. 2.11 illustrates the precedence graph for the unbounded timestamp sys-\ntem used in the Bakery lock. Not surprisingly, the graph is in\ufb01nite: there is one\nnode for each natural number, with a directed edge from node ato nodebwhen-\nevera>b .\nConsider the precedence graph T2shown in Fig. 2.12. This graph has three\nnodes, labeled 0, 1, and 2, and its edges de\ufb01ne an ordering relation on the nodes\nin which 0 is less than 1, 1 is less than 2, and 2 is less than 0. If there are only two\nthreads, then we can use this graph to de\ufb01ne a bounded (sequential) timestamp-\ning system. The system satis\ufb01es the following invariant: the two threads always\nhave tokens located on adjacent nodes, with the direction of the edge indicating\ntheir relative order. Suppose A\u2019s token is on Node 0, and B\u2019s token on Node 1 (so\nBhas the later timestamp). For B, the label () method is trivial: it already has\nthe latest timestamp, so it does nothing. For A, the label () method \u201cleapfrogs\u201d\nB\u2019s node by jumping from 0 to 2.\nRecall that a cycle1in a directed graph is a set of nodes n0,n1,:::,nksuch that\nthere is an edge from n0ton1, fromn1ton2, and eventually from nk\u00001tonk, and\nback fromnkton0.\nThe only cycle in the graph T2has length three, and there are only two threads,\nso the order among the threads is never ambiguous. T o go beyond two threads,\nwe need additional conceptual tools. Let Gbe a precedence graph, and AandB\nsubgraphs of G(possibly single nodes). We say that AdominatesBinGif every\nnode ofAhas edges directed to every node of B. Let graph multiplication be the\nfollowing noncommutative composition operator for graphs (denoted G\u000eH):\nReplace every node vofGby a copy of H(denotedHv), and letHvdominate\nHuinG\u000eHifvdominatesuinG.\nDe\ufb01ne the graph Tkinductively to be:\n1.T1is a single node.\n2.T2is the three-node graph de\ufb01ned earlier.\n3.Fork>2,Tk=T2\u000eTk\u00001.\nFor example, the graph T3is illustrated in Fig. 2.12.\n1The word \u201ccycle\u201d comes from the same Greek root as \u201ccircle.\u201d\n36 Chapter 2 Mutual Exclusion\nT1 5 T3 5\nT2 5\nTk 5 T2 * Tk210\n1 20\n1200\n1 20\n211 2\nFigure 2.12 The precedence graph for a bounded timestamping system. Consider an initial\nsituation in which there is a token Aon Node 12 (Node 2 in subgraph 1) and tokens Band\nCon Nodes 21 and 22 (Nodes 1 and 2 in subgraph 2) respectively. T oken Bwill move to 20\nto dominate the others. T oken Cwill then move to 21 to dominate the others, and BandC\ncan continue to cycle in the T2subgraph 2 forever. If Ais to move to dominate BandC, it\ncannot pick a node in subgraph 2 since it is full (any Tksubgraph can accommodate at most k\ntokens). Instead, token Amoves to Node 00. If Bnow moves, it will choose Node 01, Cwill\nchoose 10 and so on.\nThe precedence graph Tnis the basis for an", "doc_id": "7e6c13c0-f221-4a18-80c9-84f64cd19328", "embedding": null, "doc_hash": "ce8216630fbac416451227ccd8761a9b80dcd07f823ccf95fc9d7a85b2d63538", "extra_info": null, "node_info": {"start": 127811, "end": 130807}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5a8b5ca4-5a57-4ab4-b76b-ab33597531df", "3": "ac2310dd-3a13-4576-acdb-adf1b2f0a1c3"}}, "__type__": "1"}, "ac2310dd-3a13-4576-acdb-adf1b2f0a1c3": {"__data__": {"text": "1 and 2 in subgraph 2) respectively. T oken Bwill move to 20\nto dominate the others. T oken Cwill then move to 21 to dominate the others, and BandC\ncan continue to cycle in the T2subgraph 2 forever. If Ais to move to dominate BandC, it\ncannot pick a node in subgraph 2 since it is full (any Tksubgraph can accommodate at most k\ntokens). Instead, token Amoves to Node 00. If Bnow moves, it will choose Node 01, Cwill\nchoose 10 and so on.\nThe precedence graph Tnis the basis for an n-thread bounded sequential\ntimestamping system. We can \u201caddress\u201d any node in the Tngraph withn\u00001\ndigits, using ternary notation. For example, the nodes in graph T2are addressed\nby 0, 1, and 2. The nodes in graph T3are denoted by 00, 01, :::, 22, where the\nhigh-order digit indicates one of the three subgraphs, and the low-order digit\nindicates one node within that subgraph.\nThe key to understanding the n-thread labeling algorithm is that the nodes\ncovered by tokens can never form a cycle. As mentioned, two threads can never\nform a cycle on T2, because the shortest cycle in T2requires three nodes.\nHow does the label () method work for three threads? When Acalls label (),\nif both the other threads have tokens on the same T2subgraph, then move to\na node on the next highest T2subgraph, the one whose nodes dominate that\nT2subgraph. For example, consider the graph T3as illustrated in Fig. 2.12. We\nassume an initial acyclic situation in which there is a token Aon Node 12 (Node\n2 in subgraph 1) and tokens BandC, respectively, on Nodes 21 and 22 (Nodes\n1 and 2 in subgraph 2). T oken Bwill move to 20 to dominate all others. T oken C\nwill then move to 21 to dominate all others, and BandCcan continue to cycle\nin theT2subgraph 2 forever. If Ais to move to dominate BandC, it cannot pick\na node in subgraph 2 since it is full (any Tksubgraph can accommodate at most\nktokens). T oken Athus moves to Node 00. If Bnow moves, it will choose Node\n01,Cwill choose 10 and so on.\n2.8 Lower Bounds on the Number of Locations 37\n2.8 Lower Bounds on the Number of Locations\nTheBakery lock is succinct, elegant, and fair. So why is it not considered practi-\ncal? The principal drawback is the need to read and write ndistinct locations,\nwheren(which may be very large) is the maximum number of concurrent\nthreads.\nIs there a clever Lock algorithm based on reading and writing memory that\navoids this overhead? We now demonstrate that the answer is no. Any deadlock-\nfreeLock algorithm requires allocating and then reading or writing at least n\ndistinct locations in the worst case. This result is crucially important, because\nit motivates us to add to our multiprocessor machines, synchronization opera-\ntions stronger than read-write, and use them as the basis of our mutual exclusion\nalgorithms.\nIn this section, we observe why this linear bound is inherent before we dis-\ncuss practical mutual exclusion algorithms in later chapters. We will observe the\nfollowing important limitation of memory locations accessed solely by read or\nwrite instructions (in practice these are called loads and stores ): any information\nwritten by a thread to a given location could be overwritten (stored-to) without\nany other thread ever seeing it.\nOur proof requires us to argue about the state of all memory used by\na given multithreaded program. An object\u2019s state is just the state of its \ufb01elds.\nA thread\u2019s local state is the state of", "doc_id": "ac2310dd-3a13-4576-acdb-adf1b2f0a1c3", "embedding": null, "doc_hash": "e49b05418c47a61f6cd962f281f7ba38175775adf95d6ea7b3d47192c863995d", "extra_info": null, "node_info": {"start": 130848, "end": 134244}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7e6c13c0-f221-4a18-80c9-84f64cd19328", "3": "b3a28a36-39ba-4075-83b3-aa8eaa51307c"}}, "__type__": "1"}, "b3a28a36-39ba-4075-83b3-aa8eaa51307c": {"__data__": {"text": "bound is inherent before we dis-\ncuss practical mutual exclusion algorithms in later chapters. We will observe the\nfollowing important limitation of memory locations accessed solely by read or\nwrite instructions (in practice these are called loads and stores ): any information\nwritten by a thread to a given location could be overwritten (stored-to) without\nany other thread ever seeing it.\nOur proof requires us to argue about the state of all memory used by\na given multithreaded program. An object\u2019s state is just the state of its \ufb01elds.\nA thread\u2019s local state is the state of its program counters and local variables. A\nglobal state orsystem state is the state of all objects, plus the local states of the\nthreads.\nDe\ufb01nition 2.8.1. ALock object statesisinconsistent in any global state where\nsome thread is in the critical section, but the lock state is compatible with a global\nstate in which no thread is in the critical section or is trying to enter the critical\nsection.\nLemma 2.8.1. No deadlock-free Lock algorithm can enter an inconsistent\nstate.\nProof: Suppose the Lock object is in an inconsistent state s, where some thread\nAis in the critical section. If thread Btries to enter the critical section, it must\neventually succeed because the algorithm is deadlock-free and Bcannot deter-\nmine thatAis in the critical section, a contradiction. 2\nAny Lock algorithm that solves deadlock-free mutual exclusion must have\nndistinct locations. Here, we consider only the 3-thread case, showing that a\n38 Chapter 2 Mutual Exclusion\ndeadlock-free Lock algorithm accessed by three threads must use three distinct\nlocations.\nDe\ufb01nition 2.8.2. Acovering state for a Lock object is one in which there is at least\none thread about to write to each shared location, but the Lock object\u2019s locations\n\u201clook\u201d like the critical section is empty (i.e., the locations\u2019 states appear as if there\nis no thread either in the critical section or trying to enter the critical section).\nIn a covering state, we say that each thread covers the location it is about to\nwrite.\nTheorem 2.8.1. AnyLock algorithm that, by reading and writing memory, solves\ndeadlock-free mutual exclusion for three threads, must use at least three distinct\nmemory locations.\nProof: Assume by way of contradiction that we have a deadlock-free Lock algo-\nrithm for three threads with only two locations. Initially, in state s, no thread is in\nthe critical section or trying to enter. If we run any thread by itself, then it must\nwrite to at least one location before entering the critical section, as otherwise it\ncreates an inconsistent state.\nIt follows that every thread must write at least one location before entering.\nIf the shared locations are single-writer locations as in the Bakery lock, then it is\nimmediate that three distinct locations are needed.\nNow consider multiwriter locations such as the victim location in Peterson\u2019s\nalgorithm (Fig. 2.6). Lets assume that we can bring the system to a covering Lock\nstateswhereAandBrespectively cover distinct locations. Consider this possible\nexecution starting from state sas depicted in Fig. 2.13:\nLetCrun alone. Because the Lock algorithm satis\ufb01es the deadlock-free property,\nCenters the critical section eventually. Then let AandBrespectively update their\ncovered locations, leaving the Lock object in state s0.\nThe states0is inconsistent because no thread can tell whether Cis in the critical\nsection, so a lock with two locations is impossible.\nIt remains to be shown how to maneuver threads AandBinto a covering state.\nConsider an execution in which Bruns through the critical section three times.\nEach time around, it must write some location, so consider the \ufb01rst location", "doc_id": "b3a28a36-39ba-4075-83b3-aa8eaa51307c", "embedding": null, "doc_hash": "4ebb00633dfef460257957af089bb0391d82da424535e494db1ac9e4444a29b6", "extra_info": null, "node_info": {"start": 134145, "end": 137841}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ac2310dd-3a13-4576-acdb-adf1b2f0a1c3", "3": "c421249a-126c-460d-8f11-25b05d0b6d24"}}, "__type__": "1"}, "c421249a-126c-460d-8f11-25b05d0b6d24": {"__data__": {"text": "alone. Because the Lock algorithm satis\ufb01es the deadlock-free property,\nCenters the critical section eventually. Then let AandBrespectively update their\ncovered locations, leaving the Lock object in state s0.\nThe states0is inconsistent because no thread can tell whether Cis in the critical\nsection, so a lock with two locations is impossible.\nIt remains to be shown how to maneuver threads AandBinto a covering state.\nConsider an execution in which Bruns through the critical section three times.\nEach time around, it must write some location, so consider the \ufb01rst location it\nwrites when trying to enter the critical section. Since there are only two locations,\nBmust \u201cwrite \ufb01rst\u201d to the same location twice. Call that location LB.\nLetBrun until it is poised to write location LBfor the \ufb01rst time. If Aruns\nnow, it would enter the critical section, since Bhas not written anything. Amust\nwriteLAbefore entering the critical section. Otherwise, if Awrites onlyLB, then\nletAenter the critical section, let Bwrite toLB(obliterating A\u2019s last write). The\nresult is an inconsistent state: Bcannot tell whether Ais in the critical section.\nLetArun until it is poised to write LA. This state is not a covering state,\nbecauseAcould have written something to LBindicating to thread Cthat it is\ntrying to enter the critical section. Let Brun, obliterating any value Amight have\n2.8 Lower Bounds on the Number of Locations 39\nA B\n1.The system is \nin a covering state .WAWBAssume only 2 locations .\nC\nCS2. C runs . It \n    possibly writes     all locations     and enters     the CS .\nCS3.Run the other threads \nA and B. They overwrite \nwhat  C wrote and one of \nthem must enter the CS \u2013\na contradiction!\nFigure 2.13 Contradiction using a covering state for two locations. Initially both locations\nhave the empty value ?.\nwritten toLB, entering and leaving the critical section at most three times, and\nhalting just before its second write to LB. Notice that every time Benters and\nleaves the critical section, whatever it wrote to the locations is no longer relevant.\nIn this state, Ais about to write LA,Bis about to write LB, and the locations\nare consistent with no thread trying to enter or in the critical section, as required\nin a covering state. Fig. 2.14 illustrates this scenario. 2\nThe same line of argument can be extended to show that n-thread deadlock-\nfree mutual exclusion requires ndistinct locations. The Peterson andBakery\nlocks are thus optimal (within a constant factor). However, as we note, the need\nto allocatenlocations per Lock makes them impractical.\nThis proof shows the inherent limitation of read and write operations: infor-\nmation written by a thread may be overwritten without any other thread ever\nreading it. We will remember this limitation when we move on to design other\nalgorithms.\nIn later chapters, we will see that modern machine architectures provide spe-\ncialized instructions that overcome the \u201coverwriting\u201d limitation of read and write\ninstructions, allowing n-thread Lock implementations that use only a constant\nnumber of memory locations. We will also see that making effective use of these\ninstructions to solve mutual exclusion is far from trivial.\n40 Chapter 2 Mutual Exclusion\nB\n1.Start in a covering\nstate for LB. WBWA\nLBLA2.Run system until A is \nabout to write LA. There \nmust be such a case; otherwise let A enter the \nCS and then B can overwrite \nits value . But there could be \ntraces left by A in L\nB.3.Run B again . It \nerases traces in LB.\nThen let it enter the \nCS and return again . \nIf one repeats this pattern twice more, B", "doc_id": "c421249a-126c-460d-8f11-25b05d0b6d24", "embedding": null, "doc_hash": "0ba9ee2dd66950eca461d0bd99aafb7908699293738f07c8ee4e0e8f0b07d36d", "extra_info": null, "node_info": {"start": 137841, "end": 141418}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b3a28a36-39ba-4075-83b3-aa8eaa51307c", "3": "ea312dac-7d27-4ec9-8736-50f1240376b1"}}, "__type__": "1"}, "ea312dac-7d27-4ec9-8736-50f1240376b1": {"__data__": {"text": "will also see that making effective use of these\ninstructions to solve mutual exclusion is far from trivial.\n40 Chapter 2 Mutual Exclusion\nB\n1.Start in a covering\nstate for LB. WBWA\nLBLA2.Run system until A is \nabout to write LA. There \nmust be such a case; otherwise let A enter the \nCS and then B can overwrite \nits value . But there could be \ntraces left by A in L\nB.3.Run B again . It \nerases traces in LB.\nThen let it enter the \nCS and return again . \nIf one repeats this pattern twice more, B must return to a \ncovering state for the exact same location (in the figure it is L\nB).CS CSA\nFigure 2.14 Reaching a covering state. In the initial covering state for LBboth locations have\nthe empty value?.\n2.9 Chapter Notes\nIsaac Newton\u2019s ideas about the \ufb02ow of time appear in his famous Principia [121].\nThe \u201c!\u201d formalism is due to Leslie Lamport [89]. The \ufb01rst three algorithms\nin this chapter are due to Gary Peterson, who published them in a two-page\npaper in 1981 [124]. The Bakery lock presented here is a simpli\ufb01cation of the\noriginal Bakery Algorithm due to Leslie Lamport [88]. The sequential time-\nstamp algorithm is due to Amos Israeli and Ming Li [77], who invented the\nnotion of a bounded timestamping system. Danny Dolev and Nir Shavit [34]\ninvented the \ufb01rst bounded concurrent timestamping system. Other bounded\ntimestamping schemes include Sibsankar Haldar and Paul Vit \u00b4anyi [51], and\nCynthia Dwork and Orli Waarts [37]. The lower bound on the number of\nlock \ufb01elds is due to Jim Burns and Nancy Lynch [23]. Their proof technique,\ncalled a covering argument , has since been widely used to prove lower bounds\nin distributed computing. Readers interested in further reading can \ufb01nd a\nhistorical survey of mutual exclusion algorithms in a classic book by Michel\nRaynal [131].\n2.10 Exercises 41\n2.10 Exercises\nExercise 9. De\ufb01ner-bounded waiting for a given mutual exclusion algorithm to\nmean that if Dj\nA!Dk\nBthen CSj\nA!CSk+r\nB. Is there a way to de\ufb01ne a doorway for\nthePeterson algorithm such that it provides r-bounded waiting for some value\nofr?\nExercise 10. Why do we need to de\ufb01ne a doorway section, and why cannot we\nde\ufb01ne FCFS in a mutual exclusion algorithm based on the order in which the\n\ufb01rst instruction in the lock () method was executed? Argue your answer in a\ncase-by-case manner based on the nature of the \ufb01rst instruction executed by the\nlock (): a read or a write, to separate locations or the same location.\nExercise 11. Programmers at the Flaky Computer Corporation designed the pro-\ntocol shown in Fig. 2.15 to achieve n-thread mutual exclusion. For each question,\neither sketch a proof, or display an execution where it fails.\n\u0004Does this protocol satisfy mutual exclusion?\n\u0004Is this protocol starvation-free?\n\u0004is this protocol deadlock-free?\nExercise 12. Show that the Filter lock allows some threads to overtake others an\narbitrary number of times.\nExercise 13. Another way to generalize the two-thread Peterson lock is to arrange\na number of 2-thread Peterson locks in a binary tree. Suppose nis a power of two.\n1class Flaky implements Lock {\n2 private int turn;\n3 private boolean busy = false ;\n4 public void lock() {\n5", "doc_id": "ea312dac-7d27-4ec9-8736-50f1240376b1", "embedding": null, "doc_hash": "a47d4aa1f238f08b3fd41cc604d01193ff512abe139160179146622406bb554c", "extra_info": null, "node_info": {"start": 141503, "end": 144658}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "c421249a-126c-460d-8f11-25b05d0b6d24", "3": "dcb27f33-95a7-4c57-93cc-87259d92679f"}}, "__type__": "1"}, "dcb27f33-95a7-4c57-93cc-87259d92679f": {"__data__": {"text": "where it fails.\n\u0004Does this protocol satisfy mutual exclusion?\n\u0004Is this protocol starvation-free?\n\u0004is this protocol deadlock-free?\nExercise 12. Show that the Filter lock allows some threads to overtake others an\narbitrary number of times.\nExercise 13. Another way to generalize the two-thread Peterson lock is to arrange\na number of 2-thread Peterson locks in a binary tree. Suppose nis a power of two.\n1class Flaky implements Lock {\n2 private int turn;\n3 private boolean busy = false ;\n4 public void lock() {\n5 int me = ThreadID.get();\n6 do{\n7 do{\n8 turn = me;\n9 }while (busy);\n10 busy = true ;\n11 }while (turn != me);\n12 }\n13 public void unlock() {\n14 busy = false ;\n15 }\n16 }\nFigure 2.15 The Flaky lock used in Exercise 11.\n42 Chapter 2 Mutual Exclusion\nEach thread is assigned a leaf lock which it shares with one other thread. Each lock\ntreats one thread as thread 0 and the other as thread 1.\nIn the tree-lock\u2019s acquire method, the thread acquires every two-thread Peter-\nson lock from that thread\u2019s leaf to the root. The tree-lock\u2019s release method for the\ntree-lock unlocks each of the 2-thread Peterson locks that thread has acquired,\nfrom the root back to its leaf. At any time, a thread can be delayed for a \ufb01nite\nduration. (In other words, threads can take naps, or even vacations, but they do\nnot drop dead.) For each property, either sketch a proof that it holds, or describe\na (possibly in\ufb01nite) execution where it is violated.\n1.mutual exclusion.\n2.freedom from deadlock.\n3.freedom from starvation.\nIs there an upper bound on the number of times the tree-lock can be acquired\nand released between the time a thread starts acquiring the tree-lock and when it\nsucceeds?\nExercise 14. The`-exclusion problem is a variant of the starvation-free mutual\nexclusion problem. We make two changes: as many as `threads may be in the\ncritical section at the same time, and fewer than `threads might fail (by halting)\nin the critical section.\nAn implementation must satisfy the following conditions:\n`-Exclusion: At any time, at most `threads are in the critical section.\n`-Starvation-Freedom: As long as fewer than `threads are in the critical sec-\ntion, then some thread that wants to enter the critical section will eventually\nsucceed (even if some threads in the critical section have halted).\nModify then-process Filter mutual exclusion algorithm to turn it into an\n`-exclusion algorithm.\nExercise 15. In practice, almost all lock acquisitions are uncontended, so the most\npractical measure of a lock\u2019s performance is the number of steps needed for a\nthread to acquire a lock when no other thread is concurrently trying to acquire\nthe lock.\nScientists at Cantaloupe-Melon University have devised the following \u201cwrap-\nper\u201d for an arbitrary lock, shown in Fig. 2.16. They claim that if the base Lock\nclass provides mutual exclusion and is starvation-free, so does the FastPath lock,\nbut it can be acquired in a constant number of steps in the absence of contention.\nSketch an argument why they are right, or give a counterexample.\n2.10 Exercises 43\n1class FastPath implements Lock {\n2 private static ThreadLocal<Integer> myIndex;\n3 private Lock lock;\n4 private int x, y = -1;\n5 public void lock() {\n6 int i = myIndex.get();\n7 x = i; // I\u2019m here\n8 while (y != -1) {} // is the lock free?\n9 y = i;", "doc_id": "dcb27f33-95a7-4c57-93cc-87259d92679f", "embedding": null, "doc_hash": "23a1b76744a707ba00c5b6d24495bdcbc6d8f6852ea13eb1f53e6ea5819b1d90", "extra_info": null, "node_info": {"start": 144631, "end": 147929}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ea312dac-7d27-4ec9-8736-50f1240376b1", "3": "5d737a46-0905-4864-9f3d-d9c0332eee07"}}, "__type__": "1"}, "5d737a46-0905-4864-9f3d-d9c0332eee07": {"__data__": {"text": "so does the FastPath lock,\nbut it can be acquired in a constant number of steps in the absence of contention.\nSketch an argument why they are right, or give a counterexample.\n2.10 Exercises 43\n1class FastPath implements Lock {\n2 private static ThreadLocal<Integer> myIndex;\n3 private Lock lock;\n4 private int x, y = -1;\n5 public void lock() {\n6 int i = myIndex.get();\n7 x = i; // I\u2019m here\n8 while (y != -1) {} // is the lock free?\n9 y = i; // me again?\n10 if(x != i) // Am I still here?\n11 lock.lock(); // slow path\n12 }\n13 public void unlock() {\n14 y = -1;\n15 lock.unlock();\n16 }\n17 }\nFigure 2.16 Fast path mutual exclusion algorithm used in Exercise 15.\n1class Bouncer {\n2 public static final int DOWN = 0;\n3 public static final int RIGHT = 1;\n4 public static final int STOP = 2;\n5 private boolean goRight = false ;\n6 private ThreadLocal<Integer> myIndex; // initialize myIndex\n7 private int last = -1;\n8 int visit() {\n9 int i = myIndex.get();\n10 last = i;\n11 if(goRight)\n12 return RIGHT;\n13 goRight = true ;\n14 if(last == i)\n15 return STOP;\n16 else\n17 return DOWN;\n18 }\n19 }\nFigure 2.17 TheBouncer class implementation.\nExercise 16. Supposenthreads call the visit () method of the Bouncer class\nshown in Fig. 2.17. Prove that\u2014\n\u0004At most one thread gets the value STOP .\n\u0004At mostn\u00001 threads get the value DOWN .\n\u0004At mostn\u00001 threads get the value RIGHT .\nNote that the last two proofs are notsymmetric.\n44 Chapter 2 Mutual Exclusion\n0 1 3 6\n2 4 7\n5 8\n9\nFigure 2.18 Array layout for Bouncer objects.\nExercise 17. So far, we have assumed that all nthreads have unique, small indexes.\nHere is one way to assign unique small indexes to threads. Arrange Bouncer\nobjects in a triangular matrix, where each Bouncer is given an id as shown in\nFig. 2.18. Each thread starts by visiting Bouncer zero. If it gets STOP , it stops. If\nit gets RIGHT , it visits 1, and if it gets DOWN , it visits 2. In general, if a thread gets\nSTOP , it stops. If it gets RIGHT , it visits the next Bouncer on that row, and if it gets\nDOWN , it visits the next Bouncer in that column. Each thread takes the id of the\nBouncer object where it stops.\n\u0004Prove that each thread eventually stops at some Bouncer object.\n\u0004How many Bouncer objects do you need in the array if you know in advance\nthe total number nof threads?\nExercise 18. Prove, by way of a counterexample, that the sequential time-stamp\nsystemT3, started in a valid state (with no cycles among the labels), does not\nwork for three threads in the concurrent case. Note that it is not a problem to\nhave two identical labels since one can break such ties using thread IDs. The\ncounterexample should display a state of the execution where three labels are not\ntotally ordered.\nExercise 19. The sequential time-stamp system T3had a range of O(3n) different\npossible label values. Design a sequential time-stamp system that requires only\nO(n2n) labels. Note that in a time-stamp system, one may look at all the labels\nto choose a new label, yet once a label is chosen, it should be comparable to any\nother label without knowing what the other labels in the system are. Hint: think\nof the labels in terms of", "doc_id": "5d737a46-0905-4864-9f3d-d9c0332eee07", "embedding": null, "doc_hash": "fc18506a13478828c884686bbda6a092620e8c8b20a3c101601f80da6193b46c", "extra_info": null, "node_info": {"start": 148003, "end": 151133}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "dcb27f33-95a7-4c57-93cc-87259d92679f", "3": "fc0c55f3-477b-433a-8847-d1839c59f91f"}}, "__type__": "1"}, "fc0c55f3-477b-433a-8847-d1839c59f91f": {"__data__": {"text": "IDs. The\ncounterexample should display a state of the execution where three labels are not\ntotally ordered.\nExercise 19. The sequential time-stamp system T3had a range of O(3n) different\npossible label values. Design a sequential time-stamp system that requires only\nO(n2n) labels. Note that in a time-stamp system, one may look at all the labels\nto choose a new label, yet once a label is chosen, it should be comparable to any\nother label without knowing what the other labels in the system are. Hint: think\nof the labels in terms of their bit representation.\nExercise 20. Give Java code to implement the Timestamp interface of Fig. 2.10\nusing unbounded labels. Then, show how to replace the pseudocode of the\nBakery lock of Fig. 2.9 using your Timestamp Java code [82].\n3Concurrent Objects\nThe behavior of concurrent objects is best described through their safety and\nliveness properties, often referred to as correctness and progress . In this chapter\nwe examine various ways of specifying correctness and progress.\nWhile all notions of correctness for concurrent objects are based on some\nnotion of equivalence with sequential behavior, different notions are appropriate\nfor different systems. We examine three correctness conditions. Quiescent consis-\ntency is appropriate for applications that require high performance at the cost of\nplacing relatively weak constraints on object behavior. Sequential consistency is\na stronger condition, often useful for describing low-level systems such as hard-\nware memory interfaces. Linearizability , even stronger, is useful for describing\nhigher-level systems composed from linearizable components.\nAlong a different dimension, different method implementations provide dif-\nferent progress guarantees. Some are blocking , where the delay of any one thread\ncan delay others, and some are nonblocking , where the delay of a thread cannot\ndelay the others.\n3.1 Concurrency and Correctness\nWhat does it mean for a concurrent object to be correct? Fig. 3.1 shows a simple\nlock-based concurrent FIFO queue. The enq() and deq() methods synchronize\nby a mutual exclusion lock of the kind studied in Chapter 2. It is easy to see that\nthis implementation is a correct concurrent FIFO queue. Because each method\naccesses and updates \ufb01elds while holding an exclusive lock, the method calls take\neffect sequentially.\nThis idea is illustrated in Fig. 3.2, which shows an execution in which Aenq-\nueuesa,Benqueuesb, andCdequeues twice, \ufb01rst throwing EmptyException ,\nand second returning b. Overlapping intervals indicate concurrent method calls.\nAll three method calls overlap in time. In this \ufb01gure, as in others, time moves\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00003-4\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.45\n46 Chapter 3 Concurrent Objects\n1class LockBasedQueue<T> {\n2 int head, tail;\n3 T[] items;\n4 Lock lock;\n5 public LockBasedQueue( int capacity) {\n6 head = 0; tail = 0;\n7 lock = new ReentrantLock();\n8 items = (T[]) new Object[capacity];\n9 }\n10 public void enq(T x) throws FullException {\n11 lock.lock();\n12 try {\n13 if(tail - head == items.length)\n14 throw new FullException();\n15 items[tail % items.length] = x;\n16 tail++;\n17 }finally {\n18 lock.unlock();\n19 }\n20 }\n21 public T deq() throws EmptyException {\n22 lock.lock();\n23 try {\n24 if(tail == head)\n25 throw new EmptyException();\n26 T x = items[head % items.length];\n27 head++;\n28 return x;\n29", "doc_id": "fc0c55f3-477b-433a-8847-d1839c59f91f", "embedding": null, "doc_hash": "8c2ca31f692d8e95de2b7cc42df8fffb1135ab9336868bdfc30b651facf58d73", "extra_info": null, "node_info": {"start": 151046, "end": 154486}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5d737a46-0905-4864-9f3d-d9c0332eee07", "3": "69495d68-cd1b-4936-8294-7b5af706c19d"}}, "__type__": "1"}, "69495d68-cd1b-4936-8294-7b5af706c19d": {"__data__": {"text": "new Object[capacity];\n9 }\n10 public void enq(T x) throws FullException {\n11 lock.lock();\n12 try {\n13 if(tail - head == items.length)\n14 throw new FullException();\n15 items[tail % items.length] = x;\n16 tail++;\n17 }finally {\n18 lock.unlock();\n19 }\n20 }\n21 public T deq() throws EmptyException {\n22 lock.lock();\n23 try {\n24 if(tail == head)\n25 throw new EmptyException();\n26 T x = items[head % items.length];\n27 head++;\n28 return x;\n29 }finally {\n30 lock.unlock();\n31 }\n32 }\n33 }\nFigure 3.1 A lock-based FIFO queue. The queue\u2019s items are kept in an array items , where\nhead is the index of the next item to dequeue, and tail is the index of the \ufb01rst open array\nslot (modulo the capacity). The lock \ufb01eld is a lock that ensures that methods are mutually\nexclusive. Initially head andtail are zero, and the queue is empty. If enq()\ufb01nds the queue\nis full, i.e., head andtail differ by the queue size, then it throws an exception. Otherwise,\nthere is room, so enq()stores the item at array entry tail, and then increments tail. The\ndeq()method works in a symmetric way.\nfrom left to right, and dark lines indicate intervals. The intervals for a single\nthread are displayed along a single horizontal line. When convenient, the thread\nname appears on the left. A bar represents an interval with a \ufb01xed start and stop\ntime. A bar with dotted lines on the right represents an interval with a \ufb01xed\nstart-time and an unknown stop-time. The label \u201c q:enq(x)\u201d means that a thread\nenqueues item xat objectq, while \u201cq:deq(x)\u201d means that the thread dequeues\nitemxfrom objectq.\nThe timeline shows which thread holds the lock. Here, Cacquires the lock,\nobserves the queue to be empty, releases the lock, and throws an exception. It\ndoes not modify the queue. Bacquires the lock, inserts b, and releases the lock. A\n3.1 Concurrency and Correctness 47\nq.enq(a)\nq.enq(b)\nq.deq(b)A\nB\nC\nLock\nHolderTimeline B\nenq(b)C\ndeq(b)A\nenq(a)lock() enq( a) unlock()\ndeq(b) unlock()lock() enq( b) unlock()\nlock() unlock()\nC\ndeq(empty) lock()\nFigure 3.2 Locking queue execution. Here, Cacquires the lock, observes the queue to be empty, releases\nthe lock, and throws an exception. Bacquires the lock, inserts b, and releases the lock. Aacquires the lock,\ninserts a, and releases the lock. Cre-acquires the lock, dequeues b, releases the lock, and returns.\nacquires the lock, inserts a, and releases the lock. Creacquires the lock, dequeues\nb, releases the lock, and returns. Each of these calls takes effect sequentially, and\nwe can easily verify that dequeuing bbeforeais consistent with our understand-\ning of sequential FIFO queue behavior.\nLet us consider, however, the alternative concurrent queue implementation in\nFig. 3.3. (This queue is correct only if it is shared by a single enqueuer and a single\ndequeuer.) It has almost the same internal representation as the lock-based queue\nof Fig. 3.1. The only difference is the absence of a lock. We claim this is a correct\nimplementation of a single-enqueuer/single-dequeuer FIFO queue, although it is\nno longer easy to explain", "doc_id": "69495d68-cd1b-4936-8294-7b5af706c19d", "embedding": null, "doc_hash": "f74652694549f88c0be89c406126a1003e2290207087103fee5a1bd1b95cb9b9", "extra_info": null, "node_info": {"start": 154562, "end": 157604}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "fc0c55f3-477b-433a-8847-d1839c59f91f", "3": "fde63e85-8dec-4b3a-bfaf-b52c29429abc"}}, "__type__": "1"}, "fde63e85-8dec-4b3a-bfaf-b52c29429abc": {"__data__": {"text": "easily verify that dequeuing bbeforeais consistent with our understand-\ning of sequential FIFO queue behavior.\nLet us consider, however, the alternative concurrent queue implementation in\nFig. 3.3. (This queue is correct only if it is shared by a single enqueuer and a single\ndequeuer.) It has almost the same internal representation as the lock-based queue\nof Fig. 3.1. The only difference is the absence of a lock. We claim this is a correct\nimplementation of a single-enqueuer/single-dequeuer FIFO queue, although it is\nno longer easy to explain why. It may not even be clear what it means for a queue\nto be FIFO when enqueues and dequeues are concurrent.\nUnfortunately, it follows from Amdahl\u2019s Law (Chapter 1) that concurrent\nobjects whose methods hold exclusive locks, and therefore effectively execute one\nafter the other, are less desirable than ones with \ufb01ner-grained locking or no locks\nat all. We therefore need a way to specify the behavior of concurrent objects, and\nto reason about their implementations, without relying on method-level lock-\ning. Nevertheless, the lock-based queue example illustrates a useful principle: it\nis easier to reason about concurrent objects if we can somehow map their con-\ncurrent executions to sequential ones, and limit our reasoning to these sequential\nexecutions. This principle is the key to the correctness properties introduced in\nthis chapter.\n48 Chapter 3 Concurrent Objects\n1class WaitFreeQueue<T> {\n2 volatile int head = 0, tail = 0;\n3 T[] items;\n4 public WaitFreeQueue( int capacity) {\n5 items = (T[]) new Object[capacity];\n6 }\n7 public void enq(T x) throws FullException {\n8 if(tail - head == items.length)\n9 throw new FullException();\n10 items[tail % items.length] = x;\n11 tail++;\n12 }\n13 public T deq() throws EmptyException {\n14 if(tail - head == 0)\n15 throw new EmptyException();\n16 T x = items[head % items.length];\n17 head++;\n18 return x;\n19 }\n20 }\nFigure 3.3 A single-enqueuer/single-dequeuer FIFO queue. The structure is identical to that\nof the lock-based FIFO queue, except that there is no need for the lock to coordinate access.\n3.2 Sequential Objects\nAnobject in languages such as Java and C++ is a container for data. Each object\nprovides a set of methods which are the only way to manipulate that object. Each\nobject has a class, which de\ufb01nes the object\u2019s methods and how they behave. An\nobject has a well-de\ufb01ned state (for example, the FIFO queue\u2019s current sequence of\nitems). There are many ways to describe how an object\u2019s methods behave, ranging\nfrom formal speci\ufb01cations to plain English. The application program interface\n(API) documentation that we use every day lies somewhere in between.\nThe API documentation typically says something like the following: if the\nobject is in such-and-such a state before you call the method, then the object\nwill be in some other state when the method returns, and the call will return a\nparticular value, or throw a particular exception. This kind of description divides\nnaturally into a precondition (describing the object\u2019s state before invoking the\nmethod) and a postcondition , describing, once the method returns, the object\u2019s\nstate and return value. A change to an object\u2019s state is sometimes called a side\neffect . For example, consider how one might specify a \ufb01rst-in-\ufb01rst-out (FIFO)\nqueue class. The class provides two methods: enq() and deq(). The queue state\nis just a sequence of items, possibly empty. If the queue state is a sequence\nq(precondition), then a call to", "doc_id": "fde63e85-8dec-4b3a-bfaf-b52c29429abc", "embedding": null, "doc_hash": "4978413618232124c4f9aaa8cf1b44e7ce0482f292213b615a7b8b867e18d696", "extra_info": null, "node_info": {"start": 157513, "end": 161008}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "69495d68-cd1b-4936-8294-7b5af706c19d", "3": "a2798db4-1ff4-4c0a-96e9-052e969fe8e3"}}, "__type__": "1"}, "a2798db4-1ff4-4c0a-96e9-052e969fe8e3": {"__data__": {"text": "of description divides\nnaturally into a precondition (describing the object\u2019s state before invoking the\nmethod) and a postcondition , describing, once the method returns, the object\u2019s\nstate and return value. A change to an object\u2019s state is sometimes called a side\neffect . For example, consider how one might specify a \ufb01rst-in-\ufb01rst-out (FIFO)\nqueue class. The class provides two methods: enq() and deq(). The queue state\nis just a sequence of items, possibly empty. If the queue state is a sequence\nq(precondition), then a call to enq(z) leaves the queue in state q\u0001z, where\n\u201c\u0001\u201d denotes concatenation. If the queue object is nonempty (precondition), say\na\u0001q, then the deq() method removes and returns the sequence\u2019s \ufb01rst element a\n3.3 Quiescent Consistency 49\n(postcondition), leaving the queue in state q(side effect). If, instead, the queue\nobject is empty (precondition), the method throws EmptyException and leaves\nthe queue state unchanged (postcondition).\nThis style of documentation, called a sequential speci\ufb01cation , is so familiar that\nit is easy to overlook how elegant and powerful it is. The length of the object\u2019s\ndocumentation is linear in the number of methods, because each method can be\ndescribed in isolation. There are a vast number of potential interactions among\nmethods, and all such interactions are characterized succinctly by the methods\u2019\nside effects on the object state. The object\u2019s documentation describes the object\nstate before and after each call, and we can safely ignore any intermediate states\nthat the object may assume while the method call is in progress.\nDe\ufb01ning objects in terms of preconditions and postconditions makes perfect\nsense in a sequential model of computation where a single thread manipulates a\ncollection of objects. Unfortunately, for objects shared by multiple threads, this\nsuccessful and familiar style of documentation falls apart. If an object\u2019s meth-\nods can be invoked by concurrent threads, then the method calls can overlap in\ntime, and it no longer makes sense to talk about their order. What does it mean,\nin a multithreaded program, if xandyare enqueued on a FIFO queue during\noverlapping intervals? Which will be dequeued \ufb01rst? Can we continue to describe\nmethods in isolation, via preconditions and postconditions, or must we provide\nexplicit descriptions of every possible interaction among every possible collection\nof concurrent method calls?\nEven the notion of an object\u2019s state becomes confusing. In single-threaded\nprograms, an object must assume a meaningful state only between method calls.1\nFor concurrent objects, however, overlapping method calls may be in progress at\nevery instant, so the object may never be between method calls. Any method call\nmust be prepared to encounter an object state that re\ufb02ects the incomplete effects\nof other concurrent method calls, a problem that simply does not arise in single-\nthreaded programs.\n3.3 Quiescent Consistency\nOne way to develop an intuition about how concurrent objects should behave is\nto review examples of concurrent computations involving simple objects, and to\ndecide, in each case, whether the behavior agrees with our intuition about how a\nconcurrent object should behave.\nMethod calls take time. A method call is the interval that starts with an\ninvocation event and ends with a response event. Method calls by concurrent\nthreads may overlap, while method calls by a single thread are always sequential\n1There is an exception: care must be taken if one method partially changes an object\u2019s state and\nthen calls another method of that same object.\n50 Chapter 3 Concurrent Objects\nr.write(7)\nr.write(23) r.read(27)Thread", "doc_id": "a2798db4-1ff4-4c0a-96e9-052e969fe8e3", "embedding": null, "doc_hash": "ed82bf83018582f6811c7c50a7b5f06b56ead221c8f37f73bb7d27faa21a2d6e", "extra_info": null, "node_info": {"start": 161025, "end": 164682}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "fde63e85-8dec-4b3a-bfaf-b52c29429abc", "3": "93ef8a94-ca9c-4089-addc-ea989b7eea94"}}, "__type__": "1"}, "93ef8a94-ca9c-4089-addc-ea989b7eea94": {"__data__": {"text": "in each case, whether the behavior agrees with our intuition about how a\nconcurrent object should behave.\nMethod calls take time. A method call is the interval that starts with an\ninvocation event and ends with a response event. Method calls by concurrent\nthreads may overlap, while method calls by a single thread are always sequential\n1There is an exception: care must be taken if one method partially changes an object\u2019s state and\nthen calls another method of that same object.\n50 Chapter 3 Concurrent Objects\nr.write(7)\nr.write(23) r.read(27)Thread A\nThread B\nFigure 3.4 Why each method call should appear to take effect instantaneously. T wo threads\nconcurrently write \u00003 and 7 to a shared register r. Later, one thread reads rand returns the\nvalue\u00007. We expect to \ufb01nd either 7 or \u00003 in the register, not a mixture of both.\n(non-overlapping, one-after-the-other). We say a method call is pending if its call\nevent has occurred, but not its response event.\nFor historical reasons, the object version of a read\u2013write memory location is\ncalled a register (see Chapter 4). In Fig. 3.4, two threads concurrently write \u00003\nand 7 to a shared register r(as before, \u201cr:read(x)\u201d means that a thread reads value\nxfrom register object r, and similarly for \u201c r:write(x).\u201d). Later, one thread reads\nrand returns the value \u00007. This behavior is clearly not acceptable. We expect to\n\ufb01nd either 7 or\u00003 in the register, not a mixture of both. This example suggests\nthe following principle:\nPrinciple 3.3.1. Method calls should appear to happen in a one-at-a-time,\nsequential order.\nBy itself, this principle is usually too weak to be useful. For example, it\npermits reads always to return the object\u2019s initial state, even in sequential\nexecutions.\nHere is a slightly stronger condition. An object is quiescent if it has no pending\nmethod calls.\nPrinciple 3.3.2. Method calls separated by a period of quiescence should appear\nto take effect in their real-time order.\nFor example, suppose AandBconcurrently enqueue xandyin a FIFO queue.\nThe queue becomes quiescent, and then Cenqueuesz. We may not be able to\npredict the relative order of xandyin the queue, but we know they are ahead\nofz.\nT ogether, Principles 3.3.1 and 3.3.2 de\ufb01ne a correctness property called qui-\nescent consistency . Informally, it says that any time an object becomes quies-\ncent, then the execution so far is equivalent to some sequential execution of the\ncompleted calls.\nAs an example of a quiescently consistent object, consider the shared counter\nfrom Chapter 1. A quiescently-consistent shared counter would return numbers,\nnot necessarily in the order of the getAndIncrement () requests, but always\nwithout duplicating or omitting a number. The execution of a quiescently\nconsistent object is somewhat like a musical-chairs game: at any point, the\nmusic might stop, that is, the state could become quiescent. At that point, each\n3.4 Sequential Consistency 51\npending method call must return an index so that all the indexes together meet\nthe speci\ufb01cation of a sequential counter, implying no duplicated or omitted\nnumbers. In other words, a quiescently consistent counter is an index distribu-\ntion mechanism, useful as a \u201cloop counter\u201d in programs that do not care about\nthe order in which indexes are issued.\n3.3.1 Remarks\nHow much does quiescent consistency limit concurrency? Speci\ufb01cally, under\nwhat circumstances does quiescent", "doc_id": "93ef8a94-ca9c-4089-addc-ea989b7eea94", "embedding": null, "doc_hash": "a6148fa56f3c8f65ea0d90e04c055cd75c48c52b6c59e0857b75ced200e28fa5", "extra_info": null, "node_info": {"start": 164663, "end": 168059}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a2798db4-1ff4-4c0a-96e9-052e969fe8e3", "3": "8e5deae7-bc71-4c67-bd61-8dccd1cb66ca"}}, "__type__": "1"}, "8e5deae7-bc71-4c67-bd61-8dccd1cb66ca": {"__data__": {"text": "Sequential Consistency 51\npending method call must return an index so that all the indexes together meet\nthe speci\ufb01cation of a sequential counter, implying no duplicated or omitted\nnumbers. In other words, a quiescently consistent counter is an index distribu-\ntion mechanism, useful as a \u201cloop counter\u201d in programs that do not care about\nthe order in which indexes are issued.\n3.3.1 Remarks\nHow much does quiescent consistency limit concurrency? Speci\ufb01cally, under\nwhat circumstances does quiescent consistency require one method call to block\nwaiting for another to complete? Surprisingly, the answer is (essentially), never .\nA method is total if it is de\ufb01ned for every object state; otherwise it is partial . For\nexample, let us consider the following alternative speci\ufb01cation for an unbounded\nsequential FIFO queue. One can always enqueue another item, but one can\ndequeue only from a nonempty queue. In the sequential speci\ufb01cation of a FIFO\nqueue, enq() is total, since its effects are de\ufb01ned in every queue state, but deq()\nis partial, since its effects are de\ufb01ned only for nonempty queues.\nIn any concurrent execution, for any pending invocation of a total method,\nthere exists a quiescently consistent response. This observation does not mean\nthat it is easy (or even always possible) to \ufb01gure out what that response is, but only\nthat the correctness condition itself does not stand in the way. We say that qui-\nescent consistency is a nonblocking correctness condition. We make this notion\nmore clear in Section 3.6.\nA correctness property Piscompositional if, whenever each object in the sys-\ntem satis\ufb01esP, the system as a whole satis\ufb01es P. Compositionality is impor-\ntant in large systems. Any suf\ufb01ciently complex system must be designed and\nimplemented in a modular fashion. Components are designed, implemented,\nand proved correct independently. Each component makes a clear distinction\nbetween its implementation , which is hidden, and its interface , which precisely\ncharacterizes the guarantees it makes to the other components. For example, if a\nconcurrent object\u2019s interface states that it is a sequentially consistent FIFO queue,\nthen users of the queue need to know nothing about how the queue is imple-\nmented. The result of composing individually correct components that rely only\non one anothers\u2019 interfaces should itself be a correct system. Can we, in fact, com-\npose a collection of independently implemented quiescently consistent objects\nto construct a quiescently consistent system? The answer is, yes: quiescent con-\nsistency is compositional, so quiescently consistent objects can be composed to\nconstruct more complex quiescently consistent objects.\n3.4 Sequential Consistency\nIn Fig. 3.5, a single thread writes 7 and then \u00003 to a shared register r. Later, it\nreadsrand returns 7. For some applications, this behavior might not be accept-\nable because the value the thread read is not the last value it wrote. The order\n52 Chapter 3 Concurrent Objects\nr.write(7) r.write( 23) r.read(7)\nFigure 3.5 Why method calls should appear to take effect in program order. This behavior is\nnot acceptable because the value the thread read is not the last value it wrote.\nq.enq( x) q.deq( y)\nq.enq( y) q.deq( x)\nFigure 3.6 There are two possible sequential orders that can justify this execution. Both\norders are consistent with the method calls\u2019 program order, and either one is enough to\nshow the execution is sequentially consistent.\nin which a single thread issues method calls is called its program order . (Method\ncalls by", "doc_id": "8e5deae7-bc71-4c67-bd61-8dccd1cb66ca", "embedding": null, "doc_hash": "5883772367ae1b913d78be98f5db67e83cadd8f95357b169b3380a125222dfd9", "extra_info": null, "node_info": {"start": 168101, "end": 171651}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "93ef8a94-ca9c-4089-addc-ea989b7eea94", "3": "d85eedae-284d-4c5e-9dcd-6f252d8558ea"}}, "__type__": "1"}, "d85eedae-284d-4c5e-9dcd-6f252d8558ea": {"__data__": {"text": "3.5 Why method calls should appear to take effect in program order. This behavior is\nnot acceptable because the value the thread read is not the last value it wrote.\nq.enq( x) q.deq( y)\nq.enq( y) q.deq( x)\nFigure 3.6 There are two possible sequential orders that can justify this execution. Both\norders are consistent with the method calls\u2019 program order, and either one is enough to\nshow the execution is sequentially consistent.\nin which a single thread issues method calls is called its program order . (Method\ncalls by different threads are unrelated by program order.)\nIn this example, we were surprised that operation calls did not take effect in\nprogram order. This example suggests an alternative principle:\nPrinciple 3.4.1. Method calls should appear to take effect in program order.\nThis principle ensures that purely sequential computations behave the way we\nwould expect.\nT ogether, Principles 3.3.1 and 3.4.1 de\ufb01ne a correctness property called\nsequential consistency , which is widely used in the literature on multiprocessor\nsynchronization.\nSequential consistency requires that method calls act as if they occurred in a\nsequential order consistent with program order. That is, in any concurrent exe-\ncution, there is a way to order the method calls sequentially so that they (1) are\nconsistent with program order, and (2) meet the object\u2019s sequential speci\ufb01cation.\nThere may be more than one order satisfying this condition. In Fig. 3.6, thread\nAenqueuesxwhileBenqueuesy, and thenAdequeuesywhileBdequeues\nx. There are two possible sequential orders that can explain these results: (1)\nAenqueuesx,Benqueuesy,Bdequeuesx, thenAdequeuesy, or (2)B\nenqueuesy,Aenqueuesx,Adequeuesy, thenBdequeuesx. Both these orders\nare consistent with the method calls\u2019 program order, and either one is enough to\nshow the execution is sequentially consistent.\n3.4.1 Remarks\nIt is worth noting that sequential consistency and quiescent consistency are\nincomparable : there exist sequentially consistent executions that are not qui-\nescently consistent, and vice versa. Quiescent consistency does not necessarily\n3.4 Sequential Consistency 53\npreserve program order, and sequential consistency is unaffected by quiescent\nperiods.\nIn most modern multiprocessor architectures, memory reads and writes are\nnot sequentially consistent: they can be typically reordered in complex ways. Most\nof the time no one can tell, because the vast majority of reads\u2013writes are not used\nfor synchronization. In those speci\ufb01c cases where programmers need sequen-\ntial consistency, they must ask for it explicitly. The architectures provide special\ninstructions (usually called memory barriers orfences ) that instruct the processor\nto propagate updates to and from memory as needed, to ensure that reads and\nwrites interact correctly. In the end, the architectures do implement sequential\nconsistency, but only on demand. We discuss further issues related to sequential\nconsistency and the Java programming language in detail in Section 3.8.\nIn Fig. 3.7, thread Aenqueuesx, and laterBenqueuesy, and \ufb01nally A\ndequeuesy. This execution may violate our intuitive notion of how a FIFO queue\nshould behave: the call enqueuing x\ufb01nishes before the call dequeuing ystarts, so\nalthoughyis enqueued after x, it is dequeued before. Nevertheless, this execution\nis sequentially consistent. Even though the call that enqueues xhappens before\nthe call that enqueues y, these calls are unrelated by program order, so", "doc_id": "d85eedae-284d-4c5e-9dcd-6f252d8558ea", "embedding": null, "doc_hash": "1a4072f4b9c80ad83bf232b4763770773c0f5a4324cfcfe4cf46ee6c33c48864", "extra_info": null, "node_info": {"start": 171643, "end": 175118}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8e5deae7-bc71-4c67-bd61-8dccd1cb66ca", "3": "0d9f4495-d3ea-47f7-8e6f-2f9615261d2a"}}, "__type__": "1"}, "0d9f4495-d3ea-47f7-8e6f-2f9615261d2a": {"__data__": {"text": "in detail in Section 3.8.\nIn Fig. 3.7, thread Aenqueuesx, and laterBenqueuesy, and \ufb01nally A\ndequeuesy. This execution may violate our intuitive notion of how a FIFO queue\nshould behave: the call enqueuing x\ufb01nishes before the call dequeuing ystarts, so\nalthoughyis enqueued after x, it is dequeued before. Nevertheless, this execution\nis sequentially consistent. Even though the call that enqueues xhappens before\nthe call that enqueues y, these calls are unrelated by program order, so sequential\nconsistency is free to reorder them.\nOne could argue whether it is acceptable to reorder method calls whose inter-\nvals do not overlap, even if they occur in different threads. For example, we might\nbe unhappy if we deposit our paycheck on Monday, but the bank bounces our rent\ncheck the following Friday because it reordered our deposit after your withdrawal.\nSequential consistency, like quiescent consistency, is nonblocking: any pend-\ning call to a total method can always be completed.\nIs sequential consistency compositional? That is, is the result of compos-\ning multiple sequentially consistent objects itself sequentially consistent? Here,\nunfortunately, the answer is no. In Fig. 3.8, two threads, AandB, call enqueue\nand dequeue methods for two queue objects, pandq. It is not hard to see that\npandqare each sequentially consistent: the sequence of method calls for pis\nthe same as in the sequentially consistent execution shown in Fig. 3.7, and the\nbehavior ofqis symmetric. Nevertheless, the execution as a whole is notsequen-\ntially consistent.\nq.enq( x) q.deq( y)\nq.enq( y)\nFigure 3.7 Sequential consistency versus real-time order. Thread Aenqueues x, and later\nthread Benqueues y, and \ufb01nally Adequeues y. This execution may violate our intuitive notion\nof how a FIFO queue should behave because the method call enqueuing x\ufb01nishes before\nthe method call dequeuing ystarts, so although yis enqueued after x, it is dequeued before.\nNevertheless, this execution is sequentially consistent.\n54 Chapter 3 Concurrent Objects\nq.enq( x) p.deq( y)\np.enq( y) q.deq( x)p.enq( x)\nq.enq( y)A\nB\nFigure 3.8 Sequential consistency is not compositional. T wo threads, AandB, call enqueue\nand dequeue methods on two queue objects, pandq. It is not hard to see that pandqare\neach sequentially consistent, yet the execution as a whole is notsequentially consistent.\nLet us check that there is no correct sequential execution in which these\nmethod calls can be ordered in a way consistent with their program order. Let\nus assume, by way of contradiction, that these method calls can be reordered to\nform a correct FIFO queue execution, where the order of the method calls is con-\nsistent with the program order. We use the following shorthand: hp:enq (x)Ai!\nhp:deq (x)Bimeans that any sequential execution must order A\u2019s enqueue of x\natpbeforeB\u2019s dequeue of xatp, and so on. Because pis FIFO and Adequeues\nyfromp,ymust have been enqueued before x:\nhp:enq (y)Bi!hp:enq (x)Ai\nLikewise,\nhq:enq (x)Ai!hq:enq (y)Bi:\nBut program order implies that\nhp:enq (x)Ai!hq:enq (x)Aiandhq:enq", "doc_id": "0d9f4495-d3ea-47f7-8e6f-2f9615261d2a", "embedding": null, "doc_hash": "e559166d1772c1fd418db4e60edc7ebb8cb2b72ad45eca81df1b63dfb36e68ff", "extra_info": null, "node_info": {"start": 175143, "end": 178210}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d85eedae-284d-4c5e-9dcd-6f252d8558ea", "3": "55b7a8c8-7321-4fc5-b228-bcc3631d36b2"}}, "__type__": "1"}, "55b7a8c8-7321-4fc5-b228-bcc3631d36b2": {"__data__": {"text": "(x)Bimeans that any sequential execution must order A\u2019s enqueue of x\natpbeforeB\u2019s dequeue of xatp, and so on. Because pis FIFO and Adequeues\nyfromp,ymust have been enqueued before x:\nhp:enq (y)Bi!hp:enq (x)Ai\nLikewise,\nhq:enq (x)Ai!hq:enq (y)Bi:\nBut program order implies that\nhp:enq (x)Ai!hq:enq (x)Aiandhq:enq (y)Bi!hp:enq (y)Bi:\nT ogether, these orderings form a cycle.\n3.5 Linearizability\nWe have seen that the principal drawback of sequential consistency is that it is not\ncompositional: the result of composing sequentially consistent components is not\nitself necessarily sequentially consistent. We propose the following way out of this\ndilemma. Let us replace the requirement that method calls appear to happen in\nprogram order with the following stronger restriction:\nPrinciple 3.5.1. Each method call should appear to take effect instantaneously at\nsome moment between its invocation and response.\nThis principle states that the real-time behavior of method calls must be pre-\nserved. We call this correctness property linearizability . Every linearizable execu-\ntion is sequentially consistent, but not vice versa.\n3.6 Formal De\ufb01nitions 55\n3.5.1 Linearization Points\nThe usual way to show that a concurrent object implementation is linearizable is\nto identify for each method a linearization point where the method takes effect.\nFor lock-based implementations, each method\u2019s critical section can serve as its\nlinearization point. For implementations that do not use locking, the lineariza-\ntion point is typically a single step where the effects of the method call become\nvisible to other method calls.\nFor example, let us recall the single-enqueuer/single-dequeuer queue of\nFig. 3.3 . This implementation has no critical sections, and yet we can identify\nits linearization points. Here, the linearization points depend on the execution.\nIf it returns an item, the deq() method has a linearization point when the head\n\ufb01eld is updated (Line 17). If the queue is empty, the deq() method has a lin-\nearization point when it throws EmptyException (Line 15). The enq() method is\nsimilar.\n3.5.2 Remarks\nSequential consistency is a good way to describe standalone systems, such as\nhardware memories, where composition is not an issue. Linearizability, by con-\ntrast, is a good way to describe components of large systems, where components\nmust be implemented and veri\ufb01ed independently. Moreover, the techniques we\nuse to implement concurrent objects, are all linearizable. Because we are inter-\nested in systems that preserve program order and compose, most (but not all)\ndata structures considered in this book are linearizable.\nHow much does linearizability limit concurrency? Linearizability, like sequen-\ntial consistency, is nonblocking. Moreover, like quiescent consistency, but unlike\nsequential consistency, linearizability is compositional; the result of composing\nlinearizable objects is linearizable.\n3.6 Formal De\ufb01nitions\nWe now consider more precise de\ufb01nitions. Here, we focus on the formal prop-\nerties of linearizability, since it is the property most often used in this book. We\nleave it as an exercise to provide the same kinds of de\ufb01nitions for quiescent con-\nsistency and sequential consistency.\nInformally, we know that a concurrent object is linearizable if each method call\nappears to take effect instantaneously at some moment between that method\u2019s\ninvocation and return events. This statement", "doc_id": "55b7a8c8-7321-4fc5-b228-bcc3631d36b2", "embedding": null, "doc_hash": "bb08f9686dff768966d748d4d7f9da8999d295a1bba3840cd6d2d07ea5bbc215", "extra_info": null, "node_info": {"start": 178353, "end": 181773}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0d9f4495-d3ea-47f7-8e6f-2f9615261d2a", "3": "f186fa3c-0da7-471d-89d6-dbf797b7499d"}}, "__type__": "1"}, "f186fa3c-0da7-471d-89d6-dbf797b7499d": {"__data__": {"text": "objects is linearizable.\n3.6 Formal De\ufb01nitions\nWe now consider more precise de\ufb01nitions. Here, we focus on the formal prop-\nerties of linearizability, since it is the property most often used in this book. We\nleave it as an exercise to provide the same kinds of de\ufb01nitions for quiescent con-\nsistency and sequential consistency.\nInformally, we know that a concurrent object is linearizable if each method call\nappears to take effect instantaneously at some moment between that method\u2019s\ninvocation and return events. This statement is probably enough for most infor-\nmal reasoning, but a more precise formulation is needed to take care of some\ntricky cases (such as method calls that have not returned), and for more rigorous\nstyles of argument.\n56 Chapter 3 Concurrent Objects\nAn execution of a concurrent system is modeled by a history , a \ufb01nite sequence\nof method invocation and response events . Asubhistory of a historyHis a subse-\nquence of the events of H. We write a method invocation as hx:m(a\u0003)Ai, where\nxis an object, ma method name, a\u0003a sequence of arguments, and Aa thread.\nWe write a method response as hx:t(r\u0003)Aiwheretis either Okor an excep-\ntion name, and r\u0003is a sequence of result values. Sometimes we refer to an event\nlabeled with thread Aas astep ofA.\nA response matches an invocation if they have the same object and thread. We\nhave been using the term \u201cmethod call\u201d informally, but here is a more formal\nde\ufb01nition: a method call in a historyHis a pair consisting of an invocation and\nthe next matching response in H. We need to distinguish calls that have returned\nfrom those that have not: An invocation is pending inHif no matching response\nfollows the invocation. An extension ofHis a history constructed by append-\ning responses to zero or more pending invocations of H. Sometimes, we ignore\nall pending invocations: complete (H) is the subsequence of Hconsisting of all\nmatching invocations and responses.\nIn some histories, method calls do not overlap: A history Hissequential if the\n\ufb01rst event of His an invocation, and each invocation, except possibly the last, is\nimmediately followed by a matching response.\nSometimes we focus on a single thread or object: a thread subhistory ,HjA\n(\u201cHatA\u201d), of a history His the subsequence of all events in Hwhose thread\nnames areA. An object subhistory Hjxis similarly de\ufb01ned for an object x. In\nthe end, all that matters is how each thread views what happened: two histories\nHandH0areequivalent if for every thread A,HjA=H0jA. Finally, we need to\nrule out histories that make no sense: A history Hiswell formed if each thread\nsubhistory is sequential. All histories we consider here are well-formed. Notice\nthat thread subhistories of a well-formed history are always sequential, but object\nsubhistories need not be.\nHow can we tell whether an object is really a FIFO queue? We simply assume\nthat we have some effective way of recognizing whether any sequential object\nhistory is or is not a legal history for that object\u2019s class. A sequential speci\ufb01cation\nfor an object is just a set of sequential histories for the object. A sequential history\nHislegal if each object subhistory is legal for that object.\nRecall from Chapter 2 that a partial order!on a setXis a relation that is\nirre\ufb02exive and transitive. That is, it is never true that x!x, and whenever x!y\nandy!z, thenx!z. Note that it is possible that there are distinct xandysuch\nthat neither x!ynory!x. Atotal order<onXis a partial order such that\nfor all distinct", "doc_id": "f186fa3c-0da7-471d-89d6-dbf797b7499d", "embedding": null, "doc_hash": "b12458143e8f2607d8347f7977ea81cedf7030dc9123446efaf35db49d784ca3", "extra_info": null, "node_info": {"start": 181594, "end": 185084}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "55b7a8c8-7321-4fc5-b228-bcc3631d36b2", "3": "5266a6f9-47f3-43c2-a34f-57b42c2a792f"}}, "__type__": "1"}, "5266a6f9-47f3-43c2-a34f-57b42c2a792f": {"__data__": {"text": "class. A sequential speci\ufb01cation\nfor an object is just a set of sequential histories for the object. A sequential history\nHislegal if each object subhistory is legal for that object.\nRecall from Chapter 2 that a partial order!on a setXis a relation that is\nirre\ufb02exive and transitive. That is, it is never true that x!x, and whenever x!y\nandy!z, thenx!z. Note that it is possible that there are distinct xandysuch\nthat neither x!ynory!x. Atotal order<onXis a partial order such that\nfor all distinct xandyinX, eitherx<y ory<x .\nAny partial order can be extended to a total order:\nFact 3.6.1. If!is a partial order on X, then there exists a total order \u201c <\u201d onX\nsuch that ifx!y, thenx<y .\nWe say that a method call m0precedes a method call m1in historyHifm0\ufb01n-\nished before m1started: that is, m0\u2019s response event occurs before m1\u2019s invocation\n3.6 Formal De\ufb01nitions 57\nevent. This notion is important enough to introduce some shorthand notation.\nGiven a history Hcontaining method calls m0andm1, we say that m0!Hm1if\nm0precedesm1inH. We leave it as an exercise to show that !His a partial order.\nNotice that if His sequential, then !His a total order. Given a history Hand an\nobjectx, such thatHjxcontains method calls m0andm1, we say that m0!xm1\nifm0precedesm1inHjx.\n3.6.1 Linearizability\nThe basic idea behind linearizability is that every concurrent history is equiva-\nlent, in the following sense, to some sequential history. The basic rule is that if\none method call precedes another, then the earlier call must have taken effect\nbefore the later call. By contrast, if two method calls overlap, then their order is\nambiguous, and we are free to order them in any convenient way.\nMore formally,\nDe\ufb01nition 3.6.1. A historyHislinearizable if it has an extension H0and there\nis a legal sequential history Ssuch that\nL1complete (H0) is equivalent to S, and\nL2if method call m0precedes method call m1inH, then the same is true in S.\nWe refer toSas alinearization ofH. (Hmay have multiple linearizations.)\nInformally, extending HtoH0captures the idea that some pending invo-\ncations may have taken effect, even though their responses have not yet been\nreturned to the caller. Fig. 3.9 illustrates the notion: we must complete the pend-\ningenq(x) method call to justify the deq() call that returns x. The second con-\ndition says that if one method call precedes another in the original history, then\nthat ordering must be preserved in the linearization.\n3.6.2 Compositional Linearizability\nLinearizability is compositional:\nTheorem 3.6.1.His linearizable if, and only if, for each object x,Hjxis lin-\nearizable.\nq.enq( x)\nq.deq( x)\nFigure 3.9 The pending enq(x)method call must take effect early to justify the deq()call\nthat returns x.\n58 Chapter 3 Concurrent Objects\nProof: The \u201conly if\u201d part is left as an exercise.\nFor each object x, pick a linearization of Hjx. LetRxbe the set of responses\nappended to Hjxto construct that linearization, and let !xbe the corresponding\nlinearization order. Let H0be the history constructed by appending to Heach\nresponse inRx.\nWe argue by induction on the number of method calls in H0. For", "doc_id": "5266a6f9-47f3-43c2-a34f-57b42c2a792f", "embedding": null, "doc_hash": "ece91ecd8ce32501030ae10def2166bc6bf9dc9909540fc0fce39dfb02181c3f", "extra_info": null, "node_info": {"start": 185117, "end": 188240}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f186fa3c-0da7-471d-89d6-dbf797b7499d", "3": "21ebb549-b90e-438e-9f82-97eacb914bbd"}}, "__type__": "1"}, "21ebb549-b90e-438e-9f82-97eacb914bbd": {"__data__": {"text": "3.9 The pending enq(x)method call must take effect early to justify the deq()call\nthat returns x.\n58 Chapter 3 Concurrent Objects\nProof: The \u201conly if\u201d part is left as an exercise.\nFor each object x, pick a linearization of Hjx. LetRxbe the set of responses\nappended to Hjxto construct that linearization, and let !xbe the corresponding\nlinearization order. Let H0be the history constructed by appending to Heach\nresponse inRx.\nWe argue by induction on the number of method calls in H0. For the base case,\nifH0contains only one method call, we are done. Otherwise, assume the claim\nfor everyHcontaining fewer than k>1 method calls. For each object x, consider\nthe last method call in H0jx. One of these calls mmust be maximal with respect\nto!H: that is, there is no m0such thatm!Hm0. LetG0be the history de\ufb01ned\nby removing mfromH0. Becausemis maximal,H0is equivalent to G0\u0001m. By the\ninduction hypothesis, G0is linearizable to a sequential history S0, and bothH0\nandHare linearizable to S0\u0001m. 2\nCompositionality is important because it allows concurrent systems to be\ndesigned and constructed in a modular fashion; linearizable objects can be imple-\nmented, veri\ufb01ed, and executed independently. A concurrent system based on a\nnoncompositional correctness property must either rely on a centralized sched-\nuler for all objects, or else satisfy additional constraints placed on objects to\nensure that they follow compatible scheduling protocols.\n3.6.3 The Nonblocking Property\nLinearizability is a nonblocking property: a pending invocation of a total method\nis never required to wait for another pending invocation to complete.\nTheorem 3.6.2. Letinv(m) be an invocation of a total method. If hxinvPiis\na pending invocation in a linearizable history H, then there exists a response\nhxresPisuch thatH\u0001hxresPiis linearizable.\nProof: LetSbe any linearization of H. IfSincludes a response hxresPito\nhxinvPi, we are done, since Sis also a linearization of H\u0001hxresPi. Oth-\nerwise,hxinvPidoes not appear in Seither, since linearizations, by de\ufb01ni-\ntion, include no pending invocations. Because the method is total, there exists\na responsehxresPisuch that\nS0=S\u0001hxinvPi\u0001hxresPi\nis legal.S0, however, is a linearization of H\u0001hxresPi, and hence is also a lin-\nearization of H. 2\nThis theorem implies that linearizability by itself never forces a thread with\na pending invocation of a total method to block. Of course, blocking (or even\ndeadlock) may occur as artifacts of particular implementations of linearizability,\nbut it is not inherent to the correctness property itself. This theorem suggests that\n3.7 Progress Conditions 59\nlinearizability is an appropriate correctness condition for systems where concur-\nrency and real-time response are important.\nThe nonblocking property does not rule out blocking in situations where it\nis explicitly intended. For example, it may be sensible for a thread attempting to\ndequeue from an empty queue to block, waiting until another thread enqueues\nan item. A queue speci\ufb01cation would capture this intention by making the deq()\nmethod\u2019s speci\ufb01cation partial, leaving its effect unde\ufb01ned when applied to an\nempty queue. The most natural concurrent interpretation of a partial sequential\nspeci\ufb01cation is simply to wait until the object reaches a state in which the method\nis de\ufb01ned.\n3.7 Progress", "doc_id": "21ebb549-b90e-438e-9f82-97eacb914bbd", "embedding": null, "doc_hash": "e37f6e77adf79bb437645bc869a727b1de3220049ae90a0ff915272d81cd7287", "extra_info": null, "node_info": {"start": 188248, "end": 191569}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5266a6f9-47f3-43c2-a34f-57b42c2a792f", "3": "1ef569a2-baa2-4856-a111-937b84abdfaa"}}, "__type__": "1"}, "1ef569a2-baa2-4856-a111-937b84abdfaa": {"__data__": {"text": "in situations where it\nis explicitly intended. For example, it may be sensible for a thread attempting to\ndequeue from an empty queue to block, waiting until another thread enqueues\nan item. A queue speci\ufb01cation would capture this intention by making the deq()\nmethod\u2019s speci\ufb01cation partial, leaving its effect unde\ufb01ned when applied to an\nempty queue. The most natural concurrent interpretation of a partial sequential\nspeci\ufb01cation is simply to wait until the object reaches a state in which the method\nis de\ufb01ned.\n3.7 Progress Conditions\nLinearizability\u2019s nonblocking property states that any pending invocation has a\ncorrect response, but does not talk about how to compute such a response. For\nexample, let us consider the scenario for the lock-based queue shown in Fig. 3.1.\nSuppose the queue is initially empty. Ahalts half-way through enqueuing x, and\nBthen invokes deq(). The nonblocking property guarantees that B\u2019s call to deq()\nhas a response: it could either throw an exception or return x. In this implemen-\ntation, however, Bis unable to acquire the lock, and will be delayed as long as A\nis delayed.\nSuch an implementation is called blocking , because an unexpected delay by one\nthread can prevent others from making progress. Unexpected thread delays are\ncommon in multiprocessors. A cache miss might delay a processor for a hundred\ncycles, a page fault for a few million cycles, preemption by the operating system\nfor hundreds of millions of cycles. These delays depend on the speci\ufb01cs of the\nmachine and the operating system.\nA method is wait-free if it guarantees that every call \ufb01nishes its execution\nin a \ufb01nite number of steps. It is bounded wait-free if there is a bound on the\nnumber of steps a method call can take. This bound may depend on the num-\nber of threads. For example, the Bakery algorithm\u2019s doorway section studied in\nChapter 2 is bounded wait-free, where the bound is the number of threads. A\nwait-free method whose performance does not depend on the number of active\nthreads is called population-oblivious . We say that an object is wait-free if its\nmethods are wait-free, and in an object oriented language, we say that a class is\nwait-free if all instances of its objects are wait-free. Being wait-free is an example\nof a nonblocking progress condition, meaning that an arbitrary and unexpected\ndelay by one thread (say, the one holding a lock) does not necessarily prevent the\nothers from making progress.\nThe queue shown in Fig. 3.3 is wait-free. For example, in the scenario where A\nhalts half-way through enqueuing x, andBthen invokes deq(), thenBwill either\nthrow EmptyException (ifAhalted before storing the item in the array) or it\nwill returnx(ifAhalted afterward). The lock-based queue is not nonblocking\n60 Chapter 3 Concurrent Objects\nbecauseBwill take an unbounded number of steps unsuccessfully trying to\nacquire the lock.\nThe wait-free property is attractive because it guarantees that every thread that\ntakes steps makes progress. However, wait-free algorithms can be inef\ufb01cient, and\nsometimes we are willing to settle for a weaker nonblocking property.\nA method is lock-free if it guarantees that in\ufb01nitely often some method call \ufb01n-\nishes in a \ufb01nite number of steps. Clearly, any wait-free method implementation\nis also lock-free, but not vice versa. Lock-free algorithms admit the possibility\nthat some threads could starve. As a practical matter, there are many situations\nin which starvation, while possible, is extremely unlikely, so a fast lock-free algo-\nrithm may be more attractive than a slower wait-free", "doc_id": "1ef569a2-baa2-4856-a111-937b84abdfaa", "embedding": null, "doc_hash": "5f3a6ced2ca6c86efa9763fe9f9ec4eda0a9ff0a52e9dbf13a8decd2756dd2d3", "extra_info": null, "node_info": {"start": 191535, "end": 195102}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "21ebb549-b90e-438e-9f82-97eacb914bbd", "3": "350d0493-624e-4f6c-9f54-6808d05ebd35"}}, "__type__": "1"}, "350d0493-624e-4f6c-9f54-6808d05ebd35": {"__data__": {"text": "we are willing to settle for a weaker nonblocking property.\nA method is lock-free if it guarantees that in\ufb01nitely often some method call \ufb01n-\nishes in a \ufb01nite number of steps. Clearly, any wait-free method implementation\nis also lock-free, but not vice versa. Lock-free algorithms admit the possibility\nthat some threads could starve. As a practical matter, there are many situations\nin which starvation, while possible, is extremely unlikely, so a fast lock-free algo-\nrithm may be more attractive than a slower wait-free algorithm.\n3.7.1 Dependent Progress Conditions\nThe wait-free and lock-free nonblocking progress conditions guarantee that the\ncomputation as a whole makes progress, independently of how the system sched-\nules threads.\nIn Chapter 2 we encountered two progress conditions for blocking imple-\nmentations: the deadlock-free and starvation-free properties. These properties\naredependent progress conditions: progress occurs only if the underlying plat-\nform (i.e., the operating system) provides certain guarantees. In principle, the\ndeadlock-free and starvation-free properties are useful when the operating sys-\ntem guarantees that every thread eventually leaves every critical section. In prac-\ntice, these properties are useful when the operating system guarantees that every\nthread eventually leaves every critical section in a timely manner .\nClasses whose methods rely on lock-based synchronization can guarantee, at\nbest, dependent progress properties. Does this observation mean that lock-based\nalgorithms should be avoided? Not necessarily. If preemption in the middle of a\ncritical section is suf\ufb01ciently rare, then dependent blocking progress conditions\nare effectively indistinguishable from their nonblocking counterparts. If preemp-\ntion is common enough to cause concern, or if the cost of preemption-based\ndelay is suf\ufb01ciently high, then it is sensible to consider nonblocking progress con-\nditions.\nThere is also a dependent nonblocking progress condition: the obstruction-free\nproperty. We say that a method call executes in isolation if no other threads take\nsteps.\nDe\ufb01nition 3.7.1. A method is obstruction-free if, from any point after which it\nexecutes in isolation, it \ufb01nishes in a \ufb01nite number of steps.\nLike the other nonblocking progress conditions, the obstruction-free condi-\ntion ensures that not all threads can be blocked by a sudden delay of one or more\nother threads. A lock-free algorithm is obstruction-free, but not vice versa.\nThe obstruction-free algorithm rules out the use of locks but does not guar-\nantee progress when multiple threads execute concurrently. It seems to defy the\n3.8 The Java Memory Model 61\nfair approach of most operating system schedulers by guaranteeing progress only\nwhen one thread is unfairly scheduled ahead of the others.\nIn practice, however, there is no problem. The obstruction-free condition does\nnot require pausing all threads, only those threads that con\ufb02ict , meaning that they\ncall the same shared object\u2019s methods. The simplest way to exploit an obstruction-\nfree algorithm is to introduce a back-off mechanism: a thread that detects a con-\n\ufb02ict pauses to give an earlier thread time to \ufb01nish. Choosing when to back off,\nand for how long, is a complicated subject discussed in detail in Chapter 7.\nPicking a progress condition for a concurrent object implementation depends\non both the needs of the application and the characteristics of the underlying\nplatform. The absolute wait-free and lock-free progress properties have good\ntheoretical properties, they work on just about any platform, and they provide\nreal-time guarantees useful to applications such as music, electronic games, and\nother interactive", "doc_id": "350d0493-624e-4f6c-9f54-6808d05ebd35", "embedding": null, "doc_hash": "08467b550b861b29a937a9ab1597371300accc1d7568ff99fbb163c02c45c6c1", "extra_info": null, "node_info": {"start": 195109, "end": 198810}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1ef569a2-baa2-4856-a111-937b84abdfaa", "3": "7ad78dd2-7941-40e2-896e-47ddb755595f"}}, "__type__": "1"}, "7ad78dd2-7941-40e2-896e-47ddb755595f": {"__data__": {"text": "a thread that detects a con-\n\ufb02ict pauses to give an earlier thread time to \ufb01nish. Choosing when to back off,\nand for how long, is a complicated subject discussed in detail in Chapter 7.\nPicking a progress condition for a concurrent object implementation depends\non both the needs of the application and the characteristics of the underlying\nplatform. The absolute wait-free and lock-free progress properties have good\ntheoretical properties, they work on just about any platform, and they provide\nreal-time guarantees useful to applications such as music, electronic games, and\nother interactive applications. The dependent obstruction-free, deadlock-free,\nand starvation-free properties rely on guarantees provided by the underlying\nplatform. Given those guarantees, however, the dependent properties often admit\nsimpler and more ef\ufb01cient implementations.\n3.8 The Java Memory Model\nThe Java programming language does not guarantee linearizability, or even\nsequential consistency, when reading or writing \ufb01elds of shared objects. Why\nnot? The principal reason is that strict adherence to sequential consistency would\noutlaw widely used compiler optimizations, such as register allocation, common\nsubexpression elimination, and redundant read elimination, all of which work\nby reordering memory reads\u2013writes. In a single-threaded computation, such\nreorderings are invisible to the optimized program, but in a multithreaded com-\nputation, one thread can spy on another and observe out-of-order executions.\nThe Java memory model satis\ufb01es the Fundamental Property of relaxed mem-\nory models: if a program\u2019s sequentially consistent executions follow certain rules,\nthen every execution of that program in the relaxed model will still be sequen-\ntially consistent. In this section, we describe rules that guarantee that the Java\nprograms are sequentially consistent. We will not try to cover the complete set of\nrules, which is rather large and complex. Instead, we focus on a set of straightfor-\nward rules that should be enough for most purposes.\nFig. 3.10 shows double-checked locking , a once-common programming idiom\nthat falls victim to Java\u2019s lack of sequential consistency. Here, the Singleton\nclass manages a single instance of a Singleton object, accessible through the\ngetInstance () method. This method creates the instance the \ufb01rst time it is\ncalled. This method must be synchronized to ensure that only one instance is\ncreated, even if several threads observe instance to be null. Once the instance\nhas been created, however, no further synchronization should be necessary. As an\noptimization, the code in Fig. 3.10 enters the synchronized block only when\n62 Chapter 3 Concurrent Objects\n1public static Singleton getInstance() {\n2 if(instance == null ) {\n3 synchronized (Singleton. class ) {\n4 if(instance == null )\n5 instance = new Singleton();\n6 }\n7 }\n8 return instance;\n9}\nFigure 3.10 Double-checked locking.\nit observes an instance to be null. Once it has entered, it double-checks that\ninstance is still null before creating the instance.\nThis pattern, once common, is incorrect. At Line 5, the constructor call\nappears to take place before the instance \ufb01eld is assigned, but the Java mem-\nory model allows these steps to occur out of order, effectively making a partially\ninitialized Singleton object visible to other programs.\nIn the Java memory model, objects reside in a shared memory and each thread\nhas a private working memory that contains cached copies of \ufb01elds it has read or\nwritten. In the absence of explicit synchronization (explained later), a thread that\nwrites to a \ufb01eld might not propagate that update to memory right away, and a\nthread that reads a \ufb01eld might not update its working memory if the \ufb01eld\u2019s", "doc_id": "7ad78dd2-7941-40e2-896e-47ddb755595f", "embedding": null, "doc_hash": "bd12f8176606d89b67e7ba86c28a01ba3f940bfb8d836d0612e7cf764291407d", "extra_info": null, "node_info": {"start": 198745, "end": 202480}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "350d0493-624e-4f6c-9f54-6808d05ebd35", "3": "18923536-bb2f-4fb3-914e-068b25b6dda3"}}, "__type__": "1"}, "18923536-bb2f-4fb3-914e-068b25b6dda3": {"__data__": {"text": "assigned, but the Java mem-\nory model allows these steps to occur out of order, effectively making a partially\ninitialized Singleton object visible to other programs.\nIn the Java memory model, objects reside in a shared memory and each thread\nhas a private working memory that contains cached copies of \ufb01elds it has read or\nwritten. In the absence of explicit synchronization (explained later), a thread that\nwrites to a \ufb01eld might not propagate that update to memory right away, and a\nthread that reads a \ufb01eld might not update its working memory if the \ufb01eld\u2019s copy\nin memory changes value. Naturally, a Java virtual machine is free to keep such\ncached copies consistent, and in practice they often do, but they are not required\nto do so. At this point, we can guarantee only that a thread\u2019s own reads\u2013writes\nappear to that thread to happen in order, and that any \ufb01eld value read by a thread\nwas written to that \ufb01eld (i.e., values do not appear out of thin air).\nCertain statements are synchronization events . Usually, the term \u201csynchroniza-\ntion\u201d implies some form of atomicity or mutual exclusion. In Java, however, it\nalso implies reconciling a thread\u2019s working memory with the shared memory.\nSome synchronization events cause a thread to write cached changes back to\nshared memory, making those changes visible to other threads. Other synchro-\nnization events cause the thread to invalidate its cached values, forcing it to reread\n\ufb01eld values from memory, making other threads\u2019 changes visible. Synchroniza-\ntion events are linearizable: they are totally ordered, and all threads agree on that\nordering. We now look at different kinds of synchronization events.\n3.8.1 Locks and Synchronized Blocks\nA thread can achieve mutual exclusion either by entering a synchronized block\nor method, which acquires an implicit lock, or by acquiring an explicit lock\n(such as the ReentrantLock from the java.util.concurrent.locks package). Both\napproaches have the same implications for memory behavior.\nIf all accesses to a particular \ufb01eld are protected by the same lock, then\nreads\u2013writes to that \ufb01eld are linearizable. Speci\ufb01cally, when a thread releases a\nlock, modi\ufb01ed \ufb01elds in working memory are written back to shared memory,\n3.8 The Java Memory Model 63\nperforming modi\ufb01cations while holding the lock accessible to other threads.\nWhen a thread acquires the lock, it invalidates its working memory to ensure\n\ufb01elds are reread from shared memory. T ogether, these conditions ensure that\nreads\u2013writes to the \ufb01elds of any object protected by a single lock are linearizable.\n3.8.2 Volatile Fields\nVolatile \ufb01elds are linearizable. Reading a volatile \ufb01eld is like acquiring a lock:\nthe working memory is invalidated and the volatile \ufb01eld\u2019s current value is reread\nfrom memory. Writing a volatile \ufb01eld is like releasing a lock: the volatile \ufb01eld is\nimmediately written back to memory.\nAlthough reading and writing a volatile \ufb01eld has the same effect on mem-\nory consistency as acquiring and releasing a lock, multiple reads\u2013writes are not\natomic. For example, if xis a volatile variable, the expression x++ will not neces-\nsarily increment xif concurrent threads can modify x. Some form of mutual\nexclusion is needed as well. One common usage pattern for volatile variables\noccurs when a \ufb01eld is read by multiple threads, but only written by one.\nThejava.util.concurrent.atomic package includes classes that provide lineariz-\nable memory such as AtomicReference<T> orAtomicInteger . The\ncompareAndSet() and set() methods", "doc_id": "18923536-bb2f-4fb3-914e-068b25b6dda3", "embedding": null, "doc_hash": "5c33395eb058aa39119093eed4daf24b276e42d3266229a7d8be44150ddd8ad7", "extra_info": null, "node_info": {"start": 202520, "end": 206030}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7ad78dd2-7941-40e2-896e-47ddb755595f", "3": "4b52b7f9-174d-491d-aea2-ed3beebfa479"}}, "__type__": "1"}, "4b52b7f9-174d-491d-aea2-ed3beebfa479": {"__data__": {"text": "as acquiring and releasing a lock, multiple reads\u2013writes are not\natomic. For example, if xis a volatile variable, the expression x++ will not neces-\nsarily increment xif concurrent threads can modify x. Some form of mutual\nexclusion is needed as well. One common usage pattern for volatile variables\noccurs when a \ufb01eld is read by multiple threads, but only written by one.\nThejava.util.concurrent.atomic package includes classes that provide lineariz-\nable memory such as AtomicReference<T> orAtomicInteger . The\ncompareAndSet() and set() methods act like volatile writes, and get() acts\nlike a volatile read.\n3.8.3 Final Fields\nRecall that a \ufb01eld declared to be final cannot be modi\ufb01ed once it has been ini-\ntialized. An object\u2019s \ufb01nal \ufb01elds are initialized in its constructor. If the constructor\nfollows certain simple rules, described in the following paragraphs, then the cor-\nrect value of any \ufb01nal \ufb01elds will be visible to other threads without synchroniza-\ntion. For example, in the code shown in Fig. 3.11, a thread that calls reader () is\n1class FinalFieldExample {\n2 final int x;int y;\n3 static FinalFieldExample f;\n4 public FinalFieldExample() {\n5 x = 3;\n6 y = 4;\n7 }\n8 static void writer() {\n9 f = new FinalFieldExample();\n10 }\n11 static void reader() {\n12 if(f != null ) {\n13 int i = f.x; int j = f.y;\n14 }\n15 }\n16 }\nFigure 3.11 Constructor with \ufb01nal \ufb01eld.\n64 Chapter 3 Concurrent Objects\n1public class EventListener {\n2 final int x;\n3 public EventListener(EventSource eventSource) {\n4 eventSource.registerListener( this );// register with event source ...\n5 }\n6 public onEvent(Event e) {\n7 ... // handle the event\n8 }\n9}\nFigure 3.12 Incorrect EventListener class.\nguaranteed to see xequal to 3, because the x\ufb01eld is \ufb01nal. There is no guarantee\nthatywill be equal to 4, because yis not \ufb01nal.\nIf a constructor is synchronized incorrectly, however, then \ufb01nal \ufb01elds may be\nobserved to change value. The rule is simple: the this reference must not be\nreleased from the constructor before the constructor returns.\nFig. 3.12 shows an example of an incorrect constructor in an event-driven\nsystem. Here, an EventListener class registers itself with an EventSource class,\nmaking a reference to the listener object accessible to other threads. This code\nmay appear safe, since registration is the last step in the constructor, but it is\nincorrect, because if another thread calls the event listener\u2019s onEvent () method\nbefore the constructor \ufb01nishes, then the onEvent () method is not guaranteed to\nsee a correct value for x.\nIn summary, reads\u2013writes to \ufb01elds are linearizable if either the \ufb01eld is volatile,\nor the \ufb01eld is protected by a unique lock which is acquired by all readers and\nwriters.\n3.9 Remarks\nWhat progress condition is right for one\u2019s application? Obviously, it depends on\nthe needs of the application and the nature of the system it is intended to run on.\nHowever, this is actually a \u201ctrick question\u201d since different methods, even ones\napplied to the same object, can have different progress conditions. A frequently\ncalled time-critical method such as a table lookup in a \ufb01rewall program, should\nbe wait-free, while an infrequent call to update a table entry can be implemented\nusing mutual exclusion. As we will see, it is quite natural to write applications\nwhose methods differ in", "doc_id": "4b52b7f9-174d-491d-aea2-ed3beebfa479", "embedding": null, "doc_hash": "ed7b2007bcf87e9c8f7bffac224b0fd5b0760ee50f718cd5422a47f4abe573c1", "extra_info": null, "node_info": {"start": 206030, "end": 209335}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "18923536-bb2f-4fb3-914e-068b25b6dda3", "3": "c2e99bde-8485-43c2-aa92-cd84ce2d8914"}}, "__type__": "1"}, "c2e99bde-8485-43c2-aa92-cd84ce2d8914": {"__data__": {"text": "progress condition is right for one\u2019s application? Obviously, it depends on\nthe needs of the application and the nature of the system it is intended to run on.\nHowever, this is actually a \u201ctrick question\u201d since different methods, even ones\napplied to the same object, can have different progress conditions. A frequently\ncalled time-critical method such as a table lookup in a \ufb01rewall program, should\nbe wait-free, while an infrequent call to update a table entry can be implemented\nusing mutual exclusion. As we will see, it is quite natural to write applications\nwhose methods differ in their progress guarantees.\nWhich correctness condition is right for one\u2019s application? Well, it depends\non the needs of the application. A lightly loaded printer server that uses a queue\nto hold, say print jobs, might be satis\ufb01ed with a quiescently-consistent queue,\nsince the order in which documents are printed is of little importance. A banking\nserver should execute customer requests in program order (transfer $100 from\n3.10 Chapter Notes 65\nsavings to checking, write a check for $50), so it should use a sequentially consis-\ntent queue. A stock-trading server is required to be fair, so orders from different\ncustomers must be executed in the order they arrive, so it would require a lin-\nearizable queue.\nThe following joke circulated in Italy in the 1920s. According to Mussolini, the\nideal citizen is intelligent, honest, and Fascist. Unfortunately, no one is perfect,\nwhich explains why everyone you meet is either intelligent and Fascist but not\nhonest, honest and Fascist but not intelligent, or honest and intelligent but not\nFascist.\nAs programmers, it would be ideal to have linearizable hardware, linearizable\ndata structures, and good performance. Unfortunately, technology is imperfect,\nand for the time being, hardware that performs well is not even sequentially con-\nsistent. As the joke goes, that leaves open the possibility that data structures might\nstill be linearizable while performing well. Nevertheless, there are many chal-\nlenges to make this vision work, and the remainder of this book is a road map\nshowing how to attain this goal.\n3.10 Chapter Notes\nThe notion of quiescent consistency was introduced implicitly by James Aspnes,\nMaurice Herlihy, and Nir Shavit [16] and more explicitly by Nir Shavit and Asaph\nZemach [143]. Leslie Lamport [90] introduced the notion of sequential consis-\ntency , while Christos Papadimitriou [123] formulated the canonical formal char-\nacterization of serializability . William Weihl [149] was the \ufb01rst to point out the\nimportance of compositionality (which he called locality ). Maurice Herlihy and\nJeannette Wing [69] introduced the notion of linearizability in 1990. Leslie Lam-\nport [93, 94] introduced the notion of an atomic register in 1986.\nT o the best of our knowledge, the notion of wait-freedom \ufb01rst appeared\nimplicitly in Leslie Lamport\u2019s Bakery algorithm [88]. Lock-freedom has had sev-\neral historical meanings and only in recent years has it converged to its cur-\nrent de\ufb01nition. Obstruction-freedom was introduced by Maurice Herlihy, Victor\nLuchangco, and Mark Moir [61]. The notion of dependent progress was intro-\nduced by Maurice Herlihy and Nir Shavit [63].\nProgramming languages such as C or C++ were not de\ufb01ned with concurrency\nin mind, so they do not de\ufb01ne a memory model. The actual behavior of a con-\ncurrent C or C++ program is the result of a complex combination of the underly-\ning hardware, the compiler, and concurrency library. See Hans Boehm", "doc_id": "c2e99bde-8485-43c2-aa92-cd84ce2d8914", "embedding": null, "doc_hash": "ddcefd8837e9717b8aa6df10d3e26ce1195b7b2b879dc55fedcb68af56b73b5d", "extra_info": null, "node_info": {"start": 209310, "end": 212842}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "4b52b7f9-174d-491d-aea2-ed3beebfa479", "3": "9d4e7f11-5c56-464b-91f2-a501006a4b6f"}}, "__type__": "1"}, "9d4e7f11-5c56-464b-91f2-a501006a4b6f": {"__data__": {"text": "Obstruction-freedom was introduced by Maurice Herlihy, Victor\nLuchangco, and Mark Moir [61]. The notion of dependent progress was intro-\nduced by Maurice Herlihy and Nir Shavit [63].\nProgramming languages such as C or C++ were not de\ufb01ned with concurrency\nin mind, so they do not de\ufb01ne a memory model. The actual behavior of a con-\ncurrent C or C++ program is the result of a complex combination of the underly-\ning hardware, the compiler, and concurrency library. See Hans Boehm [21] for a\nmore detailed discussion of these issues. The Java memory model proposed here\nis the second memory model proposed for Java. Jeremy Manson, Bill Pugh, and\nSarita Adve [111] give a more complete description of the current Java memory.\nThe 2-thread queue is considered folklore, yet as far as we are aware, it \ufb01rst\nappeared in print in a paper by Leslie Lamport [91].\n66 Chapter 3 Concurrent Objects\n3.11 Exercises\nExercise 21. Explain why quiescent consistency is compositional.\nExercise 22. Consider a memory object that encompasses two register compo-\nnents. We know that if both registers are quiescently consistent, then so is the\nmemory. Does the converse hold? If the memory is quiescently consistent, are\nthe individual registers quiescently consistent? Outline a proof, or give a coun-\nterexample.\nExercise 23. Give an example of an execution that is quiescently consistent but\nnot sequentially consistent, and another that is sequentially consistent but not\nquiescently consistent.\nExercise 24. For each of the histories shown in Figs. 3.13 and 3.14, are they qui-\nescently consistent? Sequentially consistent? Linearizable? Justify your answer.\nExercise 25. If we drop condition L2 from the linearizability de\ufb01nition, is the\nresulting property the same as sequential consistency? Explain.\nr.read(1)\nr.write(1)\nr.write(2)A\nB\nCr.read(2)\nFigure 3.13 First history for Exercise 24.\nr.read(1)\nr.write(1)\nr.write(2)A\nB\nCr.read(1)\nFigure 3.14 Second history for Exercise 24.\n3.11 Exercises 67\nExercise 26. Prove the \u201conly if\u201d part of Theorem 3.6.1\nExercise 27. The AtomicInteger class (in the java.util.concurrent.atomic pack-\nage) is a container for an integer value. One of its methods is\nboolean compareAndSet( int expect, int update).\nThis method compares the object\u2019s current value to expect . If the values are\nequal, then it atomically replaces the object\u2019s value with update and returns true.\nOtherwise, it leaves the object\u2019s value unchanged, and returns false . This class also\nprovides\nint get()\nwhich returns the object\u2019s actual value.\nConsider the FIFO queue implementation shown in Fig. 3.15. It stores its\nitems in an array items , which, for simplicity, we will assume has unbounded\nsize. It has two AtomicInteger \ufb01elds: head is the index of the next slot from\nwhich to remove an item, and tail is the index of the next slot in which to place\nan item. Give an example showing that this implementation is notlinearizable.\nExercise 28. Consider the class shown in Fig. 3.16. According to what you have\nbeen told about the Java memory model, will the reader method ever divide by\nzero?\n1class IQueue<T> {\n2 AtomicInteger head = new AtomicInteger(0);\n3 AtomicInteger tail = new AtomicInteger(0);\n4 T[] items = (T[]) new Object[Integer.MAX_VALUE];\n5 public", "doc_id": "9d4e7f11-5c56-464b-91f2-a501006a4b6f", "embedding": null, "doc_hash": "8a778d16baffc557ede6a1518f1e6495581db3a76914f0729df64e0690855fa6", "extra_info": null, "node_info": {"start": 212937, "end": 216195}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "c2e99bde-8485-43c2-aa92-cd84ce2d8914", "3": "1aa6741f-a241-4f95-8228-ee90c90592a6"}}, "__type__": "1"}, "1aa6741f-a241-4f95-8228-ee90c90592a6": {"__data__": {"text": "head is the index of the next slot from\nwhich to remove an item, and tail is the index of the next slot in which to place\nan item. Give an example showing that this implementation is notlinearizable.\nExercise 28. Consider the class shown in Fig. 3.16. According to what you have\nbeen told about the Java memory model, will the reader method ever divide by\nzero?\n1class IQueue<T> {\n2 AtomicInteger head = new AtomicInteger(0);\n3 AtomicInteger tail = new AtomicInteger(0);\n4 T[] items = (T[]) new Object[Integer.MAX_VALUE];\n5 public void enq(T x) {\n6 int slot;\n7 do{\n8 slot = tail.get();\n9 }while (! tail.compareAndSet(slot, slot+1));\n10 items[slot] = x;\n11 }\n12 public T deq() throws EmptyException {\n13 T value;\n14 int slot;\n15 do{\n16 slot = head.get();\n17 value = items[slot];\n18 if(value == null )\n19 throw new EmptyException();\n20 }while (! head.compareAndSet(slot, slot+1));\n21 return value;\n22 }\n23 }\nFigure 3.15 IQueue implementation.\n68 Chapter 3 Concurrent Objects\n1class VolatileExample {\n2 int x = 0;\n3 volatile boolean v = false ;\n4 public void writer() {\n5 x = 42;\n6 v = true ;\n7 }\n8 public void reader() {\n9 if(v == true ) {\n10 int y = 100/x;\n11 }\n12 }\n13 }\nFigure 3.16 Volatile \ufb01eld example from Exercise 28 .\nExercise 29. Is the following property equivalent to saying that object xis wait-\nfree?\nFor every in\ufb01nite history Hofx, every thread that takes an in\ufb01nite number of\nsteps inHcompletes an in\ufb01nite number of method calls.\nExercise 30. Is the following property equivalent to saying that object xis lock-\nfree?\nFor every in\ufb01nite history Hofx, an in\ufb01nite number of method calls are com-\npleted.\nExercise 31. Consider the following rather unusual implementation of a method\nm. In every history, the ithtime a thread calls m, the call returns after 2isteps. Is\nthis method wait-free, bounded wait-free, or neither?\nExercise 32. This exercise examines a queue implementation ( Fig. 3.17 ) whose\nenq() method does not have a linearization point.\nThe queue stores its items in an items array, which for simplicity we will\nassume is unbounded. The tail \ufb01eld is an AtomicInteger , initially zero. The\nenq() method reserves a slot by incrementing tail , and then stores the item at\nthat location. Note that these two steps are not atomic: there is an interval after\ntail has been incremented but before the item has been stored in the array.\nThe deq() method reads the value of tail , and then traverses the array in\nascending order from slot zero to the tail. For each slot, it swaps null with the\ncurrent contents, returning the \ufb01rst non- null item it \ufb01nds. If all slots are null, the\nprocedure is restarted.\nGive an example execution showing that the linearization point for enq() can-\nnot occur at Line 15.\nHint: give an execution where two enq() calls are not linearized in the order\nthey execute Line 15.\n3.11 Exercises 69\n1public class HWQueue<T> {\n2 AtomicReference<T>[] items;\n3 AtomicInteger tail;\n4 static final int CAPACITY = 1024;\n5\n6 public HWQueue() {\n7 items =(AtomicReference<T>[])Array.newInstance(AtomicReference. class ,\n8 CAPACITY);\n9 for (int i = 0; i < items.length; i++) {\n10", "doc_id": "1aa6741f-a241-4f95-8228-ee90c90592a6", "embedding": null, "doc_hash": "da55f510bf6628a1f8d82874675d469581f003044d5f4c3c6b52a3567a4c7488", "extra_info": null, "node_info": {"start": 216151, "end": 219261}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9d4e7f11-5c56-464b-91f2-a501006a4b6f", "3": "fb2a73df-85c0-4bc1-a84c-fb159222f647"}}, "__type__": "1"}, "fb2a73df-85c0-4bc1-a84c-fb159222f647": {"__data__": {"text": "point for enq() can-\nnot occur at Line 15.\nHint: give an execution where two enq() calls are not linearized in the order\nthey execute Line 15.\n3.11 Exercises 69\n1public class HWQueue<T> {\n2 AtomicReference<T>[] items;\n3 AtomicInteger tail;\n4 static final int CAPACITY = 1024;\n5\n6 public HWQueue() {\n7 items =(AtomicReference<T>[])Array.newInstance(AtomicReference. class ,\n8 CAPACITY);\n9 for (int i = 0; i < items.length; i++) {\n10 items[i] = new AtomicReference<T>( null );\n11 }\n12 tail = new AtomicInteger(0);\n13 }\n14 public void enq(T x) {\n15 int i = tail.getAndIncrement();\n16 items[i].set(x);\n17 }\n18 public T deq() {\n19 while (true ) {\n20 int range = tail.get();\n21 for (int i = 0; i < range; i++) {\n22 T value = items[i].getAndSet( null );\n23 if(value != null ) {\n24 return value;\n25 }\n26 }\n27 }\n28 }\n29 }\nFigure 3.17 Herlihy/Wing queue.\nGive another example execution showing that the linearization point for enq()\ncannot occur at Line 16.\nSince these are the only two memory accesses in enq(), we must conclude that\nenq() has no single linearization point. Does this mean enq() is not linearizable?\nExercise 33. Prove that sequential consistency is nonblocking.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n4Foundations of\nShared Memory\nThe foundations of sequential computing were established in the 1930s by Alan\nTuring and Alonzo Church, who independently formulated what has come to be\nknown as the Church-Turing Thesis : anything that canbe computed, can be com-\nputed by a Turing Machine (or, equivalently, by Church\u2019s Lambda Calculus). Any\nproblem that cannot be solved by a Turing Machine (such as deciding whether\na program halts on any input) is universally considered to be unsolvable by any\nkind of practical computing device. The Turing Thesis is a thesis , not a theorem,\nbecause the notion of \u201cwhat is computable\u201d can never be de\ufb01ned in a precise,\nmathematically rigorous way. Nevertheless, just about everyone believes it.\nThis chapter describes the foundations of concurrent shared memory com-\nputing . A shared-memory computation consists of multiple threads , each of\nwhich is a sequential program in its own right. These threads communicate by\ncalling methods of objects that reside in a shared memory. Threads are asyn-\nchronous , meaning that they run at different speeds, and any thread can halt\nfor an unpredictable duration at any time. This notion of asynchrony re\ufb02ects\nthe realities of modern multiprocessor architectures, where thread delays are\nunpredictable, ranging from microseconds (cache misses), to milliseconds (page\nfaults), to seconds (scheduling interruptions).\nThe classical theory of sequential computability proceeds in stages. It starts\nwith \ufb01nite-state automata, moves on to push-down automata, and culminates\nin Turing Machines. We, too, consider a progression of models for concurrent\ncomputing. In this chapter we start with the simplest form of shared-memory\ncomputation: concurrent threads apply simple read\u2013write operations to shared-\nmemory locations, called registers for historical reasons. We start with very simple\nregisters, and we show how to use them to construct a series of more complex\nregisters.\nThe classical theory of sequential computability is", "doc_id": "fb2a73df-85c0-4bc1-a84c-fb159222f647", "embedding": null, "doc_hash": "8bcf183c68b403cb27531f65d7616c7158c055dee0799e33a35db08b3aaa8be1", "extra_info": null, "node_info": {"start": 219337, "end": 222612}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1aa6741f-a241-4f95-8228-ee90c90592a6", "3": "138d870d-4e3a-44e3-904b-063bcbd32f1b"}}, "__type__": "1"}, "138d870d-4e3a-44e3-904b-063bcbd32f1b": {"__data__": {"text": "in stages. It starts\nwith \ufb01nite-state automata, moves on to push-down automata, and culminates\nin Turing Machines. We, too, consider a progression of models for concurrent\ncomputing. In this chapter we start with the simplest form of shared-memory\ncomputation: concurrent threads apply simple read\u2013write operations to shared-\nmemory locations, called registers for historical reasons. We start with very simple\nregisters, and we show how to use them to construct a series of more complex\nregisters.\nThe classical theory of sequential computability is (for the most part) not con-\ncerned with ef\ufb01ciency: to show that a problem is computable, it is enough to show\nthat it can be solved by a Turing Machine. There is little incentive to make such\na Turing Machine ef\ufb01cient, because a Turing Machine is not a practical means\nof computation. In the same way, we make little attempt to make our register\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00004-6\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.71\n72 Chapter 4 Foundations of Shared Memory\nconstructions ef\ufb01cient. We are interested in understanding whether such\nconstructions exist, and how they work. They are not intended to be a practi-\ncal model for computation. Instead, we prefer easy-to-understand but inef\ufb01cient\nconstructions over complicated but ef\ufb01cient ones. In particular, some of our\nconstructions use timestamps (counter values) to distinguish older values from\nnewer values.\nThe problem with timestamps is that they grow without bound, and even-\ntually over\ufb02ow any \ufb01xed-size variable. Bounded solutions (such as the one in\nSection 2.7 of Chapter 2) are (arguably) more intellectually satisfying, and we\nencourage readers to investigate them further through the references provided\nin the chapter notes. Here, however, we focus on simpler, unbounded construc-\ntions, because they illustrate fundamental principles of concurrent programming\nwith less danger of becoming distracted by technicalities.\n4.1 The Space of Registers\nAt the hardware level, threads communicate by reading and writing shared\nmemory. A good way to understand inter-thread communication is to abstract\naway from hardware primitives, and to think about communication as happening\nthrough shared concurrent objects . Chapter 3 provided a detailed description of\nshared objects. For now, it suf\ufb01ces to recall the two key properties of their design:\nsafety, de\ufb01ned by consistency conditions, and liveness, de\ufb01ned by progress\nconditions.\nAread\u2013write register (or just a register ), is an object that encapsulates a value\nthat can be observed by a read () method and modi\ufb01ed by a write () method\n(in real systems these method calls are often called load and store ). Fig. 4.1\nillustrates the Register<T> interface implemented by all registers. The type T\nof the value is typically either Boolean ,Integer , or a reference to an object.\nA register that implements the Register<Boolean> interface is called a Boolean\nregister (we sometimes use 1 and 0 as synonyms for true and false ). A register\nthat implements Register<Integer> for a range of Minteger values is called an\nM-valued register . We do not explicitly discuss any other kind of register, except\nto note that any algorithm that implements integer registers can be adapted to\nimplement registers that hold references to other objects, simply by treating those\nreferences as integers.\n1public interface Register<T> {\n2 T read();\n3 void write(T v);\n4}\nFigure 4.1 TheRegister<T> interface.\n4.1 The Space of Registers 73\n1public class SequentialRegister<T> implements", "doc_id": "138d870d-4e3a-44e3-904b-063bcbd32f1b", "embedding": null, "doc_hash": "65b144a436426724183529fc56e7bd8cc3cad1bd1822c1184714a004d3802d95", "extra_info": null, "node_info": {"start": 222511, "end": 226100}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "fb2a73df-85c0-4bc1-a84c-fb159222f647", "3": "5a1ca5da-7871-42e1-8b9b-a29dcb68525d"}}, "__type__": "1"}, "5a1ca5da-7871-42e1-8b9b-a29dcb68525d": {"__data__": {"text": "false ). A register\nthat implements Register<Integer> for a range of Minteger values is called an\nM-valued register . We do not explicitly discuss any other kind of register, except\nto note that any algorithm that implements integer registers can be adapted to\nimplement registers that hold references to other objects, simply by treating those\nreferences as integers.\n1public interface Register<T> {\n2 T read();\n3 void write(T v);\n4}\nFigure 4.1 TheRegister<T> interface.\n4.1 The Space of Registers 73\n1public class SequentialRegister<T> implements Register<T> {\n2 private T value;\n3 public T read() {\n4 return value;\n5 }\n6 public void write(T v) {\n7 value = v;\n8 }\n9}\nFigure 4.2 TheSequentialRegister class.\nIf method calls do not overlap, a register implementation should behave as\nshown in Fig. 4.2. On a multiprocessor, however, we expect method calls to over-\nlap all the time, so we need to specify what the concurrent method calls mean.\nOne approach is to rely on mutual exclusion: protect each register with a\nmutex lock acquired by each read () and write () call. Unfortunately, we cannot\nuse mutual exclusion here. Chapter 2 describes how to accomplish mutual exclu-\nsion using registers, so it makes little sense to implement registers using mutual\nexclusion. Moreover, as we saw in Chapter 3, using mutual exclusion, even if\nit is deadlock- or starvation-free, would mean that the computation\u2019s progress\nwould depend on the operating system scheduler to guarantee that threads\nnever get stuck in critical sections. Since we wish to examine the basic build-\ning blocks of concurrent computation using shared objects, it makes little sense\nto assume the existence of a separate entity to provide one of their key properties:\nprogress.\nHere is a different approach. Recall that an object implementation is wait-\nfree if each method call \ufb01nishes in a \ufb01nite number of steps, independently of\nhow its execution is interleaved with steps of other concurrent method calls. The\nwait-free condition may seem simple and natural, but it has far-reaching conse-\nquences. In particular, it rules out any kind of mutual exclusion, and guarantees\nindependent progress, that is, without relying on an operating system scheduler.\nWe therefore require our register implementations to be wait-free.1\nAnatomic register is a linearizable implementation of the sequential register\nclass shown in Fig. 4.2. Informally, an atomic register behaves exactly as we would\nexpect: each read returns the \u201clast\u201d value written. A model in which threads com-\nmunicate by reading and writing to atomic registers is intuitively appealing, and\nfor a long time was the standard model of concurrent computation.\nIt is also important to specify how many readers and writers are expected. Not\nsurprisingly, it is easier to implement a register that supports only a single reader\nand writer than one that supports multiple readers and writers. For brevity, we\nuse SRSW to mean \u201csingle-reader, single-writer,\u201d MRSW to mean \u201cmulti-reader,\nsingle-writer,\u201d and MRMW to mean \u201cmulti-reader, multi-writer.\u201d\n1A wait-free implementation is also lock-free.\n74 Chapter 4 Foundations of Shared Memory\nIn this chapter, we address the following fundamental question:\nCan any data structure implemented from the most powerful registers we de\ufb01ne\nalso be implemented from the weakest?\nWe recall from Chapter 1 that any useful form of inter-thread communication\nmust be persistent: the message sent must outlive the active participation of the\nsender. The weakest form of persistent synchronization is (arguably) the ability\nto set a single persistent bit in shared memory, and the weakest", "doc_id": "5a1ca5da-7871-42e1-8b9b-a29dcb68525d", "embedding": null, "doc_hash": "93e8f563c290e75c3e243d930c92da59d18a6b373106915e30169862c5834ed8", "extra_info": null, "node_info": {"start": 226101, "end": 229741}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "138d870d-4e3a-44e3-904b-063bcbd32f1b", "3": "1a152cef-294a-4783-b700-42724600b4dc"}}, "__type__": "1"}, "1a152cef-294a-4783-b700-42724600b4dc": {"__data__": {"text": "multi-writer.\u201d\n1A wait-free implementation is also lock-free.\n74 Chapter 4 Foundations of Shared Memory\nIn this chapter, we address the following fundamental question:\nCan any data structure implemented from the most powerful registers we de\ufb01ne\nalso be implemented from the weakest?\nWe recall from Chapter 1 that any useful form of inter-thread communication\nmust be persistent: the message sent must outlive the active participation of the\nsender. The weakest form of persistent synchronization is (arguably) the ability\nto set a single persistent bit in shared memory, and the weakest form of synchro-\nnization is (unarguably) none at all: if the act of setting a bit does not overlap\nthe act of reading that bit, then the value read is the same as the value written.\nOtherwise, a read overlapping a write could return any value.\nDifferent kinds of registers come with different guarantees that make them\nmore or less powerful. For example, we have seen that registers may differ in the\nrange of values they may encapsulate (for example, Boolean vs. M-valued), and\nin the number of readers and writers they support. Finally, they may differ in the\ndegree of consistency they provide.\nA single-writer, multi-reader register implementation is safeif \u2014\n\u0004Aread () call that does not overlap a write () call returns the value written by\nthe most recent write () call.\n\u0004Otherwise, if a read () call overlaps a write () call, then the read () call may\nreturn any value within the register\u2019s allowed range of values (for example,\n0 toM\u00001 for anM-valued register).\nBe aware that the term \u201csafe\u201d is a historical accident. Because they provide such\nweak guarantees, \u201csafe\u201d registers are actually quite unsafe.\nConsider the history shown in Fig. 4.3. If the register is safe, then the three\nread calls might behave as follows:\n\u0004R1returns 0, the most recently-written value.\n\u0004R2andR3are concurrent with W(1), so they could return any value in the\nrange of the register.\nW(0) W(1)R1() R2() R3()\nFigure 4.3 A single-reader, single-writer register execution: Riis the ithread and W(v) is a\nwrite of value v. Time \ufb02ows from left to right. No matter whether the register is safe,regular ,\noratomic ,R1must return 0, the most recently written value. If the register is safethen\nbecause R2andR3are concurrent with W(1), they can return any value in the range of the reg-\nister. If the register is regular ,R2andR3can each return either 0 or 1. If the register is atomic\nthen if R2returns 1 then R3also returns 1, and if R2returns 0 then R3could return 0 or 1.\n4.1 The Space of Registers 75\nIt is convenient to de\ufb01ne an intermediate level of consistency between safe and\natomic. A regular register is a multi-reader, single-writer register where writes do\nnot happen atomically. Instead, while the write () call is in progress, the value\nbeing read may \u201c\ufb02icker\u201d between the old and new value before \ufb01nally replacing\nthe older value. More precisely:\n\u0004A regular register is safe, so any read () call that does not overlap a write ()\ncall returns the most recently written value.\n\u0004Suppose a read () call overlaps one or more write () calls. Let v0be the\nvalue written by the latest preceding write () call, and let v1,:::,vkbe the\nsequence of values written by overlapping write () calls. The read () call may\nreturn any of the vi, for anyiin the range 0 :::k .\nFor the execution in Fig. 4.3, a single-reader regular register might behave as\nfollows:\n\u0004R1returns the old value,", "doc_id": "1a152cef-294a-4783-b700-42724600b4dc", "embedding": null, "doc_hash": "50db13e1630a1eb67679e5576fc0e8a55c43a232b1952eef515ce8585578fb60", "extra_info": null, "node_info": {"start": 229712, "end": 233166}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5a1ca5da-7871-42e1-8b9b-a29dcb68525d", "3": "9265669d-1acb-48b3-b487-874e8d9aaa2f"}}, "__type__": "1"}, "9265669d-1acb-48b3-b487-874e8d9aaa2f": {"__data__": {"text": "register is safe, so any read () call that does not overlap a write ()\ncall returns the most recently written value.\n\u0004Suppose a read () call overlaps one or more write () calls. Let v0be the\nvalue written by the latest preceding write () call, and let v1,:::,vkbe the\nsequence of values written by overlapping write () calls. The read () call may\nreturn any of the vi, for anyiin the range 0 :::k .\nFor the execution in Fig. 4.3, a single-reader regular register might behave as\nfollows:\n\u0004R1returns the old value, 0.\n\u0004R2andR3each return either the old value, 0, or the new value, 1.\nRegular registers are quiescently consistent (Chapter 3), but not vice versa. We\nde\ufb01ned both safe and regular registers to permit only a single writer. Note that a\nregular register is a quiescently consistent single-writer sequential register.\nFor a single-reader single-writer atomic register, the execution in Fig. 4.3\nmight produce the following results:\n\u0004R1returns the old value, 0.\n\u0004IfR2returns 1 then R3also returns 1.\n\u0004IfR2returns 0 then R3could return 0 or 1.\nFig. 4.4 shows a schematic view of the range of possible registers as a three-\ndimensional space: the register size de\ufb01nes one dimension, the number of readers\nand writers de\ufb01nes another, and the register\u2019s consistency property de\ufb01nes the\nthird. This view should not be taken literally: there are several combinations,\nsuch as multi-writer safe registers, that are not useful to de\ufb01ne.\nT o reason about algorithms for implementing regular and atomic registers,\nit is convenient to rephrase our de\ufb01nitions directly in terms of object histo-\nries. From now on, we consider only histories in which each read () call returns\na value written by some write () call (regular and atomic registers do not\nallow reads to make up return values). We assume values read or written are\nunique.2\n2If values are not inherently unique, we can use the standard technique of appending to them\nauxiliary values invisible to the algorithm itself, used only in our reasoning to distinguish one\nvalue from another.\n76 Chapter 4 Foundations of Shared Memory\nBoolean\nSafe\nRegular\nAtomicSRSWMRSWMRMW\nMulti-valued\nFigure 4.4 The three-dimensional space of possible read\u2013write register-based implementa-\ntions.\nRecall that an object history is a sequence of invocation and response events,\nwhere an invocation event occurs when a thread calls a method, and a matching\nresponse event occurs when that call returns. A method call (or just a call) is the\ninterval between matching invocation and response events. Any history induces\na partial!order on method calls, de\ufb01ned as follows: if m0andm1are method\ncalls,m0!m1ifm0\u2019s response event precedes m1\u2019s call event. (See Chapter 3 for\ncomplete de\ufb01nitions.)\nAny register implementation (whether safe, regular, or atomic) de\ufb01nes a total\norder on the write () calls called the write order , the order in which writes \u201ctake\neffect\u201d in the register. For safe and regular registers, the write order is trivial,\nbecause they allow only one writer at a time. For atomic registers, method calls\nhave a linearization order. We use this order to index the write calls: write call\nW0is ordered \ufb01rst, W1second, and so on. Note that for SRSW or MRSW safe\nor regular registers, the write order is exactly the same as the precedence order.\nWe useRito denote any read call that returns vi, the unique value written by\nWi. Remember that a history contains only one Wicall, but it might", "doc_id": "9265669d-1acb-48b3-b487-874e8d9aaa2f", "embedding": null, "doc_hash": "caca1d7cd4a5a7362e40dee119351da8e26bdd41b7289ef18bbb84fda00e9228", "extra_info": null, "node_info": {"start": 233246, "end": 236682}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1a152cef-294a-4783-b700-42724600b4dc", "3": "3583fedb-dab0-477e-9fca-59022505a5cd"}}, "__type__": "1"}, "3583fedb-dab0-477e-9fca-59022505a5cd": {"__data__": {"text": "in the register. For safe and regular registers, the write order is trivial,\nbecause they allow only one writer at a time. For atomic registers, method calls\nhave a linearization order. We use this order to index the write calls: write call\nW0is ordered \ufb01rst, W1second, and so on. Note that for SRSW or MRSW safe\nor regular registers, the write order is exactly the same as the precedence order.\nWe useRito denote any read call that returns vi, the unique value written by\nWi. Remember that a history contains only one Wicall, but it might contain\nmultipleRicalls.\nOne can show that the following conditions provide a precise statement of\nwhat it means for a register to be regular. First, no read call returns a value from\nthe future:\nit is never the case that Ri!Wi: (4.1.1)\nSecond, no read call returns a value from the distant past, that is, one that pre-\ncedes the most recently written non-overlapping value:\nit is never the case that for some jWi!Wj!Ri: (4.1.2)\n4.2 Register Constructions 77\nT o prove that a register implementation is regular, we must show that its histories\nsatisfy Conditions 4.1.1 and 4.1.2.\nAn atomic register satis\ufb01es one additional condition:\nifRi!Rjtheni6j: (4.1.3)\nThis condition states that an earlier read cannot return a value later than that\nreturned by a later read. Regular registers are notrequired to satisfy Condition\n4.1.3. T o show that a register implementation is atomic, we need \ufb01rst to de\ufb01ne a\nwrite order, and then to show that its histories satisfy Conditions 4.1.1\u20134.1.3.\n4.2 Register Constructions\nWe now show how to implement a range of surprisingly powerful registers from\nsimple single-reader single-writer Boolean safe registers. These constructions\nimply that all read\u2013write register types are equivalent, at least in terms of com-\nputability. We consider a series of constructions that implement stronger from\nweaker registers.\nThe sequence of constructions appears in Fig. 4.5:\nBase Class Implemented Class Section\nSRSW safe MRSW safe 4.2.1\nMRSW Boolean safe MRSW Boolean regular 4.2.2\nMRSW Boolean regular MRSW regular 4.2.3\nMRSW regular SRSW atomic 4.2.4\nSRSW atomic MRSW atomic 4.2.5\nMRSW atomic MRMW atomic 4.2.6\nMRSW atomic atomic snapshot 4.3\nFigure 4.5 The sequence of register constructions.\nIn the last step, we show how atomic registers (and therefore safe registers) can\nimplement an atomic snapshot: an array of MRSW registers written by different\nthreads, that can be read atomically by any thread. Some of these constructions\nare more powerful than necessary to complete the sequence of derivations (for\nexample, we do not need to provide the multi-reader property for regular and\nsafe registers to complete the derivation of a SRSW atomic register). We present\nthem anyway because they provide valuable insights.\nOur code samples follow these conventions. When we display an algorithm to\nimplement a particular kind of register, say, a safe MRSW Boolean register, we\npresent the algorithm using a form somewhat like this:\nclass SafeMRSWBooleanRegister implements Register<Boolean>\n{\n...\n}\n78 Chapter 4 Foundations of Shared Memory\nWhile this notation makes clear the properties of the register class being imple-\nmented, it becomes cumbersome when we want to use this class to implement\nother classes. Instead, when describing a class implementation, we use the follow-\ning conventions to indicate whether a particular \ufb01eld is safe, regular, or atomic.\nA \ufb01eld", "doc_id": "3583fedb-dab0-477e-9fca-59022505a5cd", "embedding": null, "doc_hash": "ad28bec93e13876ed45009077df53df72a2c26a5ba28a2fd2aba4dd8c183380e", "extra_info": null, "node_info": {"start": 236661, "end": 240094}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9265669d-1acb-48b3-b487-874e8d9aaa2f", "3": "00273862-1688-4382-8529-60c704d17dee"}}, "__type__": "1"}, "00273862-1688-4382-8529-60c704d17dee": {"__data__": {"text": "kind of register, say, a safe MRSW Boolean register, we\npresent the algorithm using a form somewhat like this:\nclass SafeMRSWBooleanRegister implements Register<Boolean>\n{\n...\n}\n78 Chapter 4 Foundations of Shared Memory\nWhile this notation makes clear the properties of the register class being imple-\nmented, it becomes cumbersome when we want to use this class to implement\nother classes. Instead, when describing a class implementation, we use the follow-\ning conventions to indicate whether a particular \ufb01eld is safe, regular, or atomic.\nA \ufb01eld otherwise named mumble is called s_mumble if it is safe, r_mumble if it is\nregular, and a_mumble if it is atomic. Other important aspects of the \ufb01eld, such\nas its type, or whether it supports multiple readers or writers, are noted as com-\nments within the code, and should also be clear from context.\n4.2.1 MRSW Safe Registers\nFig. 4.6 shows how to construct a safe MRSW register from safe SRSW registers.\nLemma 4.2.1. The construction in Fig. 4.6 is a safe MRSW register .\nProof: IfA\u2019sread () call does not overlap any write () call, then the read ()\ncall returns the value of s_table [A], which is the most recently written value.\nFor overlapping method calls, the reader may return any value, because the\ncomponent registers are safe. 2\n4.2.2 A Regular Boolean MRSW Register\nThe next construction in Fig. 4.7 builds a regular Boolean MRSW register from a\nsafe Boolean MRSW register. For Boolean registers, the only difference between\nsafe and regular arises when the newly written value xis the same as the old.\nA regular register can only return x, while a safe register may return either\nBoolean value. We circumvent this problem simply by ensuring that a value is\nwritten only if it is distinct from the previously written value.\nLemma 4.2.2. The construction in Fig. 4.7 is a regular Boolean MRSW register.\n1public class SafeBooleanMRSWRegister implements Register<Boolean> {\n2 boolean [] s_table; // array of safe SRSW registers\n3 public SafeBooleanMRSWRegister( int capacity) {\n4 s_table = new boolean [capacity];\n5 }\n6 public Boolean read() {\n7 return s_table[ThreadID.get()];\n8 }\n9 public void write(Boolean x) {\n10 for (int i = 0; i < s_table.length; i++)\n11 s_table[i] = x;\n12 }\n13 }\nFigure 4.6 TheSafeBoolMRSWRegister class: a safe Boolean MRSW register.\n4.2 Register Constructions 79\n1public class RegBooleanMRSWRegister implements Register<Boolean> {\n2 ThreadLocal<Boolean> last;\n3 boolean s_value; // safe MRSW register\n4 RegBooleanMRSWRegister( int capacity) {\n5 last = new ThreadLocal<Boolean>() {\n6 protected Boolean initialValue() { return false ; };\n7 };\n8 }\n9 public void write(Boolean x) {\n10 if(x != last.get()) {\n11 last.set(x);\n12 s_value =x;\n13 }\n14 }\n15 public Boolean read() {\n16 return s_value;\n17 }\n18 }\nFigure 4.7 TheRegBoolMRSWRegister class: a regular Boolean MRSW register constructed\nfrom a safe Boolean MRSW register.\nProof: Aread () call that does not overlap any write () call returns the most\nrecently written value. If the calls do overlap, there are two cases to consider.\n\u0004If the value being written is the same as the last value written, then the writer\navoids writing to the safe register, ensuring that the reader reads the correct\nvalue.\n\u0004If the value", "doc_id": "00273862-1688-4382-8529-60c704d17dee", "embedding": null, "doc_hash": "29cad760ba37ed454e393104a7b4c621fc51eeda79755b913c84529077011d5f", "extra_info": null, "node_info": {"start": 240071, "end": 243319}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3583fedb-dab0-477e-9fca-59022505a5cd", "3": "56cd3672-aac2-4868-83dd-6722b2a3df2b"}}, "__type__": "1"}, "56cd3672-aac2-4868-83dd-6722b2a3df2b": {"__data__": {"text": "}\n14 }\n15 public Boolean read() {\n16 return s_value;\n17 }\n18 }\nFigure 4.7 TheRegBoolMRSWRegister class: a regular Boolean MRSW register constructed\nfrom a safe Boolean MRSW register.\nProof: Aread () call that does not overlap any write () call returns the most\nrecently written value. If the calls do overlap, there are two cases to consider.\n\u0004If the value being written is the same as the last value written, then the writer\navoids writing to the safe register, ensuring that the reader reads the correct\nvalue.\n\u0004If the value written now is distinct from the last value written, then those\nvalues must be true and false because the register is Boolean. A concurrent\nread returns some value in the range of the register, namely either true or\nfalse , either of which is correct. 2\n4.2.3 A Regular M-Valued MRSW Register\nThe jump from Boolean to M-valued registers is simple, if astonishingly\ninef\ufb01cient: we represent the value in unary notation. In Fig. 4.7 we implement\nanM-valued register as an array of MBoolean registers. Initially the register is\nset to value zero, indicated by the \u201c0\u201d-th bit being set to true. A write method of\nvaluexwrites true in locationxand then in descending array-index order sets all\nlower locations to false . A reading method reads the locations in ascending index\norder until the \ufb01rst time it reads the value true in some index i. It then returns i.\nThe example in Fig. 4.9 illustrates an 8-valued register.\nLemma 4.2.3. The read () call in the construction in Fig. 4.8 always returns a\nvalue corresponding to a bit in 0.. M\u00001 set by some write () call.\nProof: The following property is invariant: if a reading thread is reading r_bit [j],\nthen some bit at index jor higher, written by a write () call, is set to true.\n80 Chapter 4 Foundations of Shared Memory\n1public class RegMRSWRegister implements Register<Byte> {\n2 private static int RANGE = Byte.MAX_VALUE - Byte.MIN_VALUE + 1;\n3 boolean [] r_bit = new boolean [RANGE]; // regular boolean MRSW\n4 public RegMRSWRegister( int capacity) {\n5 for (int i = 1; i < r_bit.length; i++)\n6 r_bit[i] = false ;\n7 r_bit[0] = true ;\n8 }\n9 public void write(Byte x) {\n10 r_bit[x] = true ;\n11 for (int i = x - 1; i >= 0; i--)\n12 r_bit[i] = false ;\n13 }\n14 public Byte read() {\n15 for (int i = 0; i < RANGE; i++)\n16 if(r_bit[i]) {\n17 return i;\n18 }\n19 return -1; // impossible\n20 }\n21 }\nFigure 4.8 TheRegMRSWRegister class: a regular M-valued MRSW register.\nReader07\nWrite r1 1 0000 004\nReader07\nWriter1 0 0000 004\nReader07\nWriter4555\n(a)\n(b)\n(c) 1 0 0000 10\nFigure 4.9 The RegMRSWRegister class: an execution of a regular 8-valued MRSW register.\nThe values trueandfalseare represented by 0 and 1. In Part (a), the value prior to the write\nwas 4, thread W\u2019s write of 7 is not read by thread Rbecause Rreaches array entry 4 before\nWoverwrites falseat that location. In Part (b), entry 4 is overwritten by Wbefore it is read,\nso the read returns 7. In Part (c), Wstarts to write 5. Since it wrote array entry 5 before it\nwas read, the reader returns 5 even though entry 7 is also set to true.\n4.2 Register Constructions 81\nWhen the register is initialized, there are no readers, and the constructor", "doc_id": "56cd3672-aac2-4868-83dd-6722b2a3df2b", "embedding": null, "doc_hash": "2349dadf49e09066598808b0a88d641d435f7e1742abdb77f56dcaf4cecf1006", "extra_info": null, "node_info": {"start": 243347, "end": 246517}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "00273862-1688-4382-8529-60c704d17dee", "3": "c5cf1cfc-0589-4595-906d-0fcef5c5adaf"}}, "__type__": "1"}, "c5cf1cfc-0589-4595-906d-0fcef5c5adaf": {"__data__": {"text": "values trueandfalseare represented by 0 and 1. In Part (a), the value prior to the write\nwas 4, thread W\u2019s write of 7 is not read by thread Rbecause Rreaches array entry 4 before\nWoverwrites falseat that location. In Part (b), entry 4 is overwritten by Wbefore it is read,\nso the read returns 7. In Part (c), Wstarts to write 5. Since it wrote array entry 5 before it\nwas read, the reader returns 5 even though entry 7 is also set to true.\n4.2 Register Constructions 81\nWhen the register is initialized, there are no readers, and the constructor (we\ntreat the constructor call as a write (0) call) sets r_bit [0] to true. Assume a\nreader is reading r_bit [j], and that r_bit [k] istrue, fork>j.\n\u0004If the reader advances from jtoj+ 1, then r_bit [j] isfalse , sok>j (i.e., a\nbit greater than or equal to j+ 1 is true).\n\u0004The writer clears r_bit [k] only if it set a higher r_bit [`] to true,\nfor`>k . 2\nLemma 4.2.4. The construction in Fig. 4.8 is a regular M-valued MRSW register.\nProof: For any read, let xbe the value written by the most recent non-overlapping\nwrite (). At the time the write () completed, a_bit [x] was set to true, and\na_bit [i] isfalse fori<x . By Lemma 4.2.3, if the reader returns a value that\nis notx, then it observed some a_bit [j],j6=xto be true, and that bit must have\nbeen set by a concurrent write, proving Conditions 4.1.1 and 4.1.2. 2\n4.2.4 An Atomic SRSW Register\nWe \ufb01rst show how to construct an atomic SRSW register from a regular SRSW\nregister. (Note that our construction uses unbounded timestamps.)\nA regular register satis\ufb01es Conditions 4.1.1 and 4.1.2, while an atomic register\nmust also satisfy Condition 4.1.3. Since an SRSW regular register has no con-\ncurrent reads, the only way Condition 4.1.3 can be violated is if two reads that\noverlap the same write read values out-of-order, the \ufb01rst returning viand the\nlatter returning vj, wherej<i .\nFig. 4.10 describes a class of values that each have an added tag that\ncontains a timestamp. As illustrated in Fig. 4.11, our implementation of an\nAtomicSRSWRegister will use these tags to order write calls so that they can\nbe ordered properly by concurrent read calls. Each read remembers the latest\n(highest timestamp) timestamp/value pair ever read, so that it is available to\nfuture reads. If a later read then reads an earlier value (one having a lower time-\nstamp), it ignores that value and simply uses the remembered latest value.\nSimilarly, the writer remembers the latest timestamp it wrote, and tags each\nnewly written value with a later timestamp (a timestamp greater by 1).\nThis algorithm requires the ability to read\u2013write a value and a timestamp\nas a single unit. In a language such as C, we would treat both the value and\nthe timestamp as uninterpreted bits (\u201craw seething bits\u201d), and use bit-shifting\nand logical masking to pack and unpack both values in and out of one or\nmore words. In Java, it is easer to create a StampedValue<T> structure that\nholds a timestamp/value pair, and to store a reference to that structure in the\nregister.\n82 Chapter 4 Foundations of Shared Memory\n1public class StampedValue<T> {\n2 public long stamp;\n3 public T value;\n4 // initial value with zero timestamp\n5 public", "doc_id": "c5cf1cfc-0589-4595-906d-0fcef5c5adaf", "embedding": null, "doc_hash": "cc5420142b45fbbfc8c7448d28e079daeda2fa22067c77525da4516cdf9a05dd", "extra_info": null, "node_info": {"start": 246514, "end": 249716}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "56cd3672-aac2-4868-83dd-6722b2a3df2b", "3": "57c5666f-caf3-4c0b-843d-b85ebdb2b8cf"}}, "__type__": "1"}, "57c5666f-caf3-4c0b-843d-b85ebdb2b8cf": {"__data__": {"text": "as C, we would treat both the value and\nthe timestamp as uninterpreted bits (\u201craw seething bits\u201d), and use bit-shifting\nand logical masking to pack and unpack both values in and out of one or\nmore words. In Java, it is easer to create a StampedValue<T> structure that\nholds a timestamp/value pair, and to store a reference to that structure in the\nregister.\n82 Chapter 4 Foundations of Shared Memory\n1public class StampedValue<T> {\n2 public long stamp;\n3 public T value;\n4 // initial value with zero timestamp\n5 public StampedValue(T init) {\n6 stamp = 0;\n7 value = init;\n8 }\n9 // later values with timestamp provided\n10 public StampedValue( long stamp, T value) {\n11 stamp = stamp;\n12 value = value;\n13 }\n14 public static StampedValue max(StampedValue x, StampedValue y) {\n15 if(x.stamp > y.stamp) {\n16 return x;\n17 }else {\n18 return y;\n19 }\n20 }\n21 public static StampedValue MIN_VALUE =\n22 new StampedValue( null );\n23 }\nFigure 4.10 TheStampedValue<T> class: allows a timestamp and a value to be read or writ-\nten together.\nLemma 4.2.5. The construction in Fig. 4.11 is an atomic SRSW register.\nProof: The register is regular, so Conditions 4.1.1 and 4.1.2 are met. The\nalgorithm satis\ufb01es Condition 4.1.3 because writes are totally ordered by their\ntimestamps, and if a read returns a given value, a later read cannot read an earlier\nwritten value, since it would have a lower timestamp. 2\n4.2.5 An Atomic MRSW Register\nT o understand how to construct an atomic MRSW register from atomic SRSW\nregisters, we \ufb01rst consider a simple algorithm based on direct use of the construc-\ntion in Section 4.2.1, which took us from SRSW to MRSW safe registers. Let the\nSRSW registers composing the table array a_table [0::n\u00001] be atomic instead\nof safe, with all other calls remaining the same: the writer writes the array loca-\ntions in increasing index order and then each reader reads and returns its associ-\nated array entry. The result is not a multi-reader atomic register. Condition 4.1.3\nholds for any single reader because each reader reads from an atomic register, yet\nit does not hold for distinct readers. Consider, for example, a write that starts\nby setting the \ufb01rst SRSW register a_table [0], and is delayed before writing the\nremaining locations a_table [1::n\u00001]. A subsequent read by thread 0 returns\nthe correct new value, but a subsequent read by thread 1 that completely follows\nthe read by thread 0, reads and returns the earlier value because the writer has yet\n4.2 Register Constructions 83\n1public class AtomicSRSWRegister<T> implements Register<T> {\n2 ThreadLocal<Long> lastStamp;\n3 ThreadLocal<StampedValue<T>> lastRead;\n4 StampedValue<T> r_value; // regular SRSW timestamp-value pair\n5 public AtomicSRSWRegister(T init) {\n6 r_value = new StampedValue<T>(init);\n7 lastStamp = new ThreadLocal<Long>() {\n8 protected Long initialValue() { return 0; };\n9 };\n10 lastRead = new ThreadLocal<StampedValue<T>>() {\n11 protected StampedValue<T> initialValue() { return r_value; };\n12 };\n13 }\n14 public T read() {\n15 StampedValue<T> value = r_value;\n16 StampedValue<T> last = lastRead.get();\n17 StampedValue<T> result = StampedValue.max(value, last);\n18", "doc_id": "57c5666f-caf3-4c0b-843d-b85ebdb2b8cf", "embedding": null, "doc_hash": "d3665afd249b25a728098b6cd58b408d7f08d9826a5fc54eeba052bee366fab9", "extra_info": null, "node_info": {"start": 249729, "end": 252885}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "c5cf1cfc-0589-4595-906d-0fcef5c5adaf", "3": "06f0db36-d782-41c3-b7be-70341a380d7f"}}, "__type__": "1"}, "06f0db36-d782-41c3-b7be-70341a380d7f": {"__data__": {"text": "init) {\n6 r_value = new StampedValue<T>(init);\n7 lastStamp = new ThreadLocal<Long>() {\n8 protected Long initialValue() { return 0; };\n9 };\n10 lastRead = new ThreadLocal<StampedValue<T>>() {\n11 protected StampedValue<T> initialValue() { return r_value; };\n12 };\n13 }\n14 public T read() {\n15 StampedValue<T> value = r_value;\n16 StampedValue<T> last = lastRead.get();\n17 StampedValue<T> result = StampedValue.max(value, last);\n18 lastRead.set(result);\n19 return result.value;\n20 }\n21 public void write(T v) {\n22 long stamp = lastStamp.get() + 1;\n23 r_value = new StampedValue(stamp, v);\n24 lastStamp.set(stamp);\n25 }\n26 }\nFigure 4.11 The AtomicSRSWRegister class: an atomic SRSW register constructed from a\nregular SRSW register.\nto update a_table [1::n\u00001]. We address this problem by having earlier reader\nthreads help out later threads by telling them which value they read.\nThis implementation appears in Fig. 4.12. The nthreads share an n-by-narray\na_table [0::n\u00001][0::n\u00001] of stamped values. As in Section 4.2.4, we use time-\nstamped values to allow early reads to tell later reads which of the values read is\nthe latest. The locations along the diagonal, a_table [i][i] for alli, correspond to\nthe registers in our failed simple construction mentioned earlier. The writer sim-\nply writes the diagonal locations one after the other with a new value and a times-\ntamp that increases from one write () call to the next. Each reader A\ufb01rst reads\na_table [A][A] as in the earlier algorithm. It then uses the remaining SRSW\nlocations a_table [A][B],A6=Bfor communication between readers AandB.\nEach reader A, after reading a_table [A][A], checks to see if some other reader\nhas read a later value by traversing its corresponding column ( a_table [B][A]\nfor allB), and checking if it contains a later value (one with a higher timestamp).\nThe reader then lets all later readers know the latest value it read by writing this\nvalue to all locations in its corresponding row ( a_table [A][B] for allB). It thus\nfollows that after a read by Ais completed, every later read by a Bsees the last\nvalueAread (since it reads a_table [A][B]). Fig. 4.13 shows an example execu-\ntion of the algorithm.\n84 Chapter 4 Foundations of Shared Memory\n1public class AtomicMRSWRegister<T> implements Register<T> {\n2 ThreadLocal<Long> lastStamp;\n3 private StampedValue<T>[][] a_table; // each entry is SRSW atomic\n4 public AtomicMRSWRegister(T init, int readers) {\n5 lastStamp = new ThreadLocal<Long>() {\n6 protected Long initialValue() { return 0; };\n7 };\n8 a_table = (StampedValue<T>[][]) new StampedValue[readers][readers];\n9 StampedValue<T> value = new StampedValue<T>(init);\n10 for (int i = 0; i < readers; i++) {\n11 for (int j = 0; j < readers; j++) {\n12 a_table[i][j] = value;\n13 }\n14 }\n15 }\n16 public T read() {\n17 int me = ThreadID.get();\n18 StampedValue<T> value = a_table[me][me];\n19 for (int i = 0; i < a_table.length; i++) {\n20 value = StampedValue.max(value, a_table[i][me]);\n21 }\n22 for", "doc_id": "06f0db36-d782-41c3-b7be-70341a380d7f", "embedding": null, "doc_hash": "c7cef4ca8b226f4b0216b4aeee50f8b6813e7621b6654a5165ae0066b5cd348a", "extra_info": null, "node_info": {"start": 252948, "end": 255921}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "57c5666f-caf3-4c0b-843d-b85ebdb2b8cf", "3": "5a2811a4-d662-425e-a17a-86d1f5165a31"}}, "__type__": "1"}, "5a2811a4-d662-425e-a17a-86d1f5165a31": {"__data__": {"text": "value = new StampedValue<T>(init);\n10 for (int i = 0; i < readers; i++) {\n11 for (int j = 0; j < readers; j++) {\n12 a_table[i][j] = value;\n13 }\n14 }\n15 }\n16 public T read() {\n17 int me = ThreadID.get();\n18 StampedValue<T> value = a_table[me][me];\n19 for (int i = 0; i < a_table.length; i++) {\n20 value = StampedValue.max(value, a_table[i][me]);\n21 }\n22 for (int i = 0; i < a_table.length; i++) {\n23 if(i == me) continue ;\n24 a_table[me][i] = value;\n25 }\n26 return value;\n27 }\n28 public void write(T v) {\n29 long stamp = lastStamp.get() + 1;\n30 lastStamp.set(stamp);\n31 StampedValue<T> value = new StampedValue<T>(stamp, v);\n32 for (int i = 0; i < a_table.length; i++) {\n33 a_table[i][i] = value;\n34 }\n35 }\n36 }\nFigure 4.12 The AtomicMRSWRegister class: an atomic MRSW register constructed from\natomic SRSW registers.\nLemma 4.2.6. The construction in Fig. 4.12 is a MRSW atomic register.\nProof: First, no reader returns a value from the future, so Condition 4.1.1 is\nclearly satis\ufb01ed. By construction, write () calls write strictly increasing times-\ntamps. The key to understanding this algorithm is the simple observation that\nthe maximum timestamp along any row or column is also strictly increasing. If\nAwritesvwith timestamp t, then any read () call byB, whereA\u2019s call com-\npletely precedes B\u2019s, reads (from the diagonal of a_table ) a maximum times-\ntamp greater than or equal to t, satisfying Condition 4.1.2. Finally, as noted\nearlier, if a read call by Acompletely precedes a read call by B, thenAwrites\na stamped value with timestamp ttoB\u2019s row, soBchooses a value with a times-\ntamp greater than or equal to t, satisfying Condition 4.1.3. 2\n4.2 Register Constructions 85\ni 12 3 0\n01\n2\nj3ttt\nt t t\ntt t t\ntt t t\nThread 1\nreadsWriter writes\nand haltsi 12 3 0\n0\n1\n2\nj3ttt t/H115451\nt/H115451 t/H115451 t/H115451 t/H115451\nttt t\nttt tThread 1\nwrites\nThread 0\nreadsThread 3\nreadst/H115451\nt/H115451\nFigure 4.13 An execution of the MRSW atomic register. Each reader thread has an index\nbetween 0 and 4, and we refer to each thread by its index. Here, the writer writes a new value\nwith timestamp t +1 to locations a_table [0][0]anda_table [1][1]and then halts. Then,\nthread 1 reads its corresponding column a_table [i][1]for all i, and writes its corresponding\nrow a_table [1][i]for all i, returning the new value with timestamp t +1. Threads 0 and 3\nboth read completely after thread 1\u2019s read. Thread 0 reads a_table [0][0]with value t +1.\nThread 3 cannot read the new value with timestamp t +1 because the writer has yet to\nwrite a_table [3][3]. Nevertheless, it reads a_table [1][3]and returns the correct value\nwith timestamp t +1 that was read by the earlier thread 1.\n4.2.6 An Atomic MRMW Register\nHere is how to construct an atomic MRMW register from an array of", "doc_id": "5a2811a4-d662-425e-a17a-86d1f5165a31", "embedding": null, "doc_hash": "f4dcade5aa178b57d1fc7f570073840093034df0ed56bdf3107709fcc5d307a3", "extra_info": null, "node_info": {"start": 256000, "end": 258772}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "06f0db36-d782-41c3-b7be-70341a380d7f", "3": "9ef66d74-736b-450f-83a7-712c98bce50a"}}, "__type__": "1"}, "9ef66d74-736b-450f-83a7-712c98bce50a": {"__data__": {"text": "[1][i]for all i, returning the new value with timestamp t +1. Threads 0 and 3\nboth read completely after thread 1\u2019s read. Thread 0 reads a_table [0][0]with value t +1.\nThread 3 cannot read the new value with timestamp t +1 because the writer has yet to\nwrite a_table [3][3]. Nevertheless, it reads a_table [1][3]and returns the correct value\nwith timestamp t +1 that was read by the earlier thread 1.\n4.2.6 An Atomic MRMW Register\nHere is how to construct an atomic MRMW register from an array of atomic\nMRSW registers, one per thread.\nT o write to the register, Areads all the array elements, chooses a timestamp\nhigher than any it has observed, and writes a stamped value to array element A.\nT o read the register, a thread reads all the array elements, and returns the one\nwith the highest timestamp. This is exactly the timestamp algorithm used by the\nBakery algorithm of Section 2.6. As in the Bakery algorithm, we resolve ties in\nfavor of the thread with the lesser index; in other words, we use a lexicographic\norder on pairs of timestamp and thread ids.\nLemma 4.2.7. The construction in Fig. 4.14 is an atomic MRMW register.\nProof: De\ufb01ne the write order among write () calls based on the lexicographic\norder of their timestamps and thread ids so that the write () call byAwith\ntimestamptAprecedes the write () call byBwith timestamp tBiftA<tB, or\niftA=tBandA<B . We leave as an exercise to the reader to show that this\nlexicographic order is consistent with !. As usual, index write () calls in write\norder:W0,W1,:::.\nClearly a read () call cannot read a value written in a_table [] after it is\ncompleted, and any write () call completely preceded by the read has a\n86 Chapter 4 Foundations of Shared Memory\n1public class AtomicMRMWRegister<T> implements Register<T>{\n2 private StampedValue<T>[] a_table; // array of atomic MRSW registers\n3 public AtomicMRMWRegister( int capacity, T init) {\n4 a_table = (StampedValue<T>[]) new StampedValue[capacity];\n5 StampedValue<T> value = new StampedValue<T>(init);\n6 for (int j = 0; j < a_table.length; j++) {\n7 a_table[j] = value;\n8 }\n9 }\n10 public void write(T value) {\n11 int me = ThreadID.get();\n12 StampedValue<T> max = StampedValue.MIN_VALUE;\n13 for (int i = 0; i < a_table.length; i++) {\n14 max = StampedValue.max(max, a_table[i]);\n15 }\n16 a_table[me] = new StampedValue(max.stamp + 1, value);\n17 }\n18 public T read() {\n19 StampedValue<T> max = StampedValue.MIN_VALUE;\n20 for (int i = 0; i < a_table.length; i++) {\n21 max = StampedValue.max(max, a_table[i]);\n22 }\n23 return max.value;\n24 }\n25 }\nFigure 4.14 Atomic MRMW register.\ntimestamp higher than all those written before the read is completed, implying\nCondition 4.1.1.\nConsider Condition 4.1.2, which prohibits skipping over the most recent\npreceding write (). Suppose a write () call byApreceded a write call by B,\nwhich in turn preceded a read () byC. IfA=B, then the later write overwrites\na_table [A] and the read () does not return the value of the earlier write. If\nA6=B, then since A\u2019s timestamp is smaller than B\u2019s timestamp, any", "doc_id": "9ef66d74-736b-450f-83a7-712c98bce50a", "embedding": null, "doc_hash": "593b11b18b9b92fe4458e439ee5302a7d80188997137bf0136c3439c4ba4d7a6", "extra_info": null, "node_info": {"start": 258658, "end": 261706}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5a2811a4-d662-425e-a17a-86d1f5165a31", "3": "5589ef5d-0866-4ef0-978a-d0a2c24dea81"}}, "__type__": "1"}, "5589ef5d-0866-4ef0-978a-d0a2c24dea81": {"__data__": {"text": "Atomic MRMW register.\ntimestamp higher than all those written before the read is completed, implying\nCondition 4.1.1.\nConsider Condition 4.1.2, which prohibits skipping over the most recent\npreceding write (). Suppose a write () call byApreceded a write call by B,\nwhich in turn preceded a read () byC. IfA=B, then the later write overwrites\na_table [A] and the read () does not return the value of the earlier write. If\nA6=B, then since A\u2019s timestamp is smaller than B\u2019s timestamp, any Cthat\nsees both returns B\u2019s value (or one with higher timestamp), meeting Condi-\ntion 4.1.2.\nFinally, we consider Condition 4.1.3, which prohibits values from being read\nout of write order. Consider any read () call byAcompletely preceding a read ()\ncall byB, and any write () call byCwhich is ordered before the write () byD\nin the write order. We must show that if AreturnedD\u2019s value, then Bcould\nnot returnC\u2019s value. IftC<tD, then ifAread timestamp tDfrom a_table [D],\nBreadstDor a higher timestamp from a_table [D], and does not return the\nvalue associated with tC. IftC=tD, that is, the writes were concurrent, then from\nthe write order, C <D , so ifAread timestamp tDfrom a_table [D],Balso\nreadstDfrom a_table [D], and returns the value associated with tD(or higher),\neven if it reads tCina_table [C]. 2\nOur series of constructions shows that one can construct a wait-free MRMW\nmulti-valued atomic register from SRSW safe Boolean registers. Naturally, no one\n4.3 Atomic Snapshots 87\nwants to write a concurrent algorithm using safe registers, but these constructions\nshow that any algorithm using atomic registers can be implemented on an archi-\ntecture that supports only safe registers. Later on, when we consider more realistic\narchitectures, we return to the theme of implementing algorithms that assume\nstrong synchronization properties on architectures that directly provide only\nweak properties.\n4.3 Atomic Snapshots\nWe have seen how a register value can be read and written atomically. What if\nwe want to read multiple register values atomically? We call such an operation an\natomic snapshot .\nAn atomic snapshot constructs an instantaneous view of an array of atomic\nregisters. We construct a wait-free snapshot, meaning that a thread can take an\ninstantaneous snapshot of memory without delaying any other thread. Atomic\nsnapshots might be useful for backups or checkpoints.\nTheSnapshot interface (Fig. 4.15) is just an array of atomic MRSW registers,\none for each thread. The update () method writes a value vto the calling thread\u2019s\nregister in that array, while the scan () method returns an atomic snapshot of that\narray.\nOur goal is to construct a wait-free implementation that is equivalent (that is,\nlinearizable) to the sequential speci\ufb01cation shown in Fig. 4.16. The key property\nof this sequential implementation is that scan () returns a collection of values,\neach corresponding to the latest preceding update (), that is, it returns a collec-\ntion of register values that existed together in the same system state.\n4.3.1 An Obstruction-Free Snapshot\nWe begin with a SimpleSnapshot class for which update () is wait-free, but\nscan () is obstruction-free. We then extend this algorithm to make scan ()\nwait-free.\nJust as for the atomic MRSW register construction, each value includes a\nStampedValue<T> object with stamp andvalue \ufb01elds. Each update () call incre-\nments the timestamp.\nAcollect is the non atomic act of copying the register values one-by-one into\nan array. If we", "doc_id": "5589ef5d-0866-4ef0-978a-d0a2c24dea81", "embedding": null, "doc_hash": "966c04dfa21e3f9503db05b031999bd3ec32960939dc5d7f26281463fc73cce3", "extra_info": null, "node_info": {"start": 261709, "end": 265198}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9ef66d74-736b-450f-83a7-712c98bce50a", "3": "9d3afe5c-7588-4d15-9923-d9b0e822309b"}}, "__type__": "1"}, "9d3afe5c-7588-4d15-9923-d9b0e822309b": {"__data__": {"text": "of register values that existed together in the same system state.\n4.3.1 An Obstruction-Free Snapshot\nWe begin with a SimpleSnapshot class for which update () is wait-free, but\nscan () is obstruction-free. We then extend this algorithm to make scan ()\nwait-free.\nJust as for the atomic MRSW register construction, each value includes a\nStampedValue<T> object with stamp andvalue \ufb01elds. Each update () call incre-\nments the timestamp.\nAcollect is the non atomic act of copying the register values one-by-one into\nan array. If we perform two collects one after the other, and both collects read the\n1public interface Snapshot<T> {\n2 public void update(T v);\n3 public T[] scan();\n4}\nFigure 4.15 TheSnapshot interface.\n88 Chapter 4 Foundations of Shared Memory\n1public class SeqSnapshot<T> implements Snapshot<T> {\n2 T[] a_value;\n3 public SeqSnapshot( int capacity, T init) {\n4 a_value = (T[]) new Object[capacity];\n5 for (int i = 0; i < a_value.length; i++) {\n6 a_value[i] = init;\n7 }\n8 }\n9 public synchronized void update(T v) {\n10 a_value[ThreadID.get()] = v;\n11 }\n12 public synchronized T[] scan() {\n13 T[] result = (T[]) new Object[a_value.length];\n14 for (int i = 0; i < a_value.length; i++)\n15 result[i] = a_value[i];\n16 return result;\n17 }\n18 }\nFigure 4.16 A sequential snapshot.\nsame set of timestamps, then we know that there was an interval during which no\nthread updated its register, so the result of the collect is a snapshot of the system\nstate immediately after the end of the \ufb01rst collect. We call such a pair of collects a\nclean double collect .\nIn the construction shown in the SimpleSnapshot <T>class ( Fig. 4.17 ), each\nthread repeatedly calls collect () (Line 27), and returns as soon as it detects\na clean double collect (one in which both sets of timestamps were identical).\nThis construction always returns correct values. The update () calls are wait-free,\nbutscan () is not because any call can be repeatedly interrupted by update (),\nand may run forever without completing. It is however obstruction-free, since a\nscan () completes if it runs by itself for long enough.\n4.3.2 A Wait-Free Snapshot\nT o make the scan () method wait-free, each update () call helps ascan () it may\ninterfere with, by taking a snapshot before modifying its register. A scan () that\nrepeatedly fails in taking a double collect can use the snapshot from one of the\ninterfering update () calls as its own. The tricky part is that we must make sure\nthe snapshot taken from the helping update is one that can be linearized within\nthescan () call\u2019s execution interval.\nWe say that a thread moves if it completes an update (). If thread Afails to\nmake a clean collect because thread Bmoved, then can Asimply takeB\u2019s most\nrecent snapshot as its own? Unfortunately, no. As illustrated in Fig. 4.18 , it is\npossible for Ato seeBmove whenB\u2019s snapshot was taken before Astarted its\nscan () call, so the snapshot did not occur within the interval of A\u2019s scan.\n4.3 Atomic Snapshots 89\n1public class SimpleSnapshot<T> implements Snapshot<T> {\n2 private StampedValue<T>[] a_table; // array of atomic MRSW registers\n3 public SimpleSnapshot( int capacity, T init) {\n4 a_table = (StampedValue<T>[]) new StampedValue[capacity];\n5 for (int i = 0; i < capacity;", "doc_id": "9d3afe5c-7588-4d15-9923-d9b0e822309b", "embedding": null, "doc_hash": "ff2ab30a5f44ebec384f98496bf360edfd123062f442a0be7919c852f7b4261f", "extra_info": null, "node_info": {"start": 265159, "end": 268401}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5589ef5d-0866-4ef0-978a-d0a2c24dea81", "3": "3805a50f-5dfa-41c8-8ba3-3f67bfb03f9c"}}, "__type__": "1"}, "3805a50f-5dfa-41c8-8ba3-3f67bfb03f9c": {"__data__": {"text": ", it is\npossible for Ato seeBmove whenB\u2019s snapshot was taken before Astarted its\nscan () call, so the snapshot did not occur within the interval of A\u2019s scan.\n4.3 Atomic Snapshots 89\n1public class SimpleSnapshot<T> implements Snapshot<T> {\n2 private StampedValue<T>[] a_table; // array of atomic MRSW registers\n3 public SimpleSnapshot( int capacity, T init) {\n4 a_table = (StampedValue<T>[]) new StampedValue[capacity];\n5 for (int i = 0; i < capacity; i++) {\n6 a_table[i] = new StampedValue<T>(init);\n7 }\n8 }\n9 public void update(T value) {\n10 int me = ThreadID.get();\n11 StampedValue<T> oldValue = a_table[me];\n12 StampedValue<T> newValue =\n13 new StampedValue<T>((oldValue.stamp)+1, value);\n14 a_table[me] = newValue;\n15 }\n16 private StampedValue<T>[] collect() {\n17 StampedValue<T>[] copy = (StampedValue<T>[])\n18 new StampedValue[a_table.length];\n19 for (int j = 0; j < a_table.length; j++)\n20 copy[j] = a_table[j];\n21 return copy;\n22 }\n23 public T[] scan() {\n24 StampedValue<T>[] oldCopy, newCopy;\n25 oldCopy = collect();\n26 collect: while (true ) {\n27 newCopy = collect();\n28 if(! Arrays.equals(oldCopy, newCopy)) {\n29 oldCopy = newCopy;\n30 continue collect;\n31 }\n32 T[] result = (T[]) new Object[a_table.length];\n33 for (int j = 0; j < a_table.length; j++)\n34 result[j] = newCopy[j].value;\n35 return result;\n36 }\n37 }\n38 }\nFigure 4.17 Simple snapshot object.\nThe wait-free construction is based on the following observation: if a scanning\nthreadAsees a thread Bmove twice while it is performing repeated collects, then\nBexecuted a complete update () call within the interval of A\u2019sscan (), so it is\ncorrect forAto useB\u2019s snapshot.\nFigs. 4.19, 4.20, and 4.21 show the wait-free snapshot algorithm. Each\nupdate () call calls scan (), and appends the result of the scan to the value\u2019s\nlabel. More precisely, each value written to a register has the structure shown\nin Fig. 4.19: a stamp \ufb01eld incremented each time the thread updates its value,\navalue \ufb01eld containing the register\u2019s actual value, and a snap \ufb01eld containing\nthat thread\u2019s most recent scan. The snapshot algorithm is described in Fig. 4.21.\n90 Chapter 4 Foundations of Shared Memory\nScan1st collect 2nd collect\nScan\nScan UpdateThread A\nThread B\nThread CUpdate\nFigure 4.18 Here is why a thread Athat fails to complete a clean double collect cannot simply\ntake the latest snapshot of a thread Bthat performed an update ()during A\u2019s second collect.\nB\u2019s snapshot was taken before Astarted its scan (), i.e., B\u2019s snapshot did not overlap A\u2019s scan.\nThe danger, illustrated here, is that a thread Ccould have called update ()after B\u2019sscan ()\nand before A\u2019sscan (), making it incorrect for Ato use the results of B\u2019sscan ().\n1public class StampedSnap<T> {\n2 public long stamp;\n3 public T value;\n4 public T[] snap;\n5 public StampedSnap(T value) {\n6 stamp = 0;\n7 value = value;\n8 snap = null ;\n9 }\n10 public StampedSnap( long label, T value, T[] snap)", "doc_id": "3805a50f-5dfa-41c8-8ba3-3f67bfb03f9c", "embedding": null, "doc_hash": "2cf7aa4fb56c717ac6497360bdeb36e8201de9b0f5ca9d74c67427bc144ec962", "extra_info": null, "node_info": {"start": 268467, "end": 271375}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9d3afe5c-7588-4d15-9923-d9b0e822309b", "3": "b8ee1d32-493b-449a-beb8-d8520a37802f"}}, "__type__": "1"}, "b8ee1d32-493b-449a-beb8-d8520a37802f": {"__data__": {"text": "B\u2019s snapshot did not overlap A\u2019s scan.\nThe danger, illustrated here, is that a thread Ccould have called update ()after B\u2019sscan ()\nand before A\u2019sscan (), making it incorrect for Ato use the results of B\u2019sscan ().\n1public class StampedSnap<T> {\n2 public long stamp;\n3 public T value;\n4 public T[] snap;\n5 public StampedSnap(T value) {\n6 stamp = 0;\n7 value = value;\n8 snap = null ;\n9 }\n10 public StampedSnap( long label, T value, T[] snap) {\n11 stamp = label;\n12 value = value;\n13 snap = snap;\n14 }\n15 }\nFigure 4.19 The stamped snapshot class.\nA scanning thread creates a Boolean array called moved [] (Line 13), which records\nwhich threads have been observed to move in the course of the scan. As before,\neach thread performs two collects ( 14and16) and tests whether any thread\u2019s label\nhas changed. If no thread\u2019s label has changed, then the collect is clean, and the\nscan returns the result of the collect. If any thread\u2019s label has changed (Line 18)\nthe scanning thread tests the moved [] array to detect whether this is the second\ntime this thread has moved (Line 19). If so, it returns that thread\u2019s scan (Line 20),\nand otherwise it updates moved [] and resumes the outer loop (Line 21).\n4.3.3 Correctness Arguments\nIn this section, we review the correctness arguments for the wait-free snapshot\nalgorithm a little more carefully.\n4.3 Atomic Snapshots 91\n1public class WFSnapshot<T> implements Snapshot<T> {\n2 private StampedSnap<T>[] a_table; // array of atomic MRSW registers\n3 public WFSnapshot( int capacity, T init) {\n4 a_table = (StampedSnap<T>[]) new StampedSnap[capacity];\n5 for (int i = 0; i < a_table.length; i++) {\n6 a_table[i] = new StampedSnap<T>(init);\n7 }\n8 }\n9\n10 private StampedSnap<T>[] collect() {\n11 StampedSnap<T>[] copy = (StampedSnap<T>[])\n12 new StampedSnap[a_table.length];\n13 for (int j = 0; j < a_table.length; j++)\n14 copy[j] = a_table[j];\n15 return copy;\n16 }\nFigure 4.20 The single-writer atomic snapshot class and collect ()method.\n1 public void update(T value) {\n2 int me = ThreadID.get();\n3 T[] snap = scan();\n4 StampedSnap<T> oldValue = a_table[me];\n5 StampedSnap<T> newValue =\n6 new StampedSnap<T>(oldValue.stamp+1, value, snap);\n7 a_table[me] = newValue;\n8 }\n9\n10 public T[] scan() {\n11 StampedSnap<T>[] oldCopy;\n12 StampedSnap<T>[] newCopy;\n13 boolean [] moved = new boolean [a_table.length];\n14 oldCopy = collect();\n15 collect: while (true ) {\n16 newCopy = collect();\n17 for (int j = 0; j < a_table.length; j++) {\n18 if(oldCopy[j].stamp != newCopy[j].stamp) {\n19 if(moved[j]) {\n20 return newCopy[j].snap;\n21 }else {\n22 moved[j] = true ;\n23 oldCopy = newCopy;\n24 continue collect;\n25 }\n26 }\n27 }\n28 T[] result = (T[]) new Object[a_table.length];\n29 for (int j = 0; j < a_table.length; j++)\n30 result[j] = newCopy[j].value;\n31 return result;\n32 }\n33 }\n34 }\nFigure", "doc_id": "b8ee1d32-493b-449a-beb8-d8520a37802f", "embedding": null, "doc_hash": "f18a1b4993ac031d66db1a9fdef5a88126960ebc27a9f6d8cda3b79778638d38", "extra_info": null, "node_info": {"start": 271395, "end": 274198}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3805a50f-5dfa-41c8-8ba3-3f67bfb03f9c", "3": "1516baf2-cd5f-4208-93b0-a58a19e03362"}}, "__type__": "1"}, "1516baf2-cd5f-4208-93b0-a58a19e03362": {"__data__": {"text": "{\n18 if(oldCopy[j].stamp != newCopy[j].stamp) {\n19 if(moved[j]) {\n20 return newCopy[j].snap;\n21 }else {\n22 moved[j] = true ;\n23 oldCopy = newCopy;\n24 continue collect;\n25 }\n26 }\n27 }\n28 T[] result = (T[]) new Object[a_table.length];\n29 for (int j = 0; j < a_table.length; j++)\n30 result[j] = newCopy[j].value;\n31 return result;\n32 }\n33 }\n34 }\nFigure 4.21 Single-writer atomic snapshot update ()andscan ()methods.\n92 Chapter 4 Foundations of Shared Memory\nLemma 4.3.1. If a scanning thread makes a clean double collect, then the val-\nues it returns were the values that existed in the registers in some state of the\nexecution.\nProof: Consider the interval between the last read of the \ufb01rst collect, and the \ufb01rst\nread of the second collect. If any register were updated in that interval, the labels\nwould not match, and the double collect would not be clean. 2\nLemma 4.3.2. If a scanning thread Aobserves changes in another thread B\u2019s label\nduring two different double collects, then the value of B\u2019s register read during the\nlast collect was written by an update () call that began after the \ufb01rst of the four\ncollects started.\nProof: If during a scan (), two successive reads by AofB\u2019s register return differ-\nent label values, then at least one write by Boccurs between this pair of reads.\nThreadBwrites to its register as the \ufb01nal step of an update () call, so some\nupdate () call byBended sometime after the \ufb01rst read by A, and the write step\nof another occurs between the last pair of reads by A. The claim follows because\nonlyBwrites to its register. 2\nLemma 4.3.3. The values returned by a scan () were in the registers at some state\nbetween the call\u2019s invocation and response.\nProof: If the scan () call made a clean double collect, then the claim follows from\nLemma 4.3.1. If the call took the scan value from another thread B\u2019s register, then\nby Lemma 4.3.2, the scan value found in B\u2019s register was obtained by a scan ()\ncall byBwhose interval lies between A\u2019s \ufb01rst and second reads of B\u2019s register.\nEitherB\u2019sscan () call had a clean double collect, in which case the result follows\nfrom Lemma 4.3.1, or there is an embedded scan () call by a thread Coccurring\nwithin the interval of B\u2019sscan () call. This argument can be applied inductively,\nnoting that there can be at most n\u00001 nested calls before we run out of threads,\nwherenis the maximum number of threads (see Fig. 4.22). Eventually, some\nnested scan () call must have a clean double collect. 2\nLemma 4.3.4. Every scan () or update () returns after at most O(n2) reads or\nwrites.\nProof: For a given scan (), since there are only n\u00001 other threads, after n+ 1\ndouble collects, either one is clean, or some thread is observed to move twice. It\nfollows that scan () calls are wait-free, and so are update () calls. 2\nBy Lemma 4.3.3, the values returned by a scan () form a snapshot as they are\nall in the registers in some state during the call: linearize the call at that point.\nSimilarly, linearize update () calls at the point the register is written.\nTheorem 4.3.1. Figs. 4.20 and 4.21 provide a wait-free snapshot implementation.\n4.4 Chapter Notes 93\nScan\nScan\nScan UpdateThread 0\nThread 1\nThread", "doc_id": "1516baf2-cd5f-4208-93b0-a58a19e03362", "embedding": null, "doc_hash": "fddb2c8eda3054c25ecfc32a3894ef1578c3609e2cb8987fafa23c34f8bd1317", "extra_info": null, "node_info": {"start": 274263, "end": 277427}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b8ee1d32-493b-449a-beb8-d8520a37802f", "3": "5d063730-1e0d-44be-8ed4-cff4e5fdbb0b"}}, "__type__": "1"}, "5d063730-1e0d-44be-8ed4-cff4e5fdbb0b": {"__data__": {"text": "or some thread is observed to move twice. It\nfollows that scan () calls are wait-free, and so are update () calls. 2\nBy Lemma 4.3.3, the values returned by a scan () form a snapshot as they are\nall in the registers in some state during the call: linearize the call at that point.\nSimilarly, linearize update () calls at the point the register is written.\nTheorem 4.3.1. Figs. 4.20 and 4.21 provide a wait-free snapshot implementation.\n4.4 Chapter Notes 93\nScan\nScan\nScan UpdateThread 0\nThread 1\nThread n/H115461Update\nFigure 4.22 There can be at most n\u2013 1 nested calls of scan ()before we run out of threads,\nwhere nis the maximum number of threads. The scan ()by thread n\u2013 1, contained in the\nintervals of all other scan ()calls, must have a clean double collect.\n4.4 Chapter Notes\nAlonzo Church introduced Lambda Calculus around 1934-5 [29]. Alan Turing\nde\ufb01ned the Turing Machine in a classic paper in 1936-7 [146]. Leslie Lamport \ufb01rst\nde\ufb01ned the notions of safe,regular , and atomic registers and the register hierarchy,\nand was the \ufb01rst to show that one could implement non trivial shared memory\nfrom safe bits [93, 94]. Gary Peterson \ufb01rst suggested the problem of construct-\ning atomic registers [125]. Jaydev Misra gave an axiomatic treatment of atomic\nregisters [116]. The notion of linearizability , which generalizes Leslie Lamport\nand Jaydev Misra\u2019s notions of atomic registers, is due to Herlihy and Wing [69].\nSusmita Haldar and Krishnamurthy Vidyasankar gave a bounded MRSW atomic\nregister construction from regular registers [50]. The problem of constructing a\nmulti-reader atomic register from single-reader atomic registers was mentioned\nas an open problem by Leslie Lamport [93, 94], and by Paul Vit \u00b4anyi and Baruch\nAwerbuch [148]. Paul Vit \u00b4anyi and Baruch Awerbuch were the \ufb01rst to propose an\napproach for MRMW atomic register design [148]. The \ufb01rst solution is due to\nJim Anderson, Mohamed Gouda, and Ambuj Singh [12, 13]. Other atomic reg-\nister constructions, to name only a few, were proposed by Jim Burns and Gary\nPeterson [24], Richard Newman-Wolfe [150], Lefteris Kirousis, Paul Spirakis, and\nPhilippas Tsigas [83], Amos Israeli and Amnon Shaham [78], and Ming Li and\nJohn Tromp and Paul Vit \u00b4anyi [104]. The simple timestamp-based atomic MRMW\nconstruction we present here is due to Danny Dolev and Nir Shavit [34].\nCollect operations were \ufb01rst formalized by Mike Saks, Nir Shavit, and Heather\nWoll [135]. The \ufb01rst atomic snapshot constructions were discovered concurrently\nand independently by Jim Anderson [10], and by Y ehuda Afek, Hagit Attiya,\nDanny Dolev, Eli Gafni, Michael Merritt and Nir Shavit [2]. The latter algorithm is\nthe one presented here. Later snapshot algorithms are due to Elizabeth Borowsky\nand Eli Gafni [22] and Y ehuda Afek, Gideon Stupp, and Dan T ouitou [4].\n94 Chapter 4 Foundations of Shared Memory\nThe timestamps in all the algorithms mentioned in this chapter can be\nbounded so that the constructions themselves use registers of bounded size.\nBounded timestamp systems were introduced by Amos Israeli and Ming Li [77],\nand bounded concurrent timestamp systems by Danny Dolev and Nir Shavit [34].\n4.5 Exercises\nExercise 34. Consider the safe", "doc_id": "5d063730-1e0d-44be-8ed4-cff4e5fdbb0b", "embedding": null, "doc_hash": "26d283ea1ce7a03344dfd6607b53971e17883b8edf0d8a1c0d4dd3875fdca613", "extra_info": null, "node_info": {"start": 277309, "end": 280507}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1516baf2-cd5f-4208-93b0-a58a19e03362", "3": "4cede6f6-d359-42ae-aa01-1c98fd837794"}}, "__type__": "1"}, "4cede6f6-d359-42ae-aa01-1c98fd837794": {"__data__": {"text": "Later snapshot algorithms are due to Elizabeth Borowsky\nand Eli Gafni [22] and Y ehuda Afek, Gideon Stupp, and Dan T ouitou [4].\n94 Chapter 4 Foundations of Shared Memory\nThe timestamps in all the algorithms mentioned in this chapter can be\nbounded so that the constructions themselves use registers of bounded size.\nBounded timestamp systems were introduced by Amos Israeli and Ming Li [77],\nand bounded concurrent timestamp systems by Danny Dolev and Nir Shavit [34].\n4.5 Exercises\nExercise 34. Consider the safe Boolean MRSW construction shown in Fig. 4.6.\nTrue or false: if we replace the safe Boolean SRSW register array with an array\nof safeM-valued SRSW registers, then the construction yields a safe M-valued\nMRSW register. Justify your answer.\nExercise 35. Consider the safe Boolean MRSW construction shown in Fig. 4.6.\nTrue or false: if we replace the safe Boolean SRSW register array with an array of\nregular Boolean SRSW registers, then the construction yields a regular Boolean\nMRSW register. Justify your answer.\nExercise 36. Consider the atomic MRSW construction shown in Fig. 4.12. True\nor false: if we replace the atomic SRSW registers with regular SRSW registers,\nthen the construction still yields an atomic MRSW register. Justify your answer.\nExercise 37. Give an example of a quiescently-consistent register execution that\nis not regular.\nExercise 38. Consider the safe Boolean MRSW construction shown in Fig. 4.6.\nTrue or false: if we replace the safe Boolean SRSW register array with an array\nofregularM-valued SRSW registers, then the construction yields a regular\nM-valued MRSW register. Justify your answer.\nExercise 39. Consider the regular Boolean MRSW construction shown in Fig. 4.7.\nTrue or false: if we replace the safe Boolean MRSW register with a safe M-valued\nMRSW register, then the construction yields a regular M-valued MRSW register.\nJustify your answer.\nExercise 40. Does Peterson\u2019s two-thread mutual exclusion algorithm work if the\nshared atomic flag registers are replaced by regular registers?\nExercise 41. Consider the following implementation of a Register in a distri-\nbuted, message-passing system. There are nprocessorsP0,:::,Pn\u00001arranged in\na ring, where Pican send messages only to Pi+1 modn. Messages are delivered in\nFIFO order along each link.\n4.5 Exercises 95\nEach processor keeps a copy of the shared register.\n\u0004T o read a register, the processor reads the copy in its local memory.\n\u0004A processor Pistarts a write () call of value vto registerx, by sending the\nmessage \u201cPi: writevtox\u201d toPi+1 modn.\n\u0004IfPireceives a message \u201c Pj: writevtox,\u201d fori6=j, then it writes vto its local\ncopy ofx, and forwards the message to Pi+1 modn.\n\u0004IfPireceives a message \u201c Pi: writevtox,\u201d then it writes vto its local copy of x,\nand discards the message. The write () call is now complete.\nGive a short justi\ufb01cation or counterexample.\nIfwrite () calls never overlap,\n\u0004Is this register implementation regular?\n\u0004Is it atomic?\nIf multiple processors call write (),\n\u0004Is this register implementation safe?\nExercise 42. Y ou learn that your competitor, the Acme Atomic Register Com-\npany, has developed a way to use Boolean (single-bit) atomic registers to construct\nan ef\ufb01cient write-once single-reader single-writer atomic register. Through your\nspies, you acquire the code fragment shown in Fig. 4.23, which", "doc_id": "4cede6f6-d359-42ae-aa01-1c98fd837794", "embedding": null, "doc_hash": "491feb90d031eb6fb6bc044db72e269b7df381ec2c0fee2f454f3c19edb09ee4", "extra_info": null, "node_info": {"start": 280492, "end": 283829}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5d063730-1e0d-44be-8ed4-cff4e5fdbb0b", "3": "1a97aecc-de37-42eb-9ebb-67098c4102d2"}}, "__type__": "1"}, "1a97aecc-de37-42eb-9ebb-67098c4102d2": {"__data__": {"text": "call is now complete.\nGive a short justi\ufb01cation or counterexample.\nIfwrite () calls never overlap,\n\u0004Is this register implementation regular?\n\u0004Is it atomic?\nIf multiple processors call write (),\n\u0004Is this register implementation safe?\nExercise 42. Y ou learn that your competitor, the Acme Atomic Register Com-\npany, has developed a way to use Boolean (single-bit) atomic registers to construct\nan ef\ufb01cient write-once single-reader single-writer atomic register. Through your\nspies, you acquire the code fragment shown in Fig. 4.23, which is unfortunately\nmissing the code for read (). Y our job is to devise a read () method that works for\n1class AcmeRegister implements Register{\n2 // N is the length of the register\n3 // Atomic multi-reader single-writer registers\n4 private BoolRegister[] b = new BoolMRSWRegister[3 *N];\n5 public void write( int x) {\n6 boolean [] v = intToBooleanArray(x);\n7 // copy v[i] to b[i] in ascending order of i\n8 for (int i = 0; i < N; i++)\n9 b[i].write(v[i]);\n10 // copy v[i] to b[N+i] in ascending order of i\n11 for (int i = 0; i < N; i++)\n12 b[N+i].write(v[i]);\n13 // copy v[i] to b[2N+i] in ascending order of i\n14 for (int i = 0; i < N; i++)\n15 b[(2 *N)+i].write(v[i]);\n16 }\n17 public int read() {\n18 // missing code\n19 }\n20 }\nFigure 4.23 Partial acme register implementation.\n96 Chapter 4 Foundations of Shared Memory\nthis class, and to justify (informally) why it works. (Remember that the register\nis write-once, meaning that your read will overlap at most one write.)\nExercise 43. Prove that the safe Boolean MRSW register construction from safe\nBoolean SRSW registers illustrated in Fig. 4.6 is a correct implementation of a\nregular MRSW register if the component registers are regular SRSW registers.\nExercise 44. A monotonic counter is a data structure c=c1...cm(i.e.,cis\ncomposed of the individual digits cj,j>0) such that c06c16c26..., where\nc0,c1,c2,... denote the successive values assumed by c.\nIfcis not a single digit, then reading and writing cmay involve several sepa-\nrate operations. A read of cwhich is performed concurrently with one or more\nwrites tocmay obtain a value different from any of the versions cj,j>0. The\nvalue obtained may contain traces of several different versions. If a read obtains\ntraces of versions ci1, ...,cim, then we say that it obtained a value of ck,lwherek=\nminimum (i1, ...,im) andl= maximum ( i1, ...,im), so 0 6k6l. Ifk=l, then\nck,l=ckand the read obtained is a consistent version of c.\nHence, the correctness condition for such a counter simply asserts that if a\nread obtains the value ck,l, thenck6ck,l6cl. The individual digits cjare also\nassumed to be atomic digits.\nGive an implementation of a monotonic counter, given that the following\ntheorems are true:\nTheorem 4.5.1. Ifc=c1...cmis always written from right to left, then a read\nfrom left to right obtains a sequence of values ck1,`1\n1,:::,ckm,`m\nm withk16`16\nk26:::6km6`m.\nTheorem 4.5.2. Letc=c1...cmand assume that c06c16...", "doc_id": "1a97aecc-de37-42eb-9ebb-67098c4102d2", "embedding": null, "doc_hash": "96a87545554223a9b906e2ced606a9bfa227e4e767fe81ef16897334e38f4995", "extra_info": null, "node_info": {"start": 283802, "end": 286771}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "4cede6f6-d359-42ae-aa01-1c98fd837794", "3": "47c935ce-e1a6-461e-8b40-e6a4d223bfce"}}, "__type__": "1"}, "47c935ce-e1a6-461e-8b40-e6a4d223bfce": {"__data__": {"text": "thenck6ck,l6cl. The individual digits cjare also\nassumed to be atomic digits.\nGive an implementation of a monotonic counter, given that the following\ntheorems are true:\nTheorem 4.5.1. Ifc=c1...cmis always written from right to left, then a read\nfrom left to right obtains a sequence of values ck1,`1\n1,:::,ckm,`m\nm withk16`16\nk26:::6km6`m.\nTheorem 4.5.2. Letc=c1...cmand assume that c06c16... .\n1.Ifi16...6im6ithenci1\n1...cim\nm6ci.\n2.Ifi1>...>im>ithenci1\n1...cim\nm>ci.\nTheorem 4.5.3. Letc=c1...cmand assume that c06c16... and the digits ciare\natomic.\n1.Ifcis always written from right to left, then a read from left to right obtains a\nvalueck,l6cl.\n2.Ifcis always written from left to right, then a read from right to left obtains a\nvalueck,l>cl.\nNote that:\nIf a read ofcobtains traces of version cj,j>0, then:\n\u0004The beginning of the read preceded the end of the write of cj+1.\n\u0004The end of the read followed the beginning of the write of cj.\n4.5 Exercises 97\nFurthermore, we say that a read (write) of cis performed from left to right if\nfor eachj>0, the read (write) of cjis completed before the read (write) of cj+1\nhas begun. Reading or writing from right to left is de\ufb01ned in the analogous way.\nFinally, always remember that subscripts refer to individual digits of cwhile\nsuperscripts refer to successive values assumed by c.\nExercise 45. Prove Theorem 4.5.1 of Exercise 44. Note that since kj6`j, you need\nonly to show that `j6kj+1if 16j<m .\n(2) Prove Theorem 4.5.3 of Exercise 44, given that the Lemma is true.\nExercise 46. We saw safe and regular registers earlier in this chapter. De\ufb01ne a\nwraparound register that has the property that there is a value vsuch that adding\n1 tovyields 0, not v+ 1.\nIf we replace the Bakery algorithm\u2019s shared variables with either (a) \ufb02ickering,\n(b) safe, (c) or wraparound registers, then does it still satisfy (1) mutual exclu-\nsion, (2) \ufb01rst-come-\ufb01rst-served ordering?\nY ou should provide six answers (some may imply others). Justify each claim.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n5The Relative Power of Primitive\nSynchronization Operations\nImagine you are in charge of designing a new multiprocessor. What kinds of\natomic instructions should you include? The literature includes a bewildering\narray of different choices: reading and writing to memory, getAndDecrement (),\nswap (),getAndComplement (),compareAndSet() , and many, many others. Sup-\nporting them all would be complicated and inef\ufb01cient, but supporting the wrong\nones could make it dif\ufb01cult or even impossible to solve important synchroniza-\ntion problems.\nOur goal is to identify a set of primitive synchronization operations powerful\nenough to solve synchronization problems likely to arise in practice. (Of course,\nwe might want to support other, nonessential synchronization operations for\nconvenience.) T o this end, we need some way to evaluate the power of various\nsynchronization primitives:", "doc_id": "47c935ce-e1a6-461e-8b40-e6a4d223bfce", "embedding": null, "doc_hash": "741a591f4e90d26f742a0980634a76e513eeece856dd470bfe994b8956b6c77b", "extra_info": null, "node_info": {"start": 286894, "end": 289865}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1a97aecc-de37-42eb-9ebb-67098c4102d2", "3": "0016cb3b-4268-4a70-8cc7-850b383f101d"}}, "__type__": "1"}, "0016cb3b-4268-4a70-8cc7-850b383f101d": {"__data__": {"text": "(),compareAndSet() , and many, many others. Sup-\nporting them all would be complicated and inef\ufb01cient, but supporting the wrong\nones could make it dif\ufb01cult or even impossible to solve important synchroniza-\ntion problems.\nOur goal is to identify a set of primitive synchronization operations powerful\nenough to solve synchronization problems likely to arise in practice. (Of course,\nwe might want to support other, nonessential synchronization operations for\nconvenience.) T o this end, we need some way to evaluate the power of various\nsynchronization primitives: what synchronization problems they can solve, and\nhow ef\ufb01ciently they can solve them.\nA concurrent object implementation is wait-free if each method call \ufb01nishes\nin a \ufb01nite number of steps. A method is lock-free if it guarantees that in\ufb01nitely\noften some method call \ufb01nishes in a \ufb01nite number of steps. We have already seen\nwait-free (and therefore by de\ufb01nition also lock-free) register implementations\nin Chapter 4. One way to evaluate the power of synchronization instructions is\nto see how well they support implementations of shared objects such as queues,\nstacks, trees, and so on. As we explained in Chapter 4, we evaluate solutions that\nare wait-free or lock-free, that is, guarantee progress without relying on outside\nsupport.1\nWe will see that all synchronization instructions are not created equal. If\none thinks of primitive synchronization instructions as objects whose exported\nmethods are the instructions themselves (in the literature these objects are often\nreferred to as synchronization primitives ), one can show that there is an in\ufb01nite\n1It makes no sense to evaluate solutions that only meet dependent progress conditions. This is\nbecause the real power of solutions based on dependent conditions such as obstruction-freedom\nor deadlock-freedom is masked by the contribution of the operating system they depend on.\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00005-8\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.99\n100 Chapter 5 The Relative Power of Primitive Synchronization Operations\nhierarchy of synchronization primitives, such that no primitive at one level can\nbe used for a wait-free or lock-free implementation of any primitives at higher\nlevels. The basic idea is simple: each class in the hierarchy has an associated con-\nsensus number , which is the maximum number of threads for which objects of the\nclass can solve an elementary synchronization problem called consensus . We will\nsee that in a system of nor more concurrent threads, it is impossible to construct\na wait-free or lock-free implementation of an object with consensus number n\nfrom an object with a lower consensus number.\n5.1 Consensus Numbers\nConsensus is an innocuous-looking, somewhat abstract problem that will have\nenormous consequences for everything from algorithm design to hardware\narchitecture. A consensus object provides a single method decide (), as shown\nin Fig. 5.1. Each thread calls the decide () method with its input vat most\nonce. The object\u2019s decide () method will return a value meeting the following\nconditions:\n\u0004consistent : all threads decide the same value,\n\u0004valid : the common decision value is some thread\u2019s input.\nIn other words, a concurrent consensus object is linearizable to a sequential con-\nsensus object in which the thread whose value was chosen completes its decide ()\n\ufb01rst. Sometimes it is useful to focus on consensus problems where all inputs are\neither zero or one. We call this specialized problem binary consensus . T o simplify\nthe presentation, we focus here on binary consensus, but our claims apply verba-\ntim to consensus in general.\nWe are", "doc_id": "0016cb3b-4268-4a70-8cc7-850b383f101d", "embedding": null, "doc_hash": "64c6128c6de413c78d5f3e951c45c3036edb4c84f9d94b4e0ce95f08a94a55e2", "extra_info": null, "node_info": {"start": 289722, "end": 293413}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "47c935ce-e1a6-461e-8b40-e6a4d223bfce", "3": "355ab3b4-d78b-4eae-8fd2-045a52d16b70"}}, "__type__": "1"}, "355ab3b4-d78b-4eae-8fd2-045a52d16b70": {"__data__": {"text": ": all threads decide the same value,\n\u0004valid : the common decision value is some thread\u2019s input.\nIn other words, a concurrent consensus object is linearizable to a sequential con-\nsensus object in which the thread whose value was chosen completes its decide ()\n\ufb01rst. Sometimes it is useful to focus on consensus problems where all inputs are\neither zero or one. We call this specialized problem binary consensus . T o simplify\nthe presentation, we focus here on binary consensus, but our claims apply verba-\ntim to consensus in general.\nWe are interested in wait-free solutions to the consensus problem, that is, wait-\nfree concurrent implementations of consensus objects. The reader will notice that\nsince the decide () method of a given consensus object is executed only once by\neach thread, and there are a \ufb01nite number of threads, by de\ufb01nition a lock-free\nimplementation would also be wait-free and vice versa. Henceforth, we men-\ntion only wait-free implementations, and for historical reasons, call any class that\nimplements consensus in a wait-free manner a consensus protocol .\n1public interface Consensus<T> {\n2 T decide(T value);\n3}\nFigure 5.1 Consensus object interface.\n5.1 Consensus Numbers 101\nWe will restrict ourselves to object classes with deterministic sequential speci-\n\ufb01cations (i.e., ones in which each sequential method call has a single outcome).2\nWe want to understand whether a particular class of objects is powerful\nenough to solve the consensus problem. How can we make this notion more\nprecise? If we think of such objects as supported by a lower level of the system,\nperhaps the operating system, or even the hardware, then we care about the prop-\nerties of the class, not about the number of objects. (If the system can provide\none object of this class, it can probably provide more.) Second, it is reasonable to\nsuppose that any modern system can provide a generous amount of read\u2013write\nmemory for bookkeeping. These two observations suggest the following de\ufb01ni-\ntion.\nDe\ufb01nition 5.1.1. A classCsolvesn-thread consensus if there exist a consensus\nprotocol using any number of objects of class Cand any number of atomic\nregisters.\nDe\ufb01nition 5.1.2. The consensus number of a classCis the largest nfor which\nthat class solves n-thread consensus. If no largest nexists, we say the consensus\nnumber of the class is in\ufb01nite .\nCorollary 5.1.1. Suppose one can implement an object of class Cfrom one or\nmore objects of class D, together with some number of atomic registers. If class\nCsolvesn-consensus, then so does class D.\n5.1.1 States and Valence\nA good place to start is to think about the simplest interesting case: binary con-\nsensus (i.e., inputs 0 or 1) for 2 threads (call them AandB). Each thread makes\nmoves until it decides on a value. Here, a move is a method call to a shared object.\nAprotocol state consists of the states of the threads and the shared objects. An ini-\ntial state is a protocol state before any thread has moved, and a \ufb01nal state is a\nprotocol state after all threads have \ufb01nished. The decision value of any \ufb01nal state\nis the value decided by all threads in that state.\nA wait-free protocol\u2019s set of possible states forms a tree, where each node rep-\nresents a possible protocol state, and each edge represents a possible move by\nsome thread. Fig. 5.2 shows the tree for a 2-thread protocol in which each thread\nmoves twice. An edge for Afrom nodesto nodes0means that if Amoves in pro-\ntocol states, then the new protocol state is s0.", "doc_id": "355ab3b4-d78b-4eae-8fd2-045a52d16b70", "embedding": null, "doc_hash": "0a1d4214d93270117bfce3e81527ccfad81c0b5264bd4ebc0e185d1e32a5dbec", "extra_info": null, "node_info": {"start": 293445, "end": 296934}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0016cb3b-4268-4a70-8cc7-850b383f101d", "3": "dcd02c74-6e10-462d-b51f-bd0f856c25f0"}}, "__type__": "1"}, "dcd02c74-6e10-462d-b51f-bd0f856c25f0": {"__data__": {"text": "state after all threads have \ufb01nished. The decision value of any \ufb01nal state\nis the value decided by all threads in that state.\nA wait-free protocol\u2019s set of possible states forms a tree, where each node rep-\nresents a possible protocol state, and each edge represents a possible move by\nsome thread. Fig. 5.2 shows the tree for a 2-thread protocol in which each thread\nmoves twice. An edge for Afrom nodesto nodes0means that if Amoves in pro-\ntocol states, then the new protocol state is s0. We refer to s0as asuccessor state to\ns. Because the protocol is wait-free, the tree must be \ufb01nite. Leaf nodes represent\n\ufb01nal protocol states, and are labeled with their decision values, either 0 or 1.\n2We avoid nondeterministic objects since their structure is signi\ufb01cantly more complex. See the\ndiscussion in the notes at the end of this chapter.\n102 Chapter 5 The Relative Power of Primitive Synchronization Operations\nA moves\nB moves\nfinal states with decision valuesinitial state (bivalent) \nunivalent state\n1 0 1 1 1 1\nFigure 5.2 An execution tree for two threads AandB. The dark shaded nodes denote biva-\nlent states, and the lighter ones univalent states.\nA protocol state is bivalent if the decision value is not yet \ufb01xed: there is some\nexecution starting from that state in which the threads decide 0, and one in which\nthey decide 1. By contrast, the protocol state is univalent if the outcome is \ufb01xed:\nevery execution starting from that state decides the same value. A protocol state\nis 1-valent if it is univalent, and the decision value will be 1, and similarly for\n0-valent . As illustrated in Fig. 5.2, a bivalent state is a node whose descendants\nin the tree include both leaves labeled with 0 and leaves labeled with 1, while a\nunivalent state is a node whose descendants include only leaves labeled with a\nsingle decision value.\nOur next lemma says that an initial bivalent state exists. This observation\nmeans that the outcome of the protocol cannot be \ufb01xed in advance, but must\ndepend on how reads andwrites are interleaved.\nLemma 5.1.1. Every 2-thread consensus protocol has a bivalent initial state.\nProof: Consider the initial state where Ahas input 0 and Bhas input 1. If A\n\ufb01nishes the protocol before Btakes a step, then Amust decide 0, because it must\ndecide some thread\u2019s input, and 0 is the only input it has seen (it cannot decide\n1 because it has no way of distinguishing this state from the one in which Bhas\ninput 0). Symmetrically, if B\ufb01nishes the protocol before Atakes a step, then B\nmust decide 1, because it must decide some thread\u2019s input, and 1 is the only input\nit has seen. It follows that the initial state where Ahas input 0 and Bhas input 1\nis bivalent. 2\n5.2 Atomic Registers 103\nLemma 5.1.2. Every n-thread consensus protocol has a bivalent initial state.\nProof: Left as an exercise. 2\nA protocol state is critical if:\n\u0004It is bivalent, and\n\u0004if any thread moves, the protocol state becomes univalent.\nLemma 5.1.3. Every wait-free consensus protocol has a critical state.\nProof: Suppose not. By Lemma 5.1.2, the protocol has a bivalent initial state. Start\nthe protocol in this state. As long as there is some thread that can move without\nmaking the protocol state univalent, let that thread move. If the protocol runs\nforever, then it is not wait-free. Otherwise, the protocol eventually enters a state\nwhere", "doc_id": "dcd02c74-6e10-462d-b51f-bd0f856c25f0", "embedding": null, "doc_hash": "b7cb351d0e37dd79745da4798590223d989be2fe210c8107aa0761681ccf164c", "extra_info": null, "node_info": {"start": 296983, "end": 300326}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "355ab3b4-d78b-4eae-8fd2-045a52d16b70", "3": "5514f934-fdae-4c8b-a25c-320b993e9a8a"}}, "__type__": "1"}, "5514f934-fdae-4c8b-a25c-320b993e9a8a": {"__data__": {"text": "2\nA protocol state is critical if:\n\u0004It is bivalent, and\n\u0004if any thread moves, the protocol state becomes univalent.\nLemma 5.1.3. Every wait-free consensus protocol has a critical state.\nProof: Suppose not. By Lemma 5.1.2, the protocol has a bivalent initial state. Start\nthe protocol in this state. As long as there is some thread that can move without\nmaking the protocol state univalent, let that thread move. If the protocol runs\nforever, then it is not wait-free. Otherwise, the protocol eventually enters a state\nwhere no such move is possible, which must be a critical state. 2\nEverything we have proved so far applies to any consensus protocol, no matter\nwhat class (or classes) of shared objects it uses. Now we turn our attention to\nspeci\ufb01c classes of objects.\n5.2 Atomic Registers\nThe obvious place to begin is to ask whether we can solve consensus using atomic\nregisters. Surprisingly, perhaps, the answer is no. We will show that there is no\nbinary consensus protocol for two threads. We leave it as an exercise to show that\nif two threads cannot reach consensus on two values, then nthreads cannot reach\nconsensus on kvalues, where n>2 andk>2.\nOften, when we argue about whether or not there exists a protocol that solves\na particular problem, we construct a scenario of the form: \u201cif we had such a pro-\ntocol, it would behave like this under these circumstances . . . \u201d . One particularly\nuseful scenario is to have one thread, say A, run completely by itself until it \ufb01ni-\nshes the protocol. This particular scenario is common enough that we give it its\nown name:Aruns solo.\nTheorem 5.2.1. Atomic registers have consensus number 1.\nProof: Suppose there exists a binary consensus protocol for two threads AandB.\nWe will reason about the properties of such a protocol and derive a contradiction.\nBy Lemma 5.1.3, we can run the protocol until it reaches a critical state s.\nSupposeA\u2019s next move carries the protocol to a 0-valent state, and B\u2019s next move\ncarries the protocol to a 1-valent state. (If not, then switch thread names.) What\n104 Chapter 5 The Relative Power of Primitive Synchronization Operations\nB executes\none operations\ns\u00b4\u00b4A reads\ns\u00b4\nB executes\none operation\n1 B runs soloB runs solo\n0\nFigure 5.3 Case: Areads \ufb01rst. In the \ufb01rst execution scenario, Bmoves \ufb01rst, driving the\nprotocol to a 1-valent state s0, and then Bruns solo and eventually decides 1. In the second\nexecution scenario, Amoves \ufb01rst, driving the protocol to a 0-valent state s00.Bthen runs solo\nstarting in s00and eventually decides 0.\nmethods could AandBbe about to call? We now consider an exhaustive list of\nthe possibilities: one of them reads from a register, they both write to separate\nregisters, or they both write to the same register.\nSupposeAis about to read a given register ( Bmay be about to either read or\nwrite the same register or a different register), as depicted in Fig. 5.3. Consider\ntwo possible execution scenarios. In the \ufb01rst scenario, Bmoves \ufb01rst, driving the\nprotocol to a 1-valent state s0, and thenBruns solo and eventually decides 1.\nIn the second execution scenario, Amoves \ufb01rst, then Bexecutes one operation,\ndriving the protocol to a 0-valent state s.Bthen runs solo starting in s00and\neventually decides 0. The problem is that the states s0ands00are indistinguishable\ntoB(the", "doc_id": "5514f934-fdae-4c8b-a25c-320b993e9a8a", "embedding": null, "doc_hash": "d1343fac12ffac15a8e359e5601d04006d69acf9701aedffd1bae76a42f02cfb", "extra_info": null, "node_info": {"start": 300291, "end": 303594}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "dcd02c74-6e10-462d-b51f-bd0f856c25f0", "3": "f95150df-3418-4f10-8ba4-9ece4ce0426a"}}, "__type__": "1"}, "f95150df-3418-4f10-8ba4-9ece4ce0426a": {"__data__": {"text": "same register or a different register), as depicted in Fig. 5.3. Consider\ntwo possible execution scenarios. In the \ufb01rst scenario, Bmoves \ufb01rst, driving the\nprotocol to a 1-valent state s0, and thenBruns solo and eventually decides 1.\nIn the second execution scenario, Amoves \ufb01rst, then Bexecutes one operation,\ndriving the protocol to a 0-valent state s.Bthen runs solo starting in s00and\neventually decides 0. The problem is that the states s0ands00are indistinguishable\ntoB(the readAperformed could only change its thread-local state which is not\nvisible toB), which means that Bmust decide the same value in both scenarios,\na contradiction.\nSuppose, instead of this scenario, both threads are about to write to different\nregisters, as depicted in Fig. 5.4.Ais about to write to r0andBtor1. Let us\nconsider two possible execution scenarios. In the \ufb01rst, Awrites tor0and thenB\nwrites tor1, so the resulting protocol state is 0-valent because Awent \ufb01rst. In the\nsecond,Bwrites tor1and thenAwrites tor0, so the resulting protocol state is\n1-valent because Bwent \ufb01rst.\nThe problem is that both scenarios lead to indistinguishable protocol states.\nNeitherAnorBcan tell which move was \ufb01rst. The resulting state is therefore\nboth 0-valent and 1-valent, a contradiction.\nFinally, suppose both threads write to the same register r, as depicted in Fig. 5.5.\nAgain, consider two possible execution scenarios. In one scenario Awrites \ufb01rst,\nthe resulting protocol state s0is 0-valent,Bthen runs solo and decides 0. In\n5.2 Atomic Registers 105\nB writes r1\nB writes r1s\nA writes r0\nA writes r0\n0-valent 1-valent\nFigure 5.4 Case: AandBwrite to different registers.\nB writes rs\ns\u00b4A writes r\ns\u00b4\u00b4\nB writes r\n1B runs solo\n0B runs solo\nFigure 5.5 Case: AandBwrite to the same register.\nanother scenario, Bwrites \ufb01rst, the resulting protocol state s00is 1-valent,Bthen\nruns solo and decides 1. The problem is that Bcannot tell the difference between\ns0ands00(because in both s0ands00it overwrote the register rand obliterated\nany trace ofA\u2019s write) soBmust decide the same value starting from either state,\na contradiction. 2\nCorollary 5.2.1. It is impossible to construct a wait-free implementation of any\nobject with consensus number greater than 1 using atomic registers.\n106 Chapter 5 The Relative Power of Primitive Synchronization Operations\n1public abstract class ConsensusProtocol<T> implements Consensus<T> {\n2 protected T[] proposed = (T[]) new Object[N];\n3 // announce my input value to the other threads\n4 void propose(T value) {\n5 proposed[ThreadID.get()] = value;\n6 }\n7 // figure out which thread was first\n8 abstract public T decide(T value);\n9}\nFigure 5.6 The generic consensus protocol.\nThe aforementioned corollary is perhaps one of the most striking impossibil-\nity results in Computer Science. It explains why, if we want to implement lock-\nfree concurrent data structures on modern multiprocessors, our hardware must\nprovide primitive synchronization operations other than loads and stores (reads\u2013\nwrites).\n5.3 Consensus Protocols\nWe now consider a variety of interesting object classes, asking how well each\ncan solve the consensus problem. These protocols have a generic", "doc_id": "f95150df-3418-4f10-8ba4-9ece4ce0426a", "embedding": null, "doc_hash": "3a0b5abc7c7c1792b136f7954a032a25bf6a66a8ed82360d5bbf5f0d6c71c63d", "extra_info": null, "node_info": {"start": 303629, "end": 306801}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5514f934-fdae-4c8b-a25c-320b993e9a8a", "3": "0ef9c813-217f-49eb-9801-4173c367b7ec"}}, "__type__": "1"}, "0ef9c813-217f-49eb-9801-4173c367b7ec": {"__data__": {"text": "abstract public T decide(T value);\n9}\nFigure 5.6 The generic consensus protocol.\nThe aforementioned corollary is perhaps one of the most striking impossibil-\nity results in Computer Science. It explains why, if we want to implement lock-\nfree concurrent data structures on modern multiprocessors, our hardware must\nprovide primitive synchronization operations other than loads and stores (reads\u2013\nwrites).\n5.3 Consensus Protocols\nWe now consider a variety of interesting object classes, asking how well each\ncan solve the consensus problem. These protocols have a generic form, which\nwe describe in Fig. 5.6. The object has an array of atomic registers in which each\ndecide () method proposes its input value and then goes on to execute a sequence\nof steps in order to decide on one of the proposed values. We will devise different\nimplementations of the decide () method using various synchronization objects.\n5.4 FIFO Queues\nIn Chapter 3, we saw a wait-free FIFO queue implementation using only atomic\nregisters, subject to the limitation that only one thread could enqueue to the\nqueue, and only one thread could dequeue from the queue. It is natural to ask\nwhether one can provide a wait-free implementation of a FIFO queue that sup-\nports multiple enqueuers and dequeuers. For now, let us focus on a more speci\ufb01c\nproblem: can we provide a wait-free implementation of a two-dequeuer FIFO\nqueue using atomic registers?\nTheorem 5.4.1. The two-dequeuer FIFO queue class has consensus number at\nleast 2.\nProof: Fig. 5.7 shows a two-thread consensus protocol using a single FIFO\nqueue. Here, the queue stores integers. The queue is initialized by enqueuing the\nvalue WINfollowed by the value LOSE . As in all the consensus protocol considered\n5.4 FIFO Queues 107\n1public class QueueConsensus<T> extends ConsensusProtocol<T> {\n2 private static final int WIN = 0; // first thread\n3 private static final int LOSE = 1; // second thread\n4 Queue queue;\n5 // initialize queue with two items\n6 public QueueConsensus() {\n7 queue = new Queue();\n8 queue.enq(WIN);\n9 queue.enq(LOSE);\n10 }\n11 // figure out which thread was first\n12 public T decide(T Value) {\n13 propose(value);\n14 int status = queue.deq();\n15 int i = ThreadID.get();\n16 if(status == WIN)\n17 return proposed[i];\n18 else\n19 return proposed[1-i];\n20 }\n21 }\nFigure 5.7 2-thread consensus using a FIFO queue.\nhere, decide () \ufb01rst calls propose (v), which stores vinproposed [], a shared\narray of proposed input values. It then proceeds to dequeue the next item from\nthe queue. If that item is the value WIN, then the calling thread was \ufb01rst, and it\ndecides on its own value. If that item is the value LOSE , then the other thread\nwas \ufb01rst, so the calling thread returns the other thread\u2019s input, as declared in the\nproposed [] array.\nThe protocol is wait-free, since it contains no loops. If each thread returns\nits own input, then they must both have dequeued WIN, violating the FIFO\nqueue speci\ufb01cation. If each returns the other\u2019s input, then they must both have\ndequeued LOSE , also violating the queue speci\ufb01cation.\nThe validity condition follows from the observation that the thread that\ndequeued WIN stored its input in the proposed [] array before any value was\ndequeued. 2\nTrivial variations of this program yield protocols for stacks, priority queues,\nlists, sets, or any object with methods that return different results if applied in\ndifferent", "doc_id": "0ef9c813-217f-49eb-9801-4173c367b7ec", "embedding": null, "doc_hash": "1fb5197ead45fa1603c8dd8be55da53349322bf33f5755139abe98fb2f76ca98", "extra_info": null, "node_info": {"start": 306717, "end": 310119}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f95150df-3418-4f10-8ba4-9ece4ce0426a", "3": "96755489-7e42-41f8-a78b-1db7a7fb1a4a"}}, "__type__": "1"}, "96755489-7e42-41f8-a78b-1db7a7fb1a4a": {"__data__": {"text": "own input, then they must both have dequeued WIN, violating the FIFO\nqueue speci\ufb01cation. If each returns the other\u2019s input, then they must both have\ndequeued LOSE , also violating the queue speci\ufb01cation.\nThe validity condition follows from the observation that the thread that\ndequeued WIN stored its input in the proposed [] array before any value was\ndequeued. 2\nTrivial variations of this program yield protocols for stacks, priority queues,\nlists, sets, or any object with methods that return different results if applied in\ndifferent orders.\nCorollary 5.4.1. It is impossible to construct a wait-free implementation of a\nqueue, stack, priority queue, set, or list from a set of atomic registers.\nAlthough FIFO queues solve two-thread consensus, they cannot solve 3-thread\nconsensus.\nTheorem 5.4.1. FIFO queues have consensus number 2.\n108 Chapter 5 The Relative Power of Primitive Synchronization Operations\nB deqs\nA  deq\nB deq A deq\n0 1C runs solo C runs soloxyqueue tail queue head\ny y\nFigure 5.8 Case: AandBboth call deq().\nProof: By contradiction. Assume we have a consensus protocol for a thread A,\nB, andC. By Lemma 5.1.3, the protocol has a critical state s. Without loss of\ngenerality, we can assume that A\u2019s next move takes the protocol to a 0-valent\nstate, andB\u2019s next move takes the protocol to a 1-valent state. The rest, as before,\nis a case analysis.\nFirst, we know that AandB\u2019s pending moves cannot commute, implying that\nthey are both about to call methods of the same object. Second, we know that A\nandBcannot be about to read or write shared registers. It follows that they are\nabout to call methods of a single queue object.\nFirst, suppose AandBboth call deq(), as depicted in Fig. 5.8. Let s0be the\nprotocol state if Adequeues and then Bdequeues, and let s00be the state if the\ndequeues occur in the opposite order. Since s0is 0-valent, if Cruns uninterrupted\nfroms0, then it decides 0. Since s00is 1-valent, if Cruns uninterrupted from s00,\nthen it decides 1. But s0ands00are indistinguishable to C(the same two items\nwere removed from the queue), so Cmust decide the same value in both states,\na contradiction.\nSecond, suppose Acalls enq(a) andBcalls deq(). If the queue is nonempty,\nthe contradiction is immediate because the two methods commute (each oper-\nates on a different end of the queue): Ccannot observe the order in which they\noccurred. If the queue is empty, then the 1-valent state reached if Bexecutes\na dequeue on the empty queue and then Aenqueues is indistinguishable to C\nfrom the 0-valent state reached if Aalone enqueues. Notice that we do not really\ncare what a deq() on an empty queue does, that is, aborts or waits, since this will\nnot affect the state visible to C.\nFinally, suppose Acalls enq(a) andBcalls enq(b), as depicted in Fig. 5.9. Let\ns0be the state at the end of the following execution:\n5.4 FIFO Queues 109\nB enq bs\nA enq a\nB enq b A enq a\n0 1C runs solo C runs soloqueue head queue tail\nrun A till deq b\nrun B till deq arun A till deq a\nrun B till deq bb\nb aa\nab ba\nFigure 5.9 Case: Acalls enq(a)andBcalls enq(b). Notice that a", "doc_id": "96755489-7e42-41f8-a78b-1db7a7fb1a4a", "embedding": null, "doc_hash": "0d89f692e5881fbc08633187782645166a7037abb50e1a5f413175cf6779986f", "extra_info": null, "node_info": {"start": 310156, "end": 313246}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0ef9c813-217f-49eb-9801-4173c367b7ec", "3": "a029b7ea-41df-405e-a502-9fda07d4dc3b"}}, "__type__": "1"}, "a029b7ea-41df-405e-a502-9fda07d4dc3b": {"__data__": {"text": "enq(a) andBcalls enq(b), as depicted in Fig. 5.9. Let\ns0be the state at the end of the following execution:\n5.4 FIFO Queues 109\nB enq bs\nA enq a\nB enq b A enq a\n0 1C runs solo C runs soloqueue head queue tail\nrun A till deq b\nrun B till deq arun A till deq a\nrun B till deq bb\nb aa\nab ba\nFigure 5.9 Case: Acalls enq(a)andBcalls enq(b). Notice that a new item is enqueued by A\nafter AandBenqueued their respective items and before it dequeued (and Bcould have also\nenqueued items before dequeuing), but that this item is the same in both of the execution\nscenarios.\n1.LetAandBenqueue items aandbin that order.\n2.RunAuntil it dequeues a. (Since the only way to observe the queue\u2019s state is\nvia the deq() method,Acannot decide before it observes one of aorb.)\n3.BeforeAtakes any further steps, run Buntil it dequeues b.\nLets00be the state after the following alternative execution:\n1.LetBandAenqueue items bandain that order.\n2.RunAuntil it dequeues b.\n3.BeforeAtakes any further steps, run Buntil it dequeues a.\nClearly,s0is 0-valent and s00is 1-valent. Both A\u2019s executions are identical until\nAdequeuesaorb. SinceAis halted before it can modify any other objects, B\u2019s\nexecutions are also identical until it dequeues aorb. By a now familiar argument,\na contradiction arises because s0ands00are indistinguishable to C. 2\n110 Chapter 5 The Relative Power of Primitive Synchronization Operations\nTrivial variations of this argument can be applied to show that many similar\ndata types, such as sets, stacks, double-ended queues, and priority queues, all\nhave consensus number exactly two.\n5.5 Multiple Assignment Objects\nIn the (m,n-assignment ) problem, for n>m> 1 (sometimes called multiple\nassignment ), we are given an object with n\ufb01elds (sometimes an n-element array).\nThe assign () method takes as arguments mvaluesvi,i20,:::,m\u00001, andm\nindex values ij,j20,:::,m\u00001,ij20,:::,n\u00001. It atomically assigns vjto array\nelementij. The read () method takes an index argument, i, and returns the ith\narray element. This problem is the dual of the atomic snapshot object (Chapter 4),\nwhere we assign to one \ufb01eld and read multiple \ufb01elds atomically. Because snap-\nshots can be implemented from read\u2013write registers, Theorem 5.2.1 implies\nshapshot objects have consensus number 1.\nFig. 5.10 shows a lock-based implementation of a (2, 3)-assignment object.\nHere, threads can assign atomically to any two out of three array entries.\nTheorem 5.5.1. There is no wait-free implementation of an ( m,n)-assignment\nobject by atomic registers for any n>m> 1.\nProof: It is enough to show that we can solve 2-consensus given two threads and\na (2, 3)-assignment object. (Exercise 75 asks one to justify this claim.) As usual,\nthedecide () method must \ufb01gure out which thread went \ufb01rst. All array entries\nare initialized with null values. Fig. 5.11 shows the protocol. Thread Awrites\n(atomically) to \ufb01elds 0 and 1, while thread Bwrites (atomically) to \ufb01elds 1 and\n2. Then they try to determine who went \ufb01rst. From A\u2019s point", "doc_id": "a029b7ea-41df-405e-a502-9fda07d4dc3b", "embedding": null, "doc_hash": "5ffba7f9b2516a21cb0a2329b14c6253ee8b5ded9235e8df96ca097bfd125612", "extra_info": null, "node_info": {"start": 313423, "end": 316417}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "96755489-7e42-41f8-a78b-1db7a7fb1a4a", "3": "54930f9c-2fe6-4491-9cba-afe2a9ee3a2d"}}, "__type__": "1"}, "54930f9c-2fe6-4491-9cba-afe2a9ee3a2d": {"__data__": {"text": "enough to show that we can solve 2-consensus given two threads and\na (2, 3)-assignment object. (Exercise 75 asks one to justify this claim.) As usual,\nthedecide () method must \ufb01gure out which thread went \ufb01rst. All array entries\nare initialized with null values. Fig. 5.11 shows the protocol. Thread Awrites\n(atomically) to \ufb01elds 0 and 1, while thread Bwrites (atomically) to \ufb01elds 1 and\n2. Then they try to determine who went \ufb01rst. From A\u2019s point of view, there are\nthree cases, as shown in Fig. 5.12:\n1public class Assign23 {\n2 int[] r = new int [3];\n3 public Assign23( int init) {\n4 for (int i = 0; i < r.length; i++)\n5 r[i] = init;\n6 }\n7 public synchronized void assign(T v0, T v1, int i0, int i1) {\n8 r[i0] = v0;\n9 r[i1] = v1;\n10 }\n11 public synchronized int read( int i) {\n12 return r[i];\n13 }\n14 }\nFigure 5.10 A lock-based implementation of a (2,3)-assignment object.\n5.5 Multiple Assignment Objects 111\n\u0004IfA\u2019s assignment was ordered \ufb01rst, and B\u2019s assignment has not happened,\nthen \ufb01elds 0 and 1 have A\u2019s value, and \ufb01eld 2 is null.Adecides its own input.\n\u0004IfA\u2019s assignment was ordered \ufb01rst, and B\u2019s second, then \ufb01eld 0 has A\u2019s value,\nand \ufb01elds 1 and 2 have B\u2019s.Adecides its own input.\n\u0004IfB\u2019s assignment was ordered \ufb01rst, and A\u2019s second, then \ufb01elds 0 and 1 have\nA\u2019s value, and 2 has B\u2019s.AdecidesB\u2019s input.\nA similar analysis holds for B. 2\nTheorem 5.5.2. Atomic (n,n(n+1)\n2)-register assignment for n> 1 has consensus\nnumber at least n.\nProof: We design a consensus protocol for nthreads 0,:::,n\u00001. The protocol\nuses an (n,n(n+1)\n2)-assignment object. For convenience we name the object \ufb01elds\nas follows. There are n\ufb01eldsr0,:::,rn\u00001where thread iwrites to register ri, and\nn(n\u00001)=2 \ufb01eldsrij, wherei>j , where threads iandjboth write to \ufb01eld rij. All\n\ufb01elds are initialized to null. Each thread iatomically assigns its input value to n\n\ufb01elds: its single-writer \ufb01eld riand itsn\u00001 multi-writer registers rij. The protocol\ndecides the \ufb01rst value to be assigned.\n1public class MultiConsensus<T> extends ConsensusProtocol<T> {\n2 private final int NULL = -1;\n3 Assign23 assign23 = new Assign23(NULL);\n4 public T decide(T value) {\n5 propose(value);\n6 int i = ThreadID.get();\n7 int j = 1-i;\n8 // double assignment\n9 assign23.assign(i, i, i, i+1);\n10 int other = assign23.read((i+2) % 3);\n11 if(other == NULL || other == assign23.read(1))\n12 return proposed[i]; // I win\n13 else\n14 return proposed[j]; // I lose\n15 }\n16 }\nFigure 5.11 2-thread consensus using (2,3)-multiple assignment.\nA", "doc_id": "54930f9c-2fe6-4491-9cba-afe2a9ee3a2d", "embedding": null, "doc_hash": "e077d532b1cbacb901556033eacd1e1ae34c88d36fa01fb9305c6a22171f29e6", "extra_info": null, "node_info": {"start": 316328, "end": 318809}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a029b7ea-41df-405e-a502-9fda07d4dc3b", "3": "358df029-e3d5-47f2-8a91-ecac427758d8"}}, "__type__": "1"}, "358df029-e3d5-47f2-8a91-ecac427758d8": {"__data__": {"text": "T decide(T value) {\n5 propose(value);\n6 int i = ThreadID.get();\n7 int j = 1-i;\n8 // double assignment\n9 assign23.assign(i, i, i, i+1);\n10 int other = assign23.read((i+2) % 3);\n11 if(other == NULL || other == assign23.read(1))\n12 return proposed[i]; // I win\n13 else\n14 return proposed[j]; // I lose\n15 }\n16 }\nFigure 5.11 2-thread consensus using (2,3)-multiple assignment.\nA decides a012\nA decides b012Case 1 Case 3\naa\nA decides a012Case 2\nabb aab\nFigure 5.12 Consensus using multiple assignment: possible views.\n112 Chapter 5 The Relative Power of Primitive Synchronization Operations\nCase 1 Case 2\nri012 3\nbd\ni12\n0\n1\n2rij\nj3\nbb\ndd\ndi12\n0\n1\n2rij\nj3\nbaa\ndd\ndri012 3\nab d\nFigure 5.13 T wo possible views of (4,10)-assignment solving consensus for 4 threads. In Part\n1 only threads BandDshow up. Bis the \ufb01rst to assign and wins the consensus. In Part 2\nthere are three threads A,B, and D, and as before, Bwins by assigning \ufb01rst and Dassigns last.\nThe order among the threads can be determined by looking at the pairwise order among any\ntwo. Because the assignments are atomic, these individual orders are always consistent and\nde\ufb01ne the total order among the calls.\nAfter assigning to its \ufb01elds, a thread determines the relative ordering of the\nassignments for every two threads iandjas follows:\n\u0004Readrij. If the value is null, then neither assignment has occurred.\n\u0004Otherwise, read riandrj. Ifri\u2019s value is null, thenjprecedesi, and similarly\nforrj.\n\u0004If neitherrinorrjisnull, rereadrij. If its value is equal to the value read from\nri, thenjprecedesi, else vice versa.\nRepeating this procedure, a thread can determine which value was written by the\nearliest assignment. Two example orderings appear in Fig. 5.13. 2\nNote that multiple assignment solves consensus for any m>n> 1 threads\nwhile its dual structures and atomic snapshots, have consensus number at most\none. Although these two problems may appear similar, we have just shown that\nwriting atomically to multiple memory locations requires more computational\npower than reading atomically.\n5.6 Read\u2013Modify\u2013Write Operations\nMany, if not all, of the classical synchronization operations provided by multipro-\ncessors in hardware can be expressed as read\u2013modify\u2013write (RMW) operations,\nor, as they are called in their object form, read\u2013modify\u2013write registers . Consider\n5.6 Read\u2013Modify\u2013Write Operations 113\na RMW register that encapsulates integer values, and let Fbe a set of functions\nfrom integers to integers.3\nA method is an RMW for the function set Fif it atomically replaces the cur-\nrent register value vwithf(v), for somef2F, and returns the original value v.\n(SometimesFis a singleton set.) We (mostly) follow the Java convention that an\nRMW method that applies the function mumble is called getAndMumble() .\nFor example, the java.util.concurrent package provides an AtomicInteger\nclass with a rich set of RMW methods.\n\u0004The getAndSet (v) method atomically replaces the register\u2019s current value\nwithvand returns the prior value. This method (also called swap ()) is an\nRMW method for the set of constant functions of the type fv(x) =v.\n\u0004The getAndIncrement () method atomically", "doc_id": "358df029-e3d5-47f2-8a91-ecac427758d8", "embedding": null, "doc_hash": "5415f1d14d2378b54567b597527a92382502ef221164023738351526827072f3", "extra_info": null, "node_info": {"start": 318862, "end": 322001}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "54930f9c-2fe6-4491-9cba-afe2a9ee3a2d", "3": "cf19eb7b-b6ce-44ea-bd71-23475b0672da"}}, "__type__": "1"}, "cf19eb7b-b6ce-44ea-bd71-23475b0672da": {"__data__": {"text": "value v.\n(SometimesFis a singleton set.) We (mostly) follow the Java convention that an\nRMW method that applies the function mumble is called getAndMumble() .\nFor example, the java.util.concurrent package provides an AtomicInteger\nclass with a rich set of RMW methods.\n\u0004The getAndSet (v) method atomically replaces the register\u2019s current value\nwithvand returns the prior value. This method (also called swap ()) is an\nRMW method for the set of constant functions of the type fv(x) =v.\n\u0004The getAndIncrement () method atomically adds 1 to the register\u2019s cur-\nrent value and returns the prior value. This method (also called fetch-and-\nincrement ) is an RMW method for the function f(x) =x+ 1.\n\u0004ThegetAndAdd (k) method atomically adds kto the register\u2019s current value\nand returns the prior value. This method (also called fetch-and-add ) is an\nRMW method for the set of functions fk(x) =x+k.\n\u0004ThecompareAndSet() method takes two values, an expected valuee, and an\nupdate valueu. If the register value is equal to e, it is atomically replaced with u,\nand otherwise it is unchanged. Either way, the method returns a Boolean value\nindicating whether the value was changed. Informally, fe,u(x) =xifx6=eand\nuotherwise. Strictly speaking however, compareAndSet() is not an RMW\nmethod forfe,u, because an RMW method would return the register\u2019s prior\nvalue instead of a Boolean value, but this distinction is a technicality.\n\u0004The get() method returns the register\u2019s value. This method is an RMW\nmethod for the identity function f(v) =v.\nThe RMW methods are interesting precisely because they are potential hardware\nprimitives, engraved not in stone, but in silicon. Here, we de\ufb01ne RMW registers\nand their methods in terms of synchronized Java methods, but, pragmatically,\nthey correspond (exactly or nearly) to many real or proposed hardware synchro-\nnization primitives.\nAn RMW method is nontrivial if its set of functions includes at least one func-\ntion that is not the identity function.\nTheorem 5.6.1. Any nontrivial RMW register has consensus number at least 2.\nProof: Fig. 5.14 shows a 2-thread consensus protocol. Since there exists finF\nthat is not the identity, there exists a value vsuch thatf(v)6=v. In the decide ()\nmethod, as usual, the propose (v) method writes the thread\u2019s input vto the\n3For brevity, we consider only registers that hold integer values, but they could equally well hold\nreferences to other objects.\n114 Chapter 5 The Relative Power of Primitive Synchronization Operations\n1class RMWConsensus extends ConsensusProtocol {\n2 // initialize to v such that f(v) != v\n3 private RMWRegister r = new RMWRegister(v);\n4 public Object decide(Object value) {\n5 propose(value);\n6 int i = ThreadID.get(); // my index\n7 int j = 1-i; // other\u2019s index\n8 if(r.rmw() == v) // I\u2019m first, I win\n9 return proposed[i];\n10 else // I\u2019m second, I lose\n11 return proposed[j];\n12 }\n13 }\nFigure 5.14 2-thread consensus using RMW.\nproposed [] array. Then each thread applies the RMW method to a shared reg-\nister. If a thread\u2019s call returns v, it is linearized \ufb01rst, and it decides its own value.\nOtherwise, it is linearized second, and it decides the other thread\u2019s proposed\nvalue.", "doc_id": "cf19eb7b-b6ce-44ea-bd71-23475b0672da", "embedding": null, "doc_hash": "4a33410fccd337e386069e9a145abda204591d010757535ff38bc83491397b15", "extra_info": null, "node_info": {"start": 321872, "end": 325046}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "358df029-e3d5-47f2-8a91-ecac427758d8", "3": "e6409ea2-8171-4a68-b5bf-d8d027eee305"}}, "__type__": "1"}, "e6409ea2-8171-4a68-b5bf-d8d027eee305": {"__data__": {"text": "index\n8 if(r.rmw() == v) // I\u2019m first, I win\n9 return proposed[i];\n10 else // I\u2019m second, I lose\n11 return proposed[j];\n12 }\n13 }\nFigure 5.14 2-thread consensus using RMW.\nproposed [] array. Then each thread applies the RMW method to a shared reg-\nister. If a thread\u2019s call returns v, it is linearized \ufb01rst, and it decides its own value.\nOtherwise, it is linearized second, and it decides the other thread\u2019s proposed\nvalue. 2\nCorollary 5.6.1. It is impossible to construct a wait-free implementation of any\nnontrivial RMW method from atomic registers for two or more threads.\n5.7 Common2 RMW Operations\nWe now identify a class of RMW registers, called Common2 , that correspond to\nmany of the common synchronization primitives provided by processors in the\nlate Twentieth Century. Although Common2 registers, like all nontrivial RMW\nregisters, are more powerful than atomic registers, we will show that they have\nconsensus number exactly 2, implying that they have limited synchronization\npower. Fortunately, these synchronization primitives have by-and-large fallen\nfrom favor in contemporary processor architectures.\nDe\ufb01nition 5.7.1. A set of functions Fbelongs to Common2 if for all values vand\nallfiandfjinF, either:\n\u0004fiandfjcommute :fi(fj(v)) =fj(fi(v)), or\n\u0004one function overwrites the other:fi(fj(v)) =fi(v) orfj(fi(v)) =fj(v).\nDe\ufb01nition 5.7.2. A RMW register belongs to Common2 if its set of functions F\nbelongs to Common2 .\nFor example, many RMW registers in the literature provide only one nontrivial\nfunction. For example, getAndSet () uses a constant function, which overwrites\n5.7 Common2 RMW Operations 115\nany prior value. The getAndIncrement () and getAndAdd () methods use func-\ntions that commute with one another.\nVery informally, here is why RMW registers in Common2 cannot solve 3-\nthread consensus. The \ufb01rst thread (the winner ) can always tell it was \ufb01rst, and\neach of the second and third threads (the losers ) can each tell that they were losers.\nHowever, because the functions de\ufb01ning the state following operations in Com-\nmon2 commute or overwrite, a loser thread cannot tell which of the others went\n\ufb01rst (was the winner), and because the protocol is wait-free, it cannot wait to \ufb01nd\nout. Let us make this argument more precise.\nTheorem 5.7.1. Any RMW register in Common2 has consensus number\n(exactly) 2.\nProof: Theorem 5.6.1 states that any such register has consensus number at least\n2. We need only to show that any Common2 register cannot solve consensus for\nthree threads.\nAssume by way of contradiction that a 3-thread protocol exists using only\nCommon2 registers and read\u2013write registers. Suppose threads A,B, andCreach\nconsensus through Common2 registers. By Lemma 5.1.3, any such protocol has a\ncritical statesin which the protocol is bivalent, but any method call by any thread\nwill cause the protocol to enter a univalent state.\nWe now do a case analysis, examining each possible method call. The kind of\nreasoning used in the proof of Theorem 5.2.1 shows that the pending methods\ncannot be reads or writes, nor can the threads be about to call methods of differ-\nent objects. It follows that the threads are about to call RMW methods of a", "doc_id": "e6409ea2-8171-4a68-b5bf-d8d027eee305", "embedding": null, "doc_hash": "723f18e8a96179cff647d65b5579516f2c80f156e8c57bfb1f0daa029361679e", "extra_info": null, "node_info": {"start": 325142, "end": 328322}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "cf19eb7b-b6ce-44ea-bd71-23475b0672da", "3": "ad94849d-7e39-4dc1-a546-fce839c90333"}}, "__type__": "1"}, "ad94849d-7e39-4dc1-a546-fce839c90333": {"__data__": {"text": "andCreach\nconsensus through Common2 registers. By Lemma 5.1.3, any such protocol has a\ncritical statesin which the protocol is bivalent, but any method call by any thread\nwill cause the protocol to enter a univalent state.\nWe now do a case analysis, examining each possible method call. The kind of\nreasoning used in the proof of Theorem 5.2.1 shows that the pending methods\ncannot be reads or writes, nor can the threads be about to call methods of differ-\nent objects. It follows that the threads are about to call RMW methods of a single\nregisterr.\nSupposeAis about to call a method for function fA, sending the protocol to\na 0-valent state, and Bis about to call a method for fB, sending the protocol to a\n1-valent state. There are two possible cases:\n1.As depicted in Fig. 5.15, one function overwrites the other: fB(fA(v)) =\nfB(v). Lets0be the state that results if AappliesfAand thenBappliesfB.\nBecauses0is 0-valent,Cwill decide 0 if it runs alone until it \ufb01nishes the pro-\ntocol. Lets00be the state that results if Balone callsfB. Becauses00is 1-valent,\nCwill decide 1 if it runs alone until it \ufb01nishes the protocol. The problem is\nthat the two possible register states fB(fA(v)) andfB(v) are the same, so s0\nands00differ only in the internal states of AandB. If we now let thread C\nexecute, since Ccompletes the protocol without communicating with Aor\nB, these two states look identical to C, so it cannot possibly decide different\nvalues from the two states.\n2.The functions commute: fA(fB(v)) =fB(fA(v)). Lets0be the state that results\nifAappliesfAand thenBappliesfB. Becauses0is 0-valent,Cwill decide 0 if\nit runs alone until it \ufb01nishes the protocol. Let s00be the state that results if A\nandBperform their calls in the reverse order. Because s00is 1-valent,Cwill\ndecide 1 if it runs alone until it \ufb01nishes the protocol. The problem is that the\n116 Chapter 5 The Relative Power of Primitive Synchronization Operations\ns\ns\u00b4fA(v)\ns\u00b4\u00b4\n01\nC runs soloC runs solofB(v)\nfB(fA(v))\nFigure 5.15 Case: two functions that overwrite.\ntwo possible register states fA(fB(v)) andfB(fA(v)) are the same, so s0ands00\ndiffer only in the internal states of AandB. Now let thread Cexecute. Since C\ncompletes the protocol without communicating with AorB, these two states\nlook identical to C, so it cannot possibly decide different values from the two\nstates. 2\n5.8 The compareAndSet() Operation\nWe consider the compareAndSet() operation, a synchronization operation\nsupported by several contemporary architectures. (For example, it is called\nCMPXCHG on the Intel PentiumTM). This method is also known in the lit-\nerature as compare-and-swap . AcompareAndSet() takes two arguments: an\nexpected value and an update value. If the current register value is equal to the\nexpected value, then it is replaced by the update value; otherwise the value is\nleft unchanged. The method call returns a Boolean indicating whether the value\nchanged.\nTheorem 5.8.1. A register providing compareAndSet() andget() methods has\nan in\ufb01nite consensus number.\nProof: Fig. 5.16 shows a consensus protocol for nthreads 0,:::,n\u00001 using\ntheAtomicInteger class\u2019s", "doc_id": "ad94849d-7e39-4dc1-a546-fce839c90333", "embedding": null, "doc_hash": "be893f6672c95a94190bf87cc5d154d7ac01efffa566f381c57acc863cf41e7b", "extra_info": null, "node_info": {"start": 328234, "end": 331358}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e6409ea2-8171-4a68-b5bf-d8d027eee305", "3": "884903e2-f29f-46ee-9696-f0f4e37e4492"}}, "__type__": "1"}, "884903e2-f29f-46ee-9696-f0f4e37e4492": {"__data__": {"text": "compare-and-swap . AcompareAndSet() takes two arguments: an\nexpected value and an update value. If the current register value is equal to the\nexpected value, then it is replaced by the update value; otherwise the value is\nleft unchanged. The method call returns a Boolean indicating whether the value\nchanged.\nTheorem 5.8.1. A register providing compareAndSet() andget() methods has\nan in\ufb01nite consensus number.\nProof: Fig. 5.16 shows a consensus protocol for nthreads 0,:::,n\u00001 using\ntheAtomicInteger class\u2019s compareAndSet() method. The threads share an\nAtomicInteger object, initialized to a constant FIRST , distinct from any thread\nindex. Each thread calls compareAndSet() with FIRST as the expected value, and\n5.9 Chapter Notes 117\n1class CASConsensus extends ConsensusProtocol {\n2 private final int FIRST = -1;\n3 private AtomicInteger r = new AtomicInteger(FIRST);\n4 public Object decide(Object value) {\n5 propose(value);\n6 int i = ThreadID.get();\n7 if(r.compareAndSet(FIRST, i)) // I won\n8 return proposed[i];\n9 else // I lost\n10 return proposed[r.get()];\n11 }\n12 }\nFigure 5.16 Consensus using compareAndSwap() .\nits own index as the new value. If thread A\u2019s call returns true, then that method\ncall was \ufb01rst in the linearization order, so Adecides its own value. Otherwise, A\nreads the current AtomicInteger value, and takes that thread\u2019s input from the\nproposed [] array. 2\nWe note that having the compareAndSet() register in Theorem 5.8.1 provide\naget() method is only a convenience, and so it follows that:\nCorollary 5.8.1. A register providing only compareAndSet() has an in\ufb01nite\nconsensus number.\nAs we will see in Chapter 6, machines that provide primitive operations like\ncompareAndSet()4are asynchronous computation\u2019s equivalents of the Turing\nMachines of sequential computation: any concurrent object that can be imple-\nmented, can be implemented in a wait-free manner on such machines. Thus,\nin the words of Maurice Sendak, compareAndSet() is the \u201cking of all wild\nthings.\u201d\n5.9 Chapter Notes\nMichael Fischer, Nancy Lynch, and Michael Paterson [40] were the \ufb01rst to prove\nthat consensus is impossible in a message-passing system where a single thread\ncan halt. Their seminal paper introduced the \u201cbivalency\u201d style of impossibil-\nity argument widely used in the \ufb01eld of distributed computing. M. Loui and\nH. Abu-Amara [108] and Herlihy [62] were the \ufb01rst to extend this result to shared\nmemory.\n4Some architectures provide a pair of operations similar to get()/compareAndSet() called load-\nlinked/store-conditional . In general, the load-linked method marks a location as loaded, and the\nstore-conditional method fails if another thread modi\ufb01ed that location since it was loaded. See\nChapter 18 and Appendix B.\n118 Chapter 5 The Relative Power of Primitive Synchronization Operations\nClyde Kruskal, Larry Rudolph, and Marc Snir [133] coined the term read\u2013\nmodify\u2013write operation as part of the NYU Ultracomputer project.\nMaurice Herlihy [62] introduced the notion of a consensus number as a mea-\nsure of computational power, and was the \ufb01rst to prove most of the impossibility\nand universality results presented in this chapter and Chapter 6.\nThe class Common2 that includes several common primitive synchronization\noperations was", "doc_id": "884903e2-f29f-46ee-9696-f0f4e37e4492", "embedding": null, "doc_hash": "e15818f98f804f1a488a33dd361896d9d9027df3beb5e9501c94dd318bf48416", "extra_info": null, "node_info": {"start": 331363, "end": 334609}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ad94849d-7e39-4dc1-a546-fce839c90333", "3": "54793541-16f0-4a99-b831-2924af4a25b0"}}, "__type__": "1"}, "54793541-16f0-4a99-b831-2924af4a25b0": {"__data__": {"text": "B.\n118 Chapter 5 The Relative Power of Primitive Synchronization Operations\nClyde Kruskal, Larry Rudolph, and Marc Snir [133] coined the term read\u2013\nmodify\u2013write operation as part of the NYU Ultracomputer project.\nMaurice Herlihy [62] introduced the notion of a consensus number as a mea-\nsure of computational power, and was the \ufb01rst to prove most of the impossibility\nand universality results presented in this chapter and Chapter 6.\nThe class Common2 that includes several common primitive synchronization\noperations was de\ufb01ned by Y ehuda Afek and Eytan Weisberger and Hanan Weis-\nman [5]. The \u201csticky-bit\u201d object used in the exercises is due to Serge Plotkin\n[126].\nThen-bounded compareAndSet() object with arbitrary consensus number\nnin Exercise 5.10 is based on a construction by Prasad Jayanti and Sam T oueg\n[81]. In the hierarchy used here, we say that Xsolves consensus if one can con-\nstruct a wait-free consensus protocol from any number of instances of Xand any\namount of read\u2013write memory. Prasad Jayanti [79] observed that one could also\nde\ufb01ne resource-bounded hierarchies where one is restricted to using only a \ufb01xed\nnumber of instances of X, or a \ufb01xed amount of memory. The unbounded hier-\narchy used here seems to be the most natural one, since any other hierarchy is a\ncoarsening of the unbounded one.\nJayanti also raised the question whether the hierarchy is robust , that is, whether\nan objectXat levelmcan be \u201cboosted\u201d to a higher consensus level by combin-\ning it with another object Yat the same or lower level. Wai-Kau Lo and Vassos\nHadzilacos [106] and Eric Schenk [144] showed that the consensus hierarchy is\nnot robust: certain objects can be boosted. Informally, their constructions went\nlike this: let Xbe an object with the following curious properties. Xsolvesn-\nthread consensus but \u201crefuses\u201d to reveal the results unless the caller can prove\nhe or she can solve an intermediate task weaker than n-thread consensus, but\nstronger than any task solvable by atomic read/write registers. If Yis an object\nthat can be used to solve the intermediate task, Ycan boostXby convincing X\nto reveal the outcome of an n-thread consensus. The objects used in these proofs\nare nondeterministic.\nThe Maurice Sendak quote is from Where the Wild Things Are [140].\n5.10 Exercises\nExercise 47. Prove Lemma 5.1.2.\nExercise 48. Prove that every n-thread consensus protocol has a bivalent initial\nstate.\nExercise 49. Prove that in a critical state, one successor state must be 0-valent,\nand the other 1-valent.\nExercise 50. Show that if binary consensus using atomic registers is impossible for\ntwo threads, then it is also impossible for nthreads, where n>2. (Hint: argue by\n5.10 Exercises 119\nreduction : if we had a protocol to solve binary consensus for nthreads, then we\ncan transform it into a two-thread protocol.)\nExercise 51. Show that if binary consensus using atomic registers is impossible\nfornthreads, then so is consensus over kvalues, where k>2.\nExercise 52. Show that with suf\ufb01ciently many n-thread binary consensus objects\nand atomic registers one can implement n-thread consensus over nvalues.\nExercise 53. TheStack class provides two methods: push (x) pushes a value onto\nthe top of the stack, and pop() removes and returns the most recently pushed\nvalue. Prove that the Stack class has", "doc_id": "54793541-16f0-4a99-b831-2924af4a25b0", "embedding": null, "doc_hash": "dd208f66e8a1755b714eb869f22049ba61200a2b1f16a962f9db608be744255b", "extra_info": null, "node_info": {"start": 334602, "end": 337916}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "884903e2-f29f-46ee-9696-f0f4e37e4492", "3": "61e70219-c049-46c2-988e-61bc2808fd9c"}}, "__type__": "1"}, "61e70219-c049-46c2-988e-61bc2808fd9c": {"__data__": {"text": "then we\ncan transform it into a two-thread protocol.)\nExercise 51. Show that if binary consensus using atomic registers is impossible\nfornthreads, then so is consensus over kvalues, where k>2.\nExercise 52. Show that with suf\ufb01ciently many n-thread binary consensus objects\nand atomic registers one can implement n-thread consensus over nvalues.\nExercise 53. TheStack class provides two methods: push (x) pushes a value onto\nthe top of the stack, and pop() removes and returns the most recently pushed\nvalue. Prove that the Stack class has consensus number exactly two.\nExercise 54. Suppose we augment the FIFO Queue class with a peek () method\nthat returns but does not remove the \ufb01rst element in the queue. Show that the\naugmented queue has in\ufb01nite consensus number.\nExercise 55. Consider three threads, A,B, andC, each of which has a MRSW\nregister,XA,XB, andXC, that it alone can write and the others can read.\nIn addition, each pair shares a RMWRegister register that provides only a\ncompareAndSet() method:AandBshareRAB,BandCshareRBC, andA\nandCshareRAC. Only the threads that share a register can call that register\u2019s\ncompareAndSet() method or read its value.\nY our mission: either give a consensus protocol and explain why it works, or\nsketch an impossibility proof.\nExercise 56. Consider the situation described in Exercise 5.55, except that A,B,\nandCcan apply a double compareAndSet() to both registers at once.\nExercise 57. In the consensus protocol shown in 5.7, what would happen if we\nannounced the thread\u2019s value after dequeuing from the queue?\nExercise 58. Objects of the StickyBit class have three possible states ?, 0, 1, ini-\ntially?. A call to write (v), wherevis 0 or 1, has the following effects:\n\u0004If the object\u2019s state is ?, then it becomes v.\n\u0004If the object\u2019s state is 0 or 1, then it is unchanged.\nA call to read () returns the object\u2019s current state.\n1.Show that such an object can solve wait-free binary consensus (that is, all\ninputs are 0 or 1) for any number of threads.\n2.Show that an array of log2mStickyBit objects with atomic registers can\nsolve wait-free consensus for any number of threads when there are mpossi-\nble inputs. (Hint: you need to give each thread one single-writer, multi-reader\natomic register.)\nExercise 59. TheSetAgree class, like the Consensus class, provides a decide ()\nmethod whose call returns a value that was the input of some thread\u2019s decide ()\n120 Chapter 5 The Relative Power of Primitive Synchronization Operations\ncall. However, unlike the Consensus class, the values returned by decide () calls\nare not required to agree. Instead, these calls may return no more than kdistinct\nvalues. (When kis 1,SetAgree is the same as consensus.) What is the consensus\nnumber of the SetAgree class whenk>1?\nExercise 60. The two-thread approximate agreement class for a given \u000fis de\ufb01ned\nas follows. Given two threads AandB, each can call decide (xa) and decide (xb)\nmethods, where xaandxbare real numbers. These methods return values\nyaandybrespectively, such that yaandybboth lie in the closed interval\n[min(xa,xb), max(xa,xb)], andjya\u0000ybj6\u000ffor\u000f>0. Note that this object is\nnondeterministic.\nWhat is the consensus number of the approximate agreement object?\nExercise 61. Consider a distributed system where threads communicate by\nmessage-passing. A type A broadcast", "doc_id": "61e70219-c049-46c2-988e-61bc2808fd9c", "embedding": null, "doc_hash": "3997b9208c36a0ea90017aa56d926c186696b2dcd4433b34d2f392125f72e6da", "extra_info": null, "node_info": {"start": 337907, "end": 341220}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "54793541-16f0-4a99-b831-2924af4a25b0", "3": "77f35ad4-56f4-432f-9bc7-a50bfc4a59bf"}}, "__type__": "1"}, "77f35ad4-56f4-432f-9bc7-a50bfc4a59bf": {"__data__": {"text": "follows. Given two threads AandB, each can call decide (xa) and decide (xb)\nmethods, where xaandxbare real numbers. These methods return values\nyaandybrespectively, such that yaandybboth lie in the closed interval\n[min(xa,xb), max(xa,xb)], andjya\u0000ybj6\u000ffor\u000f>0. Note that this object is\nnondeterministic.\nWhat is the consensus number of the approximate agreement object?\nExercise 61. Consider a distributed system where threads communicate by\nmessage-passing. A type A broadcast guarantees:\n1.every nonfaulty thread eventually gets each message,\n2.ifPbroadcastsM1thenM2, then every thread receives M1beforeM2, but\n3.messages broadcast by different threads may be received in different orders at\ndifferent threads.\nAtype B broadcast guarantees:\n1.every nonfaulty thread eventually gets each message,\n2.ifPbroadcastsM1andQbroadcastsM2, then every thread receives M1and\nM2in the same order.\nFor each kind of broadcast,\n\u0004give a consensus protocol if possible,\n\u0004and otherwise sketch an impossibility proof.\nExercise 62. Consider the following 2-thread QuasiConsensus problem:\nTwo threads, AandB, are each given a binary input. If both have input v, then\nboth must decide v. If they have mixed inputs, then either they must agree, or B\nmay decide 0 and Amay decide 1 (but not vice versa).\nHere are three possible exercises (only one of which works). (1) Give a 2-\nthread consensus protocol using QuasiConsensus showing it has consensus\nnumber 2, or (2) give a critical-state proof that this object\u2019s consensus number is\n1, or (3) give a read\u2013write implementation of QuasiConsensus , thereby showing\nit has consensus number 1.\nExercise 63. Explain why the critical-state proof of the impossibility of consensus\nfails if the shared object is, in fact, a Consensus object.\nExercise 64. In this chapter we showed that there is a bivalent initial state for 2-\nthread consensus. Give a proof that there is a bivalent initial state for nthread\nconsensus.\n5.10 Exercises 121\nExercise 65. Ateam consensus object provides the same decide () method as con-\nsensus. A team consensus object solves consensus as long as no more than two\ndistinct values are ever proposed. (If more than two are proposed, the results are\nunde\ufb01ned.)\nShow how to solve n-thread consensus, with up to ndistinct input values, from\na supply of team consensus objects.\nExercise 66. Atrinary register holds values ?, 0, 1, and provides compareAndSet()\nandget() methods with the usual meaning. Each such register is initially ?. Give\na protocol that uses one such register to solve n-thread consensus if the inputs of\nthe threads are binary , that is, either 0 or 1.\nCan you use multiple such registers (perhaps with atomic read\u2013write regis-\nters) to solve n-thread consensus even if the threads\u2019 inputs are in the range\n0:::2K\u00001, forK > 1. (Y ou may assume an input \ufb01ts in an atomic register.)\nImportant: remember that a consensus protocol must be wait-free.\n\u0004Devise a solution that uses at most O(n) trinary registers.\n\u0004Devise a solution that uses O(K) trinary registers.\nFeel free to use all the atomic registers you want (they are cheap).\nExercise 67. Earlier we de\ufb01ned lock-freedom. Prove that there is no lock-free\nimplementation of consensus using read\u2013write registers for two or more threads.\nExercise 68. Fig. 5.17 shows a FIFO queue implemented with read, write,\ngetAndSet ()", "doc_id": "77f35ad4-56f4-432f-9bc7-a50bfc4a59bf", "embedding": null, "doc_hash": "aacd1e25ee9704277a0af13f6841bb3583fae734e38bc46130b43658899ca3ae", "extra_info": null, "node_info": {"start": 341263, "end": 344600}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "61e70219-c049-46c2-988e-61bc2808fd9c", "3": "0ed60197-76aa-483a-bb67-ab9461d80071"}}, "__type__": "1"}, "0ed60197-76aa-483a-bb67-ab9461d80071": {"__data__": {"text": "an atomic register.)\nImportant: remember that a consensus protocol must be wait-free.\n\u0004Devise a solution that uses at most O(n) trinary registers.\n\u0004Devise a solution that uses O(K) trinary registers.\nFeel free to use all the atomic registers you want (they are cheap).\nExercise 67. Earlier we de\ufb01ned lock-freedom. Prove that there is no lock-free\nimplementation of consensus using read\u2013write registers for two or more threads.\nExercise 68. Fig. 5.17 shows a FIFO queue implemented with read, write,\ngetAndSet () (that is, swap) and getAndIncrement () methods. Y ou may assume\n1class Queue {\n2 AtomicInteger head = new AtomicInteger(0);\n3 AtomicReference items[] =\n4 new AtomicReference[Integer.MAX_VALUE];\n5 void enq(Object x){\n6 int slot = head.getAndIncrement();\n7 items[slot] = x;\n8 }\n9 Object deq() {\n10 while (true ) {\n11 int limit = head.get();\n12 for (int i = 0; i < limit; i++) {\n13 Object y = items[i].getAndSet(); // swap\n14 if(y != null )\n15 return y;\n16 }\n17 }\n18 }\n19 }\nFigure 5.17 Queue implementation.\n122 Chapter 5 The Relative Power of Primitive Synchronization Operations\nthis queue is linearizable, and wait-free as long as deq() is never applied to an\nempty queue. Consider the following sequence of statements.\n\u0004Both getAndSet () and getAndIncrement () methods have consensus num-\nber 2.\n\u0004We can add a peek () simply by taking a snapshot of the queue (using the\nmethods studied earlier in the course) and returning the item at the head of\nthe queue.\n\u0004UsingtheprotocoldevisedforExercise54,wecanusetheresultingqueueto\nsolven-consensus for any n.\nWe have just constructed an n-thread consensus protocol using only objects with\nconsensus number 2. Identify the faulty step in this chain of reasoning, and\nexplain what went wrong.\nExercise 69. Recall that in our de\ufb01nition of compareAndSet() we noted that\nstrictly speaking, compareAndSet() is not a RMW method for fe,u, because a\nRMW method would return the register\u2019s prior value instead of a Boolean value.\nUse an object that supports compareAndSet() andget() to provide a new object\nwith a linearizable NewCompareAndSet() method that returns the register\u2019s cur-\nrent value instead of a Boolean.\nExercise 70. De\ufb01ne ann-bounded compareAndSet() object as follows. It pro-\nvides a compareAndSet() method that takes two values, an expected valuee, and\nanupdate valueu. For the \ufb01rst ntimes compareAndSet() is called, it behaves\nlike a conventional compareAndSet() register: if the register value is equal to\ne, it is atomically replaced with u, and otherwise it is unchanged, and returns a\nBoolean value indicating whether the change occurred. After compareAndSet()\nhas been called ntimes, however, the object enters a faulty state, and all subse-\nquent method calls return ?.\nShow that an n-bounded compareAndSet() object has consensus number\nexactlyn.\nExercise 71. Provide a wait-free implementation of a two-thread three-location\nAssign23 multiple assignment object from three compareAndSet() objects (that\nis, objects supporting the operations compareAndSet() andget()).\nExercise 72. In the proof of Theorem 5.5.1 , we claimed that it is enough to show\nthat we can solve 2-consensus given two threads and an (2, 3)-assignment object.\nJustify this claim.\nExercise 73. Prove Corollary", "doc_id": "0ed60197-76aa-483a-bb67-ab9461d80071", "embedding": null, "doc_hash": "005c3cb636f60817b04490bc925739c42f52c7886ebfb277f648aad08d911412", "extra_info": null, "node_info": {"start": 344580, "end": 347830}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "77f35ad4-56f4-432f-9bc7-a50bfc4a59bf", "3": "bcba8925-4287-486d-addb-7fc0dc7b084e"}}, "__type__": "1"}, "bcba8925-4287-486d-addb-7fc0dc7b084e": {"__data__": {"text": "return ?.\nShow that an n-bounded compareAndSet() object has consensus number\nexactlyn.\nExercise 71. Provide a wait-free implementation of a two-thread three-location\nAssign23 multiple assignment object from three compareAndSet() objects (that\nis, objects supporting the operations compareAndSet() andget()).\nExercise 72. In the proof of Theorem 5.5.1 , we claimed that it is enough to show\nthat we can solve 2-consensus given two threads and an (2, 3)-assignment object.\nJustify this claim.\nExercise 73. Prove Corollary 5.8.1 .\nExercise 74. We can treat the scheduler as an adversary who uses the knowledge\nof our protocols and input values to frustrate our attempts at reaching consensus.\nOne way to outwit an adversary is through randomization. Assume there are two\nthreads that want to reach consensus, each can \ufb02ip an unbiased coin, and the\nadversary cannot control future coin \ufb02ips.\n5.10 Exercises 123\n1Object prefer[2] = { null ,null };\n2\n3Object decide(Object input) {\n4 int i = Thread.getID();\n5 int j = 1-i;\n6 prefer[i] = input;\n7 while (true ) {\n8 if(prefer[j] == null ) {\n9 return prefer[i];\n10 }else if (prefer[i] == prefer[j]) {\n11 return prefer[i];\n12 }else {\n13 if(flip()) {\n14 prefer[i] = prefer[j];\n15 }\n16 }\n17 }\n18 }\nFigure 5.18 Is this a randomized consensus protocol?\nAssume the adversary scheduler can observe the result of each coin \ufb02ip and\neach value read or written. It can stop a thread before or after a coin \ufb02ip or a read\nor write to a shared register.\nArandomized consensus protocol terminates with probability one against an\nadversary scheduler. Fig. 5.18 shows a plausible-looking randomized consensus\nprotocol. Give an example showing that this protocol is incorrect.\nExercise 75. One can implement a consensus object using read\u2013write registers\nby implementing a deadlock- or starvation-free mutual exclusion lock. However,\nthis implementation provides only dependent progress, and the operating system\nmust make sure that threads do not get stuck in the critical section so that the\ncomputation as a whole progresses.\n\u0004Is the same true for obstruction-freedom, the nonblocking dependent progress\ncondition? Show an obstruction-free implementation of a consensus object\nusing only atomic registers.\n\u0004What is the role of the operating system in the obstruction-free solution to\nconsensus? Explain where the critical-state-based proof of the impossibility of\nconsensus breaks down if we repeatedly allow an oracle to halt threads so as to\nallow others to make progress.\n(Hint, think of how you could restrict the set of allowed executions.)\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n6Universality of Consensus\n6.1 Introduction\nIn Chapter 5, we considered a simple technique for proving statements of the\nform \u201cthere is no wait-free implementation of XbyY:\u201d We considered object\nclasses with deterministic sequential speci\ufb01cations.1We derived a hierarchy in\nwhich no object from one level can implement an object at a higher level (see\nFig. 6.1). Recall that each object has an associated consensus number , which is the\nmaximum number of threads for which the object can solve the consensus prob-\nlem. In a system of nor more concurrent threads, it is impossible to construct a\nwait-free implementation of an object with consensus number nfrom an object\nwith a lower consensus", "doc_id": "bcba8925-4287-486d-addb-7fc0dc7b084e", "embedding": null, "doc_hash": "b06ce7efdaafef020239bd71cdb99de0159d5278690a6184cc676df372649d45", "extra_info": null, "node_info": {"start": 347817, "end": 351183}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0ed60197-76aa-483a-bb67-ab9461d80071", "3": "89b40abf-2d85-444f-a58c-81b9a7260493"}}, "__type__": "1"}, "89b40abf-2d85-444f-a58c-81b9a7260493": {"__data__": {"text": "\u201cthere is no wait-free implementation of XbyY:\u201d We considered object\nclasses with deterministic sequential speci\ufb01cations.1We derived a hierarchy in\nwhich no object from one level can implement an object at a higher level (see\nFig. 6.1). Recall that each object has an associated consensus number , which is the\nmaximum number of threads for which the object can solve the consensus prob-\nlem. In a system of nor more concurrent threads, it is impossible to construct a\nwait-free implementation of an object with consensus number nfrom an object\nwith a lower consensus number. The same result holds for lock-free implementa-\ntions, and henceforth unless we explicitly state otherwise, it will be implied that\na result that holds for wait-free implementations holds for lock-free ones.\nThe impossibility results of Chapter 5 do not by any means imply that wait-\nfree synchronization is impossible or infeasible. In this chapter, we show that there\nexist classes of objects that are universal : given suf\ufb01ciently many of them, one can\nconstruct a wait-free linearizable implementation of anyconcurrent object.\nA class is universal in a system of nthreads if, and only if it has a consensus\nnumber greater than or equal to n. In Fig. 6.1, each class at level nis universal for a\nsystem ofnthreads. A machine architecture or programming language is compu-\ntationally powerful enough to support arbitrary wait-free synchronization if, and\nonly if it provides objects of a universal class as primitives. For example, modern\nmultiprocessor machines that provide a compareAndSet() operation are uni-\nversal for any number of threads: they can implement any concurrent object in a\nwait-free manner.\nThis chapter describes how to use consensus objects to build a universal con-\nstruction that implements any concurrent object. The chapter does notdescribe\n1The situation with nondeterministic objects is signi\ufb01cantly more complicated.\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00006-X\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.125\n126 Chapter 6 Universality of Consensus\nConsensus\nNumber Object\n1 atomic registers\n2 getAndSet (),getAndAdd (),Queue ,Stack\n......\nm (m,m(m+ 1)=2)-register assignment\n......\n1 memory-to-memory move ,compareAndSet() ,Load-Linked/StoreConditional2\nFigure 6.1 Concurrent computability and the universality hierarchy of synchronization\noperations.\npractical techniques for implementing wait-free objects. Like classical com-\nputability theory, understanding the universal construction and its implications\nwill allow us to avoid the na\u00a8 ve mistake of trying to solve unsolvable problems.\nOnce we understand why consensus is powerful enough to implement any kind\nof object, we will be better prepared to undertake the engineering effort needed\nto make such constructions ef\ufb01cient.\n6.2 Universality\nA classCisuniversal if one can construct a wait-free implementation of any\nobject from some number of objects of Cand some number of read\u2013write regis-\nters. Our construction uses multiple objects of class Cbecause we are ultimately\ninterested in understanding the synchronization power of machine instructions,\nand most machines allow their instructions to be applied to multiple mem-\nory locations. We allow an implementation to use multiple read\u2013write registers\nbecause it is convenient for bookkeeping, and memory is usually in plentiful sup-\nply on modern architectures. T o avoid distraction, we use an unlimited number\nof read\u2013write registers and consensus objects, leaving the question of recycling\nmemory as an exercise. We begin by presenting a lock-free implementation, later\nextending it to a slightly more complex wait-free one.\n6.3 A Lock-Free Universal", "doc_id": "89b40abf-2d85-444f-a58c-81b9a7260493", "embedding": null, "doc_hash": "b15c8e02b32f3d65d025cffc22847285aaaf0d38e2da8b7a936bd45746377038", "extra_info": null, "node_info": {"start": 351155, "end": 354875}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "bcba8925-4287-486d-addb-7fc0dc7b084e", "3": "1cfcc608-1e8e-4aa8-a300-93d781fd7bde"}}, "__type__": "1"}, "1cfcc608-1e8e-4aa8-a300-93d781fd7bde": {"__data__": {"text": "power of machine instructions,\nand most machines allow their instructions to be applied to multiple mem-\nory locations. We allow an implementation to use multiple read\u2013write registers\nbecause it is convenient for bookkeeping, and memory is usually in plentiful sup-\nply on modern architectures. T o avoid distraction, we use an unlimited number\nof read\u2013write registers and consensus objects, leaving the question of recycling\nmemory as an exercise. We begin by presenting a lock-free implementation, later\nextending it to a slightly more complex wait-free one.\n6.3 A Lock-Free Universal Construction\nFig. 6.2 shows a generic de\ufb01nition for a sequential object, based on the invocation\u2013\nresponse formulation of Chapter 3. Each object is created in a \ufb01xed initial\n2See Appendix B for details.\n6.3 A Lock-Free Universal Construction 127\n1 public interface SeqObject {\n2 public abstract Response apply(Invoc invoc);\n3}\nFigure 6.2 A Generic sequential object: the apply ()method applies the invocation and\nreturns a response.\n1public class Node {\n2 public Invoc invoc; // method name and args\n3 public Consensus<Node> decideNext; // decide next Node in list\n4 public Node next; // the next node\n5 public int seq; // sequence number\n6 public Node(Invoc invoc) {\n7 invoc = invoc;\n8 decideNext = new Consensus<Node>()\n9 seq = 0;\n10 }\n11 public static Node max(Node[] array) {\n12 Node max = array[0];\n13 for (int i = 1; i < array.length; i++)\n14 if(max.seq < array[i].seq)\n15 max = array[i];\n16 return max;\n17 }\n18 }\nFigure 6.3 TheNode class.\nstate. The apply () method takes as argument an invocation which describes the\nmethod being called and its arguments, and returns a response , containing the\ncall\u2019s termination condition (normal or exceptional) and the return value, if any.\nFor example, a stack invocation might be push () with an argument, and the cor-\nresponding response would be normal and void.\nFigs. 6.3 and 6.4 show a universal construction that transforms any sequential\nobject into a lock-free linearizable concurrent object. This construction assumes\nthat sequential objects are deterministic : if we apply a method to an object in a\nparticular state, then there is only one possible response, and one possible new\nobject state. We can represent any object as a combination of a sequential object in\nits initial state and a log: a linked list of nodes representing the sequence of method\ncalls applied to the object (and hence the object\u2019s sequence of state transitions).\nA thread executes a method call by adding the new call to the head of the list. It\nthen traverses the list, from tail to head, applying the method calls to a private copy\nof the object. The thread \ufb01nally returns the result of applying its own operation.\nIt is important to understand that only the head of the log is mutable: the initial\nstate and nodes preceding the head never change.\nHow do we make this log-based construction concurrent, that is, allow threads\nto make concurrent calls to apply ()? A thread attempting to call apply () cre-\nates a node to hold its invocation. The threads then compete to append their\n128 Chapter 6 Universality of Consensus\n1public class LFUniversal {\n2 private Node[] head;\n3 private Node tail;\n4 public Universal() {\n5 tail = new Node();\n6 tail.seq = 1;\n7 for (int i = 0; i < n; i++)\n8 head[i] = tail\n9 }\n10 public Response apply(Invoc invoc) {\n11 int i = ThreadID.get();\n12 Node prefer = new Node(invoc);\n13 while (prefer.seq == 0) {\n14 Node before = Node.max(head);\n15 Node after =", "doc_id": "1cfcc608-1e8e-4aa8-a300-93d781fd7bde", "embedding": null, "doc_hash": "62c50f8d96cd8cbda21db4a29e1ccaf4fa147ac42877190ad74a9bd9f205791a", "extra_info": null, "node_info": {"start": 354853, "end": 358358}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "89b40abf-2d85-444f-a58c-81b9a7260493", "3": "668c827f-783f-41ab-a6d7-2aecba18a25d"}}, "__type__": "1"}, "668c827f-783f-41ab-a6d7-2aecba18a25d": {"__data__": {"text": "The threads then compete to append their\n128 Chapter 6 Universality of Consensus\n1public class LFUniversal {\n2 private Node[] head;\n3 private Node tail;\n4 public Universal() {\n5 tail = new Node();\n6 tail.seq = 1;\n7 for (int i = 0; i < n; i++)\n8 head[i] = tail\n9 }\n10 public Response apply(Invoc invoc) {\n11 int i = ThreadID.get();\n12 Node prefer = new Node(invoc);\n13 while (prefer.seq == 0) {\n14 Node before = Node.max(head);\n15 Node after = before.decideNext.decide(prefer);\n16 before.next = after;\n17 after.seq = before.seq + 1;\n18 head[i] = after;\n19 }\n20 SeqObject myObject = new SeqObject();\n21 Node current = tail.next;\n22 while (current != prefer){\n23 myObject.apply(current.invoc);\n24 current = current.next;\n25 }\n26 return myObject.apply(current.invoc);\n27 }\n28 }\nFigure 6.4 The lock-free universal algorithm.\nrespective nodes to the head of the log by running an n-thread consensus proto-\ncol to agree which node was appended to the log. The inputs to this consensus\nare references to the threads\u2019 nodes, and the result is the unique winning node.\nThe winner can then proceed to compute its response. It does so by creating\na local copy of the sequential object and traversing the log, following next ref-\nerences from tail to head, applying the operations in the log to its copy, \ufb01nally\nreturning the response associated with its own invocation. This algorithm works\neven when apply () calls are concurrent, because the pre\ufb01x of the log up to the\nthread\u2019s own node never changes. The losing threads, which were not chosen by\nthe consensus object, must try again to set the node currently at the head of the\nlog (which changes between attempts) to point to them.\nWe now consider this construction in detail. The code for the lock-free univer-\nsal construction appears in Fig. 6.4. A sample execution appears in Fig. 6.5. The\nobject state is de\ufb01ned by a linked list of nodes, each one containing an invocation.\nThe code for a node appears in Fig. 6.3. The node\u2019s decideNext \ufb01eld is a consen-\nsus object used to decide which node is appended next in the list, and next is the\n\ufb01eld in which the outcome of that consensus, the reference to the next node, is\nrecorded. The seq\ufb01eld is the node\u2019s sequence number in the list. This \ufb01eld is zero\nwhile the node is not yet threaded onto the list, and positive otherwise. Sequence\n6.3 A Lock-Free Universal Construction 129\nTail\ninvoc()\ndecideNextnextsentinel\nseq1 2 3Head\n40 2 n\u20131 ... 5 ... 7 ... ...\nFigure 6.5 Execution of the lock-free universal construction. Thread 2 appends the second\nnode in the log winning consensus on decideNext in the sentinel node. It then sets the node\u2019s\nsequence number from 0 to 2, and refers to it from its entry in the head []array. Thread 7\nloses the decideNext consensus at the sentinel node, sets the next reference and sequence\nnumber of the decided successor node to 2 (they were already set to the same values by\nthread 2), and refers to the node from its entry in the head []array. Thread 5 appends the\nthird node, updates its sequence number to 3 and updates its entry in the head []array to this\nnode. Finally, thread 2 appends the fourth node, sets its sequence number to 4, and refers to\nit from its entry in the head []array. The maximal value in the head array keeps track of the\nhead of the log.\nnumbers for successive nodes in the list", "doc_id": "668c827f-783f-41ab-a6d7-2aecba18a25d", "embedding": null, "doc_hash": "dbc2504b14d7e69d2e678329a2ab569c4b3d57eeb26245772a14df8c0e94934c", "extra_info": null, "node_info": {"start": 358492, "end": 361827}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1cfcc608-1e8e-4aa8-a300-93d781fd7bde", "3": "d7f8cc98-ee5a-4c4c-8a5c-00616c06b870"}}, "__type__": "1"}, "d7f8cc98-ee5a-4c4c-8a5c-00616c06b870": {"__data__": {"text": "consensus at the sentinel node, sets the next reference and sequence\nnumber of the decided successor node to 2 (they were already set to the same values by\nthread 2), and refers to the node from its entry in the head []array. Thread 5 appends the\nthird node, updates its sequence number to 3 and updates its entry in the head []array to this\nnode. Finally, thread 2 appends the fourth node, sets its sequence number to 4, and refers to\nit from its entry in the head []array. The maximal value in the head array keeps track of the\nhead of the log.\nnumbers for successive nodes in the list increase by one. Initially, the log consists\nof a unique sentinel node with sequence number 1.\nThe hard part about designing the concurrent lock-free universal construction\nis that consensus objects can be used only once.3\nIn our lock-free algorithm in Fig. 6.4, each thread allocates a node holding\nits invocation, and repeatedly tries to append that node to the head of the log.\nEach node has a decideNext \ufb01eld, which is a consensus object. A thread tries\nto append its node by proposing as input to a consensus protocol on the head\u2019s\ndecideNext \ufb01eld. Because threads that do not participate in this consensus may\nneed to traverse the list, the result of this consensus is stored in the node\u2019s next\n\ufb01eld. Multiple threads may update this \ufb01eld simultaneously, but they all write\nthe same value. When the thread\u2019s node is appended, it sets the node\u2019s sequence\nnumber.\nOnce a thread\u2019s node is part of the log, it computes the response to its invocation\nby traversing the log from the tail to the newly added node. It applies each of the\ninvocations to a private copy of the object, and returns the response from its own\n3Creating a reusable consensus object, or even one whose decision is readable, is not a simple\ntask. It is essentially the same problem as the universal construction we are about to design. For\nexample, consider the queue-based consensus protocol in Chapter 5. It is not obvious how to use\naQueue to allow repeated reading of the consensus object state after it is decided.\n130 Chapter 6 Universality of Consensus\ninvocation. Notice that when a thread computes its response, all its predecessors\u2019\nnext references must already be set, because these nodes have already have been\nadded to the head of the list. Any thread that added a node to the list has updated\nitsnext reference with the result of the decideNext consensus.\nHow do we locate the head of the log? We cannot track the head with a consen-\nsus object because the head must be updated repeatedly, and consensus objects\ncan only be accessed once by each thread. Instead, we create a per-thread struc-\nture of the kind used in the Bakery algorithm of Chapter 2. We use an n-entry\narray head [], where head [i] is the last node in the list that thread ihas observed.\nInitially all entries refer to the tail sentinel node. The head is the node with the\nmaximum sequence number among the nodes referenced in the head [] array.\nThemax() method in Fig. 6.3 performs a collect, reading the head [] entries and\nreturning the node with the highest sequence number.\nThe construction is a linearizable implementation of the sequential object.\nEach apply () call can be linearized at the point of the consensus call adding the\nnode to the log.\nWhy is this construction lock-free? The head of the log, the latest node appen-\nded, is added to the head [] array within a \ufb01nite number of steps. The node\u2019s pre-\ndecessor must appear in the head array, so any node repeatedly attempting to add\na new node will repeatedly run the max() function on the head array. It detects\nthis predecessor, applies consensus on its decideNext \ufb01eld, and then updates", "doc_id": "d7f8cc98-ee5a-4c4c-8a5c-00616c06b870", "embedding": null, "doc_hash": "806c09db8c07fddc14933851454d9ac2c326b6821100fddf2f07eb4ccc0d5b72", "extra_info": null, "node_info": {"start": 361717, "end": 365416}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "668c827f-783f-41ab-a6d7-2aecba18a25d", "3": "cb216f3d-d657-4f40-8dfd-e34ab75d0140"}}, "__type__": "1"}, "cb216f3d-d657-4f40-8dfd-e34ab75d0140": {"__data__": {"text": "the sequential object.\nEach apply () call can be linearized at the point of the consensus call adding the\nnode to the log.\nWhy is this construction lock-free? The head of the log, the latest node appen-\nded, is added to the head [] array within a \ufb01nite number of steps. The node\u2019s pre-\ndecessor must appear in the head array, so any node repeatedly attempting to add\na new node will repeatedly run the max() function on the head array. It detects\nthis predecessor, applies consensus on its decideNext \ufb01eld, and then updates the\nwinning node\u2019s \ufb01elds, and including its sequence number. Finally, it stores the\ndecided node in that thread\u2019s head array entry. The new head node always even-\ntually appears in head []. It follows that the only way a thread can repeatedly fail\nto add its own node to the log is if other threads repeatedly succeed in appending\ntheir own nodes to the log. Thus, a node can starve only if other nodes are con-\ntinually completing their invocations, implying that the construction is lock-free.\n6.4 A Wait-Free Universal Construction\nHow do we make a lock-free algorithm wait-free? The full wait-free algorithm\nappears in Fig. 6.6. We must guarantee that every thread completes an apply ()\ncall within a \ufb01nite number of steps, that is, no thread starves. T o guarantee this\nproperty, threads making progress must help less fortunate threads to complete\ntheir calls. This helping pattern will show up later in a specialized form in other\nwait-free algorithms.\nT o allow helping, each thread must share with other threads the apply ()\ncall that it is trying to complete. We add an n-element announce [] array,\nwhere announce [i] is the node thread iis currently trying to append to the\nlist. Initially, all entries refer to the sentinel node, which has a sequence num-\nber 1. A thread iannounces a node when it stores the node in announce [i].\nT o execute apply (), a thread \ufb01rst announces its new node. This step ensures\nthat if the thread itself does not succeed in appending its node onto the list,\n6.4 A Wait-Free Universal Construction 131\n1public class Universal {\n2 private Node[] announce; // array added to coordinate helping\n3 private Node[] head;\n4 private Node tail = new node(); tail.seq = 1;\n5 for (int j=0; j < n; j++){head[j] = tail; announce[j] = tail};\n6 public Response apply(Invoc invoc) {\n7 int i = ThreadID.get();\n8 announce[i] = new Node(invoc);\n9 head[i] = Node.max(head);\n10 while (announce[i].seq == 0) {\n11 Node before = head[i];\n12 Node help = announce[(before.seq + 1) % n];\n13 if(help.seq == 0)\n14 prefer = help;\n15 else\n16 prefer = announce[i];\n17 Node after = before.decideNext.decide(prefer);\n18 before.next = after;\n19 after.seq = before.seq + 1;\n20 head[i] = after;\n21 }\n22 SeqObject MyObject = new SeqObject();\n23 Node current = tail.next;\n24 while (current != announce[i]){\n25 MyObject.apply(current.invoc);\n26 current = current.next;\n27 }\n28 head[i] = announce[i];\n29 return MyObject.apply(current.invoc);\n30 }\n31 }\nFigure 6.6 The wait-free universal algorithm.\nsome other thread will append that node on its behalf. It then proceeds as before,\nattempting to append the node into the log. T o do so, it reads the head [] array\nonly once (Line 9), and then enters the main", "doc_id": "cb216f3d-d657-4f40-8dfd-e34ab75d0140", "embedding": null, "doc_hash": "af456ffa3d290f1c9d58e70b528fbd6b037ece6097ab3b655f5e9e2fd9cf2011", "extra_info": null, "node_info": {"start": 365464, "end": 368696}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d7f8cc98-ee5a-4c4c-8a5c-00616c06b870", "3": "4f14005d-bd5d-44a3-adcb-558597007bf9"}}, "__type__": "1"}, "4f14005d-bd5d-44a3-adcb-558597007bf9": {"__data__": {"text": "Node current = tail.next;\n24 while (current != announce[i]){\n25 MyObject.apply(current.invoc);\n26 current = current.next;\n27 }\n28 head[i] = announce[i];\n29 return MyObject.apply(current.invoc);\n30 }\n31 }\nFigure 6.6 The wait-free universal algorithm.\nsome other thread will append that node on its behalf. It then proceeds as before,\nattempting to append the node into the log. T o do so, it reads the head [] array\nonly once (Line 9), and then enters the main loop of the algorithm, which it\nexecutes until its own node has been threaded onto the list (detected when its\nsequence number becomes non zero in Line 10). Here is a change from the lock-\nfree algorithm. A thread \ufb01rst checks to see if there is a node that needs help ahead\nof it in the announce [] array (Line 12). The node to be helped must be deter-\nmined dynamically because nodes are continually added to the log. A thread\nattempts to help nodes in the announce [] array in increasing order, determined\nby the sequence number modulo the width nof the announce [] array. We will\nprove that this approach guarantees that any node that does not make progress\non its own will eventually be helped by others once its owner thread\u2019s index\nmatches the maximal sequence number modulo n. If this helping step were omit-\nted, then an individual thread could be overtaken an arbitrary number of times.\nIf the node selected for help does not require help (its sequence number is non\nzero in Line 13), then each thread attempts to append its own node (Line 16).\n132 Chapter 6 Universality of Consensus\n(All announce [] array entries are initialized to the sentinel node which has a\nnon zero sequence number.) The rest of the algorithm is almost the same as in\nthe lock-free algorithm. A node is appended when its sequence number becomes\nnon zero. In this case, the thread proceeds as before to compute its result based\non the immutable segment of the log from the tail to its own node.\nFig. 6.7 shows an execution of the wait-free universal construction in which,\nstarting from the initial state, thread 5 announces its new node and appends\nit to the log, and pauses before adding the node to head []. Thread 7 then\ntakes steps. The value of ( before.seq + 1) mod n is 3, so thread 7 tries to\nhelp thread 2. Thread 7 loses the consensus on the sentinel node\u2019s decideNext\nreference since thread 5 already won it, and thus completes the operations\nof thread 5, setting the node\u2019s sequence number to 2 and adding thread 5\u2019s\nnode to the head [] array. Now imagine that thread 3 immediately announces\n0 3 n\u20131 ... 5 ... 7 ... ...Head0 3 n\u20131 ... 5 ... 7 ... ...\nTail\nsentinel1 2\n3 0\nAnnouce(all entries initially point to the sentinel)\n(all entries initially point to the sentinel)\nFigure 6.7 Execution of the wait-free universal construction. Thread 5 announces its new\nnode and appends it to the log, but halts before adding the node to the head []array.\nAnother thread 7 will not see thread 5\u2019s node in the head []array, and will attempt to help\nthread (before.seq + 1 mod n) , which is equal to 3. When attempting to help thread 3,\nthread 7 loses the consensus on the sentinel node\u2019s decideNext reference since thread\n5 already won. Thread 7 therefore completes updating the \ufb01elds of thread 5\u2019s node, set-\nting the node\u2019s sequence number to 2, and adding the node to the head []array. Notice\nthat thread 5\u2019s own entry in the head []array is not yet set to its announced node.\nNext, thread 2 announces its node and thread 7 succeeds in appending thread 3\u2019s node,\nsetting", "doc_id": "4f14005d-bd5d-44a3-adcb-558597007bf9", "embedding": null, "doc_hash": "cfcae8222434e832c16c5f68eedce62b0e5dce22ae854da70ccc742855b354dd", "extra_info": null, "node_info": {"start": 368738, "end": 372256}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "cb216f3d-d657-4f40-8dfd-e34ab75d0140", "3": "e8bf6b39-eb18-43a8-a2f9-7cf39e0399d2"}}, "__type__": "1"}, "e8bf6b39-eb18-43a8-a2f9-7cf39e0399d2": {"__data__": {"text": "+ 1 mod n) , which is equal to 3. When attempting to help thread 3,\nthread 7 loses the consensus on the sentinel node\u2019s decideNext reference since thread\n5 already won. Thread 7 therefore completes updating the \ufb01elds of thread 5\u2019s node, set-\nting the node\u2019s sequence number to 2, and adding the node to the head []array. Notice\nthat thread 5\u2019s own entry in the head []array is not yet set to its announced node.\nNext, thread 2 announces its node and thread 7 succeeds in appending thread 3\u2019s node,\nsetting thread 3\u2019s node\u2019s sequence number to 3. Now thread 3 wakes up. It will not\nenter the main loop because its node\u2019s sequence number is non zero, but will continue\nto update the head []array and compute its output value using a copy of the sequential\nobject.\n6.4 A Wait-Free Universal Construction 133\nits node. Then thread 7 succeeds in appending thread 3\u2019s node, but again pauses\nimmediately after setting thread 3\u2019s node\u2019s sequence number to 3, but before\nadding it to head []. Now thread 3 wakes up. It will not enter the main loop\nbecause its node\u2019s sequence number is non zero, but will continue to update\nhead [] at Line 28and compute its output value using a copy of the sequential\nobject.\nThere is a delicate point to understand about these modi\ufb01cations to the lock-\nfree algorithm. Since more than one thread can attempt to append a particu-\nlar node to the log, we must make sure that no node is appended twice. One\nthread might append the node, and set the node\u2019s sequence number, at the same\ntime that another thread appended the same node and set its sequence number.\nThe algorithm avoids this error because of the order in which threads read\nthe maximum head [] array value and the sequence number of a node in the\nannounce [] array. Let abe a node created by thread Aand appended by threads\nAandB. It must be added at least once to head [] before the second append.\nNotice, however, that the before node read from head [A] byB(Line 11) must\nbeaitself, or a successor of ain the log. Moreover, before any node is added to\nhead [] (either in Line 20or in Line 28), its sequence number is made non zero\n(Line 19). The order of operations ensures that Bsets its head [B] entry (the\nentry based on which B\u2019sbefore variable will be set, resulting in an erroneous\nappend) in Lines 9or20, and only then validates that the sequence number of a\nis non zero in Lines 10or13(depending whether Aor another thread performs\nthe operation). It follows that the validation of the erroneous second append will\nfail because the sequence number of node awill already be non zero, and it will\nnot be added to the log a second time.\nLinearizability follows because no node is ever added twice, and the order in\nwhich nodes are appended to the log is clearly compatible with the natural partial\norder of the corresponding method calls.\nT o prove that the algorithm is wait-free, we need to show that the helping\nmechanism will guarantee that any node that is announced is eventually added\nto the head [] array (implying that it is in the log) and the announcing thread\ncan complete computation of its outcome. T o assist in the proof, it is convenient\nto de\ufb01ne some notation. Let max( head []) be the node with the largest sequence\nnumber in the head [] array, and let \u201c c2head []\u201d denote the assertion that node c\nhas been assigned to head [i], for somei.\nAnauxiliary variable (sometimes called a ghost variable) is one that does not\nappear explicitly in the code, does not alter the program\u2019s behavior in any way,\nyet helps us reason about the behavior of the algorithm. We use the following\nauxiliary variables:\n\u0004concur", "doc_id": "e8bf6b39-eb18-43a8-a2f9-7cf39e0399d2", "embedding": null, "doc_hash": "ab807aa75cc55d27308d96227e4b609f73102c23890af1081c8a1980a6ca16ae", "extra_info": null, "node_info": {"start": 372234, "end": 375852}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "4f14005d-bd5d-44a3-adcb-558597007bf9", "3": "996b257d-0970-40e6-968e-5bad1dc7afe4"}}, "__type__": "1"}, "996b257d-0970-40e6-968e-5bad1dc7afe4": {"__data__": {"text": "of its outcome. T o assist in the proof, it is convenient\nto de\ufb01ne some notation. Let max( head []) be the node with the largest sequence\nnumber in the head [] array, and let \u201c c2head []\u201d denote the assertion that node c\nhas been assigned to head [i], for somei.\nAnauxiliary variable (sometimes called a ghost variable) is one that does not\nappear explicitly in the code, does not alter the program\u2019s behavior in any way,\nyet helps us reason about the behavior of the algorithm. We use the following\nauxiliary variables:\n\u0004concur (A) is the set of nodes that have been stored in the head [] array since\nthreadA\u2019s last announcement.\n\u0004start (A) is the sequence number of max( head []) when thread Alast\nannounced.\n134 Chapter 6 Universality of Consensus\nThe code re\ufb02ecting the auxiliary variables and how they are updated appears in\nFig. 6.8. For example, the statement\n(8j)concur (j) =concur (j)[after\nmeans that the node after is added to concur (j) for all threads j. The code state-\nments within the angled brackets are considered to be executed atomically. This\natomicity can be assumed because auxiliary variables do not affect the compu-\ntation in any way. For brevity let us slightly abuse the notation by letting the\nfunction max() applied to a node or array of nodes return the maximal among\ntheir sequence numbers.\nNotice that the following property is invariant throughout the execution of\nthe universal algorithm:\njconcur (A)j+start (A) = max( head []): (6.4.1)\n1public class Universal {\n2 private Node[] announce;\n3 private Node[] head;\n4 private Node tail = new node(); tail.seq = 1;\n5 for (int j=0; j < n; j++){head[j] = tail; announce[j] = tail};\n6 public Response apply(Invoc invoc) {\n7 int i = ThreadID.get();\n8 <announce[i] = new Node(invoc); start(i) = max(head);>\n9 head[i] = Node.max(head);\n10 while (announce[i].seq == 0) {\n11 Node before = head[i];\n12 Node help = announce[(before.seq + 1) % n];\n13 if(help.seq == 0)\n14 prefer = help;\n15 else\n16 prefer = announce[i];\n17 Node after = before.decideNext.decide(prefer);\n18 before.next = after;\n19 after.seq = before.seq + 1;\n20 <head[i] = after; ( 8j) (concur(j) = concur(j) [{after})>\n21 }\n22 SeqObject MyObject = new SeqObject();\n23 Node current = tail.next;\n24 while (current != announce[i]){\n25 MyObject.apply(current.invoc);\n26 current = current.next;\n27 }\n28 <head[i] = announce[i]; ( 8j) (concur(j) = concur(j) [{after})>\n29 return MyObject.apply(current.invoc);\n30 }\n31 }\nFigure 6.8 The wait-free universal algorithm with auxiliary variables. Operations in angled\nbrackets are assumed to happen atomically.\n6.4 A Wait-Free Universal Construction 135\nLemma 6.4.1. For all threads A, the following claim is always true:\njconcur (A)j>n)announce [A]2head []:\nProof: Ifjconcur (A)j>n, then concur (A) includes successive nodes bandc\n(appended to the log by threads BandC) whose respective sequence num-\nbers plus one modulo nare equal to A\u00001 andA(note thatbandcare\nthe nodes thread BandCadded to the log, not necessarily the ones", "doc_id": "996b257d-0970-40e6-968e-5bad1dc7afe4", "embedding": null, "doc_hash": "601b1aafd7d1f3bf5e8b17186eb166cde51162902c8b70144dacaa84b7d77efe", "extra_info": null, "node_info": {"start": 375830, "end": 378832}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e8bf6b39-eb18-43a8-a2f9-7cf39e0399d2", "3": "acedce96-7968-4f7b-a093-4b09eec7239a"}}, "__type__": "1"}, "acedce96-7968-4f7b-a093-4b09eec7239a": {"__data__": {"text": "are assumed to happen atomically.\n6.4 A Wait-Free Universal Construction 135\nLemma 6.4.1. For all threads A, the following claim is always true:\njconcur (A)j>n)announce [A]2head []:\nProof: Ifjconcur (A)j>n, then concur (A) includes successive nodes bandc\n(appended to the log by threads BandC) whose respective sequence num-\nbers plus one modulo nare equal to A\u00001 andA(note thatbandcare\nthe nodes thread BandCadded to the log, not necessarily the ones they\nannounced). Thread Cwill, based on the code of Lines 12through 16, append\nto the log a node located in A\u2019s entry in the announce [] array. We need to\nshow that when it does so, announce [A] was already announced, so cappends\nannounce [A], or announce [A] was already appended. Later, when cis added\ntohead [] andjconcur (A)j>n,announce [A] will be in head [] as the Lemma\nrequires.\nT o see why announce [A] was already announced when Creached Lines 12\nthrough 16, notice that (1) because Cappended its node ctob, it must have read\nbas the before node in Line 11, implying that Bappendedbbefore it was read\nfrom head [] byCin Line 11, and (2) because bis in concur (A),Aannounced\nbeforebwas added to head []. From (1) and (2) by transitivity it follows that A\nannounced before Cexecuted Lines 12through 16, and the claim follows. 2\nLemma 6.4.1 places a bound on the number of nodes that can be appended\nwhile a method call is in progress. We now give a sequence of lemmas showing\nthat whenA\ufb01nishes scanning the head [] array, either announce [A] is appended,\norhead [A] lies within n+ 1 nodes of the end of the list.\nLemma 6.4.2. The following property always holds:\nmax( head [])>start (A):\nProof: The sequence number for each head [i] is nondecreasing. 2\nLemma 6.4.3. The following is a loop invariant for Line 13ofFig. 6.3 (i.e., it\nholds during each iteration of the loop):\nmax( head [A],head [j],:::,head [n]\u00001)>start (A):\nwherejis the loop index.\nIn other words, the maximum sequence number of head [A] and all head []\nentries from the current value of jto the end of the loop never become smaller\nthan the maximum value in the array when Aannounced.\nProof: Whenjis 0, the assertion is implied by Lemma 6.4.2 . The truth of the\nassertion is preserved at each iteration, when head [A] is replaced by the node\nwith the sequence number max( head [A],head [j]). 2\n136 Chapter 6 Universality of Consensus\nLemma 6.4.4. The following assertion holds just before Line 10:\nhead [A]:seq>start (A):\nProof: Notice that head [A] is set to point to A\u2019s last appended node either in\nLine 20or Line 28. Thus, after the call to Node.max () at Line 9, max( head [A],\nhead [0],:::,head [n\u00001]) is just head [A].seq, and the result follows from\nLemma 6.4.3 . 2\nLemma 6.4.5. The following property always holds:\njconcur (A)j>head [A]:seq\u0000start (A)>0:\nProof: The lower bound follows from Lemma 6.4.4 , and the upper bound follows\nfrom Eq. 6.4.1 . 2\nTheorem 6.4.1. The algorithm in Fig. 6.6 is correct and wait-free.\nProof: T o see that", "doc_id": "acedce96-7968-4f7b-a093-4b09eec7239a", "embedding": null, "doc_hash": "0cde6323e14c070519fb00e50a966484f3f8b2797ea58eec7c50cb0a56efc564", "extra_info": null, "node_info": {"start": 378885, "end": 381859}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "996b257d-0970-40e6-968e-5bad1dc7afe4", "3": "6a96ca53-f468-45de-a1bf-09a3cd088779"}}, "__type__": "1"}, "6a96ca53-f468-45de-a1bf-09a3cd088779": {"__data__": {"text": "[A],\nhead [0],:::,head [n\u00001]) is just head [A].seq, and the result follows from\nLemma 6.4.3 . 2\nLemma 6.4.5. The following property always holds:\njconcur (A)j>head [A]:seq\u0000start (A)>0:\nProof: The lower bound follows from Lemma 6.4.4 , and the upper bound follows\nfrom Eq. 6.4.1 . 2\nTheorem 6.4.1. The algorithm in Fig. 6.6 is correct and wait-free.\nProof: T o see that the algorithm is wait-free, notice that Acan execute the main\nloop no more than n+1 times. At each successful iteration, head [A].seqincreases\nby one. After n+ 1 iterations, Lemma 6.4.5 implies that\njconcur (A)j>head [A]:seq\u0000start (A)>n:\nLemma 6.4.1 implies that announce [A] must have been added to head []. 2\n6.5 Chapter Notes\nThe universal construction described here is adapted from Maurice Herlihy\u2019s\n1991 paper [ 62]. An alternative lock-free universal construction using load-\nlinked\u2013store-conditional appears in [ 60]. The complexity of this construction can\nbe improved in several ways. Y ehuda Afek, Dalia Dauber, and Dan T ouitou [ 3]\nshow how to improve the time complexity to depend on the number of concur-\nrent threads, not the maximum possible number of threads. Mark Moir [ 118]\nshows how to design lock-free and wait-free constructions that do not require\ncopying the entire object. James Anderson and Mark Moir [ 11] extend the con-\nstruction to allow multiple objects to be updated. Prasad Jayanti [ 80] shows that\nany universal construction has worst-case \u00d2(n) complexity, where nis the max-\nimal number of threads. Tushar Chandra, Prasad Jayanti, and King Tan [ 26]\nidentify a large class of objects for which a more ef\ufb01cient universal construction\nexists.\n6.6 Exercises 137\n6.6 Exercises\nExercise 76. Give an example showing how the universal construction can fail for\nobjects with nondeterministic sequential speci\ufb01cations.\nExercise 77. Propose a way to \ufb01x the universal construction of Figure 6.8 to work\nfor objects with nondeterministic sequential speci\ufb01cations.\nExercise 78. In both the lock-free and wait-free universal constructions, the\nsequence number of the sentinel node at the tail of the list is initially set to\n1. Which of these algorithms, if any, would cease to work correctly if the sentinel\nnode\u2019s sequence number was initially set to 0?\nExercise 79. Suppose, instead of a universal construction, you simply want to\nuse consensus to implement a wait-free linearizable register with read () and\ncompareAndSet() methods. Show how you would adapt this algorithm to\ndo so.\nExercise 80. In the construction shown here, each thread \ufb01rst looks for another\nthread to help, and then tries to to append its own node.\nSuppose instead, each thread \ufb01rst tries to append its own node, and then tries\nto help the other thread. Explain whether this alternative approach works. Justify\nyour answer.\nExercise 81. In the construction in Fig. 6.4 we use a \u201cdistributed\u201d implementation\nof a \u201chead\u201d reference (to the node whose decideNext \ufb01eld it will try to modify)\nto avoid having to create an object that allows repeated consensus. Replace this\nimplementation with one that has no head reference at all, and \ufb01nds the next\n\u201chead\u201d by traversing down the log from the start until it reaches a node with a\nsequence number of 0 or with the highest non zero sequence.\nExercise", "doc_id": "6a96ca53-f468-45de-a1bf-09a3cd088779", "embedding": null, "doc_hash": "fca0794a25d4c41254f2e7b6a3579f0a3b771c5a384932c68ffc0a32853da8e6", "extra_info": null, "node_info": {"start": 381936, "end": 385197}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "acedce96-7968-4f7b-a093-4b09eec7239a", "3": "07813d15-c94f-426a-ac8a-4cab81a6c1b1"}}, "__type__": "1"}, "07813d15-c94f-426a-ac8a-4cab81a6c1b1": {"__data__": {"text": "whether this alternative approach works. Justify\nyour answer.\nExercise 81. In the construction in Fig. 6.4 we use a \u201cdistributed\u201d implementation\nof a \u201chead\u201d reference (to the node whose decideNext \ufb01eld it will try to modify)\nto avoid having to create an object that allows repeated consensus. Replace this\nimplementation with one that has no head reference at all, and \ufb01nds the next\n\u201chead\u201d by traversing down the log from the start until it reaches a node with a\nsequence number of 0 or with the highest non zero sequence.\nExercise 82. A small addition we made to the wait-free protocol was to have a\nthread add its newly appended node to the head array in Line 28even though\nit may have already added it in Line 20. This is necessary because, unlike in the\nlock-free protocol, it could be that the thread\u2019s node was added by another thread\nin Line 20, and that \u201chelping\u201d thread stopped at Line 20right after updating the\nnode\u2019s sequence number but before updating the head array.\n1.Explain how removing Line 28would violate Lemma 6.4.4 .\n2.Would the algorithm still work correctly?\n138 Chapter 6 Universality of Consensus\nExercise 83. Propose a way to \ufb01x the universal construction to work with a\nbounded amount of memory, that is, a bounded number of consensus objects\nand a bounded number of read\u2013write registers.\nHint: add a before \ufb01eld to the nodes and build a memory recycling scheme into\nthe code.\nExercise 84. Implement a consensus object that is accessed more than once by\neach thread using read () and compareAndSet() methods, creating a \u201cmultiple\naccess\u201d consensus object. Do not use the universal construction.\nIIPractice\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n7Spin Locks and Contention\nWhen writing programs for uniprocessors, it is usually safe to ignore the\nunderlying system\u2019s architectural details. Unfortunately, multiprocessor pro-\ngramming has yet to reach that state, and for the time being, it is crucial to\nunderstand the underlying machine architecture. The goal of this chapter is\nto understand how architecture affects performance, and how to exploit this\nknowledge to write ef\ufb01cient concurrent programs. We revisit the familiar mutual\nexclusion problem, this time with the aim of devising mutual exclusion protocols\nthat work well with today\u2019s multiprocessors.\nAny mutual exclusion protocol poses the question: what do you do if you can-\nnot acquire the lock? There are two alternatives. If you keep trying, the lock is\ncalled a spin lock , and repeatedly testing the lock is called spinning , or busy\u2013\nwaiting . The Filter andBakery algorithms are spin locks. Spinning is sensi-\nble when you expect the lock delay to be short. For obvious reasons, spinning\nmakes sense only on multiprocessors. The alternative is to suspend yourself and\nask the operating system\u2019s scheduler to schedule another thread on your proces-\nsor, which is sometimes called blocking . Because switching from one thread to\nanother is expensive, blocking makes sense only if you expect the lock delay to\nbe long. Many operating systems mix both strategies, spinning for a short time\nand then blocking. Both spinning and blocking are important techniques. In this\nchapter, we turn our attention to locks that use spinning.\n7.1 Welcome to the Real World\nWe approach real-world mutual exclusion using the Lock interface from the\njava.util.concurrent.locks package. For now, we consider only the two principal\nThe Art of Multiprocessor Programming. DOI:", "doc_id": "07813d15-c94f-426a-ac8a-4cab81a6c1b1", "embedding": null, "doc_hash": "b6c642544b033fecd6422d1571fcb25d25fb9eb2952b10a3067117ed4df8f662", "extra_info": null, "node_info": {"start": 385063, "end": 388576}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6a96ca53-f468-45de-a1bf-09a3cd088779", "3": "21e0cea0-063c-4048-a322-fa7e5ce07b56"}}, "__type__": "1"}, "21e0cea0-063c-4048-a322-fa7e5ce07b56": {"__data__": {"text": "which is sometimes called blocking . Because switching from one thread to\nanother is expensive, blocking makes sense only if you expect the lock delay to\nbe long. Many operating systems mix both strategies, spinning for a short time\nand then blocking. Both spinning and blocking are important techniques. In this\nchapter, we turn our attention to locks that use spinning.\n7.1 Welcome to the Real World\nWe approach real-world mutual exclusion using the Lock interface from the\njava.util.concurrent.locks package. For now, we consider only the two principal\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00007-1\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.141\n142 Chapter 7 Spin Locks and Contention\nmethods: lock () and unlock (). In most cases, these methods should be used in\nthe following structured way:\n1Lock mutex = new LockImpl(...); // lock implementation\n2...\n3mutex.lock();\n4try {\n5 ... // body\n6 }finally {\n7 mutex.unlock();\n8 }\nWe create a new Lock object called mutex (Line 1). Because Lock is an inter-\nface and not a class, we cannot create Lock objects directly. Instead, we create an\nobject that implements theLock interface. (The java.util.concurrent.locks pack-\nage includes a number of classes that implement Lock , and we provide others in\nthis chapter.) Next, we acquire the lock (Line 3), and enter the critical section,\natry block (Line 4). The finally block (Line 6) ensures that no matter what,\nthe lock is released when control leaves the critical section. Do not put the lock ()\ncall inside the tryblock, because the lock () call might throw an exception before\nacquiring the lock, causing the finally block to call unlock () when the lock has\nnot actually been acquired.\nIf we want to implement an ef\ufb01cient Lock , why not use one of the algorithms\nwe studied in Chapter 2 , such as Filter orBakery ? One problem with this\napproach is clear from the space lower bound we proved in Chapter 2 : no matter\nwhat we do, mutual exclusion using reads andwrites requires space linear in n,\nthe number of threads that may potentially access the location. It gets worse.\nConsider, for example, the 2-thread Peterson lock algorithm of Chapter 2 ,\npresented again in Fig. 7.1 . There are two threads, AandB, with IDs either\n0 or 1. When thread Awants to acquire the lock, it sets flag [A] to true, sets\nvictim toA, and tests victim andflag [1\u0000A]. As long as the test fails, the\nthread spins, repeating the test. Once it succeeds, it enters the critical section, low-\nering flag [A] tofalse as it leaves. We know, from Chapter 2 , that the Peterson\nlock provides starvation-free mutual exclusion.\n1class Peterson implements Lock {\n2 private boolean [] flag = new boolean [2];\n3 private int victim;\n4 public void lock() {\n5 int i = ThreadID.get(); // either 0 or 1\n6 int j = 1-i;\n7 flag[i] = true ;\n8 victim = i;\n9 while (flag[j] && victim == i) {}; // spin\n10 }\n11 }\nFigure 7.1 ThePeterson class ( Chapter 2 ): the order of reads\u2013writes in Lines 7,8, and9is\ncrucial to providing mutual exclusion.\n7.1 Welcome to the Real World 143\nSuppose we write a simple concurrent program in which each of the two\nthreads repeatedly acquires the Peterson lock, increments a shared counter, and\nthen releases the lock. We run it on a multiprocessor, where each thread executes\nthis acquire\u2013increment\u2013release cycle, say, half a million times. On most modern\narchitectures, the threads \ufb01nish quickly.", "doc_id": "21e0cea0-063c-4048-a322-fa7e5ce07b56", "embedding": null, "doc_hash": "e6286ea0ffd457f4c1df58b4168898c2bac13ccd7dcbf2ad11c91b73eebeffa1", "extra_info": null, "node_info": {"start": 388512, "end": 391945}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "07813d15-c94f-426a-ac8a-4cab81a6c1b1", "3": "277cc3f0-056e-4379-ad8f-3c2701309caa"}}, "__type__": "1"}, "277cc3f0-056e-4379-ad8f-3c2701309caa": {"__data__": {"text": "}\nFigure 7.1 ThePeterson class ( Chapter 2 ): the order of reads\u2013writes in Lines 7,8, and9is\ncrucial to providing mutual exclusion.\n7.1 Welcome to the Real World 143\nSuppose we write a simple concurrent program in which each of the two\nthreads repeatedly acquires the Peterson lock, increments a shared counter, and\nthen releases the lock. We run it on a multiprocessor, where each thread executes\nthis acquire\u2013increment\u2013release cycle, say, half a million times. On most modern\narchitectures, the threads \ufb01nish quickly. Alarmingly, however, we may discover\nthat the counter\u2019s \ufb01nal value may be slightly off from the expected million mark.\nProportionally, the error is probably tiny, but why is there any error at all? Some-\nhow, it must be that both threads are occasionally in the critical section at the\nsame time, even though we have proved that this cannot happen. T o quote Sher-\nlock Holmes\nHow often have I said to you that when you have eliminated the impossible, what-\never remains, however improbable, must be the truth?\nIt must be that our proof fails, not because there is anything wrong with our\nlogic, but because our assumptions about the real world are mistaken.\nWhen programming our multiprocessor, we naturally assumed that read\u2013\nwrite operations are atomic, that is, they are linearizable to some sequential\nexecution, or at the very least, that they are sequentially consistent. (Recall that\nlinearizability implies sequential consistency.) As we saw in Chapter 3 , sequen-\ntial consistency implies that there is some global order on all operations in which\neach thread\u2019s operations take effect as ordered by its program. Without calling\nattention to it at the time, we relied on the assumption that memory is sequen-\ntially consistent when proving the Peterson lock correct. In particular, mutual\nexclusion depends on the order of the steps in Lines 7,8, and 9ofFig. 7.1 . Our\nproof that the Peterson lock provided mutual exclusion implicitly relied on the\nassumption that any two memory accesses by the same thread, even to separate\nvariables, take effect in program order. (Speci\ufb01cally, it was crucial that B\u2019s write\ntoflag [B] take effect before its write to victim Eq.( 2.3).9and thatA\u2019s write to\nvictim take effect before its read of flag [B]Eq.( 2.3).11.)\nUnfortunately, modern multiprocessors typically do not provide sequentially\nconsistent memory, nor do they necessarily guarantee program order among\nreads\u2013writes by a given thread.\nWhy not? The \ufb01rst culprits are compilers that reorder instructions to enhance\nperformance. Most programming languages preserve program order for each\nindividual variable, but not across multiple variables. It is therefore possible that\nthe order of writes of flag [B] and victim by threadBwill be reversed by the\ncompiler, invalidating Eq. 2.3.9 . A second culprit is the multiprocessor hardware\nitself. ( Appendix B has a much more complete discussion of the multiproces-\nsor architecture issues raised in this chapter.) Hardware vendors make no secret\nof the fact that writes to multiprocessor memory do not necessarily take effect\nwhen they are issued, because in most programs the vast majority of writes do\nnotneed to take effect in shared memory right away. Thus, on many multiproces-\nsor architectures, writes to shared memory are buffered in a special write buffer\n(sometimes called a store buffer ), to be written to memory only when needed. If\nthreadA\u2019s write to victim is delayed in a write buffer, it may arrive in memory\nonly afterAreads flag [B],invalidatingEq.2.3.1.1\n144", "doc_id": "277cc3f0-056e-4379-ad8f-3c2701309caa", "embedding": null, "doc_hash": "eb9f0e6c45d207a799cace9a5af08844a1803753ff0f952c35657099fc18d81b", "extra_info": null, "node_info": {"start": 392015, "end": 395565}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "21e0cea0-063c-4048-a322-fa7e5ce07b56", "3": "017a6232-70bf-4b59-9d7a-e6982c90c219"}}, "__type__": "1"}, "017a6232-70bf-4b59-9d7a-e6982c90c219": {"__data__": {"text": "make no secret\nof the fact that writes to multiprocessor memory do not necessarily take effect\nwhen they are issued, because in most programs the vast majority of writes do\nnotneed to take effect in shared memory right away. Thus, on many multiproces-\nsor architectures, writes to shared memory are buffered in a special write buffer\n(sometimes called a store buffer ), to be written to memory only when needed. If\nthreadA\u2019s write to victim is delayed in a write buffer, it may arrive in memory\nonly afterAreads flag [B],invalidatingEq.2.3.1.1\n144 Chapter 7 Spin Locks and Contention\nHow then does one program multiprocessors given such weak memory con-\nsistency guarantees? T o prevent the reordering of operations resulting from write\nbuffering, modern architectures provide a special memory barrier instruction\n(sometimes called a memory fence ) that forces outstanding operations to take\neffect. It is the programmer\u2019s responsibility to know where to insert a memory\nbarrier (e.g., the Peterson lock can be \ufb01xed by placing a barrier right before\neach read). Not surprisingly, memory barriers are expensive, about as expensive\nas an atomic compareAndSet() instruction, so we want to minimize their use.\nIn fact, synchronization instructions such as getAndSet () or compareAndSet()\ndescribed in earlier chapters include a memory barrier on many architectures, as\ndoreads andwrites tovolatile \ufb01elds.\nGiven that barriers cost about as much as synchronization instructions, it may\nbe sensible to design mutual exclusion algorithms directly to use operations such\nasgetAndSet () or compareAndSet() . These operations have higher consensus\nnumbers than reads andwrites , and they can be used in a straightforward way\nto reach a kind of consensus on who can and cannot enter the critical section.\n7.2 T est-And-Set Locks\nThe testAndSet () operation, with consensus number two, was the principal\nsynchronization instruction provided by many early multiprocessor architec-\ntures. This instruction operates on a single memory word (or byte). That word\nholds a binary value, true orfalse . The testAndSet () instruction atomically\nstores true in the word, and returns that word\u2019s previous value, swapping the\nvalue true for the word\u2019s current value. At \ufb01rst glance, this instruction seems\nideal for implementing a spin lock. The lock is free when the word\u2019s value is false ,\nand busy when it is true. The lock () method repeatedly applies testAndSet ()\nto the location until that instruction returns false (i.e., until the lock is free). The\nunlock () method simply writes the value false to it.\nThejava.util.concurrent package includes an AtomicBoolean class that stores\na Boolean value. It provides a set(b) method to replace the stored value with\nvalueb, and a getAndSet (b) that atomically replaces the current value with b,\nand returns the previous value. The archaic testAndSet () instruction is the same\nas a call to getAndSet (true). We use the term test-and-set in prose to remain\ncompatible with common usage, but we use the expression getAndSet (true) in\nour code examples to remain compatible with Java. The TASLock class shown in\nFig. 7.2 shows a lock algorithm based on the testAndSet () instruction.\nNow consider the alternative to the TASLock algorithm illustrated in Fig. 7.3.\nInstead of performing the testAndSet () directly, the thread repeatedly reads the\nlock until it appears to be free (i.e., until get() returns false ). Only after the lock\nappears to be free does the thread apply testAndSet (). This technique is called\ntest-and-test-and-set and the lock a TTASLock .\n7.2 T", "doc_id": "017a6232-70bf-4b59-9d7a-e6982c90c219", "embedding": null, "doc_hash": "c7160d38be86c75a18fbeda6ff99aeb4c811dcc4ddad791c60911af0a1b353e8", "extra_info": null, "node_info": {"start": 395546, "end": 399137}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "277cc3f0-056e-4379-ad8f-3c2701309caa", "3": "0d1deb84-4c47-45c6-b2c1-12f65f88351c"}}, "__type__": "1"}, "0d1deb84-4c47-45c6-b2c1-12f65f88351c": {"__data__": {"text": "code examples to remain compatible with Java. The TASLock class shown in\nFig. 7.2 shows a lock algorithm based on the testAndSet () instruction.\nNow consider the alternative to the TASLock algorithm illustrated in Fig. 7.3.\nInstead of performing the testAndSet () directly, the thread repeatedly reads the\nlock until it appears to be free (i.e., until get() returns false ). Only after the lock\nappears to be free does the thread apply testAndSet (). This technique is called\ntest-and-test-and-set and the lock a TTASLock .\n7.2 T est-And-Set Locks 145\n1public class TASLock implements Lock {\n2 AtomicBoolean state = new AtomicBoolean( false );\n3 public void lock() {\n4 while (state.getAndSet( true )) {}\n5 }\n6 public void unlock() {\n7 state.set( false );\n8 }\n9}\nFigure 7.2 TheTASLock class.\n1public class TTASLock implements Lock {\n2 AtomicBoolean state = new AtomicBoolean( false );\n3 public void lock() {\n4 while (true ) {\n5 while (state.get()) {};\n6 if(!state.getAndSet( true ))\n7 return ;\n8 }\n9 }\n10 public void unlock() {\n11 state.set( false );\n12 }\n13 }\nFigure 7.3 TheTTASLock class.\nClearly, the TASLock andTTASLock algorithms are equivalent from the point\nof view of correctness: each one guarantees deadlock-free mutual exclusion.\nUnder the simple model we have been using so far, there should be no differ-\nence between these two algorithms.\nHow do they compare on a real multiprocessor? Experiments that measure\nthe elapsed time for nthreads to execute a short critical section invariably yield\nthe results shown schematically in Fig. 7.4. Each data point represents the same\namount of work, so in the absence of contention effects, all curves would be \ufb02at.\nThe top curve is the TASLock , the middle curve is the TTASLock , and the bottom\ncurve shows the time that would be needed if the threads did not interfere at all.\nThe difference is dramatic: the TASLock performs very poorly, and the TTASLock\nperformance, while substantially better, still falls far short of the ideal.\nThese differences can be explained in terms of modern multiprocessor archi-\ntectures. First, a word of caution. Modern multiprocessors encompass a variety\nof architectures, so we must be careful about overgeneralizing. Nevertheless,\n(almost) all modern architectures have similar issues concerning caching and\nlocality. The details differ, but the principles remain the same.\nFor simplicity, we consider a typical multiprocessor architecture in which pro-\ncessors communicate by a shared broadcast medium called a bus(like a tiny Eth-\nernet). Both the processors and the memory controller can broadcast on the bus,\n146 Chapter 7 Spin Locks and Contention\nTASLock\nTTASLock\nIdealLocktime\nnumber of threads\nFigure 7.4 Schematic performance of a TASLock , aTTASLock , and an ideal lock with no\noverhead.\nbut only one processor (or memory) can broadcast on the bus at a time. All proces-\nsors (and memory) can listen. T oday, bus-based architectures are common because\nthey are easy to build, although they scale poorly to large numbers of processors.\nEach processor has a cache , a small high-speed memory where the processor\nkeeps data likely to be of interest. A memory access typically requires orders of\nmagnitude more machine cycles than a cache access. T echnology trends are not\nhelping: it is unlikely that memory access times will catch up with processor cycle\ntimes in the near future, so cache performance is critical to the overall perfor-\nmance of a", "doc_id": "0d1deb84-4c47-45c6-b2c1-12f65f88351c", "embedding": null, "doc_hash": "45f8b6cc490df61c0918f7ab9fbaa47d31b59ca6252b2f5d66b775bd5a2e0215", "extra_info": null, "node_info": {"start": 399154, "end": 402601}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "017a6232-70bf-4b59-9d7a-e6982c90c219", "3": "2a9ecc0a-3ce6-468c-9016-88149b9fa0d6"}}, "__type__": "1"}, "2a9ecc0a-3ce6-468c-9016-88149b9fa0d6": {"__data__": {"text": "on the bus at a time. All proces-\nsors (and memory) can listen. T oday, bus-based architectures are common because\nthey are easy to build, although they scale poorly to large numbers of processors.\nEach processor has a cache , a small high-speed memory where the processor\nkeeps data likely to be of interest. A memory access typically requires orders of\nmagnitude more machine cycles than a cache access. T echnology trends are not\nhelping: it is unlikely that memory access times will catch up with processor cycle\ntimes in the near future, so cache performance is critical to the overall perfor-\nmance of a multiprocessor architecture.\nWhen a processor reads from an address in memory, it \ufb01rst checks whether\nthat address and its contents are present in its cache. If so, then the processor\nhas a cache hit , and can load the value immediately. If not, then the processor\nhas a cache miss , and must \ufb01nd the data either in the memory, or in another\nprocessor\u2019s cache. The processor then broadcasts the address on the bus. The\nother processors snoop on the bus. If one processor has that address in its cache,\nthen it responds by broadcasting the address and value. If no processor has that\naddress, then the memory itself responds with the value at that address.\n7.3 TAS-Based Spin Locks Revisited\nWe now consider how the simple TASLock algorithm performs on a shared-bus\narchitecture. Each getAndSet () call is broadcast on the bus. Because all threads\nmust use the bus to communicate with memory, these getAndSet () calls delay\nall threads, even those not waiting for the lock. Even worse, the getAndSet () call\nforces other processors to discard their own cached copies of the lock, so every\n7.4 Exponential Backoff 147\nspinning thread encounters a cache miss almost every time, and must use the\nbus to fetch the new, but unchanged value. Adding insult to injury, when the\nthread holding the lock tries to release it, it may be delayed because the bus is\nmonopolized by the spinners. We now understand why the TASLock performs so\npoorly.\nNow consider the behavior of the TTASLock algorithm while the lock is held\nby a threadA. The \ufb01rst time thread Breads the lock it takes a cache miss, forc-\ningBto block while the value is loaded into B\u2019s cache. As long as Aholds the\nlock,Brepeatedly rereads the value, but hits in the cache every time. Bthus pro-\nduces no bus traf\ufb01c, and does not slow down other threads\u2019 memory accesses.\nMoreover, a thread that releases a lock is not delayed by threads spinning on\nthat lock.\nThe situation deteriorates, however, when the lock is released. The lock holder\nreleases the lock by writing false to the lock variable, which immediately invali-\ndates the spinners\u2019 cached copies. Each one takes a cache miss, rereads the new\nvalue, and they all (more-or-less simultaneously) call getAndSet () to acquire the\nlock. The \ufb01rst to succeed invalidates the others, who must then reread the value,\ncausing a storm of bus traf\ufb01c. Eventually, the threads settle down once again to\nlocal spinning.\nThis notion of local spinning , where threads repeatedly reread cached values\ninstead of repeatedly using the bus, is an important principle critical to the design\nof ef\ufb01cient spin locks.\n7.4 Exponential Backoff\nWe now consider how to re\ufb01ne the TTASLock algorithm. First, some terminology:\ncontention occurs when multiple threads try to acquire a lock at the same time.\nHigh contention means there are many such threads, and low contention means\nthe opposite.\nRecall that in the TTASLock class, the lock () method takes two steps: it repeat-\nedly reads the lock, and", "doc_id": "2a9ecc0a-3ce6-468c-9016-88149b9fa0d6", "embedding": null, "doc_hash": "e4cc838c039f1f9ae2ad6e92f103a759d0d12d158e88f4b988432291a47b3493", "extra_info": null, "node_info": {"start": 402538, "end": 406131}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0d1deb84-4c47-45c6-b2c1-12f65f88351c", "3": "60bfa312-8993-4d4f-95cc-621ceca115a0"}}, "__type__": "1"}, "60bfa312-8993-4d4f-95cc-621ceca115a0": {"__data__": {"text": ", where threads repeatedly reread cached values\ninstead of repeatedly using the bus, is an important principle critical to the design\nof ef\ufb01cient spin locks.\n7.4 Exponential Backoff\nWe now consider how to re\ufb01ne the TTASLock algorithm. First, some terminology:\ncontention occurs when multiple threads try to acquire a lock at the same time.\nHigh contention means there are many such threads, and low contention means\nthe opposite.\nRecall that in the TTASLock class, the lock () method takes two steps: it repeat-\nedly reads the lock, and when the lock appears to be free, it attempts to acquire the\nlock by calling getAndSet (true). Here is a key observation: if some other thread\nacquires the lock between the \ufb01rst and second step, then, most likely, there is high\ncontention for that lock. Clearly, it is a bad idea to try to acquire a lock for which\nthere is high contention. Such an attempt contributes to bus traf\ufb01c (making the\ntraf\ufb01c jam worse), at a time when the thread\u2019s chances of acquiring the lock are\nslim. Instead, it is more effective for the thread to back off for some duration,\ngiving competing threads a chance to \ufb01nish.\nFor how long should the thread back off before retrying? A good rule of\nthumb is that the larger the number of unsuccessful tries, the higher the likely\ncontention, and the longer the thread should back off. Here is a simple approach.\nWhenever the thread sees the lock has become free but fails to acquire it, it backs\n148 Chapter 7 Spin Locks and Contention\noff before retrying. T o ensure that concurrent con\ufb02icting threads do not fall into\nlock-step, all trying to acquire the lock at the same time, the thread backs off for\na random duration. Each time the thread tries and fails to get the lock, it doubles\nthe expected back-off time, up to a \ufb01xed maximum.\nBecause backing off is common to several locking algorithms, we encapsulate\nthis logic in a simple Backoff class, shown in Fig. 7.5. The constructor takes these\narguments: minDelay is the initial minimum delay (it makes no sense for the\nthread to back off for too short a duration), and maxDelay is the \ufb01nal maximum\ndelay (a \ufb01nal limit is necessary to prevent unlucky threads from backing off for\nmuch too long). The limit \ufb01eld controls the current delay limit. The backoff ()\nmethod computes a random delay between zero and the current limit, and blocks\nthe thread for that duration before returning. It doubles the limit for the next\nback-off, up to maxDelay .\nFig. 7.6 illustrates the BackoffLock class. It uses a Backoff object whose min-\nimum and maximum back-off durations are governed by the constants chosen\nforminDelay andmaxDelay . It is important to note that the thread backs off\nonly when it fails to acquire a lock that it had immediately before observed to\nbe free. Observing that the lock is held by another thread says nothing about the\nlevel of contention.\nTheBackoffLock is easy to implement, and typically performs signi\ufb01cantly\nbetter than TASLock on many architectures. Unfortunately, its performance is\nsensitive to the choice of minDelay andmaxDelay values. T o deploy this lock\non a particular architecture, it is easy to experiment with different values, and\nto choose the ones that work best. Experience shows, however, that these opti-\nmal values are sensitive to the number of processors and their speed, so it is not\n1public class Backoff {\n2 final int minDelay, maxDelay;\n3 int limit;\n4 final Random random;\n5 public Backoff( int min, int max) {\n6 minDelay = min;\n7 maxDelay = max;\n8 limit =", "doc_id": "60bfa312-8993-4d4f-95cc-621ceca115a0", "embedding": null, "doc_hash": "114566b758da7d5da8605c5188d807cfe3c17a82d2168076739de92c68b1f8e2", "extra_info": null, "node_info": {"start": 406187, "end": 409709}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2a9ecc0a-3ce6-468c-9016-88149b9fa0d6", "3": "cb73713a-2980-4eb6-a636-e270920adf39"}}, "__type__": "1"}, "cb73713a-2980-4eb6-a636-e270920adf39": {"__data__": {"text": "Unfortunately, its performance is\nsensitive to the choice of minDelay andmaxDelay values. T o deploy this lock\non a particular architecture, it is easy to experiment with different values, and\nto choose the ones that work best. Experience shows, however, that these opti-\nmal values are sensitive to the number of processors and their speed, so it is not\n1public class Backoff {\n2 final int minDelay, maxDelay;\n3 int limit;\n4 final Random random;\n5 public Backoff( int min, int max) {\n6 minDelay = min;\n7 maxDelay = max;\n8 limit = minDelay;\n9 random = new Random();\n10 }\n11 public void backoff() throws InterruptedException {\n12 int delay = random.nextInt(limit);\n13 limit = Math.min(maxDelay, 2 *limit);\n14 Thread.sleep(delay);\n15 }\n16 }\nFigure 7.5 TheBackoff class: adaptive backoff logic. T o ensure that concurrently contending\nthreads do not repeatedly try to acquire the lock at the same time, threads back off for a\nrandom duration. Each time the thread tries and fails to get the lock, it doubles the expected\ntime to back off, up to a \ufb01xed maximum.\n7.5 Queue Locks 149\n1public class BackoffLock implements Lock {\n2 private AtomicBoolean state = new AtomicBoolean( false );\n3 private static final int MIN_DELAY = ...;\n4 private static final int MAX_DELAY = ...;\n5 public void lock() {\n6 Backoff backoff = new Backoff(MIN_DELAY, MAX_DELAY);\n7 while (true ) {\n8 while (state.get()) {};\n9 if(!state.getAndSet( true )) {\n10 return ;\n11 }else {\n12 backoff.backoff();\n13 }\n14 }\n15 }\n16 public void unlock() {\n17 state.set( false );\n18 }\n19 ...\n20 }\nFigure 7.6 The Exponential Backoff lock. Whenever the thread fails to acquire a lock that\nbecame free, it backs off before retrying.\neasy to tune the BackoffLock class to be portable across a range of different\nmachines.\n7.5 Queue Locks\nWe now explore a different approach to implementing scalable spin locks,\none that is slightly more complicated than backoff locks, but inherently more\nportable. There are two problems with the BackoffLock algorithm.\n\u0004Cache-coherence Traf\ufb01c : All threads spin on the same shared location causing\ncache-coherence traf\ufb01c on every successful lock access (though less than with\ntheTASLock ).\n\u0004Critical Section Underutilization : Threads delay longer than necessary, causing\nthe critical section to be underutilized.\nOne can overcome these drawbacks by having threads form a queue . In a\nqueue, each thread can learn if its turn has arrived by checking whether its pre-\ndecessor has \ufb01nished. Cache-coherence traf\ufb01c is reduced by having each thread\nspin on a different location. A queue also allows for better utilization of the crit-\nical section, since there is no need to guess when to attempt to access it: each\nthread is noti\ufb01ed directly by its predecessor in the queue. Finally, a queue pro-\nvides \ufb01rst-come-\ufb01rst-served fairness, the same high level of fairness achieved by\n150 Chapter 7 Spin Locks and Contention\ntheBakery algorithm. We now explore different ways to implement queue locks ,\na family of locking algorithms that exploit these insights.\n7.5.1 Array-Based Locks\nFigs. 7.7 and 7.8show the ALock ,1a simple array-based queue lock. The threads\nshare an AtomicInteger tail \ufb01eld, initially zero. T o acquire the lock, each\nthread atomically increments tail (Line 17). Call the resulting value", "doc_id": "cb73713a-2980-4eb6-a636-e270920adf39", "embedding": null, "doc_hash": "1ea7f40f9793f3cabf3e3c912601132a1d748e254ebc50e00ad363a3bfb87195", "extra_info": null, "node_info": {"start": 409719, "end": 413007}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "60bfa312-8993-4d4f-95cc-621ceca115a0", "3": "0fa51dd4-b29e-441f-b240-e99776adbecf"}}, "__type__": "1"}, "0fa51dd4-b29e-441f-b240-e99776adbecf": {"__data__": {"text": "fairness, the same high level of fairness achieved by\n150 Chapter 7 Spin Locks and Contention\ntheBakery algorithm. We now explore different ways to implement queue locks ,\na family of locking algorithms that exploit these insights.\n7.5.1 Array-Based Locks\nFigs. 7.7 and 7.8show the ALock ,1a simple array-based queue lock. The threads\nshare an AtomicInteger tail \ufb01eld, initially zero. T o acquire the lock, each\nthread atomically increments tail (Line 17). Call the resulting value the thread\u2019s\nslot. The slot is used as an index into a Boolean flag array. If flag [j] istrue, then\nthe thread with slot jhas permission to acquire the lock. Initially, flag [0] is true.\nT o acquire the lock, a thread spins until the \ufb02ag at its slot becomes true (Line 19).\nT o release the lock, the thread sets the \ufb02ag at its slot to false (Line 23), and sets\nthe \ufb02ag at the next slot to true (Line 24). All arithmetic is modulo n, wherenis\nat least as large as the maximum number of concurrent threads.\nInthe ALock algorithm, mySlotIndex isathread-local variable(see AppendixA ).\nThread-local variables differ from their regular counterparts in that each thread\nhas its own, independently initialized copy of each variable. Thread-local vari-\nables need not be stored in shared memory, do not require synchronization, and\n1public class ALock implements Lock {\n2 ThreadLocal<Integer> mySlotIndex = new ThreadLocal<Integer> (){\n3 protected Integer initialValue() {\n4 return 0;\n5 }\n6 };\n7 AtomicInteger tail;\n8 volatile boolean [] flag;\n9 int size;\n10 public ALock( int capacity) {\n11 size = capacity;\n12 tail = new AtomicInteger(0);\n13 flag = new boolean [capacity];\n14 flag[0] = true ;\n15 }\n16 public void lock() {\n17 int slot = tail.getAndIncrement() % size;\n18 mySlotIndex.set(slot);\n19 while (! flag[slot]) {};\n20 }\n21 public void unlock() {\n22 int slot = mySlotIndex.get();\n23 flag[slot] = false ;\n24 flag[(slot + 1) % size] = true ;\n25 }\n26 }\nFigure 7.7 Array-based Queue Lock.\n1Most of our lock classes use the initials of their inventors, as explained in Section 7.10 .\n7.5 Queue Locks 151\ndo not generate any coherence traf\ufb01c since they are accessed by only one thread.\nThe value of a thread-local variable is accessed by get() and set() methods.\nTheflag [] array, on the other hand, is shared2. However, contention on the\narray locations is minimized since each thread, at any given time, spins on its\nlocally cached copy of a single array location, greatly reducing invalidation traf\ufb01c.\nNote that contention may still occur because of a phenomenon called false\nsharing , which occurs when adjacent data items (such as array elements) share\na single cache line. A write to one item invalidates that item\u2019s cache line, which\ncauses invalidation traf\ufb01c to processors that are spinning on unchanged but near\nitems that happen to fall in the same cache line. In the example in Fig. 7.8, threads\naccessing the 8 ALock locations may suffer unnecessary invalidations because the\nlocations were all cached in the same two 4-word lines. One way to avoid false\nsharing is to pad array elements so that distinct elements are mapped to dis-\ntinct cache lines. Padding is easier in low-level languages like C or C++ where\nthe programmer has a direct control over the layout of objects in memory. In\nthe example in Fig. 7.8, we pad the eight original ALock locations by increasing\nthe lock array", "doc_id": "0fa51dd4-b29e-441f-b240-e99776adbecf", "embedding": null, "doc_hash": "20313cc894b3061f3513546e7fcbde52a2d7f2acf5e1b280833c129b4ebc5614", "extra_info": null, "node_info": {"start": 413044, "end": 416417}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "cb73713a-2980-4eb6-a636-e270920adf39", "3": "450677bb-6f0d-4014-afc2-67e5ea845ec3"}}, "__type__": "1"}, "450677bb-6f0d-4014-afc2-67e5ea845ec3": {"__data__": {"text": "that happen to fall in the same cache line. In the example in Fig. 7.8, threads\naccessing the 8 ALock locations may suffer unnecessary invalidations because the\nlocations were all cached in the same two 4-word lines. One way to avoid false\nsharing is to pad array elements so that distinct elements are mapped to dis-\ntinct cache lines. Padding is easier in low-level languages like C or C++ where\nthe programmer has a direct control over the layout of objects in memory. In\nthe example in Fig. 7.8, we pad the eight original ALock locations by increasing\nthe lock array size fourfold, and placing the locations four words apart so that no\ntwo locations can fall in the same cache line. (We increment from one location i\nto the next by computing 4( i+ 1) mod 32 instead of i+ 1 mod 8).\n7.5.2 The CLH Queue Lock\nTheALock improves on BackoffLock because it reduces invalidations to a mini-\nmum, and minimizes the interval between when a lock is freed by one thread and\nwhen it is acquired by another. Unlike the TASLock andBackoffLock , this algo-\nrithm guarantees that no starvation occurs, and provides \ufb01rst-come-\ufb01rst-served\nfairness.\nUnfortunately, the ALock lock is not space-ef\ufb01cient. It requires a known\nboundnon the maximum number of concurrent threads, and it allocates an\narray of that size per lock. Thus, synchronizing Ldistinct objects requires O(Ln)\nspace, even if a thread accesses only one lock at a time.\nWe now turn our attention to a different style of queue lock. Fig. 7.9 shows the\nCLHLock class\u2019s \ufb01elds and constructor. This class records each thread\u2019s status in a\nQNode object, which has a Boolean locked \ufb01eld. If that \ufb01eld is true, then the cor-\nresponding thread has either acquired the lock, or is waiting for the lock. If that\n\ufb01eld is false , then the thread has released the lock. The lock itself is represented\nas a virtual linked list of QNode objects. We use the term \u201cvirtual\u201d because the\nlist is implicit: each thread refers to its predecessor through a thread-local pred\nvariable. The public tail \ufb01eld is an AtomicReference<QNode> to the node most\nrecently added to the queue.\nAs shown in Fig. 7.10, to acquire the lock, a thread sets the locked \ufb01eld of its\nQNode totrue, indicating that the thread is not ready to release the lock. The thread\n2The role of a volatile statement here is not to introduce a memory barrier but rather to prevent\nthe compiler from applying any optimizations to the loop in Line 19.\n152 Chapter 7 Spin Locks and Contention\n(a)\n(b)mySlot\nThread C\n(will get \nslot 4 )mySlot\nThread A\n(in CS )mySlot\nThread B\n(spinning )10\n2\n37\n6\n45falsefalse\ntrue\nfalse falsefalsefalsefalse\n4tail\nline k\nline k11\nline nCache\nline 0\ntrue false false false\nfalse false false false\ni11 mod 8\nline nCache\nline 0\nline k17line k12line k11line k40\n8\n1228\n24\n1620false\nfalse\ntrue\nfalsefalse\nfalse\nfalse\nfalse16tail\n4(i11) mod 32\nmySlot mySlot mySlot\nThread C\n(will get\nslot 16 )Thread B\n(spinning )Thread A\n(in CS )false\nfalse\ntrue\nfalse\nFigure 7.8 TheALock with padding to avoid false sharing. In Part (a) the ALock has 8 slots which are accessed\nvia a modulo 8 counter. Array entries are typically mapped into cache lines consecutively. As can be seen,\nwhen thread Achanges the status of its entry, thread Bwhose entry is mapped to the same cache line k\nincurs", "doc_id": "450677bb-6f0d-4014-afc2-67e5ea845ec3", "embedding": null, "doc_hash": "eeb573a6a9becf41757ba57889654dedaf9f8b9c38feb18f071ce428ace6bbe3", "extra_info": null, "node_info": {"start": 416354, "end": 419647}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0fa51dd4-b29e-441f-b240-e99776adbecf", "3": "6cd36fb3-e29d-4da2-956c-db40c3ee8441"}}, "__type__": "1"}, "6cd36fb3-e29d-4da2-956c-db40c3ee8441": {"__data__": {"text": "mod 32\nmySlot mySlot mySlot\nThread C\n(will get\nslot 16 )Thread B\n(spinning )Thread A\n(in CS )false\nfalse\ntrue\nfalse\nFigure 7.8 TheALock with padding to avoid false sharing. In Part (a) the ALock has 8 slots which are accessed\nvia a modulo 8 counter. Array entries are typically mapped into cache lines consecutively. As can be seen,\nwhen thread Achanges the status of its entry, thread Bwhose entry is mapped to the same cache line k\nincurs a false invalidation. In Part (b) each location is padded so it is 4 apart from the others with a modulo\n32 counter. Even if array entries are mapped consecutively, the entry for Bis mapped to a different cache line\nfrom that of A, so if Ainvalidates its entry this does not cause Bto be invalidated.\n7.5 Queue Locks 153\n1public class CLHLock implements Lock {\n2 AtomicReference<Qnode> tail;\n3 ThreadLocal<QNode> myPred;\n4 ThreadLocal<QNode> myNode;\n5 public CLHLock() {\n6 tail = new AtomicReference<QNode>( null );\n7 myNode = new ThreadLocal<QNode>() {\n8 protected QNode initialValue() {\n9 return new QNode();\n10 }\n11 };\n12 myPred = new ThreadLocal<QNode>() {\n13 protected QNode initialValue() {\n14 return null ;\n15 }\n16 };\n17 }\n18 ...\n19 }\nFigure 7.9 TheCLHLock class: \ufb01elds and constructor.\n20 public void lock() {\n21 QNode qnode = myNode.get();\n22 qnode.locked = true ;\n23 QNode pred = tail.getAndSet(qnode);\n24 myPred.set(pred);\n25 while (pred.locked) {}\n26 }\n27 public void unlock() {\n28 QNode qnode = myNode.get();\n29 qnode.locked = false ;\n30 myNode.set(myPred.get());\n31 }\n32 }\nFigure 7.10 TheCLHLock class: lock ()andunlock ()methods.\napplies getAndSet () to the tail \ufb01eld to make its own node the tail of the queue,\nsimultaneously acquiring a reference to its predecessor\u2019s QNode . The thread then\nspins on the predecessor\u2019s locked \ufb01eld until the predecessor releases the lock. T o\nrelease the lock, the thread sets its node\u2019s locked \ufb01eld to false . It then reuses its\npredecessor\u2019s QNode as its new node for future lock accesses. It can do so because\nat this point the thread\u2019s predecessor\u2019s QNode is no longer used by the predecessor,\nand the thread\u2019s old QNode could be referenced both by the thread\u2019s successor and\nby the tail .3Although we do not do so in our examples, it is possible to recycle\n3It is not necessary for correctness to reuse nodes in garbage-collected languages such as Java\nor C#, but reuse would be needed in languages such as C++ or C.\n154 Chapter 7 Spin Locks and Contention\n(a) (b)\n(c)tail.getAndSet()\nA:lock()\nA:unlock()\nB:lock()Thread A \nmyNode 5myPredmyNode myPred\nThread B Thread AmyNode myPred myNode myPredfalse false true\nfalse false trueInitiallytail tail\ntail\nFigure 7.11 CLHLock class: lock acquisition and release. Initially the tail \ufb01eld refers to a QNode whose\nlocked \ufb01eld is false. Thread Athen applies getAndSet ()to the tail \ufb01eld to insert its QNode at the tail of\nthe queue, simultaneously acquiring a reference to its predecessor\u2019s QNode . Next, Bdoes the same to insert\nitsQNode at the tail of the queue. Athen releases the lock by setting its node\u2019s locked", "doc_id": "6cd36fb3-e29d-4da2-956c-db40c3ee8441", "embedding": null, "doc_hash": "04ab0330f1be467a724445066dcd09a68327dfc94acb0020e9df03b94336f66e", "extra_info": null, "node_info": {"start": 419751, "end": 422806}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "450677bb-6f0d-4014-afc2-67e5ea845ec3", "3": "56b59297-9e5d-4630-a73d-fc2a6fd9397a"}}, "__type__": "1"}, "56b59297-9e5d-4630-a73d-fc2a6fd9397a": {"__data__": {"text": "myPred\nThread B Thread AmyNode myPred myNode myPredfalse false true\nfalse false trueInitiallytail tail\ntail\nFigure 7.11 CLHLock class: lock acquisition and release. Initially the tail \ufb01eld refers to a QNode whose\nlocked \ufb01eld is false. Thread Athen applies getAndSet ()to the tail \ufb01eld to insert its QNode at the tail of\nthe queue, simultaneously acquiring a reference to its predecessor\u2019s QNode . Next, Bdoes the same to insert\nitsQNode at the tail of the queue. Athen releases the lock by setting its node\u2019s locked \ufb01eld to false. It then\nrecycles the QNode referenced by pred for future lock accesses.\nnodes so that if there are Llocks, and each thread accesses at most one lock at a\ntime, then the CLHLock class needs only O(L+n) space, as compared to O(Ln)\nfor the ALock class. Fig. 7.11 shows a typical CLHLock execution.\nLike the ALock , this algorithm has each thread spin on a distinct location, so\nwhen one thread releases its lock, it invalidates only its successor\u2019s cache. This\nalgorithm requires much less space than the ALock class, and does not require\nknowledge of the number of threads that might access the lock. Like the ALock\nclass, it provides \ufb01rst-come-\ufb01rst-served fairness.\nPerhaps the only disadvantage of this lock algorithm is that it performs poorly\non cache-less NUMA architectures. Each thread spins waiting for its predeces-\nsor\u2019s node\u2019s locked \ufb01eld to become false . If this memory location is remote, then\nperformance suffers. On cache-coherent architectures, however, this approach\nshould work well.\n7.5.3 The MCS Queue Lock\nFig. 7.12 shows the \ufb01elds and constructor for the MCSLock class. Here, too, the\nlock is represented as a linked list of QNode objects, where each QNode represents\n7.5 Queue Locks 155\n1public class MCSLock implements Lock {\n2 AtomicReference<QNode> tail;\n3 ThreadLocal<QNode> myNode;\n4 public MCSLock() {\n5 tail = new AtomicReference<QNode>( null );\n6 myNode = new ThreadLocal<QNode>() {\n7 protected QNode initialValue() {\n8 return new QNode();\n9 }\n10 };\n11 }\n12 ...\n13 class QNode {\n14 boolean locked = false ;\n15 QNode next = null ;\n16 }\n17 }\nFigure 7.12 MCSLock class: \ufb01elds, constructor and QNode class.\n18 public void lock() {\n19 QNode qnode = myNode.get();\n20 QNode pred = tail.getAndSet(qnode);\n21 if(pred != null ) {\n22 qnode.locked = true ;\n23 pred.next = qnode;\n24 // wait until predecessor gives up the lock\n25 while (qnode.locked) {}\n26 }\n27 }\n28 public void unlock() {\n29 QNode qnode = myNode.get();\n30 if(qnode.next == null ) {\n31 if(tail.compareAndSet(qnode, null ))\n32 return ;\n33 // wait until successor fills in the next field\n34 while (qnode.next == null ) {}\n35 }\n36 qnode.next.locked = false ;\n37 qnode.next = null ;\n38 }\nFigure 7.13 MCSLock class: lock ()andunlock ()methods.\neither a lock holder or a thread waiting to acquire the lock. Unlike the CLHLock\nclass, the list is explicit, not virtual: instead of embodying the list in thread-local\nvariables, it is embodied in the (globally accessible) QNode objects, via their next\n\ufb01elds.\nFig. 7.13 shows the MCSLock class\u2019s lock () and unlock", "doc_id": "56b59297-9e5d-4630-a73d-fc2a6fd9397a", "embedding": null, "doc_hash": "cee1e222eaeada62dc9e79a8d53dd6596e170d6c56b6e63e77581695638f270c", "extra_info": null, "node_info": {"start": 422743, "end": 425812}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6cd36fb3-e29d-4da2-956c-db40c3ee8441", "3": "aec39ccd-c1a3-4439-b0ac-cef5088016bf"}}, "__type__": "1"}, "aec39ccd-c1a3-4439-b0ac-cef5088016bf": {"__data__": {"text": "== null ) {}\n35 }\n36 qnode.next.locked = false ;\n37 qnode.next = null ;\n38 }\nFigure 7.13 MCSLock class: lock ()andunlock ()methods.\neither a lock holder or a thread waiting to acquire the lock. Unlike the CLHLock\nclass, the list is explicit, not virtual: instead of embodying the list in thread-local\nvariables, it is embodied in the (globally accessible) QNode objects, via their next\n\ufb01elds.\nFig. 7.13 shows the MCSLock class\u2019s lock () and unlock () methods. T o acquire\nthe lock, a thread appends its own QNode at the tail of the list (Line 20). If the\n156 Chapter 7 Spin Locks and Contention\nqueue was not previously empty, it sets the predecessor\u2019s QNode \u2019snext \ufb01eld to\nrefer to its own QNode . The thread then spins on a (local) locked \ufb01eld in its own\nQNode waiting until its predecessor sets this \ufb01eld to false (Lines 21\u201326).\nTheunlock () method checks whether the node\u2019s next \ufb01eld is null (Line 30).\nIf so, then either no other thread is contending for the lock, or there is another\nthread, but it is slow. Let qbe the thread\u2019s current node. T o distinguish between\nthese cases, the method applies compareAndSet (q,null) to the tail \ufb01eld. If the\ncall succeeds, then no other thread is trying to acquire the lock, tail is set to null,\nand the method returns. Otherwise, another (slow) thread is trying to acquire the\nlock, so the method spins waiting for it to \ufb01nish (Line 34). In either case, once\nthe successor has appeared, the unlock () method sets its successor\u2019s locked \ufb01eld\ntofalse , indicating that the lock is now free. At this point, no other thread can\naccess this QNode , and so it can be reused. Fig. 7.14 shows an example execution\nof the MCSLock .\nThis lock shares the advantages of the CLHLock , in particular, the property\nthat each lock release invalidates only the successor\u2019s cache entry. It is better\nsuited to cache-less NUMA architectures because each thread controls the loca-\ntion on which it spins. Like the CLHLock , nodes can be recycled so that this lock\nfalse(a)\nInitially\nA:lock()B:lock()\nC:lock()\nA:unlock()false true true\nfalse true falseThread C myNode\ntailThread B myNode\nThread A myNode(c)\n(b) (d)\nThread CmyNode\nThread BmyNode\nThread AmyNode\nThread A myNodetailtail tail\ntail.getAndSet()\nFigure 7.14 A lock acquisition and release in an MCSLock . (a) Initially the tail is null. (b) T o acquire the\nlock, thread Aplaces its own QNode at the tail of the list and since it has no predecessor it enters the critical\nsection. (c) thread Benqueues its own QNode at the tail of the list and modi\ufb01es its predecessor\u2019s QNode to\nrefer back to its own. Thread Bthen spins on its locked \ufb01eld waiting until A, its predecessor, sets this \ufb01eld\nfrom true to false. Thread Crepeats this sequence. (d) T o release the lock, Afollows its next \ufb01eld to its\nsuccessor Band sets B\u2019slocked \ufb01eld to false. It can now reuse its QNode .\n7.6 A Queue Lock with Timeouts 157\nhas space complexity O(L+n). One drawback of the MCSLock algorithm is that\nreleasing a lock requires spinning. Another is that it requires more reads, writes,\nandcompareAndSet() calls than the CLHLock algorithm.\n7.6", "doc_id": "aec39ccd-c1a3-4439-b0ac-cef5088016bf", "embedding": null, "doc_hash": "d4a61404be60c49c72e901401e7a5fc0b925e8d64975af18bef8c164c71386a8", "extra_info": null, "node_info": {"start": 425871, "end": 428975}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "56b59297-9e5d-4630-a73d-fc2a6fd9397a", "3": "03a6a44f-8504-4045-8836-b5aa2191535c"}}, "__type__": "1"}, "03a6a44f-8504-4045-8836-b5aa2191535c": {"__data__": {"text": "its predecessor, sets this \ufb01eld\nfrom true to false. Thread Crepeats this sequence. (d) T o release the lock, Afollows its next \ufb01eld to its\nsuccessor Band sets B\u2019slocked \ufb01eld to false. It can now reuse its QNode .\n7.6 A Queue Lock with Timeouts 157\nhas space complexity O(L+n). One drawback of the MCSLock algorithm is that\nreleasing a lock requires spinning. Another is that it requires more reads, writes,\nandcompareAndSet() calls than the CLHLock algorithm.\n7.6 A Queue Lock with Timeouts\nThe Java Lock interface includes a tryLock () method that allows the caller to\nspecify a timeout : a maximum duration the caller is willing to wait to acquire the\nlock. If the timeout expires before the caller acquires the lock, the attempt is aban-\ndoned. A Boolean return value indicates whether the lock attempt succeeded.\n(For an explanation why these methods throw InterruptedException , see\nPragma 8.2.2 in Chapter 8.)\nAbandoning a BackoffLock request is trivial, because a thread can simply\nreturn from the tryLock () call. Timing out is wait-free, requiring only a constant\nnumber of steps. By contrast, timing out any of the queue lock algorithms is\nfar from trivial: if a thread simply returns, the threads queued up behind it will\nstarve.\nHere is a bird\u2019s-eye view of a queue lock with timeouts. As in the CLHLock, the\nlock is a virtual queue of nodes, and each thread spins on its predecessor\u2019s node\nwaiting for the lock to be released. As noted, when a thread times out, it can-\nnot simply abandon its queue node, because its successor will never notice when\nthe lock is released. On the other hand, it seems extremely dif\ufb01cult to unlink a\nqueue node without disrupting concurrent lock releases. Instead, we take a lazy\napproach: when a thread times out, it marks its node as abandoned. Its successor\nin the queue, if there is one, notices that the node on which it is spinning has\nbeen abandoned, and starts spinning on the abandoned node\u2019s predecessor. This\napproach has the added advantage that the successor can recycle the abandoned\nnode.\nFig. 7.15 shows the \ufb01elds, constructor, and QNode class for the TOLock (time-\nout lock) class, a queue lock based on the CLHLock class that supports wait-free\ntime-out even for threads in the middle of the list of nodes waiting for the lock.\nWhen a QNode \u2019spred \ufb01eld is null, the associated thread has either not acquired\nthe lock or has not released it yet. When a QNode\u2019s pred \ufb01eld refers to a distin-\nguished, static QNode called AVAILABLE, the associated thread has released the\nlock. Finally, if the pred \ufb01eld refers to some other QNode , the associated thread\nhas abandoned the lock request, so the thread owning the successor node should\nwait on the abandoned node\u2019s predecessor.\nFig. 7.16 shows the TOLock class\u2019s tryLock () and unlock () methods. The\ntryLock () method creates a new QNode with a nullpred \ufb01eld and appends it to\nthe list as in the CLHLock class (Lines 5\u20138). If the lock was free (Line 9), the thread\nenters the critical section. Otherwise, it spins waiting for its predecessor\u2019s QNode \u2019s\npred \ufb01eld to change (Lines 12\u201319). If the predecessor thread times out, it sets\nthepred \ufb01eld to its own predecessor, and the thread spins instead on the new\n158 Chapter 7 Spin Locks and Contention\n1public class TOLock implements", "doc_id": "03a6a44f-8504-4045-8836-b5aa2191535c", "embedding": null, "doc_hash": "2510a92bc383f19fb99c28306f9732b9057bf556cdadf9702f654b73f9f8e168", "extra_info": null, "node_info": {"start": 428963, "end": 432252}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "aec39ccd-c1a3-4439-b0ac-cef5088016bf", "3": "f0f7e263-498f-4d45-9593-b72cd7a12d17"}}, "__type__": "1"}, "f0f7e263-498f-4d45-9593-b72cd7a12d17": {"__data__": {"text": "() method creates a new QNode with a nullpred \ufb01eld and appends it to\nthe list as in the CLHLock class (Lines 5\u20138). If the lock was free (Line 9), the thread\nenters the critical section. Otherwise, it spins waiting for its predecessor\u2019s QNode \u2019s\npred \ufb01eld to change (Lines 12\u201319). If the predecessor thread times out, it sets\nthepred \ufb01eld to its own predecessor, and the thread spins instead on the new\n158 Chapter 7 Spin Locks and Contention\n1public class TOLock implements Lock{\n2 static QNode AVAILABLE = new QNode();\n3 AtomicReference<QNode> tail;\n4 ThreadLocal<QNode> myNode;\n5 public TOLock() {\n6 tail = new AtomicReference<QNode>( null );\n7 myNode = new ThreadLocal<QNode>() {\n8 protected QNode initialValue() {\n9 return new QNode();\n10 }\n11 };\n12 }\n13 ...\n14 static class QNode {\n15 public QNode pred = null ;\n16 }\n17 }\nFigure 7.15 TOLock class: \ufb01elds, constructor, and QNode class.\n1 public boolean tryLock( long time, TimeUnit unit)\n2 throws InterruptedException {\n3 long startTime = System.currentTimeMillis();\n4 long patience = TimeUnit.MILLISECONDS.convert(time, unit);\n5 QNode qnode = new QNode();\n6 myNode.set(qnode);\n7 qnode.pred = null ;\n8 QNode myPred = tail.getAndSet(qnode);\n9 if(myPred == null || myPred.pred == AVAILABLE) {\n10 return true ;\n11 }\n12 while (System.currentTimeMillis() - startTime < patience) {\n13 QNode predPred = myPred.pred;\n14 if(predPred == AVAILABLE) {\n15 return true ;\n16 }else if (predPred != null ) {\n17 myPred = predPred;\n18 }\n19 }\n20 if(!tail.compareAndSet(qnode, myPred))\n21 qnode.pred = myPred;\n22 return false ;\n23 }\n24 public void unlock() {\n25 QNode qnode = myNode.get();\n26 if(!tail.compareAndSet(qnode, null ))\n27 qnode.pred = AVAILABLE;\n28 }\n29 }\nFigure 7.16 TOLock class: tryLock ()andunlock ()methods.\n7.7 A Composite Lock 159\nThread EmyNode myPred\nThread CmyNode myPredtail\nThread AmyNode myPred\nThread D Thread Bavail\nFigure 7.17 Timed-out nodes that must be skipped to acquire the TOLock . Threads Band\nDhave timed out, redirecting their pred \ufb01elds to their predecessors in the list. Thread C\nnotices that B\u2019s \ufb01eld is directed at Aand so it starts spinning on A. Similarly thread Espins\nwaiting for C. When Acompletes and sets its pred toAVAILABLE ,Cwill access the critical\nsection and upon leaving it will set its pred toAVAILABLE , releasing E.\npredecessor. An example of such a sequence appears in Fig. 7.17. Finally, if the\nthread itself times out (Line 20), it attempts to remove its QNode from the list by\napplying compareAndSet() to the tail \ufb01eld. If the compareAndSet() call fails,\nindicating that the thread has a successor, the thread sets its QNode \u2019spred \ufb01eld,\npreviously null, to its predecessor\u2019s QNode , indicating that it has abandoned the\nqueue.\nIn the unlock () method, a thread checks, using compareAndSet() , whether\nit has a successor (Line 26), and if so sets its pred \ufb01eld to AVAILABLE. Note that\nit is", "doc_id": "f0f7e263-498f-4d45-9593-b72cd7a12d17", "embedding": null, "doc_hash": "827abc7bdabc97d3cb00cddeef201386c32165dbc4741b5a21401d292d780b1d", "extra_info": null, "node_info": {"start": 432250, "end": 435135}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "03a6a44f-8504-4045-8836-b5aa2191535c", "3": "05a6c422-7294-4fc6-94f1-bd7d97663266"}}, "__type__": "1"}, "05a6c422-7294-4fc6-94f1-bd7d97663266": {"__data__": {"text": "it attempts to remove its QNode from the list by\napplying compareAndSet() to the tail \ufb01eld. If the compareAndSet() call fails,\nindicating that the thread has a successor, the thread sets its QNode \u2019spred \ufb01eld,\npreviously null, to its predecessor\u2019s QNode , indicating that it has abandoned the\nqueue.\nIn the unlock () method, a thread checks, using compareAndSet() , whether\nit has a successor (Line 26), and if so sets its pred \ufb01eld to AVAILABLE. Note that\nit is not safe to recycle a thread\u2019s old node at this point, since the node may be\nreferenced by its immediate successor, or by a chain of such references. The nodes\nin such a chain can be recycled as soon as a thread skips over the timed-out nodes\nand enters the critical section.\nTheTOLock has many of the advantages of the original CLHLock: local spin-\nning on a cached location and quick detection that the lock is free. It also has\nthe wait-free timeout property of the BackoffLock. However, it has some draw-\nbacks, among them the need to allocate a new node per lock access, and the fact\nthat a thread spinning on the lock may have to go up a chain of timed-out nodes\nbefore it can access the critical section.\n7.7 A Composite Lock\nSpin-lock algorithms impose trade-offs. Queue locks provide \ufb01rst-come-\ufb01rst-\nserved fairness, fast lock release, and low contention, but require nontrivial\nprotocols for recycling abandoned nodes. By contrast, backoff locks support triv-\nial timeout protocols, but are inherently not scalable, and may have slow lock\nrelease if timeout parameters are not well-tuned. In this section, we consider an\nadvanced lock algorithm that combines the best of both approaches.\n160 Chapter 7 Spin Locks and Contention\nConsider the following simple observation: in a queue lock, only the threads\nat the front of the queue need to perform lock handoffs. One way to balance the\nmerits of queue locks versus backoff locks is to keep a small number of wait-\ning threads in a queue on the way to the critical section, and have the rest use\nexponential backoff while attempting to enter this short queue. It is trivial for the\nthreads employing backoff to quit.\nTheCompositeLock class keeps a short, \ufb01xed-size array of lock nodes. Each\nthread that tries to acquire the lock selects a node in the array at random. If\nthat node is in use, the thread backs off (adaptively), and tries again. Once the\nthread acquires a node, it enqueues that node in a TOLock -style queue. The thread\nspins on the preceding node, and when that node\u2019s owner signals it is done, the\nthread enters the critical section. When it leaves, either because it completed or\ntimed-out, it releases ownership of the node, and another backed-off thread may\nacquire it. The tricky part of course, is how to recycle the freed nodes of the array\nwhile multiple threads attempt to acquire control over them.\nThe CompositeLock \u2019s \ufb01elds, constructor, and unlock () method appear in\nFig. 7.18. The waiting \ufb01eld is a constant-size QNode array, and the tail\n\ufb01eld is an AtomicStampedReference<QNode> that combines a reference to\nthe queue tail with a version number needed to avoid the ABA problem on\nupdates (see Pragma 10.6.1 of Chapter 10 for a more detailed explanation of the\n1public class CompositeLock implements Lock{\n2 private static final int SIZE = ...;\n3 private static final int MIN_BACKOFF = ...;\n4 private static final int MAX_BACKOFF = ...;\n5", "doc_id": "05a6c422-7294-4fc6-94f1-bd7d97663266", "embedding": null, "doc_hash": "784b4a2120437793c386ff77393b0b74710631dd731a7865357e8473be832a46", "extra_info": null, "node_info": {"start": 435142, "end": 438533}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f0f7e263-498f-4d45-9593-b72cd7a12d17", "3": "e36c3ba6-f844-49ef-b910-fb37c1b8c050"}}, "__type__": "1"}, "e36c3ba6-f844-49ef-b910-fb37c1b8c050": {"__data__": {"text": "() method appear in\nFig. 7.18. The waiting \ufb01eld is a constant-size QNode array, and the tail\n\ufb01eld is an AtomicStampedReference<QNode> that combines a reference to\nthe queue tail with a version number needed to avoid the ABA problem on\nupdates (see Pragma 10.6.1 of Chapter 10 for a more detailed explanation of the\n1public class CompositeLock implements Lock{\n2 private static final int SIZE = ...;\n3 private static final int MIN_BACKOFF = ...;\n4 private static final int MAX_BACKOFF = ...;\n5 AtomicStampedReference<QNode> tail;\n6 QNode[] waiting;\n7 Random random;\n8 ThreadLocal<QNode> myNode = new ThreadLocal<QNode>() {\n9 protected QNode initialValue() { return null ; };\n10 };\n11 public CompositeLock() {\n12 tail = new AtomicStampedReference<QNode>( null ,0);\n13 waiting = new QNode[SIZE];\n14 for (int i = 0; i < waiting.length; i++) {\n15 waiting[i] = new QNode();\n16 }\n17 random = new Random();\n18 }\n19 public void unlock() {\n20 QNode acqNode = myNode.get();\n21 acqNode.state.set(State.RELEASED);\n22 myNode.set( null );\n23 }\n24 ...\n25 }\nFigure 7.18 TheCompositeLock class: \ufb01elds, constructor, and unlock ()method.\n7.7 A Composite Lock 161\n1 enum State {FREE, WAITING, RELEASED, ABORTED};\n2 class QNode {\n3 AtomicReference<State> state;\n4 QNode pred;\n5 public QNode() {\n6 state = new AtomicReference<State>(State.FREE);\n7 }\n8 }\nFigure 7.19 TheCompositeLock class: the QNode class.\n1 public boolean tryLock( long time, TimeUnit unit)\n2 throws InterruptedException {\n3 long patience = TimeUnit.MILLISECONDS.convert(time, unit);\n4 long startTime = System.currentTimeMillis();\n5 Backoff backoff = new Backoff(MIN_BACKOFF, MAX_BACKOFF);\n6 try {\n7 QNode node = acquireQNode(backoff, startTime, patience);\n8 QNode pred = spliceQNode(node, startTime, patience);\n9 waitForPredecessor(pred, node, startTime, patience);\n10 return true ;\n11 }catch (TimeoutException e) {\n12 return false ;\n13 }\n14 }\nFigure 7.20 TheCompositeLock class: the tryLock ()method.\nAtomicStampedReference<T> class, and Chapter 11 for a more complete dis-\ncussion of the ABA problem4). The tail \ufb01eld is either null or refers to the last\nnode inserted in the queue. Fig. 7.19 shows the QNode class. Each QNode includes\naState \ufb01eld and a reference to the predecessor node in the queue.\nAQNode has four possible states: WAITING ,RELEASED ,ABORTED , orFREE . A\nWAITING node is linked into the queue, and the owning thread is either in the\ncritical section, or waiting to enter. A node becomes RELEASED when its owner\nleaves the critical section and releases the lock. The other two states occur when\na thread abandons its attempt to acquire the lock. If the quitting thread has\nacquired a node but not enqueued it, then it marks the thread as FREE . If the\nnode is enqueued, then it is marked as ABORTED .\nFig. 7.20 shows the tryLock () method. A thread acquires the lock in three\nsteps. First, it acquires a node in the waiting array (Line 7), then it enqueues\nthat node in the queue (Line 8), and \ufb01nally it waits until that node is at the head\nof the queue (Line 9).\n4ABA is typically a", "doc_id": "e36c3ba6-f844-49ef-b910-fb37c1b8c050", "embedding": null, "doc_hash": "d0ed1c2330b2aacaf8574ed9e1bbedcd0a430dac063eff624d8b6dd7838bf7dc", "extra_info": null, "node_info": {"start": 438506, "end": 441555}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "05a6c422-7294-4fc6-94f1-bd7d97663266", "3": "dfd37746-5555-40ca-b73f-c47988f3c3ec"}}, "__type__": "1"}, "dfd37746-5555-40ca-b73f-c47988f3c3ec": {"__data__": {"text": "two states occur when\na thread abandons its attempt to acquire the lock. If the quitting thread has\nacquired a node but not enqueued it, then it marks the thread as FREE . If the\nnode is enqueued, then it is marked as ABORTED .\nFig. 7.20 shows the tryLock () method. A thread acquires the lock in three\nsteps. First, it acquires a node in the waiting array (Line 7), then it enqueues\nthat node in the queue (Line 8), and \ufb01nally it waits until that node is at the head\nof the queue (Line 9).\n4ABA is typically a problem only when using dynamically allocated memory in non garbage col-\nlected languages. We encounter it here because we are implementing a dynamic linked list using\nan array.\n162 Chapter 7 Spin Locks and Contention\n1 private QNode acquireQNode(Backoff backoff, long startTime,\n2 long patience)\n3 throws TimeoutException, InterruptedException {\n4 QNode node = waiting[random.nextInt(SIZE)];\n5 QNode currTail;\n6 while (true ) {\n7 if(node.state.compareAndSet(State.FREE, State.WAITING)) {\n8 return node;\n9 }\n10 currTail = tail.get(currStamp);\n11 State state = node.state.get();\n12 if(state == State.ABORTED || state == State.RELEASED) {\n13 if(node == currTail) {\n14 QNode myPred = null ;\n15 if(state == State.ABORTED) {\n16 myPred = node.pred;\n17 }\n18 if(tail.compareAndSet(currTail, myPred,\n19 currStamp[0], currStamp[0]+1)) {\n20 node.state.set(State.WAITING);\n21 return node;\n22 }\n23 }\n24 }\n25 backoff.backoff();\n26 if(timeout(patience, startTime)) {\n27 throw new TimeoutException();\n28 }\n29 }\n30 }\nFigure 7.21 The CompositeLock class: the acquireQNode ()method.\nThe algorithm for acquiring a node in the waiting array appears in Fig. 7.21 .\nThe thread selects a node at random and tries to acquire the node by changing\nthat node\u2019s state from FREE toWAITING (Line 7). If it fails, it examines the node\u2019s\nstatus. If the node is ABORTED orRELEASED (Line 12), the thread may \u201cclean up\u201d\nthe node. T o avoid synchronization con\ufb02icts with other threads, a node can be\ncleaned up only if it is the last queue node (that is, the value of tail ). If the\ntail node is ABORTED ,tail is redirected to that node\u2019s predecessor; otherwise\ntail is set to null. If, instead, the allocated node is WAITING , then the thread\nbacks off and retries. If the thread times out before acquiring its node, it throws\nTimeoutException (Line 27).\nOnce the thread acquires a node, the spliceQNode () method, shown in\nFig. 7.22 , splices that node into the queue. The thread repeatedly tries to set\ntail to the allocated node. If it times out, it marks the allocated node as FREE\nand throws TimeoutException . If it succeeds, it returns the prior value of tail ,\nacquired by the node\u2019s predecessor in the queue.\n7.7 A Composite Lock 163\n1 private QNode spliceQNode(QNode node, long startTime, long patience)\n2 throws TimeoutException {\n3 QNode currTail;\n4 do{\n5 currTail = tail.get(currStamp);\n6 if(timeout(startTime, patience)) {\n7 node.state.set(State.FREE);\n8 throw new TimeoutException();\n9 }\n10 }while (!tail.compareAndSet(currTail, node,\n11", "doc_id": "dfd37746-5555-40ca-b73f-c47988f3c3ec", "embedding": null, "doc_hash": "5df2c89eb7c61b86d9e73be91721cfd9ee0f64f2bc7dc68e2606df3e098ceff7", "extra_info": null, "node_info": {"start": 441556, "end": 444583}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e36c3ba6-f844-49ef-b910-fb37c1b8c050", "3": "f9d64bbc-270f-4fd1-90de-95935d9bb0b5"}}, "__type__": "1"}, "f9d64bbc-270f-4fd1-90de-95935d9bb0b5": {"__data__": {"text": "the prior value of tail ,\nacquired by the node\u2019s predecessor in the queue.\n7.7 A Composite Lock 163\n1 private QNode spliceQNode(QNode node, long startTime, long patience)\n2 throws TimeoutException {\n3 QNode currTail;\n4 do{\n5 currTail = tail.get(currStamp);\n6 if(timeout(startTime, patience)) {\n7 node.state.set(State.FREE);\n8 throw new TimeoutException();\n9 }\n10 }while (!tail.compareAndSet(currTail, node,\n11 currStamp[0], currStamp[0]+1));\n12 return currTail;\n13 }\nFigure 7.22 TheCompositeLock class: the spliceQNode ()method.\n1 private void waitForPredecessor(QNode pred, QNode node, long startTime,\n2 long patience)\n3 throws TimeoutException {\n4 int[] stamp = {0};\n5 if(pred == null ) {\n6 myNode.set(node);\n7 return ;\n8 }\n9 State predState = pred.state.get();\n10 while (predState != State.RELEASED) {\n11 if(predState == State.ABORTED) {\n12 QNode temp = pred;\n13 pred = pred.pred;\n14 temp.state.set(State.FREE);\n15 }\n16 if(timeout(patience, startTime)) {\n17 node.pred = pred;\n18 node.state.set(State.ABORTED);\n19 throw new TimeoutException();\n20 }\n21 predState = pred.state.get();\n22 }\n23 pred.state.set(State.FREE);\n24 myNode.set(node);\n25 return ;\n26 }\nFigure 7.23 TheCompositeLock class: the waitForPredecessor ()method.\nFinally, once the node has been enqueued, the thread must wait its turn by\ncalling waitForPredecessor () (Fig. 7.23). If the predecessor is null, then the\nthread\u2019s node is \ufb01rst in the queue, so the thread saves the node in the thread-\nlocal myNode \ufb01eld (for later use by unlock ()), and enters the critical section. If\nthe predecessor node is not RELEASED , the thread checks whether it is ABORTED\n164 Chapter 7 Spin Locks and Contention\nmyNode myNode myNode myNode myNode myPred myPred myPred myPred myPred\nThread C\n(waiting onNode 4)Thread B\n(times outholdingNode 4) Thread A\n(in CS)  \nThread D\n(backing offon Node 1) \nThread E\n(backing of f\non Node 4)(a)\n13\nFWnull null null null\nW  A W24\n(b)\nmyNode myNode myPred myNode myPred myNode myPred myNode myPred myPred\nThread C\n(waiting onNode 3)Thread B\n(timed out) Thread A\n(in CS) Thread D\n(backing offon Node 1)  Thread E\n(acquiresNode 4)13\nFWnull null null null\nF  A W24\n(c)\nmyNode myNode myPred myNode myPred myNode myPred myNode myPred myPred\nThread C\n(waiting onNode 3) \nThread B\n(timed out) Thread A\n(in CS)Thread D\n(backing offon Node 1) \nThread E\n(acquiresNode 4) 13\nFWnull null null\nW  F W2v\nFigure 7.24 The CompositeLock class: an execution. In Part (a) thread A(which acquired Node 3) is in the\ncritical section. Thread B(Node 4) is waiting for Ato release the critical section and thread C(Node 1) is\nin turn waiting for B. Threads DandEare backing off, waiting to acquire a node. Node 2 is free. The tail\n\ufb01eld refers to Node 1, the last node to be inserted into the queue. At this point Btimes out, inserting an\nexplicit reference to its", "doc_id": "f9d64bbc-270f-4fd1-90de-95935d9bb0b5", "embedding": null, "doc_hash": "6847c07266edc5e28b07e760445249bd2b3779acf3d779f56a8ffd3ef38ca841", "extra_info": null, "node_info": {"start": 444637, "end": 447464}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "dfd37746-5555-40ca-b73f-c47988f3c3ec", "3": "35c739ee-5096-49db-a454-1dc5daaf17c3"}}, "__type__": "1"}, "35c739ee-5096-49db-a454-1dc5daaf17c3": {"__data__": {"text": "4) 13\nFWnull null null\nW  F W2v\nFigure 7.24 The CompositeLock class: an execution. In Part (a) thread A(which acquired Node 3) is in the\ncritical section. Thread B(Node 4) is waiting for Ato release the critical section and thread C(Node 1) is\nin turn waiting for B. Threads DandEare backing off, waiting to acquire a node. Node 2 is free. The tail\n\ufb01eld refers to Node 1, the last node to be inserted into the queue. At this point Btimes out, inserting an\nexplicit reference to its predecessor, and changing Node 4\u2019s state from WAITING (denoted by W), to ABORTED\n(denoted by A). In Part (b), thread Ccleans up the ABORTED Node 4, setting its state to FREE and following\nthe explicit reference from 4 to 3 (by redirecting its local myPred \ufb01eld). It then starts waiting for A(Node 3)\nto leave the critical section. In Part (c) Eacquires the FREE Node 4, using compareAndSet() to set its state\ntoWAITING . Thread Ethen inserts Node 4 into the queue, using compareAndSet() to swap Node 4 into the\ntail, then waiting on Node 1, which was previously referred to the tail.\n7.7 A Composite Lock 165\n(Line 11). If so, the thread marks the node FREE and waits on the aborted node\u2019s\npredecessor. If the thread times out, then it marks its own node as ABORTED\nand throws TimeoutException . Otherwise, when the predecessor node becomes\nRELEASED the thread marks it FREE , records its own node in the thread-local\nmyNode \ufb01eld, and enters the critical section.\nTheunlock () method ( Fig. 7.18 ) simply retrieves its node from myNode and\nmarks it RELEASED .\nThe CompositeLock has a number of interesting properties. When threads\nback off, they access different locations, reducing contention. Lock hand-off is\nfast, just as in the CLHLock andTOLock algorithms. Abandoning a lock request is\ntrivial for threads in the backoff stage, and relatively straightforward for threads\nthat have acquired queue nodes. For Llocks andnthreads, the CompositeLock\nclass, requires only O(L) space in the worst case, as compared to the TOLock\nclass\u2019sO(L\u0001n). There is one drawback: the CompositeLock class does not guar-\nantee \ufb01rst-come-\ufb01rst-served access.\n7.7.1 A Fast-Path Composite Lock\nAlthough the CompositeLock is designed to perform well under contention,\nperformance in the absence of concurrency is also important. Ideally, for a thread\nrunning alone, acquiring a lock should be as simple as acquiring an uncontended\nTASLock . Unfortunately, in the CompositeLock algorithm, a thread running\nalone must redirect the tail \ufb01eld away from a released node, claim the node,\nand then splice it into the queue.\nAfast path is a shortcut through a complex algorithm taken by a thread run-\nning alone. We can extend the CompositeLock algorithm to encompass a fast\npath in which a solitary thread acquires an idle lock without acquiring a node\nand splicing it into the queue.\nHere is a bird\u2019s-eye view. We add an extra state, distinguishing between a lock\nheld by an ordinary thread and a lock held by a fast-path thread. If a thread\ndiscovers the lock is free, it tries a fast-path acquire. If it succeeds, then it has\nacquired the lock in a single atomic step. If it fails, then it enqueues itself just as\nbefore.\nWe now examine the algorithm in detail. T o reduce code duplication, we\nde\ufb01ne the CompositeFastPathLock class to be a subclass of", "doc_id": "35c739ee-5096-49db-a454-1dc5daaf17c3", "embedding": null, "doc_hash": "90c4f368db4a12baf866c505ad8bba7721c8b5bbc2ccc2a505a12ab2b27bed76", "extra_info": null, "node_info": {"start": 447431, "end": 450739}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f9d64bbc-270f-4fd1-90de-95935d9bb0b5", "3": "cb7d46fc-f4de-4ec7-822b-2d63104ae8f9"}}, "__type__": "1"}, "cb7d46fc-f4de-4ec7-822b-2d63104ae8f9": {"__data__": {"text": "splicing it into the queue.\nHere is a bird\u2019s-eye view. We add an extra state, distinguishing between a lock\nheld by an ordinary thread and a lock held by a fast-path thread. If a thread\ndiscovers the lock is free, it tries a fast-path acquire. If it succeeds, then it has\nacquired the lock in a single atomic step. If it fails, then it enqueues itself just as\nbefore.\nWe now examine the algorithm in detail. T o reduce code duplication, we\nde\ufb01ne the CompositeFastPathLock class to be a subclass of CompositeLock\n(seeFig. 7.25 ).\nWe use a FASTPATH \ufb02ag to indicate that a thread has acquired the lock through\nthe fast path. Because we need to manipulate this \ufb02ag together with the tail\n\ufb01eld\u2019s reference, we \u201csteal\u201d a high-order bit from the tail \ufb01eld\u2019s integer stamp\n(Line 2). The private fastPathLock () method checks whether the tail \ufb01eld\u2019s\nstamp has a clear FASTPATH \ufb02ag and a null reference. If so, it tries to acquire\nthe lock simply by applying compareAndSet() to set the FASTPATH \ufb02ag to true,\nensuring that the reference remains null. An uncontended lock acquisition thus\nrequires a single atomic operation. The fastPathLock () method returns true if\nit succeeds, and false otherwise.\n166 Chapter 7 Spin Locks and Contention\n1public class CompositeFastPathLock extends CompositeLock {\n2 private static final int FASTPATH = ...;\n3 private boolean fastPathLock() {\n4 int oldStamp, newStamp;\n5 int stamp[] = {0};\n6 QNode qnode;\n7 qnode = tail.get(stamp);\n8 oldStamp = stamp[0];\n9 if(qnode != null ) {\n10 return false ;\n11 }\n12 if((oldStamp & FASTPATH) != 0) {\n13 return false ;\n14 }\n15 newStamp = (oldStamp + 1) | FASTPATH;\n16 return tail.compareAndSet(qnode, null , oldStamp, newStamp);\n17 }\n18 public boolean tryLock( long time, TimeUnit unit)\n19 throws InterruptedException {\n20 if(fastPathLock()) {\n21 return true ;\n22 }\n23 if(super .tryLock(time, unit)) {\n24 while ((tail.getStamp() & FASTPATH ) != 0){};\n25 return true ;\n26 }\n27 return false ;\n28 }\nFigure 7.25 CompositeFastPathLock class: the private fastPathLock ()method returns\ntrue if it succeeds in acquiring the lock through the fast path.\n1 private boolean fastPathUnlock() {\n2 int oldStamp, newStamp;\n3 oldStamp = tail.getStamp();\n4 if((oldStamp & FASTPATH) == 0) {\n5 return false ;\n6 } int[] stamp = {0};\n7 QNode qnode;\n8 do{\n9 qnode = tail.get(stamp);\n10 oldStamp = stamp[0];\n11 newStamp = oldStamp & (\u02dcFASTPATH);\n12 }while (!tail.compareAndSet(qnode, qnode, oldStamp, newStamp));\n13 return true ;\n14 }\n15 public void unlock() {\n16 if(!fastPathUnlock()) {\n17 super .unlock();\n18 };\n19 }\nFigure 7.26 CompositeFastPathLock class: fastPathLock ()andunlock ()methods.\n7.8 Hierarchical Locks 167\nThe tryLock () method (Lines 18\u201328) \ufb01rst tries the fast path by calling\nfastPathLock (). If it fails, then it pursues the slow path by calling the\nCompositeLock class\u2019s tryLock () method.", "doc_id": "cb7d46fc-f4de-4ec7-822b-2d63104ae8f9", "embedding": null, "doc_hash": "dc209b60024c282da08ea6b61f6ed2b128deeb33e93c9bed522f6310d2646258", "extra_info": null, "node_info": {"start": 450725, "end": 453572}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "35c739ee-5096-49db-a454-1dc5daaf17c3", "3": "fbc31c6c-d94c-4d57-bb66-71bc64863237"}}, "__type__": "1"}, "fbc31c6c-d94c-4d57-bb66-71bc64863237": {"__data__": {"text": "qnode, oldStamp, newStamp));\n13 return true ;\n14 }\n15 public void unlock() {\n16 if(!fastPathUnlock()) {\n17 super .unlock();\n18 };\n19 }\nFigure 7.26 CompositeFastPathLock class: fastPathLock ()andunlock ()methods.\n7.8 Hierarchical Locks 167\nThe tryLock () method (Lines 18\u201328) \ufb01rst tries the fast path by calling\nfastPathLock (). If it fails, then it pursues the slow path by calling the\nCompositeLock class\u2019s tryLock () method. Before it can return from the slow\npath, however, it must ensure that no other thread holds the fast-path lock by\nwaiting until the FASTPATH \ufb02ag is clear (Line 24).\nThe fastPathUnlock () method returns false if the fast-path \ufb02ag is not set\n(Line 4). Otherwise, it repeatedly tries to clear the \ufb02ag, leaving the reference com-\nponent unchanged (Lines 8\u201312), returning true when it succeeds.\nThe CompositeFastPathLock class\u2019s unlock () method \ufb01rst calls\nfastPathUnlock () (Line 16). If that call fails to release the lock, it then calls\ntheCompositeLock \u2019sunlock () method (Line 17).\n7.8 Hierarchical Locks\nMany of today\u2019s cache-coherent architectures organize processors in clusters ,\nwhere communication within a cluster is signi\ufb01cantly faster than communica-\ntion between clusters. For example, a cluster might correspond to a group of pro-\ncessors that share memory through a fast interconnect, or it might correspond\nto the threads running on a single core in a multicore architecture. We would\nlike to design locks that are sensitive to these differences in locality. Such locks\nare called hierarchical because they take into account the architecture\u2019s memory\nhierarchy and access costs.\nArchitectures can easily have two, three, or more levels of memory hierarchy,\nbut to keep things simple, we assume there are two. We consider an architec-\nture consisting of clusters of processors, where processors in the same cluster\ncommunicate ef\ufb01ciently through a shared cache. Inter-cluster communication is\nsigni\ufb01cantly more expensive than intra-cluster communication.\nWe assume that each cluster has a unique cluster id known to each thread in the\ncluster, available via ThreadID.getCluster() . Threads do not migrate between\nclusters.\n7.8.1 A Hierarchical Backoff Lock\nA test\u2013and\u2013test\u2013and\u2013set lock can easily be adapted to exploit clustering. Suppose\nthe lock is held by thread A. If threads from A\u2019s cluster have shorter backoff\ntimes, then when the lock is released, local threads are more likely to acquire\nthe lock than remote threads, reducing the overall time needed to switch lock\nownership. Fig. 7.27 shows the HBOLock class, a hierarchical backoff lock based\non this principle.\nOne drawback of the HBOLock is that it may be toosuccessful in exploiting\nlocality. There is a danger that threads from the same cluster will repeatedly trans-\nfer the lock among themselves while threads from other clusters starve. Moreover,\nacquiring and releasing the lock invalidates remotely cached copies of the lock\n\ufb01eld, which can be expensive on cache-coherent NUMA architectures.\n168 Chapter 7 Spin Locks and Contention\n1public class HBOLock implements Lock {\n2 private static final int LOCAL_MIN_DELAY = ...;\n3 private static final int LOCAL_MAX_DELAY = ...;\n4 private static final int REMOTE_MIN_DELAY = ...;\n5 private static final int REMOTE_MAX_DELAY = ...;\n6 private static final int FREE =", "doc_id": "fbc31c6c-d94c-4d57-bb66-71bc64863237", "embedding": null, "doc_hash": "925dc1859adde1489f3bf57c93002d830ae180e2fbf2301eef2431dcf4652953", "extra_info": null, "node_info": {"start": 453615, "end": 456933}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "cb7d46fc-f4de-4ec7-822b-2d63104ae8f9", "3": "ae3ecee6-09eb-4a74-9c8b-c3dabbe70915"}}, "__type__": "1"}, "ae3ecee6-09eb-4a74-9c8b-c3dabbe70915": {"__data__": {"text": "clusters starve. Moreover,\nacquiring and releasing the lock invalidates remotely cached copies of the lock\n\ufb01eld, which can be expensive on cache-coherent NUMA architectures.\n168 Chapter 7 Spin Locks and Contention\n1public class HBOLock implements Lock {\n2 private static final int LOCAL_MIN_DELAY = ...;\n3 private static final int LOCAL_MAX_DELAY = ...;\n4 private static final int REMOTE_MIN_DELAY = ...;\n5 private static final int REMOTE_MAX_DELAY = ...;\n6 private static final int FREE = -1;\n7 AtomicInteger state;\n8 public HBOLock() {\n9 state = new AtomicInteger(FREE);\n10 }\n11 public void lock() {\n12 int myCluster = ThreadID.getCluster();\n13 Backoff localBackoff =\n14 new Backoff(LOCAL_MIN_DELAY, LOCAL_MAX_DELAY);\n15 Backoff remoteBackoff =\n16 new Backoff(REMOTE_MIN_DELAY, REMOTE_MAX_DELAY);\n17 while (true ) {\n18 if(state.compareAndSet(FREE, myCluster)) {\n19 return ;\n20 }\n21 int lockState = state.get();\n22 if(lockState == myCluster) {\n23 localBackoff.backoff();\n24 }else {\n25 remoteBackoff.backoff();\n26 }\n27 }\n28 }\n29 public void unlock() {\n30 state.set(FREE);\n31 }\n32 }\nFigure 7.27 TheHBOLock class: a hierarchical backoff lock.\n7.8.2 A Hierarchical CLH Queue Lock\nT o provide a more balanced way to exploit clustering, we now consider the design\nof a hierarchical queue lock. The challenge is to reconcile con\ufb02icting fairness\nrequirements. We would like to favor transferring locks within the same cluster\nto avoid high communication costs, but we also want to ensure some degree of\nfairness, so that remote lock requests are not excessively postponed in favor of\nlocal requests. We balance these demands by scheduling sequences of requests\nfrom the same cluster together.\nTheHCLHLock queue lock (Fig. 7.28) consists of a collection of local queues ,\none per cluster, and a single global queue . Each queue is a linked list of nodes,\nwhere the links are implicit, in the sense that they are held in thread-local \ufb01elds,\nmyQNode andmyPred .\nWe say that a thread owns itsmyQNode node. For any node in a queue\n(other than at the head), its predecessor is its owner\u2019s myPred node. Fig. 7.30\n7.8 Hierarchical Locks 169\n(a)\n(b)\n(c)localQueue\nmyNode myNode myPred1\nThread BB: insert to local\nC: release lock, recycle qnode1\nmyPred\nThread A (master)\n1T 1\nmyNode myPred myNode myPred myNode myPred myNode myPred\nThread B Thread A Thread D Thread C0globalQueue\n1T 0localQueue\n1T 1\nmyNode myPred\nThread A (master) Thread B\n0globalQueue\n1T 1A: splice to globalmyPred myNode\nmyNode myNode myPred myPred\nThread D Thread C\nFigure 7.28 Lock acquisition and release in a HCLHLock . The successorMustWait \ufb01eld is marked in the\nnodes by a 0 (for false) or a 1 (for true). A node is marked as a local tail when it is being spliced by adding the\nsymbol T. In Part (a), Binserts its node into the local queue. In Part (b), Asplices the local queue containing A\nandB\u2019s nodes onto the global queue, which already contains CandD\u2019s nodes. In Part (c), Creleases the lock\nby setting its node\u2019s successorMustWait \ufb02ag to false, and then setting", "doc_id": "ae3ecee6-09eb-4a74-9c8b-c3dabbe70915", "embedding": null, "doc_hash": "cac1701151ffab641f6663088955287ea0cd7a7ecd66d661bf52851a02566508", "extra_info": null, "node_info": {"start": 456883, "end": 459910}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "fbc31c6c-d94c-4d57-bb66-71bc64863237", "3": "489d0952-c652-4455-8647-bdff613d627d"}}, "__type__": "1"}, "489d0952-c652-4455-8647-bdff613d627d": {"__data__": {"text": "acquisition and release in a HCLHLock . The successorMustWait \ufb01eld is marked in the\nnodes by a 0 (for false) or a 1 (for true). A node is marked as a local tail when it is being spliced by adding the\nsymbol T. In Part (a), Binserts its node into the local queue. In Part (b), Asplices the local queue containing A\nandB\u2019s nodes onto the global queue, which already contains CandD\u2019s nodes. In Part (c), Creleases the lock\nby setting its node\u2019s successorMustWait \ufb02ag to false, and then setting myQNode to the predecessor node.\n170 Chapter 7 Spin Locks and Contention\n1public class HCLHLock implements Lock {\n2 static final int MAX_CLUSTERS = ...;\n3 List<AtomicReference<QNode>> localQueues;\n4 AtomicReference<QNode> globalQueue;\n5 ThreadLocal<QNode> currNode = new ThreadLocal<QNode>() {\n6 protected QNode initialValue() { return new QNode(); };\n7 };\n8 ThreadLocal<QNode> predNode = new ThreadLocal<QNode>() {\n9 protected QNode initialValue() { return null ; };\n10 };\n11 public HCLHLock() {\n12 localQueues = new ArrayList<AtomicReference<QNode>>(MAX_CLUSTERS);\n13 for (int i = 0; i < MAX_CLUSTERS; i++) {\n14 localQueues.add( new AtomicReference <QNode>());\n15 }\n16 QNode head = new QNode();\n17 globalQueue = new AtomicReference<QNode>(head);\n18 }\nFigure 7.29 TheHCLHLock class: \ufb01elds and constructor.\n1 class QNode {\n2 // private boolean tailWhenSpliced;\n3 private static final int TWS_MASK = 0x80000000;\n4 // private boolean successorMustWait = false;\n5 private static final int SMW_MASK = 0x40000000;\n6 // private int clusterID;\n7 private static final int CLUSTER_MASK = 0x3FFFFFFF;\n8 AtomicInteger state;\n9 public QNode() {\n10 state = new AtomicInteger(0);\n11 }\n12 public void unlock() {\n13 int oldState = 0;\n14 int newState = ThreadID.getCluster();\n15 // successorMustWait = true;\n16 newState |= SMW_MASK;\n17 // tailWhenSpliced = false;\n18 newState &= (\u02dcTWS_MASK);\n19 do{\n20 oldState = state.get();\n21 }while (! state.compareAndSet(oldState, newState));\n22 }\n23 public int getClusterID() {\n24 return state.get() & CLUSTER_MASK;\n25 }\n26 // other getters and setters omitted.\n27 }\nFigure 7.30 TheHCLHLock class: the inner QNode class.\n7.8 Hierarchical Locks 171\nshows the QNode class. Each node has three virtual \ufb01elds: the current (or most\nrecent) owner\u2019s ClusterId , and two Boolean \ufb01elds, successorMustWait and\ntailWhenSpliced . These \ufb01elds are virtual in the sense that they need to be\nupdated atomically, so we represent them as bit-\ufb01elds in an AtomicInteger\n\ufb01eld, using simple masking and shifting operations to extract their values.\nThe tailWhenSpliced \ufb01eld indicates whether the node is the last node in the\nsequence currently being spliced onto the global queue. The successorMustWait\n\ufb01eld is the same as in the original CLH algorithm: it is set to true before\nbeing enqueued, and set to false by the node\u2019s owner on releasing the lock.\nThus, a thread waiting to acquire the lock may proceed when its predeces-\nsor\u2019s successorMustWait \ufb01eld becomes false . Because we need to update", "doc_id": "489d0952-c652-4455-8647-bdff613d627d", "embedding": null, "doc_hash": "9ec251e82662f31b78bc0a0b927e00b5205aa4758cab32b5fb7388fae3b3b702", "extra_info": null, "node_info": {"start": 459929, "end": 462918}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ae3ecee6-09eb-4a74-9c8b-c3dabbe70915", "3": "6bdd7f73-78a1-4fa1-a1ba-46c94495e6cb"}}, "__type__": "1"}, "6bdd7f73-78a1-4fa1-a1ba-46c94495e6cb": {"__data__": {"text": "using simple masking and shifting operations to extract their values.\nThe tailWhenSpliced \ufb01eld indicates whether the node is the last node in the\nsequence currently being spliced onto the global queue. The successorMustWait\n\ufb01eld is the same as in the original CLH algorithm: it is set to true before\nbeing enqueued, and set to false by the node\u2019s owner on releasing the lock.\nThus, a thread waiting to acquire the lock may proceed when its predeces-\nsor\u2019s successorMustWait \ufb01eld becomes false . Because we need to update these\n\ufb01elds atomically, they are private, accessed indirectly through synchronized\nmethods.\nFig. 7.28 illustrates how the HCLHLock class acquires and releases a lock. The\nlock () method \ufb01rst adds the thread\u2019s node to the local queue, and then waits\nuntil either the thread can enter the critical section or its node is at the head of\nthe local queue. In the latter case, we say the thread is the cluster master , and it is\nresponsible for splicing the local queue onto the global queue.\nThe code for the lock () method appears in Fig. 7.31 . The thread\u2019s node has\nbeen initialized so that successorMustWait istrue,tailWhenSpliced isfalse ,\nand the ClusterId \ufb01eld is the caller\u2019s cluster. The thread then adds its node to\nthe end (tail) of its local cluster\u2019s queue, using compareAndSet() to change the\ntail to its node (Line 9). Upon success, the thread sets its myPred to the node it\nreplaced as the tail. We call this node the predecessor .\nThe thread then calls waitForGrantOrClusterMaster () (Line 11), which\ncauses the thread to spin until one of the following conditions is true:\n1.the predecessor node is from the same cluster, and tailWhenSpliced and\nsuccessorMustWait are both false , or\n2.the predecessor node is from a different cluster or the predecessor\u2019s \ufb02ag\ntailWhenSpliced istrue.\nIn the \ufb01rst case, the thread\u2019s node is at the head of the global queue, so it enters\nthe critical section and returns (Line 14). In the second case, as explained here,\nthe thread\u2019s node is at the head of the local queue, so the thread is the clus-\nter master, making it responsible for splicing the local queue onto the global\nqueue. (If there is no predecessor, that is, if the local queue\u2019s tail is null, then the\nthread becomes the cluster master immediately.) Most of the spinning required\nbywaitForGrantOrClusterMaster () is local and incurs little or no communi-\ncation cost.\nOtherwise, either the predecessor\u2019s cluster is different from the thread\u2019s, or\nthe predecessor\u2019s tailWhenSpliced \ufb02ag is true. If the predecessor belongs to a\ndifferent cluster, then it cannot be in this thread\u2019s local queue. The predecessor\nmust have already been moved to the global queue and recycled to a thread in a\ndifferent cluster. On the other hand, if the predecessor\u2019s tailWhenSpliced \ufb02ag\n172 Chapter 7 Spin Locks and Contention\n1 public void lock() {\n2 QNode myNode = currNode.get();\n3 AtomicReference<QNode> localQueue = localQueues.get\n4 (ThreadID.getCluster());\n5 // splice my QNode into local queue\n6 QNode myPred = null ;\n7 do{\n8 myPred = localQueue.get();\n9 }while (!localQueue.compareAndSet(myPred, myNode));\n10 if(myPred != null ) {\n11 boolean iOwnLock = myPred.waitForGrantOrClusterMaster();\n12 if(iOwnLock) {\n13 predNode.set(myPred);\n14 return ;\n15 }\n16 }\n17 // I am the", "doc_id": "6bdd7f73-78a1-4fa1-a1ba-46c94495e6cb", "embedding": null, "doc_hash": "5662bee0e0563acde090bef25410287e02da36d556faee885986caa6b68c6929", "extra_info": null, "node_info": {"start": 462884, "end": 466175}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "489d0952-c652-4455-8647-bdff613d627d", "3": "745912f2-28f4-4e90-a136-d37204c8e3fe"}}, "__type__": "1"}, "745912f2-28f4-4e90-a136-d37204c8e3fe": {"__data__": {"text": "AtomicReference<QNode> localQueue = localQueues.get\n4 (ThreadID.getCluster());\n5 // splice my QNode into local queue\n6 QNode myPred = null ;\n7 do{\n8 myPred = localQueue.get();\n9 }while (!localQueue.compareAndSet(myPred, myNode));\n10 if(myPred != null ) {\n11 boolean iOwnLock = myPred.waitForGrantOrClusterMaster();\n12 if(iOwnLock) {\n13 predNode.set(myPred);\n14 return ;\n15 }\n16 }\n17 // I am the cluster master: splice local queue into global queue.\n18 QNode localTail = null ;\n19 do{\n20 myPred = globalQueue.get();\n21 localTail = localQueue.get();\n22 }while (!globalQueue.compareAndSet(myPred, localTail));\n23 // inform successor it is the new master\n24 localTail.setTailWhenSpliced( true );\n25 while (myPred.isSuccessorMustWait()) {};\n26 predNode.set(myPred);\n27 return ;\n28 }\nFigure 7.31 The HCLHLock class: lock ()method. As in the CLHLock ,lock ()saves the pre-\ndecessor\u2019s recently released node to be used for next lock acquisition attempt.\nistrue, then the predecessor node was the last that moved to the global queue,\nand therefore the thread\u2019s node is now at the head of the local queue. It can-\nnot have been moved to the global queue because only the cluster master, the\nthread whose node is at the head of the local queue, moves nodes onto the global\nqueue.\nAs cluster master, a thread\u2019s role is to splice the nodes accumulated in the\nlocal queue onto the global queue. The threads in the local queue spin, each on\nits predecessor\u2019s node. The cluster master reads the local queue\u2019s tail and calls\ncompareAndSet() to change the global queue\u2019s tail to the node it saw at the tail\nof its local queue (Line 22). When it succeeds, myPred is the tail of the global\nqueue that it replaced (Line 20). It then sets to true thetailWhenSpliced \ufb02ag\nof the last node it spliced onto the global queue (Line 24), indicating to that\nnode\u2019s (local) successor that it is now the head of the local queue. This sequence\nof operations transfers the local nodes (up to the local tail) into the CLH-style\nglobal queue in the same order as in the local queue.\n7.10 Chapter Notes 173\n29 public void unlock() {\n30 QNode myNode = currNode.get();\n31 myNode.setSuccessorMustWait( false );\n32 QNode node = predNode.get();\n33 node.unlock();\n34 currNode.set(node);\n35 }\nFigure 7.32 The HCLHLock class: unlock ()method. This method promotes the node saved\nby the lock ()operation and initializes the QNode to be used in the next lock acquisition\nattempt.\nOnce in the global queue, the cluster master acts as though it were in an\nordinary CLHLock queue, entering the critical section when its (new) prede-\ncessor\u2019s successorMustWait \ufb01eld is false (Line 25). The other threads whose\nnodes were spliced in are not aware that anything has changed, so they con-\ntinue spinning as before. Each will enter the critical section when its predecessor\u2019s\nsuccessorMustWait \ufb01eld becomes false .\nAs in the original CLHLock algorithm, a thread releases the lock by setting\nits node\u2019s successorMustWait \ufb01eld to false (Fig. 7.32 ). When unlocking, the\nthread saves its predecessor\u2019s node to be used in its next lock acquisition attempt\n(Line 34).\nTheHCLHLock lock favors sequences of local threads, one waiting for the other,\nwithin the waiting list", "doc_id": "745912f2-28f4-4e90-a136-d37204c8e3fe", "embedding": null, "doc_hash": "b9433804affe4b05f52b433d8986808815d6692fba24dc835d0c4b912983c688", "extra_info": null, "node_info": {"start": 466265, "end": 469475}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6bdd7f73-78a1-4fa1-a1ba-46c94495e6cb", "3": "26b8738f-df9e-4842-9c76-5feba40d521e"}}, "__type__": "1"}, "26b8738f-df9e-4842-9c76-5feba40d521e": {"__data__": {"text": "has changed, so they con-\ntinue spinning as before. Each will enter the critical section when its predecessor\u2019s\nsuccessorMustWait \ufb01eld becomes false .\nAs in the original CLHLock algorithm, a thread releases the lock by setting\nits node\u2019s successorMustWait \ufb01eld to false (Fig. 7.32 ). When unlocking, the\nthread saves its predecessor\u2019s node to be used in its next lock acquisition attempt\n(Line 34).\nTheHCLHLock lock favors sequences of local threads, one waiting for the other,\nwithin the waiting list in the global queue. As with the CLHLock lock, the use of\nimplicit references minimizes cache misses, and threads spin on locally cached\ncopies of their successor\u2019s node state.\n7.9 One Lock T o Rule Them All\nIn this chapter, we have seen a variety of spin locks that vary in characteristics\nand performance. Such a variety is useful, because no single algorithm is ideal\nfor all applications. For some applications, complex algorithms work best, and\nfor others, simple algorithms are preferable. The best choice usually depends on\nspeci\ufb01c aspects of the application and the target architecture.\n7.10 Chapter Notes\nTheTTASLock is due to Larry Rudolph and Zary Segall [ 133]. Exponential back\noff is a well-known technique used in Ethernet routing, presented in the context\nof multiprocessor mutual exclusion by Anant Agarwal and Mathews Cherian [ 6].\n174 Chapter 7 Spin Locks and Contention\nT om Anderson [14] invented the ALock algorithm and was one of the \ufb01rst to\nempirically study the performance of spin locks in shared memory multipro-\ncessors. The MCSLock , due to John Mellor-Crummey and Michael Scott [113],\nis perhaps the best-known queue lock algorithm. T oday\u2019s Java Virtual Machines\nuse object synchronization based on simpli\ufb01ed monitor algorithms such as the\nThinlock of David Bacon, Ravi Konuru, Chet Murthy, and Mauricio Serrano [17],\ntheMetalock of Ole Agesen, Dave Detlefs, Alex Garthwaite, Ross Knippel, Y. S.\nRamakrishna and Derek White [7], or the RelaxedLock of Dave Dice [31]. All\nthese algorithms are variations of the MCSLock lock.\nTheCLHLock lock is due to Travis Craig, Erik Hagersten, and Anders Landin\n[30, 110]. The TOLock with nonblocking timeout is due to Bill Scherer and\nMichael Scott [138, 139]. The CompositeLock and its variations are due to Viren-\ndra Marathe, Mark Moir, and Nir Shavit [120]. The notion of using a fast-path\nin a mutual exclusion algorithm is due to Leslie Lamport [95]. Hierarchical locks\nwere invented by Zoran Radovi \u00b4c and Erik Hagersten. The HBOLock is a variant of\ntheir original algorithm [130] and the particular HCLHLock presented here is due\nto Victor Luchangco, Daniel Nussbaum, and Nir Shavit [109].\nDanny Hendler, Faith Fich, and Nir Shavit [39] have extended the work of\nJim Burns and Nancy Lynch to show that any starvation-free mutual exclusion\nalgorithm requires \u00d2(n) space, even if strong operations such as getAndSet ()\norcompareAndSet() are used, implying that all the queue-lock algorithms\nconsidered here are space-optimal.\nThe schematic performance graph in this chapter is loosely based on empirical\nstudies by T om Anderson [14], as well as on data collected by the authors on\nvarious modern machines. We chose to use schematics rather than actual data\nbecause of the great variation in machine", "doc_id": "26b8738f-df9e-4842-9c76-5feba40d521e", "embedding": null, "doc_hash": "97369fa0c5088ca9e14fd1c6b78ffbc0adc873c24d79e5f48fde09a6d5eae9e5", "extra_info": null, "node_info": {"start": 469397, "end": 472672}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "745912f2-28f4-4e90-a136-d37204c8e3fe", "3": "11161654-d461-4b4c-b381-d5599d0d625c"}}, "__type__": "1"}, "11161654-d461-4b4c-b381-d5599d0d625c": {"__data__": {"text": "have extended the work of\nJim Burns and Nancy Lynch to show that any starvation-free mutual exclusion\nalgorithm requires \u00d2(n) space, even if strong operations such as getAndSet ()\norcompareAndSet() are used, implying that all the queue-lock algorithms\nconsidered here are space-optimal.\nThe schematic performance graph in this chapter is loosely based on empirical\nstudies by T om Anderson [14], as well as on data collected by the authors on\nvarious modern machines. We chose to use schematics rather than actual data\nbecause of the great variation in machine architectures and their signi\ufb01cant effect\non lock performance.\nThe Sherlock Holmes quote is from The Sign of Four [36].\n7.11 Exercises\nExercise 85. Fig. 7.33 shows an alternative implementation of CLHLock in which\na thread reuses its own node instead of its predecessor node. Explain how this\nimplementation can go wrong.\nExercise 86. Imaginenthreads, each of which executes method foo() followed\nby method bar() . Suppose we want to make sure that no thread starts bar()\nuntil all threads have \ufb01nished foo() . For this kind of synchronization, we place\nabarrier between foo() andbar() .\nFirst barrier implementation: We have a counter protected by a test\u2013and\u2013test\u2013\nand\u2013set lock. Each thread locks the counter, increments it, releases the lock, and\nspins, rereading the counter until it reaches n.\n7.11 Exercises 175\n1public class BadCLHLock implements Lock {\n2 // most recent lock holder\n3 AtomicReference<Qnode> tail = new Qnode();\n4 // thread-local variable\n5 ThreadLocal<Qnode> myNode;\n6 public void lock() {\n7 Qnode qnode = myNode.get();\n8 qnode.locked = true ; // I\u2019m not done\n9 // Make me the new tail, and find my predecessor\n10 Qnode pred = tail.getAndSet(qnode);\n11 // spin while predecessor holds lock\n12 while (pred.locked) {}\n13 }\n14 public void unlock() {\n15 // reuse my node next time\n16 myNode.get().locked = false ;\n17 }\n18 static class Qnode { // Queue node inner class\n19 public boolean locked = false ;\n20 }\n21 }\nFigure 7.33 An incorrect attempt to implement a CLHLock .\nSecond barrier implementation: We have an n-element array b[0::n\u00001], all 0.\nThread zero sets b[0] to 1. Every thread i, for 0 <i<n\u00001, spins until b[i\u00001] is\n1, sets b[i] to 1, and waits until b[i+ 1] becomes 2, at which point it proceeds to\nleave the barrier. Thread n\u00001, upon detecting that b[n\u00002] is 1, sets b[n\u00001] to\n2 and leaves the barrier.\nCompare (in ten lines) the behavior of these two implementations on a bus-\nbased cache-coherent architecture. Explain which approach you expect will per-\nform better under low load and high load.\nExercise 87. Prove that the CompositeFastPathLock implementation guarantees\nmutual exclusion, but is not starvation-free.\nExercise 88. In the HCLHLock lock, for a given cluster master thread, in the interval\nbetween setting the global tail reference and raising the tailWhenSpliced \ufb02ag of\nthe last spliced node, the nodes spliced onto the global queue are in both its local\nqueue and the global queue. Explain why the algorithm is still correct.\nExercise 89. Notice that, in the HCLHLock lock, what will happen if the time\nbetween becoming cluster master and successfully splicing the local queue into\nthe global queue is too small? Suggest a remedy to this problem.\nExercise 90. Why is it important that the \ufb01elds of the State", "doc_id": "11161654-d461-4b4c-b381-d5599d0d625c", "embedding": null, "doc_hash": "a2202f2c3629aeef7d249459aeb025a24c343b4acc5f280a66b196ae5cb1b8e4", "extra_info": null, "node_info": {"start": 472623, "end": 475943}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "26b8738f-df9e-4842-9c76-5feba40d521e", "3": "c3eafbcc-4e46-46a2-90ff-6f8631632732"}}, "__type__": "1"}, "c3eafbcc-4e46-46a2-90ff-6f8631632732": {"__data__": {"text": "lock, for a given cluster master thread, in the interval\nbetween setting the global tail reference and raising the tailWhenSpliced \ufb02ag of\nthe last spliced node, the nodes spliced onto the global queue are in both its local\nqueue and the global queue. Explain why the algorithm is still correct.\nExercise 89. Notice that, in the HCLHLock lock, what will happen if the time\nbetween becoming cluster master and successfully splicing the local queue into\nthe global queue is too small? Suggest a remedy to this problem.\nExercise 90. Why is it important that the \ufb01elds of the State object accessed\nby the HCLHLock lock\u2019s waitForGrantOrClusterMaster () method be read and\nmodi\ufb01ed atomically? Provide the code for the HCLHLock lock\u2019s\nwaitForGrantOrClusterMaster () method. Does your implementation require\n176 Chapter 7 Spin Locks and Contention\nthe use of a compareAndSet() , and if so, can it be implemented ef\ufb01ciently with-\nout it?\nExercise 91. Design an isLocked () method that tests whether any thread is hold-\ning a lock (but does not acquire the lock). Give implementations for\n\u0004AnytestAndSet () spin lock\n\u0004The CLH queue lock, and\n\u0004The MCS queue lock.\nExercise 92. (Hard) Where does the \u00d2(n) space complexity lower bound proof\nfor deadlock-free mutual exclusion of Chapter 2 break when locks are allowed to\nuse read\u2013modify\u2013write operations?\n8Monitors and Blocking\nSynchronization\n8.1 Introduction\nMonitors are a structured way of combining synchronization and data. A class\nencapsulates both data and methods in the same way that a monitor combines\ndata, methods, and synchronization in a single modular package.\nHere is why modular synchronization is important. Let us imagine our appli-\ncation has two threads, a producer and a consumer, that communicate through\na shared FIFO queue. We could have the threads share two objects: an unsyn-\nchronized queue, and a lock to protect the queue. The producer looks something\nlike this:\nmutex.lock();\ntry {\nqueue.enq(x)\n}finally {\nmutex.unlock();\n}\nThis is no way to run a railroad. Suppose the queue is bounded, meaning that an\nattempt to add an item to a full queue cannot proceed until the queue has room.\nHere, the decision whether to block the call or to let it proceed depends on the\nqueue\u2019s internal state, which is (and should be) inaccessible to the caller. Even\nworse, suppose the application grows to have multiple producers, consumers, or\nboth. Each such thread must keep track of both the lock and the queue objects,\nand the application will be correct only if each thread follows the same locking\nconventions.\nA more sensible approach is to allow each queue to manage its own synchro-\nnization. The queue itself has its own internal lock, acquired by each method\nwhen it is called and released when it returns. There is no need to ensure that\nevery thread that uses the queue follows a cumbersome synchronization protocol.\nIf a thread tries to enqueue an item to a queue that is already full, then the enq()\nmethod itself can detect the problem, suspend the caller, and resume the caller\nwhen the queue has room.\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00008-3\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.177\n178 Chapter 8 Monitors and Blocking Synchronization\n8.2 Monitor Locks and Conditions\nJust as in Chapters 2 and 7, a Lock is the basic mechanism for ensuring mutual\nexclusion. Only one thread at a time can hold a lock. A thread acquires a lock\nwhen it \ufb01rst starts to hold the lock. A thread releases a lock when", "doc_id": "c3eafbcc-4e46-46a2-90ff-6f8631632732", "embedding": null, "doc_hash": "8210124f0e5ddfba2d89ad6262b4de575c5f5c637db34616da5391be2867a2bd", "extra_info": null, "node_info": {"start": 475939, "end": 479457}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "11161654-d461-4b4c-b381-d5599d0d625c", "3": "d8164c4b-ed1e-4fa4-9989-7b98213ec7c2"}}, "__type__": "1"}, "d8164c4b-ed1e-4fa4-9989-7b98213ec7c2": {"__data__": {"text": "the caller\nwhen the queue has room.\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00008-3\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.177\n178 Chapter 8 Monitors and Blocking Synchronization\n8.2 Monitor Locks and Conditions\nJust as in Chapters 2 and 7, a Lock is the basic mechanism for ensuring mutual\nexclusion. Only one thread at a time can hold a lock. A thread acquires a lock\nwhen it \ufb01rst starts to hold the lock. A thread releases a lock when it stops holding\nthe lock. A monitor exports a collection of methods, each of which acquires the\nlock when it is called, and releases it when it returns.\nIf a thread cannot immediately acquire a lock, it can either spin, repeatedly\ntesting whether the desired event has happened, or it can block , giving up the\nprocessor for a while to allow another thread to run.1Spinning makes sense on\na multiprocessor if we expect to wait for a short time, because blocking a thread\nrequires an expensive call to the operating system. On the other hand, blocking\nmakes sense if we expect to wait for a long time, because a spinning thread keeps\na processor busy without doing any work.\nFor example, a thread waiting for another thread to release a lock should\nspin if that particular lock is held brie\ufb02y, while a consumer thread waiting to\ndequeue an item from an empty buffer should block, since there is usually no\nway to predict how long it may have to wait. Often, it makes sense to combine\nspinning and blocking: a thread waiting to dequeue an item might spin for a brief\nduration, and then switch to blocking if the delay appears to be long. Blocking\nworks on both multiprocessors and uniprocessors, while spinning works only on\nmultiprocessors.\nPragma 8.2.1. Most of the locks in this book follow the interface shown in\nFig. 8.1. Here is an explanation of the Lock interface\u2019s methods:\n\u0004Thelock () method blocks the caller until it acquires the lock.\n\u0004The lockInterruptibly () method acts like lock (), but throws\nan exception if the thread is interrupted while it is waiting. (See\nPragma 8.2.2.)\n\u0004Theunlock () method releases the lock.\n\u0004The newCondition () method is a factory that creates and returns a\nCondition object associated with the lock (explained below.)\n\u0004The tryLock () method acquires the lock if it is free, and immediately\nreturns a Boolean indicating whether it acquired the lock. This method\ncan also be called with a timeout.\n1Elsewhere we make a distinction between blocking and nonblocking synchronization algorithms.\nThere, we mean something entirely different: a blocking algorithm is one where a delay by one\nthread can cause a delay in another.\n8.2 Monitor Locks and Conditions 179\n1 public interface Lock {\n2 void lock();\n3 void lockInterruptibly() throws InterruptedException;\n4 boolean tryLock();\n5 boolean tryLock( long time, TimeUnit unit);\n6 Condition newCondition();\n7 void unlock();\n8 }\nFigure 8.1 TheLock Interface.\n8.2.1 Conditions\nWhile a thread is waiting for something to happen, say, for another thread to\nplace an item in a queue, it is a very good idea to release the lock on the queue,\nbecause otherwise the other thread will never be able to enqueue the anticipated\nitem. After the waiting thread has released the lock, it needs a way to be noti\ufb01ed\nwhen to reacquire the lock and try again.\nIn the Java concurrency package (and in related packages such as Pthreads),\nthe ability to release a lock temporarily is provided by a Condition object associ-\nated with a lock. Fig. 8.2 shows the use of the Condition interface provided in the\njava.util.concurrent.locks library. A condition is associated with a lock, and is", "doc_id": "d8164c4b-ed1e-4fa4-9989-7b98213ec7c2", "embedding": null, "doc_hash": "f27dc957b59271406979cf2cc4d05ec70ac23f15484c27a1744ab9075ca5efa6", "extra_info": null, "node_info": {"start": 479532, "end": 483170}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "c3eafbcc-4e46-46a2-90ff-6f8631632732", "3": "203bcb74-47cf-4eab-b80b-71bfce10048a"}}, "__type__": "1"}, "203bcb74-47cf-4eab-b80b-71bfce10048a": {"__data__": {"text": "queue, it is a very good idea to release the lock on the queue,\nbecause otherwise the other thread will never be able to enqueue the anticipated\nitem. After the waiting thread has released the lock, it needs a way to be noti\ufb01ed\nwhen to reacquire the lock and try again.\nIn the Java concurrency package (and in related packages such as Pthreads),\nthe ability to release a lock temporarily is provided by a Condition object associ-\nated with a lock. Fig. 8.2 shows the use of the Condition interface provided in the\njava.util.concurrent.locks library. A condition is associated with a lock, and is cre-\nated by calling that lock\u2019s newCondition () method. If the thread holding that lock\ncalls the associated condition\u2019s await () method, it releases that lock and suspends\nitself, giving another thread the opportunity to acquire the lock. When the calling\nthread awakens, it reacquires the lock, perhaps competing with other threads.\nPragma 8.2.2. Threads in Java can be interrupted by other threads. If a thread\nis interrupted during a call to a Condition \u2019sawait () method, then the\ncall throws InterruptedException . The proper response to an interrupt\nis application-dependent. (It is not good programming practice simply to\nignore interrupts).\nFig. 8.2 shows a schematic example\n1 Condition condition = mutex.newCondition();\n2 ...\n3 mutex.lock()\n4 try {\n5 while (!property) { // not happy\n6 condition.await(); // wait for property\n7 } catch (InterruptedException e) {\n8 ... // application-dependent response\n9 }\n10 ... // happy: property must hold\n11 }\nFigure 8.2 How to use Condition objects.\n180 Chapter 8 Monitors and Blocking Synchronization\nT o avoid clutter, we usually omit InterruptedException handlers from\nexample code, even though they would be required in actual code.\nLike locks, Condition objects must be used in a stylized way. Suppose a thread\nwants to wait until a certain property holds. The thread tests the property while\nholding the lock. If the property does not hold, then the thread calls await () to\nrelease the lock and sleep until it is awakened by another thread. Here is the key\npoint: there is no guarantee that the property will hold at the time the thread\nawakens. The await () method can return spuriously (i.e., for no reason), or the\nthread that signaled the condition may have awakened too many sleeping threads.\nWhatever the reason, the thread must retest the property, and if it \ufb01nds the prop-\nerty still does not hold, it must call await () again.\nThe Condition interface in Fig. 8.3 provides several variations of this call,\nsome of which provide the ability to specify a maximum time the caller can be\nsuspended, or whether the thread can be interrupted while it is waiting. When\nthe queue changes, the thread that made the change can notify other threads\nwaiting on a condition. Calling signal () wakes up one thread waiting on a con-\ndition, while calling signalAll () wakes up all waiting threads. Fig. 8.4 describes\na schematic execution of a monitor lock.\nFig. 8.5 shows how to implement a bounded FIFO queue using explicit locks\nand conditions. The lock \ufb01eld is a lock that must be acquired by all meth-\nods. We must initialize it to hold an instance of a class that implements the\nLock interface. Here, we choose ReentrantLock , a useful lock type provided by\nthejava.util.concurrent.locks package. As discussed in Section 8.4, this lock is\nreentrant : a thread that is holding the lock can acquire it again without blocking.\nThere are two condition objects: notEmpty noti\ufb01es waiting dequeuers when\nthe queue goes from being empty to nonempty, and notFull for the opposite\ndirection. Using two conditions instead of one is more ef\ufb01cient, since fewer\nthreads are", "doc_id": "203bcb74-47cf-4eab-b80b-71bfce10048a", "embedding": null, "doc_hash": "bffdb8264a30aa8d1e4658e45a5199b85bbc95478191e0c8897a2bd4e55936bc", "extra_info": null, "node_info": {"start": 483083, "end": 486795}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d8164c4b-ed1e-4fa4-9989-7b98213ec7c2", "3": "c04c1f25-324a-4953-a92c-ec6c3f58a2f6"}}, "__type__": "1"}, "c04c1f25-324a-4953-a92c-ec6c3f58a2f6": {"__data__": {"text": "We must initialize it to hold an instance of a class that implements the\nLock interface. Here, we choose ReentrantLock , a useful lock type provided by\nthejava.util.concurrent.locks package. As discussed in Section 8.4, this lock is\nreentrant : a thread that is holding the lock can acquire it again without blocking.\nThere are two condition objects: notEmpty noti\ufb01es waiting dequeuers when\nthe queue goes from being empty to nonempty, and notFull for the opposite\ndirection. Using two conditions instead of one is more ef\ufb01cient, since fewer\nthreads are woken up unnecessarily, but it is more complex.\n1public interface Condition {\n2 void await() throws InterruptedException;\n3 boolean await( long time, TimeUnit unit)\n4 throws InterruptedException;\n5 boolean awaitUntil(Date deadline)\n6 throws InterruptedException;\n7 long awaitNanos( long nanosTimeout)\n8 throws InterruptedException;\n9 void awaitUninterruptibly();\n10 void signal(); // wake up one waiting thread\n11 void signalAll(); // wake up all waiting threads\n12 }\nFigure 8.3 The Condition interface: await ()and its variants release the lock, give up the\nprocessor, and later awaken and reacquire the lock. The signal ()andsignalAll ()methods\nawaken one or more waiting threads.\n8.2 Monitor Locks and Conditions 181\nwaiting \nroom\ncriticalsectionlock()(b)\nC\nC\nunlock()\nsignalAll()CABAB AB AB(c)\nwaiting\nroom lock() D\nDcriticalsection waitingroom \ncritical\nsection B await(cond)lock()(a)\nB\nFigure 8.4 A schematic representation of a monitor execution. In Part (a) thread Ahas acquired the monitor\nlock, called await ()on a condition, released the lock, and is now in the waiting room. Thread Bthen\ngoes through the same sequence of steps, entering the critical section, calling await ()on the condition,\nrelinquishing the lock and entering the waiting room. In Part (b) both AandBleave the waiting room after\nthread Cexits the critical section and calls signalAll ().AandBthen attempt to reacquire the monitor lock.\nHowever, thread Dmanages to acquire the critical section lock \ufb01rst, and so both AandBspin until Dleaves\nthe critical section. Notice that if Cwould have issued a signal ()instead of a signalAll (), only one of A\norBwould have left the waiting room, and the other would have continued to wait.\nThis combination of methods, mutual exclusion locks, and condition objects\nis called a monitor .\n8.2.2 The Lost-Wakeup Problem\nJust as locks are inherently vulnerable to deadlock, Condition objects are inher-\nently vulnerable to lost wakeups , in which one or more threads wait forever with-\nout realizing that the condition for which they are waiting has become true.\nLost wakeups can occur in subtle ways. Fig. 8.6 shows an ill-considered opti-\nmization of the Queue<T> class. Instead of signaling the notEmpty condition\neach time enq() enqueues an item, would it not be more ef\ufb01cient to signal\nthe condition only when the queue actually transitions from empty to non-\nempty? This optimization works as intended if there is only one producer and\none consumer, but it is incorrect if there are multiple producers or consumers.\nConsider the following scenario: consumers AandBboth try to dequeue an\nitem from an empty queue, both detect the queue is empty, and both block on\nthenotEmpty condition. Producer Cenqueues an item in the buffer, and signals\nnotEmpty , wakingA. BeforeAcan acquire the lock, however, another producer\nDputs a second item in the queue, and because the queue is not empty, it does\nnot signal notEmpty . ThenAacquires the lock, removes the", "doc_id": "c04c1f25-324a-4953-a92c-ec6c3f58a2f6", "embedding": null, "doc_hash": "023932fc213dea4ffa1a91748420b0b87856ba97f0c3e3c73acca8b9c326c47a", "extra_info": null, "node_info": {"start": 486823, "end": 490350}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "203bcb74-47cf-4eab-b80b-71bfce10048a", "3": "34edbdef-4dd0-41e4-a94c-19dcc56901ff"}}, "__type__": "1"}, "34edbdef-4dd0-41e4-a94c-19dcc56901ff": {"__data__": {"text": "as intended if there is only one producer and\none consumer, but it is incorrect if there are multiple producers or consumers.\nConsider the following scenario: consumers AandBboth try to dequeue an\nitem from an empty queue, both detect the queue is empty, and both block on\nthenotEmpty condition. Producer Cenqueues an item in the buffer, and signals\nnotEmpty , wakingA. BeforeAcan acquire the lock, however, another producer\nDputs a second item in the queue, and because the queue is not empty, it does\nnot signal notEmpty . ThenAacquires the lock, removes the \ufb01rst item, but B,\nvictim of a lost wakeup, waits forever even though there is an item in the buffer\nto be consumed.\nAlthough there is no substitute for reasoning carefully about our program,\nthere are simple programming practices that will minimize vulnerability to lost\nwakeups.\n\u0004Always signal allprocesses waiting on a condition, not just one.\n\u0004Specify a timeout when waiting.\n182 Chapter 8 Monitors and Blocking Synchronization\n1class LockedQueue<T> {\n2 final Lock lock = new ReentrantLock();\n3 final Condition notFull = lock.newCondition();\n4 final Condition notEmpty = lock.newCondition();\n5 final T[] items;\n6 int tail, head, count;\n7 public LockedQueue( int capacity) {\n8 items = (T[]) new Object[capacity];\n9 }\n10 public void enq(T x) {\n11 lock.lock();\n12 try {\n13 while (count == items.length)\n14 notFull.await();\n15 items[tail] = x;\n16 if(++tail == items.length)\n17 tail = 0;\n18 ++count;\n19 notEmpty.signal();\n20 }finally {\n21 lock.unlock();\n22 }\n23 }\n24 public T deq() {\n25 lock.lock();\n26 try {\n27 while (count == 0)\n28 notEmpty.await();\n29 T x = items[head];\n30 if(++head == items.length)\n31 head = 0;\n32 --count;\n33 notFull.signal();\n34 return x;\n35 }finally {\n36 lock.unlock();\n37 }\n38 }\n39 }\nFigure 8.5 The LockedQueue class: a FIFO queue using locks and conditions. There are two\ncondition \ufb01elds, one to detect when the queue becomes nonempty, and one to detect when\nit becomes nonfull.\nEither of these two practices would \ufb01x the bounded buffer error we just\ndescribed. Each has a small performance penalty, but negligible compared to\nthe cost of a lost wakeup.\nJava provides built-in support for monitors in the form of synchronized\nblocks and methods, as well as built-in wait (),notify (), and notifyAll ()\nmethods. (See Appendix A.)\n8.3 Readers\u2013Writers Locks 183\n1 public void enq(T x) {\n2 lock.lock();\n3 try {\n4 while (count == items.length)\n5 notFull.await();\n6 items[tail] = x;\n7 if(++tail == items.length)\n8 tail = 0;\n9 ++count;\n10 if(count == 1) { // Wrong!\n11 notEmpty.signal();\n12 }\n13 }finally {\n14 lock.unlock();\n15 }\n16 }\nFigure 8.6 This example is incorrect . It suffers from lost wakeups. The enq()method signals\nnotEmpty only if it is the \ufb01rst to place an item in an empty buffer. A lost wakeup occurs if\nmultiple consumers are waiting, but only the \ufb01rst is awakened to consume an item.\n8.3 Readers\u2013Writers Locks\nMany shared objects have the property that most method calls, called readers ,\nreturn information about the object\u2019s state without modifying the object, while\nonly a small number of calls, called writers , actually modify the", "doc_id": "34edbdef-4dd0-41e4-a94c-19dcc56901ff", "embedding": null, "doc_hash": "5d7631723e8a1c034e9dd2f93cad40d7c5019226d3d9997a80a0a18555ad7590", "extra_info": null, "node_info": {"start": 490348, "end": 493481}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "c04c1f25-324a-4953-a92c-ec6c3f58a2f6", "3": "1cb66b78-ae2b-471a-b99d-530d55815367"}}, "__type__": "1"}, "1cb66b78-ae2b-471a-b99d-530d55815367": {"__data__": {"text": "}\n16 }\nFigure 8.6 This example is incorrect . It suffers from lost wakeups. The enq()method signals\nnotEmpty only if it is the \ufb01rst to place an item in an empty buffer. A lost wakeup occurs if\nmultiple consumers are waiting, but only the \ufb01rst is awakened to consume an item.\n8.3 Readers\u2013Writers Locks\nMany shared objects have the property that most method calls, called readers ,\nreturn information about the object\u2019s state without modifying the object, while\nonly a small number of calls, called writers , actually modify the object.\nThere is no need for readers to synchronize with one another; it is perfectly\nsafe for them to access the object concurrently. Writers, on the other hand, must\nlock out readers as well as other writers. A readers\u2013writers lock allows multiple\nreaders or a single writer to enter the critical section concurrently. We use the\nfollowing interface:\npublic interface ReadWriteLock {\nLock readLock();\nLock writeLock();\n}\nThis interface exports two lock objects: the read lock and the write lock . They\nsatisfy the following safety properties:\n\u0004No thread can acquire the write lock while any thread holds either the write\nlock or the read lock.\n\u0004No thread can acquire the read lock while any thread holds the write lock.\nNaturally, multiple threads may hold the read lock at the same time.\n184 Chapter 8 Monitors and Blocking Synchronization\n8.3.1 Simple Readers\u2013Writers Lock\nWe consider a sequence of increasingly sophisticated reader\u2013writer lock imple-\nmentations. The SimpleReadWriteLock class appears in Figs. 8.7\u2013 8.9. This class\nuses a counter to keep track of the number of readers that have acquired the lock,\n1public class SimpleReadWriteLock implements ReadWriteLock {\n2 int readers;\n3 boolean writer;\n4 Lock lock;\n5 Condition condition;\n6 Lock readLock, writeLock;\n7 public SimpleReadWriteLock() {\n8 writer = false ;\n9 readers = 0;\n10 lock = new ReentrantLock();\n11 readLock = new ReadLock();\n12 writeLock = new WriteLock();\n13 condition = lock.newCondition();\n14 }\n15 public Lock readLock() {\n16 return readLock;\n17 }\n18 public Lock writeLock() {\n19 return writeLock;\n20 }\nFigure 8.7 TheSimpleReadWriteLock class: \ufb01elds and public methods.\n21 class ReadLock implements Lock {\n22 public void lock() {\n23 lock.lock();\n24 try {\n25 while (writer) {\n26 condition.await();\n27 }\n28 readers++;\n29 }finally {\n30 lock.unlock();\n31 }\n32 }\n33 public void unlock() {\n34 lock.lock();\n35 try {\n36 readers--;\n37 if(readers == 0)\n38 condition.signalAll();\n39 }finally {\n40 lock.unlock();\n41 }\n42 }\n43 }\nFigure 8.8 TheSimpleReadWriteLock class: the inner read lock.\n8.3 Readers\u2013Writers Locks 185\n44 protected class WriteLock implements Lock {\n45 public void lock() {\n46 lock.lock();\n47 try {\n48 while (readers > 0 || writer) {\n49 condition.await();\n50 }\n51 writer = true ;\n52 }finally {\n53 lock.unlock();\n54 }\n55 }\n56 public void unlock() {\n57 lock.lock();\n58 try {\n59 writer = false ;\n60 condition.signalAll();\n61 }finally {\n62 lock.unlock();\n63 }\n64 }\n65 }\n66 }\nFigure 8.9 TheSimpleReadWriteLock class: inner write lock.\nand a Boolean \ufb01eld indicating whether a writer has acquired the lock. T o de\ufb01ne\nthe associated read\u2013write locks, this code uses inner classes , a Java feature that\nallows one object (the SimpleReadWriteLock lock) to create other objects (the\nread\u2013write locks) that share the", "doc_id": "1cb66b78-ae2b-471a-b99d-530d55815367", "embedding": null, "doc_hash": "ad684589fa5990881329944f6070a0162cec09bec9f9adf3ec55205ad9b5f5c5", "extra_info": null, "node_info": {"start": 493513, "end": 496832}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "34edbdef-4dd0-41e4-a94c-19dcc56901ff", "3": "597c10b2-8c1e-4ed9-89f9-22c8c80d204c"}}, "__type__": "1"}, "597c10b2-8c1e-4ed9-89f9-22c8c80d204c": {"__data__": {"text": "}\n56 public void unlock() {\n57 lock.lock();\n58 try {\n59 writer = false ;\n60 condition.signalAll();\n61 }finally {\n62 lock.unlock();\n63 }\n64 }\n65 }\n66 }\nFigure 8.9 TheSimpleReadWriteLock class: inner write lock.\nand a Boolean \ufb01eld indicating whether a writer has acquired the lock. T o de\ufb01ne\nthe associated read\u2013write locks, this code uses inner classes , a Java feature that\nallows one object (the SimpleReadWriteLock lock) to create other objects (the\nread\u2013write locks) that share the \ufb01rst object\u2019s private \ufb01elds. Both the readLock ()\nand the writeLock () methods return objects that implement the Lock interface.\nThese objects communicate via the writeLock () object\u2019s \ufb01elds. Because the read\u2013\nwrite lock methods must synchronize with one another, they both synchronize on\ntheLock andcondition \ufb01elds of their common SimpleReadWriteLock object.\n8.3.2 Fair Readers\u2013Writers Lock\nEven though the SimpleReadWriteLock algorithm is correct, it is still not quite\nsatisfactory. If readers are much more frequent than writers, as is usually the case,\nthen writers could be locked out for a long time by a continual stream of readers.\nTheFifoReadWriteLock class, shown in Figs. 8.10\u20138.12, shows one way to give\nwriters priority. This class ensures that once a writer calls the write lock\u2019s lock ()\nmethod, then no more readers will be able to acquire the read lock until the writer\nhas acquired and released the write lock. Eventually, the readers holding the read\nlock will drain out without letting any more readers in, and the writer will acquire\nthe write lock.\nThe readAcquires \ufb01eld counts the total number of read lock acquisitions,\nand the readReleases \ufb01eld counts the total number of read lock releases. When\nthese quantities match, no thread is holding the read lock. (We are, of course,\nignoring potential integer over\ufb02ow and wraparound problems.) The class has\n186 Chapter 8 Monitors and Blocking Synchronization\n1public class FifoReadWriteLock implements ReadWriteLock {\n2 int readAcquires, readReleases;\n3 boolean writer;\n4 Lock lock;\n5 Condition condition;\n6 Lock readLock, writeLock;\n7 public FifoReadWriteLock() {\n8 readAcquires = readReleases = 0;\n9 writer = false ;\n10 lock = new ReentrantLock( true );\n11 condition = lock.newCondition();\n12 readLock = new ReadLock();\n13 writeLock = new WriteLock();\n14 }\n15 public Lock readLock() {\n16 return readLock;\n17 }\n18 public Lock writeLock() {\n19 return writeLock;\n20 }\n21 ...\n22 }\nFigure 8.10 TheFifoReadWriteLock class: \ufb01elds and public methods.\n23 private class ReadLock implements Lock {\n24 public void lock() {\n25 lock.lock();\n26 try {\n27 while (writer) {\n28 condition.await();\n29 }\n30 readAcquires++;\n31 }finally {\n32 lock.unlock();\n33 }\n34 }\n35 public void unlock() {\n36 lock.lock();\n37 try {\n38 readReleases++;\n39 if(readAcquires == readReleases)\n40 condition.signalAll();\n41 }finally {\n42 lock.unlock();\n43 }\n44 }\n45 }\nFigure 8.11 TheFifoReadWriteLock class: inner read lock class.\n8.4 Our Own Reentrant Lock 187\n46 private class WriteLock implements Lock {\n47 public void lock() {\n48 lock.lock();\n49 try {\n50 while (writer) {\n51 condition.await();\n52 }\n53 writer = true ;\n54 while (readAcquires != readReleases) {\n55 condition.await();\n56 }\n57 }finally", "doc_id": "597c10b2-8c1e-4ed9-89f9-22c8c80d204c", "embedding": null, "doc_hash": "78b82f66c184bce03a49f6f97219f2f2598f40aaf645584753cf1639faba4456", "extra_info": null, "node_info": {"start": 496859, "end": 500074}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1cb66b78-ae2b-471a-b99d-530d55815367", "3": "ae61f657-b28c-41f2-89fe-64edbaf3e5ea"}}, "__type__": "1"}, "ae61f657-b28c-41f2-89fe-64edbaf3e5ea": {"__data__": {"text": "== readReleases)\n40 condition.signalAll();\n41 }finally {\n42 lock.unlock();\n43 }\n44 }\n45 }\nFigure 8.11 TheFifoReadWriteLock class: inner read lock class.\n8.4 Our Own Reentrant Lock 187\n46 private class WriteLock implements Lock {\n47 public void lock() {\n48 lock.lock();\n49 try {\n50 while (writer) {\n51 condition.await();\n52 }\n53 writer = true ;\n54 while (readAcquires != readReleases) {\n55 condition.await();\n56 }\n57 }finally {\n58 lock.unlock();\n59 }\n60 }\n61 public void unlock() {\n62 writer = false ;\n63 condition.signalAll();\n64 }\n65 }\nFigure 8.12 TheFifoReadWriteLock class: inner write lock class.\na private lock \ufb01eld, held by readers for short durations: they acquire the lock,\nincrement the readAcquires \ufb01eld, and release the lock. Writers hold this lock\nfrom the time they try to acquire the write lock to the time they release it. This\nlocking protocol ensures that once a writer has acquired the lock , no additional\nreader can increment readAcquires , so no additional reader can acquire the\nread lock, and eventually all readers currently holding the read lock will release\nit, allowing the writer to proceed.\nHow are waiting writers noti\ufb01ed when the last reader releases its lock? When a\nwriter tries to acquire the write lock, it acquires the FifoReadWriteLock object\u2019s\nlock . A reader releasing the read lock also acquires that lock , and calls the\nassociated condition\u2019s signal () method if all readers have released their locks.\n8.4 Our Own Reentrant Lock\nUsing the locks described in Chapters 2 and 7, a thread that attempts to reac-\nquire a lock it already holds will deadlock with itself. This situation can arise if a\nmethod that acquires a lock makes a nested call to another method that acquires\nthe same lock.\nA lock is reentrant if it can be acquired multiple times by the same thread.\nWe now examine how to create a reentrant lock from a non-reentrant lock. This\nexercise is intended to illustrate how to use locks and conditions. In practice, the\njava.util.concurrent.locks package provides reentrant lock classes, so there is no\nneed to write our own.\nFig. 8.13 shows the SimpleReentrantLock class. The owner \ufb01eld holds the ID\nof the last thread to acquire the lock, and the holdCount \ufb01eld is incremented each\n188 Chapter 8 Monitors and Blocking Synchronization\n1public class SimpleReentrantLock implements Lock{\n2 Lock lock;\n3 Condition condition;\n4 int owner, holdCount;\n5 public SimpleReentrantLock() {\n6 lock = new SimpleLock();\n7 condition = lock.newCondition();\n8 owner = 0;\n9 holdCount = 0;\n10 }\n11 public void lock() {\n12 int me = ThreadID.get();\n13 lock.lock();\n14 try {\n15 if(owner == me) {\n16 holdCount++;\n17 return ;\n18 }\n19 while (holdCount != 0) {\n20 condition.await();\n21 }\n22 owner = me;\n23 holdCount = 1;\n24 }finally {\n25 lock.unlock();\n26 }\n27 }\n28 public void unlock() {\n29 lock.lock();\n30 try {\n31 if(holdCount == 0 || owner != ThreadID.get())\n32 throw new IllegalMonitorStateException();\n33 holdCount--;\n34 if(holdCount == 0) {\n35 condition.signal();\n36 }\n37 }finally {\n38 lock.unlock();\n39 }\n40 }\n41\n42 public Condition newCondition() {\n43 throw new UnsupportedOperationException(\"Not supported yet.\");\n44 }\n45 ...\n46 }\nFigure 8.13 TheSimpleReentrantLock class:", "doc_id": "ae61f657-b28c-41f2-89fe-64edbaf3e5ea", "embedding": null, "doc_hash": "ce6495654105438065a79a9244baa9a3fc49195f0e27d015a4bbfeb0d592d651", "extra_info": null, "node_info": {"start": 500119, "end": 503328}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "597c10b2-8c1e-4ed9-89f9-22c8c80d204c", "3": "826608c1-1289-49fb-a3d2-f6977d4b2638"}}, "__type__": "1"}, "826608c1-1289-49fb-a3d2-f6977d4b2638": {"__data__": {"text": "lock.unlock();\n26 }\n27 }\n28 public void unlock() {\n29 lock.lock();\n30 try {\n31 if(holdCount == 0 || owner != ThreadID.get())\n32 throw new IllegalMonitorStateException();\n33 holdCount--;\n34 if(holdCount == 0) {\n35 condition.signal();\n36 }\n37 }finally {\n38 lock.unlock();\n39 }\n40 }\n41\n42 public Condition newCondition() {\n43 throw new UnsupportedOperationException(\"Not supported yet.\");\n44 }\n45 ...\n46 }\nFigure 8.13 TheSimpleReentrantLock class: lock ()andunlock ()methods.\ntime the lock is acquired, and decremented each time it is released. The lock is\nfree when the holdCount value is zero. Because these two \ufb01elds are manipulated\natomically, we need an internal, short-term lock. The lock \ufb01eld is a lock used by\nlock () and unlock () to manipulate the \ufb01elds, and the condition \ufb01eld is used by\n8.6 Chapter Notes 189\nthreads waiting for the lock to become free. In Fig. 8.13 , we initialize the internal\nlock \ufb01eld to an object of a (\ufb01ctitious) SimpleLock class which is presumably not\nreentrant (Line 6).\nThelock () method acquires the internal lock (Line 13). If the current thread is\nalready the owner, it increments the hold count and returns (Line 15). Otherwise,\nif the hold count is not zero, the lock is held by another thread, and the caller\nreleases the lock and waits until the condition is signaled (Line 20). When the\ncaller awakens, it must still check that the hold count is zero. When the hold\ncount is established to be zero, the calling thread makes itself the owner and sets\nthe hold count to 1.\nTheunlock () method acquires the internal lock (Line 29). It throws an excep-\ntion if either the lock is free, or the caller is not the owner (Line 31). Otherwise, it\ndecrements the hold count. If the hold count is zero, then the lock is free, so the\ncaller signals the condition to wake up a waiting thread (Line 35).\n8.5 Semaphores\nAs we have seen, a mutual exclusion lock guarantees that only one thread at a\ntime can enter a critical section. If another thread wants to enter the critical\nsection while it is occupied, then it blocks, suspending itself until another thread\nnoti\ufb01es it to try again. A Semaphore is a generalization of mutual exclusion locks.\nEach Semaphore has a capacity , denoted by cfor brevity. Instead of allowing\nonly one thread at a time into the critical section, a Semaphore allows at most c\nthreads, where the capacity cis determined when the Semaphore is initialized.\nAs discussed in the chapter notes, semaphores were one of the earliest forms of\nsynchronization.\nThe Semaphore class of Fig. 8.14 provides two methods: a thread calls\nacquire () to request permission to enter the critical section, and release ()\nto announce that it is leaving the critical section. The Semaphore itself is just a\ncounter: it keeps track of the number of threads that have been granted permis-\nsion to enter. If a new acquire () call is about to exceed the capacity c, the calling\nthread is suspended until there is room. When a thread leaves the critical section,\nit calls release () to notify a waiting thread that there is now room.\n8.6 Chapter Notes\nMonitors were invented by Per Brinch-Hansen [ 52] and T ony Hoare [ 71].\nSemaphores were invented by Edsger Dijkstra [ 33]. McKenney [ 112] surveys dif-\nferent kinds of locking protocols.\n190 Chapter 8 Monitors and Blocking Synchronization\n1public class Semaphore {\n2 final int capacity;\n3 int state;\n4 Lock lock;\n5 Condition condition;\n6 public", "doc_id": "826608c1-1289-49fb-a3d2-f6977d4b2638", "embedding": null, "doc_hash": "d03be3d253fb6f4445bf9ef95479180a17efb1800272c8b2800f24ddaddf9d61", "extra_info": null, "node_info": {"start": 503305, "end": 506734}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ae61f657-b28c-41f2-89fe-64edbaf3e5ea", "3": "b411a8c0-4773-423f-bfdc-21fc7e6f1fe4"}}, "__type__": "1"}, "b411a8c0-4773-423f-bfdc-21fc7e6f1fe4": {"__data__": {"text": "until there is room. When a thread leaves the critical section,\nit calls release () to notify a waiting thread that there is now room.\n8.6 Chapter Notes\nMonitors were invented by Per Brinch-Hansen [ 52] and T ony Hoare [ 71].\nSemaphores were invented by Edsger Dijkstra [ 33]. McKenney [ 112] surveys dif-\nferent kinds of locking protocols.\n190 Chapter 8 Monitors and Blocking Synchronization\n1public class Semaphore {\n2 final int capacity;\n3 int state;\n4 Lock lock;\n5 Condition condition;\n6 public Semaphore( int c) {\n7 capacity = c;\n8 state = 0;\n9 lock = new ReentrantLock();\n10 condition = lock.newCondition();\n11 }\n12 public void acquire() {\n13 lock.lock();\n14 try {\n15 while (state == capacity) {\n16 condition.await();\n17 }\n18 state++;\n19 }finally {\n20 lock.unlock();\n21 }\n22 }\n23 public void release() {\n24 lock.lock();\n25 try {\n26 state--;\n27 condition.signalAll();\n28 }finally {\n29 lock.unlock();\n30 }\n31 }\n32 }\nFigure 8.14 Semaphore implementation.\n8.7 Exercises\nExercise 93. Reimplement the SimpleReadWriteLock class using Java\nsynchronized ,wait (),notify (), and notifyAll () constructs in place of\nexplict locks and conditions.\nHint: you must \ufb01gure out how methods of the inner read\u2013write lock classes\ncan lock the outer SimpleReadWriteLock object.\nExercise 94. The ReentrantReadWriteLock class provided by the\njava.util.concurrent.locks package does not allow a thread holding the lock in\nread mode to then access that lock in write mode (the thread will block). Jus-\ntify this design decision by sketching what it would take to permit such lock\nupgrades.\n8.7 Exercises 191\nExercise 95. Asavings account object holds a nonnegative balance, and provides\ndeposit (k) and withdraw (k) methods, where deposit (k) addskto the bal-\nance, and withdraw (k) subtractsk, if the balance is at least k, and otherwise\nblocks until the balance becomes kor greater.\n1.Implement this savings account using locks and conditions.\n2.Now suppose there are two kinds of withdrawals: ordinary and preferred .\nDevise an implementation that ensures that no ordinary withdrawal occurs\nif there is a preferred withdrawal waiting to occur.\n3.Now let us add a transfer () method that transfers a sum from one account\nto another:\nvoid transfer( int k, Account reserve) {\nlock.lock();\ntry {\nreserve.withdraw(k);\ndeposit(k);\n}finally {lock.unlock();}\n}\nWe are given a set of 10 accounts, whose balances are unknown. At 1:00, each\nofnthreads tries to transfer $100 from another account into its own account.\nAt 2:00, a Boss thread deposits $1000 to each account. Is every transfer method\ncalled at 1:00 certain to return?\nExercise 96. In the shared bathroom problem , there are two classes of threads,\ncalled male and female . There is a single bathroom resource that must be used\nin the following way:\n1.Mutual exclusion: persons of opposite sex may not occupy the bathroom\nsimultaneously,\n2.Starvation-freedom: everyone who needs to use the bathroom eventually\nenters.\nThe protocol is implemented via the following four procedures: enterMale ()\ndelays the caller until it is ok for a male to enter the bathroom, leaveMale () is\ncalled when a male leaves the bathroom, while enterFemale () and\nleaveFemale () do the same for females. For", "doc_id": "b411a8c0-4773-423f-bfdc-21fc7e6f1fe4", "embedding": null, "doc_hash": "d38809ffbba4d7749a9598c40f8a10a7e5e71ffb971b41cd01ddcf9989f16bba", "extra_info": null, "node_info": {"start": 506708, "end": 509928}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "826608c1-1289-49fb-a3d2-f6977d4b2638", "3": "7fb6f579-d8be-474c-a49d-e7b55b5cb250"}}, "__type__": "1"}, "7fb6f579-d8be-474c-a49d-e7b55b5cb250": {"__data__": {"text": "classes of threads,\ncalled male and female . There is a single bathroom resource that must be used\nin the following way:\n1.Mutual exclusion: persons of opposite sex may not occupy the bathroom\nsimultaneously,\n2.Starvation-freedom: everyone who needs to use the bathroom eventually\nenters.\nThe protocol is implemented via the following four procedures: enterMale ()\ndelays the caller until it is ok for a male to enter the bathroom, leaveMale () is\ncalled when a male leaves the bathroom, while enterFemale () and\nleaveFemale () do the same for females. For example,\nenterMale();\nteeth.brush(toothpaste);\nleaveMale();\n1.Implement this class using locks and condition variables.\n2.Implement this class using synchronized ,wait (),notify (), and\nnotifyAll ().\nFor each implementation, explain why it satis\ufb01es mutual exclusion and starvation-\nfreedom.\n192 Chapter 8 Monitors and Blocking Synchronization\nExercise 97. TheRooms class manages a collection of rooms , indexed from 0 to\nm(wheremis an argument to the constructor). Threads can enter or exit any\nroom in that range. Each room can hold an arbitrary number of threads simul-\ntaneously, but only one room can be occupied at a time. For example, if there are\ntwo rooms, indexed 0 and 1, then any number of threads might enter the room\n0, but no thread can enter the room 1 while room 0 is occupied. Fig. 8.15 shows\nan outline of the Rooms class.\nEach room can be assigned an exit handler : calling setHandler (i,h) sets the\nexit handler for room ito handlerh. The exit handler is called by the last thread to\n1public class Rooms {\n2 public interface Handler {\n3 void onEmpty();\n4 }\n5 public Rooms( int m) { ... };\n6 void enter( int i) { ... };\n7 boolean exit() { ... };\n8 public void setExitHandler( int i, Rooms.Handler h) { ... };\n9}\nFigure 8.15 TheRooms class.\n1class Driver {\n2 void main() {\n3 CountDownLatch startSignal = new CountDownLatch(1);\n4 CountDownLatch doneSignal = new CountDownLatch(n);\n5 for (int i = 0; i < n; ++i) // start threads\n6 new Thread( new Worker(startSignal, doneSignal)).start();\n7 doSomethingElse(); // get ready for threads\n8 startSignal.countDown(); // unleash threads\n9 doSomethingElse(); // biding my time ...\n10 doneSignal.await(); // wait for threads to finish\n11 }\n12 class Worker implements Runnable {\n13 private final CountDownLatch startSignal, doneSignal;\n14 Worker(CountDownLatch myStartSignal, CountDownLatch myDoneSignal) {\n15 startSignal = myStartSignal;\n16 doneSignal = myDoneSignal;\n17 }\n18 public void run() {\n19 startSignal.await(); // wait for driver\u2019s OK to start\n20 doWork();\n21 doneSignal.countDown(); // notify driver we\u2019re done\n22 }\n23 ...\n24 }\n25 }\nFigure 8.16 TheCountDownLatch class: an example usage.\n8.7 Exercises 193\nleave a room, but before any threads subsequently enter any room. This method\nis called once and while it is running, no threads are in any rooms.\nImplement the Rooms class. Make sure that:\n\u0004If some thread is in room i, then no thread is in room j6=i.\n\u0004The last thread to leave a room calls the room\u2019s exit handler, and no threads\nare in any room while that handler is running.\n\u0004Y our implementation must be fair: any thread that tries to enter a room even-\ntually succeeds. Naturally, you may assume that every thread that enters a\nroom eventually leaves.\nExercise 98. Consider an", "doc_id": "7fb6f579-d8be-474c-a49d-e7b55b5cb250", "embedding": null, "doc_hash": "12054373ea30697e6845293c89a6e50ea880d992b987821f052adb7c887725d5", "extra_info": null, "node_info": {"start": 509878, "end": 513190}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b411a8c0-4773-423f-bfdc-21fc7e6f1fe4", "3": "3baff753-dc51-4c1c-a86f-75a54efe7495"}}, "__type__": "1"}, "3baff753-dc51-4c1c-a86f-75a54efe7495": {"__data__": {"text": "subsequently enter any room. This method\nis called once and while it is running, no threads are in any rooms.\nImplement the Rooms class. Make sure that:\n\u0004If some thread is in room i, then no thread is in room j6=i.\n\u0004The last thread to leave a room calls the room\u2019s exit handler, and no threads\nare in any room while that handler is running.\n\u0004Y our implementation must be fair: any thread that tries to enter a room even-\ntually succeeds. Naturally, you may assume that every thread that enters a\nroom eventually leaves.\nExercise 98. Consider an application with distinct sets of active and passive\nthreads, where we want to block the passive threads until all active threads give\npermission for the passive threads to proceed.\nACountDownLatch encapsulates a counter, initialized to be n, the number of\nactive threads. When an active method is ready for the passive threads to run,\nit calls countDown() , which decrements the counter. Each passive thread calls\nawait (), which blocks the thread until the counter reaches zero. (See Fig. 8.16.)\nProvide a CountDownLatch implementation. Do not worry about reusing the\nCountDownLatch object.\nExercise 99. This exercise is a follow-up to Exercise 98. Provide a CountDownLatch\nimplementation where the CountDownLatch object can be reused.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n9Linked Lists: The Role\nof Locking\n9.1 Introduction\nIn Chapter 7 we saw how to build scalable spin locks that provide mutual\nexclusion ef\ufb01ciently, even when they are heavily used. We might think that it\nis now a simple matter to construct scalable concurrent data structures: take\na sequential implementation of the class, add a scalable lock \ufb01eld, and ensure\nthat each method call acquires and releases that lock. We call this approach\ncoarse-grained synchronization .\nOften, coarse-grained synchronization works well, but there are important\ncases where it does not. The problem is that a class that uses a single lock to\nmediate all its method calls is not always scalable, even if the lock itself is scalable.\nCoarse-grained synchronization works well when levels of concurrency are low,\nbut if too many threads try to access the object at the same time, then the object\nbecomes a sequential bottleneck, forcing threads to wait in line for access.\nThischapterintroducesseveralusefultechniquesthatgobeyondcoarse-grained\nlocking to allow multiple threads to access a single object at the same time.\n\u0004Fine-grained synchronization: Instead of using a single lock to synchronize\nevery access to an object, we split the object into independently synchronized\ncomponents, ensuring that method calls interfere only when trying to access\nthe same component at the same time.\n\u0004Optimistic synchronization: Many objects, such as trees or lists, consist of mul-\ntiple components linked together by references. Some methods search for a\nparticular component (e.g., a list or tree node containing a particular key).\nOne way to reduce the cost of \ufb01ne-grained locking is to search without acquir-\ning any locks at all. If the method \ufb01nds the sought-after component, it locks\nthat component, and then checks that the component has not changed in the\ninterval between when it was inspected and when it was locked. This technique\nis worthwhile only if it succeeds more often than not, which is why we call it\noptimistic.\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00009-5\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.195\n196 Chapter 9 Linked Lists: The Role of", "doc_id": "3baff753-dc51-4c1c-a86f-75a54efe7495", "embedding": null, "doc_hash": "82a92fec13622c2f09f7ea46d8355f4d47cccf0b98748059f95d5f02457d93e5", "extra_info": null, "node_info": {"start": 513212, "end": 516778}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7fb6f579-d8be-474c-a49d-e7b55b5cb250", "3": "b72f0c78-328c-4807-967f-b455dcbe931f"}}, "__type__": "1"}, "b72f0c78-328c-4807-967f-b455dcbe931f": {"__data__": {"text": "without acquir-\ning any locks at all. If the method \ufb01nds the sought-after component, it locks\nthat component, and then checks that the component has not changed in the\ninterval between when it was inspected and when it was locked. This technique\nis worthwhile only if it succeeds more often than not, which is why we call it\noptimistic.\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00009-5\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.195\n196 Chapter 9 Linked Lists: The Role of Locking\n1public interface Set<T> {\n2 boolean add(T x);\n3 boolean remove(T x);\n4 boolean contains(T x);\n5}\nFigure 9.1 The Setinterface: add()adds an item to the set (no effect if that item is already\npresent), remove ()removes it (if present), and contains ()returns a Boolean indicating\nwhether the item is present.\n\u0004Lazy synchronization: Sometimes it makes sense to postpone hard work. For\nexample, the task of removing a component from a data structure can be split\ninto two phases: the component is logically removed simply by setting a tag bit,\nand later, the component can be physically removed by unlinking it from the\nrest of the data structure.\n\u0004Nonblocking synchronization: Sometimes we can eliminate locks entirely,\nrelying on built-in atomic operations such as compareAndSet() for synchro-\nnization.\nEach of these techniques can be applied (with appropriate customization) to a\nvariety of common data structures. In this chapter we consider how to use linked\nlists to implement a set, a collection of items that contains no duplicate elements.\nFor our purposes, as illustrated in Fig. 9.1, a setprovides the following three\nmethods:\n\u0004Theadd(x) method adds xto the set, returning true if, and only if xwas not\nalready there.\n\u0004Theremove (x) method removes xfrom the set, returning true if, and only if\nxwas there.\n\u0004Thecontains (x) returns true if, and only if the set contains x.\nFor each method, we say that a call is successful if it returns true, and unsuccessful\notherwise. It is typical that in applications using sets, there are signi\ufb01cantly more\ncontains () calls than add() or remove () calls.\n9.2 List-Based Sets\nThis chapter presents a range of concurrent set algorithms, all based on the same\nbasic idea. A set is implemented as a linked list of nodes. As shown in Fig. 9.2,\ntheNode<T> class has three \ufb01elds.1Theitem \ufb01eld is the actual item of interest.\nThekey \ufb01eld is the item\u2019s hash code. Nodes are sorted in key order, providing\nan ef\ufb01cient way to detect when an item is absent. The next \ufb01eld is a reference to\n1T o be consistent with the Java memory model these \ufb01elds and their modi\ufb01cations later in the\ntext will need to be volatile, though we ignore this issue here for the sake of brevity.\n9.2 List-Based Sets 197\n1private class Node {\n2 T item;\n3 int key;\n4 Node next;\n5}\nFigure 9.2 The Node<T> class: this internal class keeps track of the item, the item\u2019s key, and\nthe next node in the list. Some algorithms require technical changes to this class.\nremove bbheadpred\ntail\nccurr\na\nbheadpred\ntail\nccurr\naadd b(a)\n(b)\nFigure 9.3 A seqential Setimplementation: adding and removing nodes. In Part (a), a thread\nadding a node buses two variables: curr is the current node, and pred is its", "doc_id": "b72f0c78-328c-4807-967f-b455dcbe931f", "embedding": null, "doc_hash": "456fa9d2dc006b43c6dc42ceb844089258c601d460ccfd43af1ecc016cdafc22", "extra_info": null, "node_info": {"start": 516795, "end": 520021}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3baff753-dc51-4c1c-a86f-75a54efe7495", "3": "7c896a4b-43b0-41e8-8b13-0e3ba062f704"}}, "__type__": "1"}, "7c896a4b-43b0-41e8-8b13-0e3ba062f704": {"__data__": {"text": "{\n2 T item;\n3 int key;\n4 Node next;\n5}\nFigure 9.2 The Node<T> class: this internal class keeps track of the item, the item\u2019s key, and\nthe next node in the list. Some algorithms require technical changes to this class.\nremove bbheadpred\ntail\nccurr\na\nbheadpred\ntail\nccurr\naadd b(a)\n(b)\nFigure 9.3 A seqential Setimplementation: adding and removing nodes. In Part (a), a thread\nadding a node buses two variables: curr is the current node, and pred is its predecessor.\nThe thread moves down the list comparing the keys for curr andb. If a match is found, the\nitem is already present, so it returns false. If curr reaches a node with a higher key, the item\nis not in the set so Set b\u2019snext \ufb01eld to curr, and pred\u2019snext \ufb01eld to b. In Part (b), to delete\ncurr, the thread sets pred\u2019snext \ufb01eld to curr\u2019snext \ufb01eld.\nthe next node in the list. (Some of the algorithms we consider require technical\nchanges to this class, such as adding new \ufb01elds, or changing the types of existing\n\ufb01elds.) For simplicity, we assume that each item\u2019s hash code is unique (relaxing\nthis assumption is left as an exercise). We associate an item with the same node\nand key throughout any given example, which allows us to abuse notation and\nuse the same symbol to refer to a node, its key, and its item. That is, node amay\nhave keyaand itema, and so on.\nThe list has two kinds of nodes. In addition to regular nodes that hold items\nin the set, we use two sentinel nodes, called head andtail , as the \ufb01rst and last\nlist elements. Sentinel nodes are never added, removed, or searched for, and their\n198 Chapter 9 Linked Lists: The Role of Locking\nkeys are the minimum and maximum integer values.2Ignoring synchronization\nfor the moment, the top part of Fig. 9.3 schematically describes how an item is\nadded to the set. Each thread Ahas two local variables used to traverse the list:\ncurrAis the current node and predAis its predecessor. T o add an item to the set,\nthreadAsets local variables predAandcurrAtohead , and moves down the list,\ncomparing currA\u2019s key to the key of the item being added. If they match, the item\nis already present in the set, so the call returns false . IfpredAprecedes currAin\nthe list, then predA\u2019s key is lower than that of the inserted item, and currA\u2019s key\nis higher, so the item is not present in the list. The method creates a new node b\nto hold the item, sets b\u2019snextA\ufb01eld to currA, then sets predAtob. Removing\nan item from the set works in a similar way.\n9.3 Concurrent Reasoning\nReasoning about concurrent data structures may seem impossibly dif\ufb01cult, but it\nis a skill that can be learned. Often, the key to understanding a concurrent data\nstructure is to understand its invariants : properties that always hold. We can show\nthat a property is invariant by showing that:\n1.The property holds when the object is created, and\n2.Once the property holds, then no thread can take a step that makes the\nproperty false .\nMost interesting invariants hold trivially when the list is created, so it makes sense\nto focus on how invariants, once established, are preserved.\nSpeci\ufb01cally, we can check that each invariant is preserved by each invocation\nofinsert (),remove (), and contains () methods. This approach works only if\nwe can assume that these methods", "doc_id": "7c896a4b-43b0-41e8-8b13-0e3ba062f704", "embedding": null, "doc_hash": "fd01ab0a6b5cb9fb395534eb723ff2021525a8bdaa0498db9352617aa563bae3", "extra_info": null, "node_info": {"start": 520074, "end": 523326}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b72f0c78-328c-4807-967f-b455dcbe931f", "3": "b9225d5f-864f-4cd9-bbe3-74d6fbefdd1c"}}, "__type__": "1"}, "b9225d5f-864f-4cd9-bbe3-74d6fbefdd1c": {"__data__": {"text": ": properties that always hold. We can show\nthat a property is invariant by showing that:\n1.The property holds when the object is created, and\n2.Once the property holds, then no thread can take a step that makes the\nproperty false .\nMost interesting invariants hold trivially when the list is created, so it makes sense\nto focus on how invariants, once established, are preserved.\nSpeci\ufb01cally, we can check that each invariant is preserved by each invocation\nofinsert (),remove (), and contains () methods. This approach works only if\nwe can assume that these methods are the only ones that modify nodes, a prop-\nerty sometimes called freedom from interference . In the list algorithms considered\nhere, nodes are internal to the list implementation, so freedom from interference\nis guaranteed because users of the list have no opportunity to modify its internal\nnodes.\nWe require freedom from interference even for nodes that have been removed\nfrom the list, since some of our algorithms permit a thread to unlink a node\nwhile it is being traversed by others. Fortunately, we do not attempt to reuse list\nnodes that have been removed from the list, relying instead on a garbage collector\nto recycle that memory. The algorithms described here work in languages with-\nout garbage collection, but sometimes require nontrivial modi\ufb01cations that are\nbeyond the scope of this chapter.\n2All algorithms presented here work for any ordered set of keys that have maximum and mini-\nmum values and that are well-founded, that is, there are only \ufb01nitely many keys smaller than any\ngiven key. For simplicity, we assume here that keys are integers.\n9.3 Concurrent Reasoning 199\nWhen reasoning about concurrent object implementations, it is important to\nunderstand the distinction between an object\u2019s abstract value (here, a set of items),\nand its concrete representation (here, a list of nodes).\nNot every list of nodes is a meaningful representation for a set. An algorithm\u2019s\nrepresentation invariant characterizes which representations make sense as\nabstract values. If aandbare nodes, we say that apoints tobifa\u2019snext \ufb01eld is a\nreference to b. We say that bisreachable if there is a sequence of nodes, starting\nathead , and ending at b, where each node in the sequence points to its successor.\nThe set algorithms in this chapter require the following invariants (some\nrequire more, as explained later). First, sentinels are neither added nor removed.\nSecond, nodes are sorted by key, and keys are unique.\nLet us think of the representation invariant as a contract among the object\u2019s\nmethods. Each method call preserves the invariant, and also relies on the other\nmethods to preserve the invariant. In this way, we can reason about each method\nin isolation, without having to consider all the possible ways they might interact.\nGiven a list satisfying the representation invariant, which set does it represent?\nThe meaning of such a list is given by an abstraction map carrying lists that satisfy\nthe representation invariant to sets. Here, the abstraction map is simple: an item\nis in the set if and only if it is reachable from head .\nWhat safety and liveness properties do we need? Our safety property is\nlinearizability . As we saw in Chapter 3, to show that a concurrent data structure\nis a linearizable implementation of a sequentially speci\ufb01ed object, it is enough to\nidentify a linearization point , a single atomic step where the method call \u201ctakes\neffect.\u201d This step can be a read, a write, or a more complex atomic operation.\nLooking at any execution history of a list-based set, it must be the case that if the\nabstraction map is applied to the representation at the linearization points, the\nresulting sequence of states and method calls de\ufb01nes a valid sequential", "doc_id": "b9225d5f-864f-4cd9-bbe3-74d6fbefdd1c", "embedding": null, "doc_hash": "67206ffbf086da359b01f04db4896fdd6fc2bbe5f24328d7d03f6ca0e1c77260", "extra_info": null, "node_info": {"start": 523233, "end": 526995}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7c896a4b-43b0-41e8-8b13-0e3ba062f704", "3": "1c822044-c953-4ee9-be1e-3c7baba319ed"}}, "__type__": "1"}, "1c822044-c953-4ee9-be1e-3c7baba319ed": {"__data__": {"text": "safety property is\nlinearizability . As we saw in Chapter 3, to show that a concurrent data structure\nis a linearizable implementation of a sequentially speci\ufb01ed object, it is enough to\nidentify a linearization point , a single atomic step where the method call \u201ctakes\neffect.\u201d This step can be a read, a write, or a more complex atomic operation.\nLooking at any execution history of a list-based set, it must be the case that if the\nabstraction map is applied to the representation at the linearization points, the\nresulting sequence of states and method calls de\ufb01nes a valid sequential set exe-\ncution. Here, add(a) addsato the abstract set, remove (a) removesafrom the\nabstract set, and contains (a) returns true orfalse , depending on whether awas\nalready in the set.\nDifferent list algorithms make different progress guarantees. Some use locks,\nand care is required to ensure they are deadlock- and starvation-free. Some\nnonblocking list algorithms do not use locks at all, while others restrict locking\nto certain methods. Here is a brief summary, from Chapter 3, of the nonblocking\nproperties we use3:\n\u0004A method is wait-free if it guarantees that every call \ufb01nishes in a \ufb01nite number\nof steps.\n\u0004A method is lock-free if it guarantees that some call always \ufb01nishes in a \ufb01nite\nnumber of steps.\nWe are now ready to consider a range of list-based set algorithms. We start with\nalgorithms that use coarse-grained synchronization, and successively re\ufb01ne them\n3Chapter 3 introduces an even weaker nonblocking property called obstruction-freedom .\n200 Chapter 9 Linked Lists: The Role of Locking\nto reduce granularity of locking. Formal proofs of correctness lie beyond the\nscope of this book. Instead, we focus on informal reasoning useful in everyday\nproblem-solving.\nAs mentioned, in each of these algorithms, methods scan through the list using\ntwo local variables: curr is the current node and pred is its predecessor. These\nvariables are thread-local,4so we use predAandcurrAto denote the instances\nused by thread A.\n9.4 Coarse-Grained Synchronization\nWe start with a simple algorithm using coarse-grained synchronization. Figs. 9.4\nand 9.5 show the add() and remove () methods for this coarse-grained algorithm.\n(The contains () method works in much the same way, and is left as an exercise.)\nThe list itself has a single lock which every method call must acquire. The principal\n1public class CoarseList<T> {\n2 private Node head;\n3 private Lock lock = new ReentrantLock();\n4 public CoarseList() {\n5 head = new Node(Integer.MIN_VALUE);\n6 head.next = new Node(Integer.MAX_VALUE);\n7 }\n8 public boolean add(T item) {\n9 Node pred, curr;\n10 int key = item.hashCode();\n11 lock.lock();\n12 try {\n13 pred = head;\n14 curr = pred.next;\n15 while (curr.key < key) {\n16 pred = curr;\n17 curr = curr.next;\n18 }\n19 if(key == curr.key) {\n20 return false ;\n21 }else {\n22 Node node = new Node(item);\n23 node.next = curr;\n24 pred.next = node;\n25 return true ;\n26 }\n27 }finally {\n28 lock.unlock();\n29 }\n30 }\nFigure 9.4 TheCoarseList class: the add()method.\n4Appendix A describes how thread-local variables work in Java.\n9.5 Fine-Grained Synchronization 201\n31 public boolean remove(T item) {\n32 Node pred, curr;\n33 int key = item.hashCode();\n34 lock.lock();\n35 try {\n36", "doc_id": "1c822044-c953-4ee9-be1e-3c7baba319ed", "embedding": null, "doc_hash": "47de3ba1c03c2e2fd5a71fb1a8ca48074c9418c528e55821a3950490da59108e", "extra_info": null, "node_info": {"start": 526982, "end": 530234}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b9225d5f-864f-4cd9-bbe3-74d6fbefdd1c", "3": "cf728e8b-a8c5-447f-8e63-f1bfd1d9fd4e"}}, "__type__": "1"}, "cf728e8b-a8c5-447f-8e63-f1bfd1d9fd4e": {"__data__": {"text": "return false ;\n21 }else {\n22 Node node = new Node(item);\n23 node.next = curr;\n24 pred.next = node;\n25 return true ;\n26 }\n27 }finally {\n28 lock.unlock();\n29 }\n30 }\nFigure 9.4 TheCoarseList class: the add()method.\n4Appendix A describes how thread-local variables work in Java.\n9.5 Fine-Grained Synchronization 201\n31 public boolean remove(T item) {\n32 Node pred, curr;\n33 int key = item.hashCode();\n34 lock.lock();\n35 try {\n36 pred = head;\n37 curr = pred.next;\n38 while (curr.key < key) {\n39 pred = curr;\n40 curr = curr.next;\n41 }\n42 if(key == curr.key) {\n43 pred.next = curr.next;\n44 return true ;\n45 }else {\n46 return false ;\n47 }\n48 }finally {\n49 lock.unlock();\n50 }\n51 }\nFigure 9.5 The CoarseList class: the remove ()method. All methods acquire a single lock,\nwhich is released on exit by the finally block.\nadvantage of this algorithm, which should not be discounted, is that it is obviously\ncorrect. All methods act on the list only while holding the lock, so the execution\nis essentially sequential. T o simplify matters, we follow the convention (for now)\nthat the linearization point for any method call that acquires a lock is the instant\nthe lock is acquired.\nTheCoarseList class satis\ufb01es the same progress condition as its lock: if the\nLock is starvation-free, so is our implementation. If contention is very low, this\nalgorithm is an excellent way to implement a list. If, however, there is contention,\nthen even if the lock itself performs well, threads will still be delayed waiting for\none another.\n9.5 Fine-Grained Synchronization\nWe can improve concurrency by locking individual nodes, rather than locking\nthe list as a whole. Instead of placing a lock on the entire list, let us add a Lock\nto each node, along with lock () and unlock () methods. As a thread traverses\nthe list, it locks each node when it \ufb01rst visits, and sometime later releases it. Such\n\ufb01ne-grained locking permits concurrent threads to traverse the list together in a\npipelined fashion.\nLet us consider two nodes aandbwhereapoints tob. It is not safe to unlock\nabefore locking bbecause another thread could remove bfrom the list in the\ninterval between unlocking aand lockingb. Instead, thread Amust acquire locks\nin a kind of \u201chand-over-hand\u201d order: except for the initial head sentinel node,\n202 Chapter 9 Linked Lists: The Role of Locking\nacquire the lock for a node only while holding the lock for its predecessor. This\nlocking protocol is sometimes called lock coupling . (Notice that there is no obvi-\nous way to implement lock coupling using Java\u2019s synchronized methods.)\nFig. 9.6 shows the FineList algorithm\u2019s add() method, and Fig. 9.7 its\nremove () method. Just as in the coarse-grained list, remove () makes currA\nunreachable by setting predA\u2019snext \ufb01eld to currA\u2019s successor. T o be safe,\nremove () must lock both predAand currA. T o see why, let us consider the\nfollowing scenario, illustrated in Fig. 9.8. Thread Ais about to remove node a,\nthe \ufb01rst node in the list, while thread Bis about to remove node b, wherea\npoints tob. SupposeAlocks head , andBlocksa.Athen sets head.next tob,\nwhileBsetsa.next toc. The net effect is to remove a, but notb. The problem is\nthat there is no overlap between the", "doc_id": "cf728e8b-a8c5-447f-8e63-f1bfd1d9fd4e", "embedding": null, "doc_hash": "3ab4b0f3ae617802b6aad7612008215a510c4bdeb9dd9684125a637ea35a8de3", "extra_info": null, "node_info": {"start": 530361, "end": 533561}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1c822044-c953-4ee9-be1e-3c7baba319ed", "3": "2c177e7c-1cf1-4858-9600-663fa6bbdff0"}}, "__type__": "1"}, "2c177e7c-1cf1-4858-9600-663fa6bbdff0": {"__data__": {"text": "\ufb01eld to currA\u2019s successor. T o be safe,\nremove () must lock both predAand currA. T o see why, let us consider the\nfollowing scenario, illustrated in Fig. 9.8. Thread Ais about to remove node a,\nthe \ufb01rst node in the list, while thread Bis about to remove node b, wherea\npoints tob. SupposeAlocks head , andBlocksa.Athen sets head.next tob,\nwhileBsetsa.next toc. The net effect is to remove a, but notb. The problem is\nthat there is no overlap between the locks held by the two remove () calls. Fig. 9.9\nillustrates how this \u201chand-over-hand\u201d locking avoids this problem.\nT o guarantee progress, it is important that all methods acquire locks in the\nsame order, starting at the head and following next references toward the tail .\nAs Fig. 9.10 shows, a deadlock could occur if different method calls were to\nacquire locks in different orders. In this example, thread A, trying to add a, has\n1 public boolean add(T item) {\n2 int key = item.hashCode();\n3 head.lock();\n4 Node pred = head;\n5 try {\n6 Node curr = pred.next;\n7 curr.lock();\n8 try {\n9 while (curr.key < key) {\n10 pred.unlock();\n11 pred = curr;\n12 curr = curr.next;\n13 curr.lock();\n14 }\n15 if(curr.key == key) {\n16 return false ;\n17 }\n18 Node newNode = new Node(item);\n19 newNode.next = curr;\n20 pred.next = newNode;\n21 return true ;\n22 }finally {\n23 curr.unlock();\n24 }\n25 }finally {\n26 pred.unlock();\n27 }\n28 }\nFigure 9.6 The FineList class: the add()method uses hand-over-hand locking to traverse\nthe list. The finally blocks release locks before returning.\n9.5 Fine-Grained Synchronization 203\n29 public boolean remove(T item) {\n30 Node pred = null , curr = null ;\n31 int key = item.hashCode();\n32 head.lock();\n33 try {\n34 pred = head;\n35 curr = pred.next;\n36 curr.lock();\n37 try {\n38 while (curr.key < key) {\n39 pred.unlock();\n40 pred = curr;\n41 curr = curr.next;\n42 curr.lock();\n43 }\n44 if(curr.key == key) {\n45 pred.next = curr.next;\n46 return true ;\n47 }\n48 return false ;\n49 }finally {\n50 curr.unlock();\n51 }\n52 }finally {\n53 pred.unlock();\n54 }\n55 }\nFigure 9.7 The FineList class: the remove ()method locks both the node to be removed\nand its predecessor before removing that node.\nb\nhead tailc\nremove b remove aa\nFigure 9.8 TheFineList class: why remove ()must acquire two locks. Thread Ais about to\nremove a, the \ufb01rst node in the list, while thread Bis about to remove b, where apoints\ntob. Suppose Alocks head, and Blocks a. Thread Athen sets head.next tob, while Bsets\na\u2019snext \ufb01eld to c. The net effect is to remove a, but not b.\nlockedband is attempting to lock head , whileB, trying to remove b, has locked\nhead and is trying to lock b. Clearly, these method calls will never \ufb01nish. Avoiding\ndeadlocks is one of the principal challenges of programming with locks.\nTheFineList algorithm maintains the representation invariant: sentinels are\nnever added or removed, and nodes are sorted by key value without duplicates.\n204 Chapter 9 Linked Lists: The Role of", "doc_id": "2c177e7c-1cf1-4858-9600-663fa6bbdff0", "embedding": null, "doc_hash": "3f844f044038cd5b992380d1ef162802ce47d8f4f0910b7499b6a8295ffbf1f7", "extra_info": null, "node_info": {"start": 533552, "end": 536480}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "cf728e8b-a8c5-447f-8e63-f1bfd1d9fd4e", "3": "920576d9-c386-4eaa-99c0-44cd2014733b"}}, "__type__": "1"}, "920576d9-c386-4eaa-99c0-44cd2014733b": {"__data__": {"text": "while Bsets\na\u2019snext \ufb01eld to c. The net effect is to remove a, but not b.\nlockedband is attempting to lock head , whileB, trying to remove b, has locked\nhead and is trying to lock b. Clearly, these method calls will never \ufb01nish. Avoiding\ndeadlocks is one of the principal challenges of programming with locks.\nTheFineList algorithm maintains the representation invariant: sentinels are\nnever added or removed, and nodes are sorted by key value without duplicates.\n204 Chapter 9 Linked Lists: The Role of Locking\nb\nhead tailc a\nremove b remove a\nFigure 9.9 TheFineList class: Hand-over-hand locking ensures that if concurrent remove ()\ncalls try to remove adjacent nodes, then they acquire con\ufb02icting locks. Thread Ais about to\nremove node a, the \ufb01rst node in the list, while thread Bis about to remove node b, where\napoints to b. Because Amust lock both head anda, and Bmust lock both aandb, they are\nguaranteed to con\ufb02ict on a, forcing one call to wait for the other.\nb\nhead tailc\naB: remove b A: add a\nFigure 9.10 The FineList class: a deadlock can occur if, for example, remove ()andadd()\ncalls acquire locks in opposite order. Thread Ais about to insert aby locking \ufb01rst band then\nhead, and thread Bis about to remove node bby locking \ufb01rst head and then b. Each thread\nholds the lock the other is waiting to acquire, so neither makes progress.\nThe abstraction map is the same as for the course-grained list: an item is in the\nset if, and only if its node is reachable.\nThe linearization point for an add(a) call depends on whether the call was\nsuccessful (i.e., whether awas already present). A successful call ( aabsent) is\nlinearized when the node with the next higher key is locked (either Line 7or13).\nThe same distinctions apply to remove (a) calls. An unsuccessful call ( a\npresent) is linearized when the predecessor node is locked (Lines 36or42).\nAn unsuccessful call ( aabsent) is linearized when the node containing the next\nhigher key is locked (Lines 36or42).\nDetermining linearization points for contains () is left as an exercise.\nThe FineList algorithm is starvation-free, but arguing this property is\nharder than in the coarse-grained case. We assume that all individual locks are\nstarvation-free. Because all methods acquire locks in the same down-the-list\n9.6 Optimistic Synchronization 205\norder, deadlock is impossible. If thread Aattempts to lock head , eventually it\nsucceeds. From that point on, because there are no deadlocks, eventually all locks\nheld by threads ahead of Ain the list will be released, and Awill succeed in\nlocking predAandcurrA.\n9.6 Optimistic Synchronization\nAlthough \ufb01ne-grained locking is an improvement over a single, coarse-grained\nlock, it still imposes a potentially long sequence of lock acquisitions and releases.\nMoreover, threads accessing disjoint parts of the list may still block one another.\nFor example, a thread removing the second item in the list blocks all concurrent\nthreads searching for later nodes.\nOne way to reduce synchronization costs is to take a chance: search without\nacquiring locks, lock the nodes found, and then con\ufb01rm that the locked nodes\nare correct. If a synchronization con\ufb02ict causes the wrong nodes to be locked,\nthen release the locks and start over. Normally, this kind of con\ufb02ict is rare, which\nis why we call this technique optimistic synchronization .\nInFig. 9.11 , threadAmakes an optimistic add(a). It traverses the list with-\nout acquiring any", "doc_id": "920576d9-c386-4eaa-99c0-44cd2014733b", "embedding": null, "doc_hash": "5aa096169b26e5c58be82441b0bda03522f657ecf84d7a19b12860ed31ed786e", "extra_info": null, "node_info": {"start": 536435, "end": 539874}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2c177e7c-1cf1-4858-9600-663fa6bbdff0", "3": "985bd4f2-a183-474f-8f34-65c813549cc8"}}, "__type__": "1"}, "985bd4f2-a183-474f-8f34-65c813549cc8": {"__data__": {"text": "blocks all concurrent\nthreads searching for later nodes.\nOne way to reduce synchronization costs is to take a chance: search without\nacquiring locks, lock the nodes found, and then con\ufb01rm that the locked nodes\nare correct. If a synchronization con\ufb02ict causes the wrong nodes to be locked,\nthen release the locks and start over. Normally, this kind of con\ufb02ict is rare, which\nis why we call this technique optimistic synchronization .\nInFig. 9.11 , threadAmakes an optimistic add(a). It traverses the list with-\nout acquiring any locks (Lines 6through 8). In fact, it ignores the locks com-\npletely. It stops the traversal when currA\u2019s key is greater than, or equal to a\u2019s.\nIt then locks predAand currA, and calls validate () to check that predAis\nreachable and its next \ufb01eld still refers to currA. If validation succeeds, then\nthreadAproceeds as before: if currA\u2019s key is greater than a, threadAadds a\nnew node with item abetween predAandcurrA, and returns true. Otherwise it\nreturns false . The remove () and contains () methods ( Figs. 9.12 and 9.13 ) oper-\nate similarly, traversing the list without locking, then locking the target nodes\nand validating they are still in the list. T o be consistent with the Java memory\nmodel, the next \ufb01elds in the nodes need to be declared volatile.\nThe code of validate () appears in Fig. 9.14 . We are reminded of the following\nstory:\nA tourist takes a taxi in a foreign town. The taxi driver speeds through a red\nlight. The tourist, frightened, asks \u201cWhat are you are doing?\u201d The driver answers:\n\u201cDo not worry, I am an expert.\u201d He speeds through more red lights, and the\ntourist, on the verge of hysteria, complains again, more urgently. The driver\nreplies, \u201cRelax, relax, you are in the hands of an expert.\u201d Suddenly, the light turns\ngreen, the driver slams on the brakes, and the taxi skids to a halt. The tourist picks\nhimself off the \ufb02oor of the taxi and asks \u201cFor crying out loud, why stop now that\nthe light is \ufb01nally green?\u201d The driver answers \u201cT oo dangerous, could be another\nexpert crossing.\u201d\nTraversing any dynamically changing lock-based data structure while ignor-\ning locks requires careful thought (there are other expert threads out there). We\nmust make sure to use some form of validation and guarantee freedom from\ninterference .\n206 Chapter 9 Linked Lists: The Role of Locking\n1 public boolean add(T item) {\n2 int key = item.hashCode();\n3 while (true ) {\n4 Node pred = head;\n5 Node curr = pred.next;\n6 while (curr.key < key) {\n7 pred = curr; curr = curr.next;\n8 }\n9 pred.lock(); curr.lock();\n10 try {\n11 if(validate(pred, curr)) {\n12 if(curr.key == key) {\n13 return false ;\n14 }else {\n15 Node node = new Node(item);\n16 node.next = curr;\n17 pred.next = node;\n18 return true ;\n19 }\n20 }\n21 }finally {\n22 pred.unlock(); curr.unlock();\n23 }\n24 }\n25 }\nFigure 9.11 The OptimisticList class: the add()method traverses the list ignoring locks,\nacquires locks, and validates before adding the new node.\n26 public boolean remove(T item) {\n27 int key = item.hashCode();\n28 while (true", "doc_id": "985bd4f2-a183-474f-8f34-65c813549cc8", "embedding": null, "doc_hash": "f11232c7c0298a695b1ae1e7af393466d1cf8e5d8e88791cbb55ebd380534972", "extra_info": null, "node_info": {"start": 539850, "end": 542883}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "920576d9-c386-4eaa-99c0-44cd2014733b", "3": "f784056a-3adf-4cda-94ee-cd31a7703a4c"}}, "__type__": "1"}, "f784056a-3adf-4cda-94ee-cd31a7703a4c": {"__data__": {"text": "== key) {\n13 return false ;\n14 }else {\n15 Node node = new Node(item);\n16 node.next = curr;\n17 pred.next = node;\n18 return true ;\n19 }\n20 }\n21 }finally {\n22 pred.unlock(); curr.unlock();\n23 }\n24 }\n25 }\nFigure 9.11 The OptimisticList class: the add()method traverses the list ignoring locks,\nacquires locks, and validates before adding the new node.\n26 public boolean remove(T item) {\n27 int key = item.hashCode();\n28 while (true ) {\n29 Node pred = head;\n30 Node curr = pred.next;\n31 while (curr.key < key) {\n32 pred = curr; curr = curr.next;\n33 }\n34 pred.lock(); curr.lock();\n35 try {\n36 if(validate(pred, curr)) {\n37 if(curr.key == key) {\n38 pred.next = curr.next;\n39 return true ;\n40 }else {\n41 return false ;\n42 }\n43 }\n44 }finally {\n45 pred.unlock(); curr.unlock();\n46 }\n47 }\n48 }\nFigure 9.12 The OptimisticList class: the remove ()method traverses ignoring locks,\nacquires locks, and validates before removing the node.\n9.6 Optimistic Synchronization 207\n49 public boolean contains(T item) {\n50 int key = item.hashCode();\n51 while (true ) {\n52 Node pred = this .head; // sentinel node;\n53 Node curr = pred.next;\n54 while (curr.key < key) {\n55 pred = curr; curr = curr.next;\n56 }\n57 pred.lock(); curr.lock();\n58 try {\n59 if(validate(pred, curr)) {\n60 return (curr.key == key);\n61 }\n62 }finally { // always unlock\n63 pred.unlock(); curr.unlock();\n64 }\n65 }\n66 }\nFigure 9.13 The OptimisticList class: the contains ()method searches, ignoring locks,\nthen it acquires locks, and validates to determine if the node is in the list.\n67 private boolean validate(Node pred, Node curr) {\n68 Node node = head;\n69 while (node.key <= pred.key) {\n70 if(node == pred)\n71 return pred.next == curr;\n72 node = node.next;\n73 }\n74 return false ;\n75 }\nFigure 9.14 The OptimisticList : validation checks that pred Apoints to curr Aand is\nreachable from head.\nAs Fig. 9.15 shows, validation is necessary because the trail of references lead-\ning to predAor the reference from predAtocurrAcould have changed between\nwhen they were last read by Aand whenAacquired the locks. In particular,\na thread could be traversing parts of the list that have already been removed.\nFor example, the node currAand all nodes between currAanda(includinga)\nmay be removed while Ais still traversing currA. ThreadAdiscovers that currA\npoints toa, and, without validation, \u201csuccessfully\u201d removes a, even though ais\nno longer in the list. A validate () call detects that ais no longer in the list,\nand the caller restarts the method.\nBecause we are ignoring the locks that protect concurrent modi\ufb01cations, each\nof the method calls may traverse nodes that have been removed from the list.\nNevertheless, absence of interference implies that once a node has been unlinked\nfrom the list, the value of its next \ufb01eld does not change, so following a sequence\nof such links eventually leads back to the list. Absence of interference, in turn,\nrelies on garbage collection to ensure that no node is recycled while it is being\ntraversed.\n208 Chapter 9 Linked", "doc_id": "f784056a-3adf-4cda-94ee-cd31a7703a4c", "embedding": null, "doc_hash": "d1b86d60fe85f03f410e14cbe4755c46bbc84ed133507378b73c1d883b1e5b7d", "extra_info": null, "node_info": {"start": 542967, "end": 545972}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "985bd4f2-a183-474f-8f34-65c813549cc8", "3": "66cbdbc6-cbb8-4ef5-b411-13ba5de8f7fe"}}, "__type__": "1"}, "66cbdbc6-cbb8-4ef5-b411-13ba5de8f7fe": {"__data__": {"text": "list,\nand the caller restarts the method.\nBecause we are ignoring the locks that protect concurrent modi\ufb01cations, each\nof the method calls may traverse nodes that have been removed from the list.\nNevertheless, absence of interference implies that once a node has been unlinked\nfrom the list, the value of its next \ufb01eld does not change, so following a sequence\nof such links eventually leads back to the list. Absence of interference, in turn,\nrelies on garbage collection to ensure that no node is recycled while it is being\ntraversed.\n208 Chapter 9 Linked Lists: The Role of Locking\na\ncurrAheadpredA\ntail\n. . .\nFigure 9.15 The OptimisticList class: why validation is needed. Thread Ais attempting\nto remove a node a. While traversing the list, currAand all nodes between curr Aanda\n(including a) might be removed (denoted by a lighter node color). In such a case, thread A\nwould proceed to the point where curr Apoints to a, and, without validation, would success-\nfully remove a, even though it is no longer in the list. Validation is required to determine that\nais no longer reachable from head.\nTheOptimisticList algorithm is not starvation-free, even if all node locks\nare individually starvation-free. A thread might be delayed forever if new nodes\nare repeatedly added and removed (see Exercise 107). Nevertheless, we would\nexpect this algorithm to do well in practice, since starvation is rare.\n9.7 Lazy Synchronization\nThe OptimisticList implementation works best if the cost of traversing the\nlist twice without locking is signi\ufb01cantly less than the cost of traversing the list\nonce with locking. One drawback of this particular algorithm is that contains ()\nacquires locks, which is unattractive since contains () calls are likely to be much\nmore common than calls to other methods.\nThe next step is to re\ufb01ne this algorithm so that contains () calls are wait-\nfree, and add() and remove () methods, while still blocking, traverse the list only\nonce (in the absence of contention). We add to each node a Boolean marked \ufb01eld\nindicating whether that node is in the set. Now, traversals do not need to lock\nthe target node, and there is no need to validate that the node is reachable by\nretraversing the whole list. Instead, the algorithm maintains the invariant that\nevery unmarked node is reachable. If a traversing thread does not \ufb01nd a node,\nor \ufb01nds it marked, then that item is not in the set. As a result, contains () needs\nonly one wait-free traversal. T o add an element to the list, add() traverses the list,\nlocks the target\u2019s predecessor, and inserts the node. The remove () method is lazy,\ntaking two steps: \ufb01rst, mark the target node, logically removing it, and second,\nredirect its predecessor\u2019s next \ufb01eld, physically removing it.\n9.7 Lazy Synchronization 209\nIn more detail, all methods traverse the list (possibly traversing logically and\nphysically removed nodes) ignoring the locks. The add() and remove () methods\nlock the predAandcurrAnodes as before (Figs. 9.17 and 9.18), but validation\ndoes not retraverse the entire list (Fig. 9.16) to determine whether a node is in\nthe set. Instead, because a node must be marked before being physically removed,\nvalidation need only check that currAhas not been marked. However, as Fig. 9.20\nshows, for insertion and deletion, since predAis the one being modi\ufb01ed, one\nmust also check that predAitself is not marked, and that it points to currA.\nLogical removals", "doc_id": "66cbdbc6-cbb8-4ef5-b411-13ba5de8f7fe", "embedding": null, "doc_hash": "8bcc6fca74969d14eaed44f26515696b4fc6a499e8320fa2b710a27baa8811f3", "extra_info": null, "node_info": {"start": 545868, "end": 549296}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f784056a-3adf-4cda-94ee-cd31a7703a4c", "3": "15411be5-ffd6-4fb5-ac14-808eb20a6a03"}}, "__type__": "1"}, "15411be5-ffd6-4fb5-ac14-808eb20a6a03": {"__data__": {"text": "the predAandcurrAnodes as before (Figs. 9.17 and 9.18), but validation\ndoes not retraverse the entire list (Fig. 9.16) to determine whether a node is in\nthe set. Instead, because a node must be marked before being physically removed,\nvalidation need only check that currAhas not been marked. However, as Fig. 9.20\nshows, for insertion and deletion, since predAis the one being modi\ufb01ed, one\nmust also check that predAitself is not marked, and that it points to currA.\nLogical removals require a small change to the abstraction map: an item is in\nthe set if, and only if it is referred to by an unmarked reachable node. Notice\n1 private boolean validate(Node pred, Node curr) {\n2 return !pred.marked && !curr.marked && pred.next == curr;\n3 }\nFigure 9.16 TheLazyList class: validation checks that neither the pred nor the curr node\nhas been logically deleted, and that pred points to curr.\n1 public boolean add(T item) {\n2 int key = item.hashCode();\n3 while (true ) {\n4 Node pred = head;\n5 Node curr = head.next;\n6 while (curr.key < key) {\n7 pred = curr; curr = curr.next;\n8 }\n9 pred.lock();\n10 try {\n11 curr.lock();\n12 try {\n13 if(validate(pred, curr)) {\n14 if(curr.key == key) {\n15 return false ;\n16 }else {\n17 Node node = new Node(item);\n18 node.next = curr;\n19 pred.next = node;\n20 return true ;\n21 }\n22 }\n23 }finally {\n24 curr.unlock();\n25 }\n26 }finally {\n27 pred.unlock();\n28 }\n29 }\n30 }\nFigure 9.17 TheLazyList class: add()method.\n210 Chapter 9 Linked Lists: The Role of Locking\n1 public boolean remove(T item) {\n2 int key = item.hashCode();\n3 while (true ) {\n4 Node pred = head;\n5 Node curr = head.next;\n6 while (curr.key < key) {\n7 pred = curr; curr = curr.next;\n8 }\n9 pred.lock();\n10 try {\n11 curr.lock();\n12 try {\n13 if(validate(pred, curr)) {\n14 if(curr.key != key) {\n15 return false ;\n16 }else {\n17 curr.marked = true ;\n18 pred.next = curr.next;\n19 return true ;\n20 }\n21 }\n22 }finally {\n23 curr.unlock();\n24 }\n25 }finally {\n26 pred.unlock();\n27 }\n28 }\n29 }\nFigure 9.18 The LazyList class: the remove ()method removes nodes in two steps, logical\nand physical.\n1 public boolean contains(T item) {\n2 int key = item.hashCode();\n3 Node curr = head;\n4 while (curr.key < key)\n5 curr = curr.next;\n6 return curr.key == key && !curr.marked;\n7 }\nFigure 9.19 TheLazyList class: the contains ()method.\nthat the path along which the node is reachable may contain marked nodes. The\nreader should check that any unmarked reachable node remains reachable, even\nif its predecessor is logically or physically deleted. As in the OptimisticList\nalgorithm, add() and remove () are not starvation-free, because list traversals may\nbe arbitrarily delayed by ongoing modi\ufb01cations.\nThecontains () method (Fig. 9.19) traverses the list once ignoring locks and\nreturns true if the node it was searching for is present and unmarked, and false\n9.7 Lazy Synchronization 211\n00 0", "doc_id": "15411be5-ffd6-4fb5-ac14-808eb20a6a03", "embedding": null, "doc_hash": "571fd514970dd2952f66178bebba605143f228699f195236d840d569c9dc2a5c", "extra_info": null, "node_info": {"start": 549358, "end": 552215}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "66cbdbc6-cbb8-4ef5-b411-13ba5de8f7fe", "3": "e5fb2c82-dceb-4684-bbb5-e5bf5159701d"}}, "__type__": "1"}, "e5fb2c82-dceb-4684-bbb5-e5bf5159701d": {"__data__": {"text": "which the node is reachable may contain marked nodes. The\nreader should check that any unmarked reachable node remains reachable, even\nif its predecessor is logically or physically deleted. As in the OptimisticList\nalgorithm, add() and remove () are not starvation-free, because list traversals may\nbe arbitrarily delayed by ongoing modi\ufb01cations.\nThecontains () method (Fig. 9.19) traverses the list once ignoring locks and\nreturns true if the node it was searching for is present and unmarked, and false\n9.7 Lazy Synchronization 211\n00 0 a 0\nhead tail0 0 0 a 1predA\nhead tailcurrA (a)\n(b)\n0predA currA\nFigure 9.20 The LazyList class: why validation is needed. In Part (a) of the \ufb01gure, thread A\nis attempting to remove node a. After it reaches the point where pred Arefers to curr A, and\nbefore it acquires locks on these nodes, the node pred Ais logically and physically removed.\nAfter Aacquires the locks, validation will detect the problem. In Part (b) of the \ufb01gure, Ais\nattempting to remove node a. After it reaches the point where pred Arefers to curr A, and\nbefore it acquires locks on these nodes, a new node is added between pred Aandcurr A. After\nAacquires the locks, even though neither pred Aorcurr Aare marked, validation detects that\npred Ais not the same as curr A, and A\u2019s call to remove ()will be restarted.\notherwise. It is thus wait-free.5A marked node\u2019s value is ignored. Each time the\ntraversal moves to a new node, the new node has a larger key than the previous\none, even if the node is logically deleted.\nThe linearization points for LazyList add () and unsuccessful remove () calls\nare the same as for the OptimisticList . A successful remove () call is linearized\nwhen the mark is set (Line 17), and a successful contains () call is linearized\nwhen an unmarked matching node is found.\nT o understand how to linearize an unsuccessful contains (), let us consider\nthe scenario depicted in Fig. 9.21 . In Part (a), node ais marked as removed (its\nmarked \ufb01eld is set) and thread Ais attempting to \ufb01nd the node matching a\u2019s key.\nWhileAis traversing the list, currAand all nodes between currAandaincluding\naare removed, both logically and physically. Thread Awould still proceed to the\npoint where currApoints toa, and would detect that ais marked and no longer\nin the abstract set. The call could be linearized at this point.\nNow let us consider the scenario depicted in Part (b). While Ais traversing\nthe removed section of the list leading to a, and before it reaches the removed\n5Notice that the list ahead of a given traversing thread cannot grow forever due to newly inserted\nkeys, since the key size is \ufb01nite.\n212 Chapter 9 Linked Lists: The Role of Locking\n0a 1100 0 b 0\n0 b(a)\n(b)pred A\ncurrAhead tail\nhead tail. . .\na 11\ncurrA. . .00a\n0\nFigure 9.21 TheLazyList class: linearizing an unsuccessful contains ()call. Dark nodes are\nphysically in the list and white nodes are physically removed. In Part (a), while thread Ais\ntraversing the list, a concurrent remove ()call disconnects the sublist referred to by curr.\nNotice that nodes with items aandbare still reachable, so whether an item is actually in the\nlist depends only on whether it is marked. Thread A\u2019s call is linearized at the point when it\nsees that ais marked and is no longer in the abstract set. Alternatively, let us consider", "doc_id": "e5fb2c82-dceb-4684-bbb5-e5bf5159701d", "embedding": null, "doc_hash": "05403281ebda471c8a079da69c5149f2d6108403593b5ecb44a52cd37f726bfe", "extra_info": null, "node_info": {"start": 552164, "end": 555484}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "15411be5-ffd6-4fb5-ac14-808eb20a6a03", "3": "1ce8130f-d2a9-412d-adb8-e6c237655759"}}, "__type__": "1"}, "1ce8130f-d2a9-412d-adb8-e6c237655759": {"__data__": {"text": "9.21 TheLazyList class: linearizing an unsuccessful contains ()call. Dark nodes are\nphysically in the list and white nodes are physically removed. In Part (a), while thread Ais\ntraversing the list, a concurrent remove ()call disconnects the sublist referred to by curr.\nNotice that nodes with items aandbare still reachable, so whether an item is actually in the\nlist depends only on whether it is marked. Thread A\u2019s call is linearized at the point when it\nsees that ais marked and is no longer in the abstract set. Alternatively, let us consider the\nscenario depicted in Part (b). While thread Ais traversing the list leading to marked node\na, another thread adds a new node with key a. It would be wrong to linearize thread A\u2019s\nunsuccessful contains ()call to when it found the marked node a, since this point occurs\nafter the insertion of the new node with key ato the list.\nnodea, another thread adds a new node with a key ato the reachable part of the\nlist. Linearizing thread A\u2019s unsuccessful contains () method at the point it \ufb01nds\nthe marked node awould be wrong, since this point occurs after the insertion\nof the new node with key ato the list. We therefore linearize an unsuccessful\ncontains () method call within its execution interval at the earlier of the fol-\nlowing points: (1) the point where a removed matching node, or a node with a\nkey greater than the one being searched for is found, and (2) the point immedi-\nately before a new matching node is added to the list. Notice that the second is\nguaranteed to be within the execution interval because the insertion of the new\nnode with the same key must have happened after the start of the contains ()\nmethod, or the contains () method would have found that item. As can be\n9.8 Non-Blocking Synchronization 213\nseen, the linearization point of the unsuccessful contains () is determined by\nthe ordering of events in the execution, and is not a predetermined point in the\nmethod\u2019s code.\nOne bene\ufb01t of lazy synchronization is that we can separate unobtrusive logi-\ncal steps such as setting a \ufb02ag, from disruptive physical changes to the structure,\nsuch as disconnecting a node. The example presented here is simple because we\ndisconnect one node at a time. In general, however, delayed operations can be\nbatched and performed lazily at a convenient time, reducing the overall disrup-\ntiveness of physical modi\ufb01cations to the structure.\nThe principal disadvantage of the LazyList algorithm is that add() and\nremove () calls are blocking: if one thread is delayed, then others may also be\ndelayed.\n9.8 Non-Blocking Synchronization\nWe have seen that it is sometimes a good idea to mark nodes as logically removed\nbefore physically removing them from the list. We now show how to extend this\nidea to eliminate locks altogether, allowing all three methods, add(),remove (),\nandcontains (), to be nonblocking. (The \ufb01rst two methods are lock-free and the\nlast wait-free). A na \u00a8\u0131ve approach would be to use compareAndSet() to change\nthenext \ufb01elds. Unfortunately, this idea does not work. In Fig. 9.22, part (a)\nshows a thread Aattempting to remove a node awhile thread Bis adding a\nnode b. Suppose Aapplies compareAndSet() tohead.next , while Bapplies\ncompareAndSet() toa.next . The net effect is that ais correctly deleted but b\nis not added to the list. In part (b) of the \ufb01gure, Aattempts to remove a, the \ufb01rst\nnode in the list, while Bis about to remove b, where a points to b. Suppose A\napplies compareAndSet()", "doc_id": "1ce8130f-d2a9-412d-adb8-e6c237655759", "embedding": null, "doc_hash": "d16d42f363184feb9d5c40a9c9c8acc7e444acd539af8bb43723de8c6fff50d6", "extra_info": null, "node_info": {"start": 555487, "end": 558962}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e5fb2c82-dceb-4684-bbb5-e5bf5159701d", "3": "eecd3f6b-2c03-4de7-9462-b5e58fb8fe71"}}, "__type__": "1"}, "eecd3f6b-2c03-4de7-9462-b5e58fb8fe71": {"__data__": {"text": "idea does not work. In Fig. 9.22, part (a)\nshows a thread Aattempting to remove a node awhile thread Bis adding a\nnode b. Suppose Aapplies compareAndSet() tohead.next , while Bapplies\ncompareAndSet() toa.next . The net effect is that ais correctly deleted but b\nis not added to the list. In part (b) of the \ufb01gure, Aattempts to remove a, the \ufb01rst\nnode in the list, while Bis about to remove b, where a points to b. Suppose A\napplies compareAndSet() tohead.next , while Bapplies compareAndSet() to\na.next . The net effect is to remove a, but not b.\nIfBwants to remove currBfrom the list, it might call compareAndSet()\nto set predB\u2019s next \ufb01eld to currB\u2019s successor. It is not hard to see that if these\ntwo threads try to remove these adjacent nodes concurrently, the list will end up\nwith b not being removed. A similar situation for a pair of concurrent add() and\nremove () methods is depicted in the upper part of Fig. 9.22.\nClearly, we need a way to ensure that a node\u2019s \ufb01elds cannot be updated, after\nthat node has been logically or physically removed from the list. Our approach is\nto treat the node\u2019s next andmarked \ufb01elds as a single atomic unit: any attempt to\nupdate the next \ufb01eld when the marked \ufb01eld is true will fail.\nPragma 9.8.1. An AtomicMarkableReference<T> is an object from the\njava.util.concurrent.atomic package that encapsulates both a reference to an\nobject of type Tand a Boolean mark . These \ufb01elds can be updated atomically,\neither together or individually. For example, the compareAndSet() method\n214 Chapter 9 Linked Lists: The Role of Locking\nb\nhead tailc a\nremove b remove aremove a\nbhead tail\nc a\nadd b(a)\n(b)\nFigure 9.22 TheLockFreeList class: why mark and reference \ufb01elds must be modi\ufb01ed atomi-\ncally. In Part (a) of the \ufb01gure, thread Ais about to remove a, the \ufb01rst node in the list, while\nBis about to add b. Suppose Aapplies compareAndSet() tohead.next , while Bapplies\ncompareAndSet() toa.next . The net effect is that ais correctly deleted but bis not added\nto the list. In Part (b) of the \ufb01gure, thread Ais about to remove a, the \ufb01rst node in the list,\nwhile Bis about to remove b, where apoints to b. Suppose Aapplies compareAndSet() to\nhead.next , while Bapplies compareAndSet() toa.next . The net effect is to remove a, but\nnotb.\ntests the expected reference and mark values, and if both tests succeed,\nreplaces them with updated reference and mark values. As shorthand, the\nattemptMark () method tests an expected reference value and if the test suc-\nceeds, replaces it with a new mark value. The get() method has an unusual\ninterface: it returns the object\u2019s reference value and stores the mark value in a\nBoolean array argument. Fig. 9.23 illustrates the interfaces of these methods.\n1public boolean compareAndSet(T expectedReference,\n2 T newReference,\n3 boolean expectedMark,\n4 boolean newMark);\n5public boolean attemptMark(T expectedReference,\n6 boolean newMark);\n7public T get( boolean [] marked);\nFigure 9.23 Some AtomicMarkableReference<T> methods: the compareAndSet()\nmethod tests and updates both the mark and reference \ufb01elds, while the attemptMark ()\nmethod updates the mark", "doc_id": "eecd3f6b-2c03-4de7-9462-b5e58fb8fe71", "embedding": null, "doc_hash": "14fd997d6cbc7ade8b5a7b13caeedc1fbedc0d75039deb3019ca562666ad7ba8", "extra_info": null, "node_info": {"start": 559051, "end": 562172}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1ce8130f-d2a9-412d-adb8-e6c237655759", "3": "2168fb38-6bad-467f-b6d8-389795bedd16"}}, "__type__": "1"}, "2168fb38-6bad-467f-b6d8-389795bedd16": {"__data__": {"text": "reference value and stores the mark value in a\nBoolean array argument. Fig. 9.23 illustrates the interfaces of these methods.\n1public boolean compareAndSet(T expectedReference,\n2 T newReference,\n3 boolean expectedMark,\n4 boolean newMark);\n5public boolean attemptMark(T expectedReference,\n6 boolean newMark);\n7public T get( boolean [] marked);\nFigure 9.23 Some AtomicMarkableReference<T> methods: the compareAndSet()\nmethod tests and updates both the mark and reference \ufb01elds, while the attemptMark ()\nmethod updates the mark if the reference \ufb01eld has the expected value. The get()method\nreturns the encapsulated reference and stores the mark at position 0 in the argument\narray.\n9.8 Non-Blocking Synchronization 215\nIn C or C++, one could provide this functionality ef\ufb01ciently by \u201csteal-\ning\u201d a bit from a pointer, using bit-wise operators to extract the mark and\nthe pointer from a single word. In Java, of course, one cannot manipulate\npointers directly, so this functionality must be provided by a library.\nAs described in detail in Pragma 9.8.1, an AtomicMarkableReference<T>\nobject encapsulates both a reference to an object of type Tand a Boolean mark .\nThese \ufb01elds can be atomically updated, either together or individually.\nWe make each node\u2019s next \ufb01eld an AtomicMarkableReference<Node> .\nThreadAlogically removes currAby setting the mark bit in the node\u2019s next\n\ufb01eld, and shares the physical removal with other threads performing add()\norremove (): as each thread traverses the list, it cleans up the list by physi-\ncally removing (using compareAndSet() ) any marked nodes it encounters. In\nother words, threads performing add() and remove () do not traverse marked\nnodes, they remove them before continuing. The contains () method remains\nthe same as in the LazyList algorithm, traversing all nodes whether they\nare marked or not, and testing if an item is in the list based on its key and\nmark.\nIt is worth pausing to consider a design decision that differentiates the\nLockFreeList algorithm from the LazyList algorithm. Why do threads that\nadd or remove nodes never traverse marked nodes, and instead physically remove\nall marked nodes they encounter? Suppose that thread Awere to traverse marked\nnodes without physically removing them, and after logically removing currA,\nwere to attempt to physically remove it as well. It could do so by calling\ncompareAndSet() to try to redirect predA\u2019snext \ufb01eld, simultaneously veri-\nfying that predAis not marked and that it refers to currA. The dif\ufb01culty is that\nbecauseAis not holding locks on predAandcurrA, other threads could insert\nnew nodes or remove predAbefore the compareAndSet() call.\nConsider a scenario in which another thread marks predA. As illustrated\nin Fig. 9.22, we cannot safely redirect the next \ufb01eld of a marked node, so A\nwould have to restart the physical removal by retraversing the list. This time,\nhowever,Awould have to physically remove predAbefore it could remove\ncurrA. Even worse, if there is a sequence of logically removed nodes leading to\npredA,Amust remove them all, one after the other, before it can remove currA\nitself.\nThis example illustrates why add() and remove () calls do not traverse marked\nnodes: when they arrive at the node to be modi\ufb01ed, they may be forced to\nretraverse the list to remove previous marked nodes. Instead, we choose to have\nboth add() and remove () physically remove any marked nodes on the path to\ntheir target node. The contains () method, by", "doc_id": "2168fb38-6bad-467f-b6d8-389795bedd16", "embedding": null, "doc_hash": "59587b4feef4ec0d7711b7ef5eeaa72d6faaf0af2063584ad51dbc4ef97e23c2", "extra_info": null, "node_info": {"start": 562081, "end": 565541}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "eecd3f6b-2c03-4de7-9462-b5e58fb8fe71", "3": "3d7e2eea-2036-49de-95fa-80ac7924a7e7"}}, "__type__": "1"}, "3d7e2eea-2036-49de-95fa-80ac7924a7e7": {"__data__": {"text": "it could remove\ncurrA. Even worse, if there is a sequence of logically removed nodes leading to\npredA,Amust remove them all, one after the other, before it can remove currA\nitself.\nThis example illustrates why add() and remove () calls do not traverse marked\nnodes: when they arrive at the node to be modi\ufb01ed, they may be forced to\nretraverse the list to remove previous marked nodes. Instead, we choose to have\nboth add() and remove () physically remove any marked nodes on the path to\ntheir target node. The contains () method, by contrast, performs no modi\ufb01-\ncation, and therefore need not participate in the cleanup of logically removed\nnodes, allowing it, as in the LazyList , to traverse both marked and unmarked\nnodes.\n216 Chapter 9 Linked Lists: The Role of Locking\nIn presenting our LockFreeList algorithm, we factor out functionality\ncommon to the add() and remove () methods by creating an inner Window class to\nhelp navigation. As shown in Fig. 9.24 , aWindow object is a structure with pred\nandcurr \ufb01elds. The find () method takes a head node and a key a, and traverses\nthe list, seeking to set pred to the node with the largest key less than a, and curr\nto the node with the least key greater than or equal to a. As threadAtraverses\nthe list, each time it advances currA, it checks whether that node is marked\n(Line 16). If so, it calls compareAndSet() to attempt to physically remove the\nnode by setting predA\u2019snext \ufb01eld to currA\u2019snext \ufb01eld. This call tests both the\n\ufb01eld\u2019s reference and Boolean mark values, and fails if either value has changed.\nA concurrent thread could change the mark value by logically removing predA,\nor it could change the reference value by physically removing currA. If the call\nfails,Arestarts the traversal from the head of the list; otherwise the traversal\ncontinues.\nTheLockFreeList algorithm uses the same abstraction map as the LazyList\nalgorithm: an item is in the set if, and only if it is in an unmarked reachable node.\n1 class Window {\n2 public Node pred, curr;\n3 Window(Node myPred, Node myCurr) {\n4 pred = myPred; curr = myCurr;\n5 }\n6 }\n7 public Window find(Node head, int key) {\n8 Node pred = null , curr = null , succ = null ;\n9 boolean [] marked = { false };\n10 boolean snip;\n11 retry: while (true ) {\n12 pred = head;\n13 curr = pred.next.getReference();\n14 while (true ) {\n15 succ = curr.next.get(marked);\n16 while (marked[0]) {\n17 snip = pred.next.compareAndSet(curr, succ, false ,false );\n18 if(!snip) continue retry;\n19 curr = succ;\n20 succ = curr.next.get(marked);\n21 }\n22 if(curr.key >= key)\n23 return new Window(pred, curr);\n24 pred = curr;\n25 curr = succ;\n26 }\n27 }\n28 }\nFigure 9.24 The Window class: the find ()method returns a structure containing the nodes\non either side of the key. It removes marked nodes when it encounters them.\n9.8 Non-Blocking Synchronization 217\nThecompareAndSet() call at Line 17of the find () method is an example of a\nbenevolent side effect : it changes the concrete list without changing the abstract\nset, because removing a marked node does not change the value of the abstrac-\ntion map.\nFig. 9.25 shows the LockFreeList class\u2019s add() method. Suppose thread A\ncalls", "doc_id": "3d7e2eea-2036-49de-95fa-80ac7924a7e7", "embedding": null, "doc_hash": "273cbe979e1b47deb4ce1ddc901641dda65b763dd6a055d9e01f7d460e7c42f8", "extra_info": null, "node_info": {"start": 565560, "end": 568721}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2168fb38-6bad-467f-b6d8-389795bedd16", "3": "b4ddf1cf-ca01-47ec-8798-2a81ea6b6f68"}}, "__type__": "1"}, "b4ddf1cf-ca01-47ec-8798-2a81ea6b6f68": {"__data__": {"text": "}\n28 }\nFigure 9.24 The Window class: the find ()method returns a structure containing the nodes\non either side of the key. It removes marked nodes when it encounters them.\n9.8 Non-Blocking Synchronization 217\nThecompareAndSet() call at Line 17of the find () method is an example of a\nbenevolent side effect : it changes the concrete list without changing the abstract\nset, because removing a marked node does not change the value of the abstrac-\ntion map.\nFig. 9.25 shows the LockFreeList class\u2019s add() method. Suppose thread A\ncalls add(a).Auses find () to locate predAandcurrA. IfcurrA\u2019s key is equal\ntoa\u2019s, the call returns false . Otherwise, add() initializes a new node ato hold\na, and setsato refer to currA. It then calls compareAndSet() (Line 11)\nto set predAtoa. Because the compareAndSet() tests both the mark and\nthe reference, it succeeds only if predAis unmarked and refers to currA. If\nthecompareAndSet() is successful, the method returns true, and otherwise it\nstarts over.\nFig. 9.26 shows the LockFreeList algorithm\u2019s remove () method. When A\ncalls remove () to remove item a, it uses find () to locate predAandcurrA. If\ncurrA\u2019s key fails to match a\u2019s, the call returns false . Otherwise, remove () uses\nacompareAndSet() to attempt to mark currAas logically removed (Line 27).\nThis call succeeds only if no other thread has set the mark \ufb01rst. If it succeeds,\nthe call returns true. A single attempt is made to physically remove the node, but\nthere is no need to try again because the node will be removed by the next thread\nto traverse that region of the list. If the compareAndSet() call fails, remove ()\nstarts over.\nThe LockFreeList algorithm\u2019s contains () method is virtually the same as\nthat of the LazyList (Fig. 9.27 ). There is one small change: to test if curr is\nmarked we must apply curr.next.get(marked) and check that marked [0] is\ntrue.\n1public boolean add(T item) {\n2 int key = item.hashCode();\n3 while (true ) {\n4 Window window = find(head, key);\n5 Node pred = window.pred, curr = window.curr;\n6 if(curr.key == key) {\n7 return false ;\n8 }else {\n9 Node node = new Node(item);\n10 node.next = new AtomicMarkableReference(curr, false );\n11 if(pred.next.compareAndSet(curr, node, false ,false )) {\n12 return true ;\n13 }\n14 }\n15 }\n16 }\nFigure 9.25 The LockFreeList class: the add()method calls find ()to locate pred Aand\ncurr A. It adds a new node only if pred Ais unmarked and refers to curr A.\n218 Chapter 9 Linked Lists: The Role of Locking\n17 public boolean remove(T item) {\n18 int key = item.hashCode();\n19 boolean snip;\n20 while (true ) {\n21 Window window = find(head, key);\n22 Node pred = window.pred, curr = window.curr;\n23 if(curr.key != key) {\n24 return false ;\n25 }else {\n26 Node succ = curr.next.getReference();\n27 snip = curr.next.compareAndSet(succ, succ, false ,true );\n28 if(!snip)\n29 continue ;\n30 pred.next.compareAndSet(curr, succ, false ,false );\n31 return true ;\n32 }\n33 }\n34 }\nFigure 9.26 TheLockFreeList class: the remove ()method calls find ()to locate pred Aand\ncurr", "doc_id": "b4ddf1cf-ca01-47ec-8798-2a81ea6b6f68", "embedding": null, "doc_hash": "3e4fd549f662c9216f03f03795e107c9ba282906836b19bf3ad9b45260d0a9cc", "extra_info": null, "node_info": {"start": 568715, "end": 571732}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3d7e2eea-2036-49de-95fa-80ac7924a7e7", "3": "8e22705b-9910-44ab-b73a-2b49e21e8939"}}, "__type__": "1"}, "8e22705b-9910-44ab-b73a-2b49e21e8939": {"__data__": {"text": "window.pred, curr = window.curr;\n23 if(curr.key != key) {\n24 return false ;\n25 }else {\n26 Node succ = curr.next.getReference();\n27 snip = curr.next.compareAndSet(succ, succ, false ,true );\n28 if(!snip)\n29 continue ;\n30 pred.next.compareAndSet(curr, succ, false ,false );\n31 return true ;\n32 }\n33 }\n34 }\nFigure 9.26 TheLockFreeList class: the remove ()method calls find ()to locate pred Aand\ncurr A, and atomically marks the node for removal.\n35 public boolean contains(T item) {\n36 boolean [] marked = false ;\n37 int key = item.hashCode();\n38 Node curr = head;\n39 while (curr.key < key) {\n40 curr = curr.next.getReference();\n41 Node succ = curr.next.get(marked);\n42 }\n43 return (curr.key == key && !marked[0])\n44 }\nFigure 9.27 The LockFreeList class: the wait-free contains ()method is almost the same\nas in the LazyList class. There is one small difference: it calls curr.next.get(marked) to\ntest whether curr is marked.\n9.9 Discussion\nWe have seen a progression of list-based lock implementations in which the\ngranularity and frequency of locking was gradually reduced, eventually reach-\ning a fully nonblocking list. The \ufb01nal transition from the LazyList to the\nLockFreeList exposes some of the design decisions that face concurrent\nprogrammers. As we will see, approaches such as optimistic and lazy syn-\nchronization will appear time and again when designing more complex data\nstructures.\n9.11 Exercises 219\nOn the one hand, the LockFreeList algorithm guarantees progress in the face\nof arbitrary delays. However, there is a price for this strong progress guarantee:\n\u0004The need to support atomic modi\ufb01cation of a reference and a Boolean mark\nhas an added performance cost.6\n\u0004Asadd() and remove () traverse the list, they must engage in concurrent\ncleanup of removed nodes, introducing the possibility of contention among\nthreads, sometimes forcing threads to restart traversals, even if there was no\nchange near the node each was trying to modify.\nOn the other hand, the lazy lock-based list does not guarantee progress in the\nface of arbitrary delays: its add() and remove () methods are blocking. However,\nunlike the lock-free algorithm, it does not require each node to include an atom-\nically markable reference. It also does not require traversals to clean up logically\nremoved nodes; they progress down the list, ignoring marked nodes.\nWhich approach is preferable depends on the application. In the end, the\nbalance of factors such as the potential for arbitrary thread delays, the relative\nfrequency of calls to the add() and remove () methods, the overhead of imple-\nmenting an atomically markable reference, and so on determine the choice of\nwhether to lock, and if so, at what granularity.\n9.10 Chapter Notes\nLock coupling was invented by Rudolf Bayer and Mario Schkolnick [19]. The \ufb01rst\ndesigns of lock-free linked-list algorithms are credited to John Valois [147]. The\nLock-free list implementation shown here is a variation on the lists of Maged\nMichael [114], who based his work on earlier linked-list algorithms by Tim\nHarris [53]. This algorithm is referred to by many as the Harris-Michael algo-\nrithm. The Harris-Michael algorithm is the one used in the Java Concurrency\nPackage. The OptimisticList algorithm was invented for this chapter, and the\nlazy algorithm is credited to Steven Heller, Maurice Herlihy, Victor", "doc_id": "8e22705b-9910-44ab-b73a-2b49e21e8939", "embedding": null, "doc_hash": "507bae35c8ca7e7010bd176c4ab60aa9b43fc091271ccda7871eca1abf315a48", "extra_info": null, "node_info": {"start": 571841, "end": 575180}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b4ddf1cf-ca01-47ec-8798-2a81ea6b6f68", "3": "930cad80-7f97-44da-b225-3f130a88d03d"}}, "__type__": "1"}, "930cad80-7f97-44da-b225-3f130a88d03d": {"__data__": {"text": "of lock-free linked-list algorithms are credited to John Valois [147]. The\nLock-free list implementation shown here is a variation on the lists of Maged\nMichael [114], who based his work on earlier linked-list algorithms by Tim\nHarris [53]. This algorithm is referred to by many as the Harris-Michael algo-\nrithm. The Harris-Michael algorithm is the one used in the Java Concurrency\nPackage. The OptimisticList algorithm was invented for this chapter, and the\nlazy algorithm is credited to Steven Heller, Maurice Herlihy, Victor Luchangco,\nMark Moir, Nir Shavit, and Bill Scherer [55].\n9.11 Exercises\nExercise 100. Describe how to modify each of the linked list algorithms if object\nhash codes are not guaranteed to be unique.\n6In the Java Concurrency Package, for example, this cost is somewhat reduced by using a reference\nto an intermediate dummy node to signify that the marked bit is set.\n220 Chapter 9 Linked Lists: The Role of Locking\nExercise 101. Explain why the \ufb01ne-grained locking algorithm is not subject to\ndeadlock.\nExercise 102. Explain why the \ufb01ne-grained list\u2019s add() method is linearizable.\nExercise 103. Explain why the optimistic and lazy locking algorithms are not sub-\nject to deadlock.\nExercise 104. Show a scenario in the optimistic algorithm where a thread is for-\never attempting to delete a node.\nHint : since we assume that all the individual node locks are starvation-free,\nthe livelock is not on any individual lock, and a bad execution must repeatedly\nadd and remove nodes from the list.\nExercise 105. Provide the code for the contains () method missing from the \ufb01ne-\ngrained algorithm. Explain why your implementation is correct.\nExercise 106. Is the optimistic list implementation still correct if we switch the\norder in which add() locks the pred andcurr entries?\nExercise 107. Show that in the optimistic list algorithm, if predAis not null, then\ntail is reachable from predA, even if predAitself is not reachable.\nExercise 108. Show that in the optimistic algorithm, the add() method needs to\nlock only pred .\nExercise 109. In the optimistic algorithm, the contains () method locks two\nentries before deciding whether a key is present. Suppose, instead, it locks no\nentries, returning true if it observes the value, and false otherwise.\nEither explain why this alternative is linearizable, or give a counterexample\nshowing it is not.\nExercise 110. Would the lazy algorithm still work if we marked a node as removed\nsimply by setting its next \ufb01eld to null? Why or why not? What about the lock-free\nalgorithm?\nExercise 111. In the lazy algorithm, can predAever be unreachable? Justify your\nanswer.\nExercise 112. Y our new employee claims that the lazy list\u2019s validation method\n(Fig. 9.16) can be simpli\ufb01ed by dropping the check that pred.next is equal to\ncurr:After all, the code always sets pred to the old value of curr , and before\npred.next can be changed, the new value of curr must be marked, causing the\nvalidation to fail. Explain the error in this reasoning.\nExercise 113. Can you modify the lazy algorithm\u2019s remove () so it locks only one\nnode?\n9.11 Exercises 221\nExercise 114. In the lock-free algorithm, argue the bene\ufb01ts and drawbacks of\nhaving the contains () method help in the cleanup of logically removed entries.\nExercise 115. In the lock-free algorithm, if an add() method call fails because\npred does not point to curr , but pred is not marked, do we need to traverse\nthe list again from head in order to", "doc_id": "930cad80-7f97-44da-b225-3f130a88d03d", "embedding": null, "doc_hash": "ef5f8674337f4d63f13f6279a9a155d2e83d33c86a08479ff96201711a61d0ad", "extra_info": null, "node_info": {"start": 575074, "end": 578532}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8e22705b-9910-44ab-b73a-2b49e21e8939", "3": "74b58bb4-f1ce-4947-b097-94d35c579e35"}}, "__type__": "1"}, "74b58bb4-f1ce-4947-b097-94d35c579e35": {"__data__": {"text": "the\nvalidation to fail. Explain the error in this reasoning.\nExercise 113. Can you modify the lazy algorithm\u2019s remove () so it locks only one\nnode?\n9.11 Exercises 221\nExercise 114. In the lock-free algorithm, argue the bene\ufb01ts and drawbacks of\nhaving the contains () method help in the cleanup of logically removed entries.\nExercise 115. In the lock-free algorithm, if an add() method call fails because\npred does not point to curr , but pred is not marked, do we need to traverse\nthe list again from head in order to attempt to complete the call?\nExercise 116. Would the contains () method of the lazy and lock-free algorithms\nstill be correct if logically removed entries were not guaranteed to be sorted?\nExercise 117. Theadd() method of the lock-free algorithm never \ufb01nds a marked\nnode with the same key. Can one modify the algorithm so that it will simply\ninsert its new added object into the existing marked node with same key if such\na node exists in the list, thus saving the need to insert a new node?\nExercise 118. Explain why the following cannot happen in the LockFreeList\nalgorithm. A node with item xis logically but not yet physically removed by some\nthread, then the same item xis added into the list by another thread, and \ufb01nally a\ncontains () call by a third thread traverses the list, \ufb01nding the logically removed\nnode, and returning false, even though the linearization order of the remove ()\nandadd() implies that xis in the set.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n10Concurrent Queues and\nthe ABA Problem\n10.1 Introduction\nIn the subsequent chapters, we look at a broad class of objects known as pools .\nA pool is similar to the Setclass studied in Chapter 9, with two main differences:\na pool does not necessarily provide a contains () method to test membership,\nand it allows the same item to appear more than once. The Pool hasget() and\nset() methods as in Fig. 10.1. Pools show up in many places in concurrent sys-\ntems. For example, in many applications, one or more producer threads produce\nitems to be consumed by one or more consumer threads. These items may be\njobs to perform, keystrokes to interpret, purchase orders to execute, or packets to\ndecode. Sometimes, producers are bursty , suddenly and brie\ufb02y producing items\nfaster than consumers can consume them. T o allow consumers to keep up, we\ncan place a buffer between the producers and the consumers. Items produced\nfaster than they can be consumed accumulate in the buffer, from which they are\nconsumed as quickly as possible. Often, pools act as producer\u2013consumer buffers.\nPools come in several varieties.\n\u0004A pool can be bounded orunbounded . A bounded pool holds a limited number\nof items. This limit is called its capacity . By contrast, an unbounded pool can\nhold any number of items. Bounded pools are useful when we want to keep\nproducer and consumer threads loosely synchronized, ensuring that produc-\ners do not get too far ahead of consumers. Bounded pools may also be simpler\nto implement than unbounded pools. On the other hand, unbounded pools\nare useful when there is no need to set a \ufb01xed limit on how far producers can\noutstrip consumers.\n\u0004Pool methods may be total ,partial , orsynchronous .\n\u2014 A method is total if calls do not wait for certain conditions to become true.\nFor example, a get() call that tries to remove an item from an empty pool\nimmediately returns a failure code or throws an exception. If the pool is\nbounded, a total set() that", "doc_id": "74b58bb4-f1ce-4947-b097-94d35c579e35", "embedding": null, "doc_hash": "2548dec5e8a45bab37b77080491bb5be7864dd75dc259670ef53243fec448929", "extra_info": null, "node_info": {"start": 578551, "end": 582069}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "930cad80-7f97-44da-b225-3f130a88d03d", "3": "3e08fb0b-7d95-4209-a3ae-fd4ef4e8a028"}}, "__type__": "1"}, "3e08fb0b-7d95-4209-a3ae-fd4ef4e8a028": {"__data__": {"text": "far ahead of consumers. Bounded pools may also be simpler\nto implement than unbounded pools. On the other hand, unbounded pools\nare useful when there is no need to set a \ufb01xed limit on how far producers can\noutstrip consumers.\n\u0004Pool methods may be total ,partial , orsynchronous .\n\u2014 A method is total if calls do not wait for certain conditions to become true.\nFor example, a get() call that tries to remove an item from an empty pool\nimmediately returns a failure code or throws an exception. If the pool is\nbounded, a total set() that tries to add an item to a full pool immediately\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00010-1\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.223\n224 Chapter 10 Concurrent Queues and the ABA Problem\n1public interface Pool<T> {\n2 void set(T item);\n3 T get();\n4}\nFigure 10.1 ThePool<T> interface.\nreturns a failure code or an exception. A total interface makes sense when\nthe producer (or consumer) thread has something better to do than wait\nfor the method call to take effect.\n\u2014 A method is partial if calls may wait for conditions to hold. For example,\na partial get() call that tries to remove an item from an empty pool blocks\nuntil an item is available to return. If the pool is bounded, a partial set()\ncall that tries to add an item to a full pool blocks until an empty slot is\navailable to \ufb01ll. A partial interface makes sense when the producer (or con-\nsumer) has nothing better to do than to wait for the pool to become nonfull\n(or nonempty).\n\u2014 A method is synchronous if it waits for another method to overlap its call\ninterval. For example, in a synchronous pool, a method call that adds an\nitem to the pool is blocked until that item is removed by another method\ncall. Symmetrically, a method call that removes an item from the pool is\nblocked until another method call makes an item available to be removed.\n(Such methods are partial.) Synchronous pools are used for communica-\ntion in programming languages such as CSP and Ada in which threads\nrendezvous to exchange information.\n\u0004Pools provide different fairness guarantees. They can be \ufb01rst-in-\ufb01rst-out\n(a queue), last-in-\ufb01rst-out (a stack), or other, weaker properties. The impor-\ntance of fairness when buffering using a pool is clear to anyone who has ever\ncalled a bank or a technical support line, only to be placed in a pool of waiting\ncalls. The longer you wait, the more consolation you draw from the recorded\nmessage asserting that calls are answered in the order they arrive. Perhaps.\n10.2 Queues\nIn this chapter we consider a kind of pool that provides \ufb01rst-in-\ufb01rst-out (FIFO)\nfairness. A sequential Queue<T> is an ordered sequence of items (of type T). It\nprovides an enq(x) method that puts item xat one end of the queue, called the\ntail, and a deq() method that removes and returns the item at the other end of the\nqueue, called the head . A concurrent queue is linearizable to a sequential queue.\nQueues are pools, where enq() implements set(), and deq() implements get().\nWe use queue implementations to illustrate a number of important principles. In\nlater chapters we consider pools that provide other fairness guarantees.\n10.3 A Bounded Partial Queue 225\n10.3 A Bounded Partial Queue\nFor simplicity, we assume it is illegal to add a null value to a queue. Of course,\nthere may be circumstances where it makes sense to add and remove null values,\nbut we leave it as an exercise to adapt our", "doc_id": "3e08fb0b-7d95-4209-a3ae-fd4ef4e8a028", "embedding": null, "doc_hash": "c07f42e9efa5fc470108c9e57d97f293b15495fd052d23b85db5fb2ea19c613f", "extra_info": null, "node_info": {"start": 582059, "end": 585508}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "74b58bb4-f1ce-4947-b097-94d35c579e35", "3": "eaf857e7-049d-4741-aefa-a3d00ab93ab0"}}, "__type__": "1"}, "eaf857e7-049d-4741-aefa-a3d00ab93ab0": {"__data__": {"text": "queue is linearizable to a sequential queue.\nQueues are pools, where enq() implements set(), and deq() implements get().\nWe use queue implementations to illustrate a number of important principles. In\nlater chapters we consider pools that provide other fairness guarantees.\n10.3 A Bounded Partial Queue 225\n10.3 A Bounded Partial Queue\nFor simplicity, we assume it is illegal to add a null value to a queue. Of course,\nthere may be circumstances where it makes sense to add and remove null values,\nbut we leave it as an exercise to adapt our algorithms to accommodate null values.\nHow much concurrency can we expect a bounded queue implementation with\nmultiple concurrent enqueuers and dequeuers to provide? Very informally, the\nenq() and deq() methods operate on opposite ends of the queue, so as long as the\nqueue is neither full nor empty, an enq() call and a deq() call should, in principle,\nbe able to proceed without interference. For the same reason, concurrent enq()\ncalls probably will interfere, and the same holds for deq() calls. This informal\nreasoning may sound convincing, and it is in fact mostly correct, but realizing\nthis level of concurrency is not trivial.\nHere, we implement a bounded queue as a linked list. (We could also have\nused an array.) Fig. 10.2 shows the queue\u2019s \ufb01elds and constructor, Figs. 10.3 and\n10.4 show the enq() and deq() methods, and Fig. 10.5 shows a queue node. Like\nthe lists studied in Chapter 9, a queue node has value andnext \ufb01elds.\nAs seen in Fig. 10.6, the queue itself has head andtail \ufb01elds that respectively\nrefer to the \ufb01rst and last nodes in the queue. The queue always contains a sen-\ntinel node acting as a place-holder. Like the sentinel nodes in Chapter 9, it marks\na position in the queue, though its value is meaningless. Unlike the list algo-\nrithms in Chapter 9, in which the same nodes always act as sentinels, the queue\nrepeatedly replaces the sentinel node. We use two distinct locks, enqLock and\ndeqLock , to ensure that at most one enqueuer, and at most one dequeuer at a\ntime can manipulate the queue object\u2019s \ufb01elds. Using two locks instead of one\nensures that an enqueuer does not lock out a dequeuer unnecessarily, and vice\nversa. Each lock has an associated condition \ufb01eld. The enqLock is associated with\n1public class BoundedQueue<T> {\n2 ReentrantLock enqLock, deqLock;\n3 Condition notEmptyCondition, notFullCondition;\n4 AtomicInteger size;\n5 volatile Node head, tail;\n6 int capacity;\n7 public BoundedQueue( int _capacity) {\n8 capacity = _capacity;\n9 head = new Node( null );\n10 tail = head;\n11 size = new AtomicInteger(0);\n12 enqLock = new ReentrantLock();\n13 notFullCondition = enqLock.newCondition();\n14 deqLock = new ReentrantLock();\n15 notEmptyCondition = deqLock.newCondition();\n16 }\nFigure 10.2 TheBoundedQueue class: \ufb01elds and constructor.\n226 Chapter 10 Concurrent Queues and the ABA Problem\n17 public void enq(T x) {\n18 boolean mustWakeDequeuers = false ;\n19 enqLock.lock();\n20 try {\n21 while (size.get() == capacity)\n22 notFullCondition.await();\n23 Node e = new Node(x);\n24 tail.next = tail; tail = e;\n25 if(size.getAndIncrement() == 0)\n26 mustWakeDequeuers = true ;\n27 }finally {\n28", "doc_id": "eaf857e7-049d-4741-aefa-a3d00ab93ab0", "embedding": null, "doc_hash": "8939e77b2c1d6ee94af8b76d956f94028af5fdb0a630573be0240f404d1116e8", "extra_info": null, "node_info": {"start": 585497, "end": 588664}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3e08fb0b-7d95-4209-a3ae-fd4ef4e8a028", "3": "3f92ec4b-ca8b-4441-aeb5-81e7e790b2b9"}}, "__type__": "1"}, "3f92ec4b-ca8b-4441-aeb5-81e7e790b2b9": {"__data__": {"text": "}\nFigure 10.2 TheBoundedQueue class: \ufb01elds and constructor.\n226 Chapter 10 Concurrent Queues and the ABA Problem\n17 public void enq(T x) {\n18 boolean mustWakeDequeuers = false ;\n19 enqLock.lock();\n20 try {\n21 while (size.get() == capacity)\n22 notFullCondition.await();\n23 Node e = new Node(x);\n24 tail.next = tail; tail = e;\n25 if(size.getAndIncrement() == 0)\n26 mustWakeDequeuers = true ;\n27 }finally {\n28 enqLock.unlock();\n29 }\n30 if(mustWakeDequeuers) {\n31 deqLock.lock();\n32 try {\n33 notEmptyCondition.signalAll();\n34 }finally {\n35 deqLock.unlock();\n36 }\n37 }\n38 }\nFigure 10.3 TheBoundedQueue class: the enq()method.\n39 public T deq() {\n40 T result;\n41 boolean mustWakeEnqueuers = false ;\n42 deqLock.lock();\n43 try {\n44 while (size.get() == 0)\n45 notEmptyCondition.await();\n46 result = head.next.value;\n47 head = head.next;\n48 if(size.getAndDecrement() == capacity) {\n49 mustWakeEnqueuers = true ;\n50 }\n51 }finally {\n52 deqLock.unlock();\n53 }\n54 if(mustWakeEnqueuers) {\n55 enqLock.lock();\n56 try {\n57 notFullCondition.signalAll();\n58 }finally {\n59 enqLock.unlock();\n60 }\n61 }\n62 return result;\n63 }\nFigure 10.4 TheBoundedQueue class: the deq()method.\n10.3 A Bounded Partial Queue 227\n64 protected class Node {\n65 public T value;\n66 public volatile Node next;\n67 public Node(T x) {\n68 value = x;\n69 next = null ;\n70 }\n71 }\n72 }\nFigure 10.5 BoundedQueue class: List Node.\nthenotFullCondition condition, used to notify waiting dequeuers when the\nqueue is no longer full. The deqLock is associated with notEmptyCondition ,\nused to notify waiting enqueuers when the queue is no longer empty.\nSince the queue is bounded, we must keep track of the number of empty\nslots. The size \ufb01eld is an AtomicInteger that tracks the number of objects cur-\nrently in the queue. This \ufb01eld is decremented by deq() calls and incremented by\nenq() calls.\nTheenq() method ( Fig. 10.3 ) works as follows. A thread acquires the enqLock\n(Line 19), and reads the size \ufb01eld (Line 21). While that \ufb01eld is equal to the\ncapacity, the queue is full, and the enqueuer must wait until a dequeuer makes\nroom. The enqueuer waits on the notFullCondition \ufb01eld (Line 22), releasing\nthe enqueue lock temporarily, and blocking until that condition is signaled. Each\ntime the thread awakens (Line 22), it checks whether there is room, and if not,\ngoes back to sleep.\nOnce the number of empty slots exceeds zero, however, the enqueuer may pro-\nceed. We note that once the enqueuer observes an empty slot, while the enqueue\nis in progress no other thread can \ufb01ll the queue, because all the other enqueuers\nare locked out, and a concurrent dequeuer can only increase the number of empty\nslots. (Synchronization for the enq() method is symmetric.)\nWe must carefully check that this implementation does not suffer from the\nkind of \u201clost-wakeup\u201d bug described in Chapter 8 . Care is needed because an\nenqueuer encounters a full queue in two steps: \ufb01rst, it sees that size is the queue\ncapacity, and second, it waits on notFullCondition until there", "doc_id": "3f92ec4b-ca8b-4441-aeb5-81e7e790b2b9", "embedding": null, "doc_hash": "c039124ab910714f72fbaf9a0fe1ca499bcde1a1f2764fc360c827873850c5ac", "extra_info": null, "node_info": {"start": 588769, "end": 591769}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "eaf857e7-049d-4741-aefa-a3d00ab93ab0", "3": "a6cb6435-905f-4067-9322-cbb82796d1a8"}}, "__type__": "1"}, "a6cb6435-905f-4067-9322-cbb82796d1a8": {"__data__": {"text": "an empty slot, while the enqueue\nis in progress no other thread can \ufb01ll the queue, because all the other enqueuers\nare locked out, and a concurrent dequeuer can only increase the number of empty\nslots. (Synchronization for the enq() method is symmetric.)\nWe must carefully check that this implementation does not suffer from the\nkind of \u201clost-wakeup\u201d bug described in Chapter 8 . Care is needed because an\nenqueuer encounters a full queue in two steps: \ufb01rst, it sees that size is the queue\ncapacity, and second, it waits on notFullCondition until there is room in the\nqueue. When a dequeuer changes the queue from full to not-full, it acquires\nenqLock and signals notFullCondition . Even though the size \ufb01eld is not pro-\ntected by the enqLock , the dequeuer acquires the enqLock before it signals the\ncondition, so the dequeuer cannot signal between the enqueuer\u2019s two steps.\nThedeq() method proceeds as follows. It reads the size \ufb01eld to check whether\nthe queue is empty. If so, the dequeuer must wait until an item is enqueued. Like\ntheenq() method, the dequeuer waits on notEmptyCondition , which temporar-\nily releases deqLock , and blocks until the condition is signaled. Each time the\nthread awakens, it checks whether the queue is empty, and if so, goes back to sleep.\nIt is important to understand that the abstract queue\u2019s head and tail items\nare not always the same as those referenced by head andtail . An item is logi-\ncally added to the queue as soon as the last node\u2019s next \ufb01eld is redirected to the\n228 Chapter 10 Concurrent Queues and the ABA Problem\n c      a b d\ne\nnext\nsentineltail headenq\nlockdeq\nlock size\nnew sentinel343 010 010\n123\nnew\nFigure 10.6 The enq()anddeq()methods of the BoundedQueue with 4 slots. First a node\nis enqueued into the queue by acquiring the enqLock . The enq()checks that the size is\n3 which is less than the bound. It then redirects the next \ufb01eld of the node referenced by\nthetail \ufb01eld (step 1), redirects tail to the new node (step 2), increments the size to 4,\nand releases the lock. Since size is now 4, any further calls to enq()will cause the threads\nto block until the notFullCondition is signalled by some deq(). Next, a node is dequeued\nfrom the queue by some thread. The deq()acquires the deqLock , reads the new value bfrom\nthe successor of the node referenced by head (this node is the current sentinel), redirects\nhead to this successor node (step 3), decrements the size to 3, and releases the lock.\nBefore completing the deq(), because the size was 4 when it started, the thread acquires\ntheenqLock and signals any enqueuers waiting on notFullCondition that they can proceed.\nnew item (the linearization point of the enq()), even if the enqueuer has not yet\nupdated tail . For example, a thread can hold the enqLock and be in the pro-\ncess of inserting a new node. Suppose it has not yet redirected the tail \ufb01eld.\nA concurrent dequeuing thread could acquire the deqLock , read and return the\nnew node\u2019s value, and redirect the head to the new node, all before the enqueuer\nredirects tail to the newly inserted node.\nOnce the dequeuer establishes that the queue is nonempty, the queue will\nremain nonempty for the duration of the deq() call, because all other dequeuers\nhave been locked out. Consider the \ufb01rst nonsentinel node in the queue (i.e., the\nnode referenced by the sentinel node\u2019s next", "doc_id": "a6cb6435-905f-4067-9322-cbb82796d1a8", "embedding": null, "doc_hash": "4a1708160a37fd11027a4d1bb4d4ccda5a96793efd851cd7f79d6b420b2191f5", "extra_info": null, "node_info": {"start": 591658, "end": 595012}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3f92ec4b-ca8b-4441-aeb5-81e7e790b2b9", "3": "d441f992-cbc7-422b-a40f-5bd54074a78d"}}, "__type__": "1"}, "d441f992-cbc7-422b-a40f-5bd54074a78d": {"__data__": {"text": "redirected the tail \ufb01eld.\nA concurrent dequeuing thread could acquire the deqLock , read and return the\nnew node\u2019s value, and redirect the head to the new node, all before the enqueuer\nredirects tail to the newly inserted node.\nOnce the dequeuer establishes that the queue is nonempty, the queue will\nremain nonempty for the duration of the deq() call, because all other dequeuers\nhave been locked out. Consider the \ufb01rst nonsentinel node in the queue (i.e., the\nnode referenced by the sentinel node\u2019s next \ufb01eld). The dequeuer reads this node\u2019s\nvalue \ufb01eld, and sets the queue\u2019s head to refer to it, making it the new sentinel\nnode. The dequeuer then decrements size and releases deqLock. If the dequeuer\nfound the former size was the queue capacity, then there may be enqueuers wait-\ning on notEmptyCondition , so the dequeuer acquires enqLock , and signals all\nsuch threads to wake up.\nOne drawback of this implementation is that concurrent enq() and deq()\ncalls interfere with each other, but not through locks. All method calls apply\ngetAndIncrement () or getAndDecrement () calls to the size \ufb01eld. These meth-\nods are more expensive than ordinary reads\u2013writes, and they could cause a\nsequential bottleneck.\n10.4 An Unbounded T otal Queue 229\nOne way to reduce such interactions is to split this \ufb01eld into two counters:\nenqSideSize is an integer \ufb01eld incremented by enq(), and deqSideSize is an\ninteger \ufb01eld decremented by deq(). A thread calling enq() tests enqSideSize , and\nas long as it is less than the capacity, it proceeds. When the \ufb01eld reaches capacity,\nthe thread locks deqLock , adds deqSize toEnqSize , and resets deqSideSize\nto 0. Instead of synchronizing on every method call, this technique synchronizes\nsporadically when the enqueuer\u2019s size estimate becomes too large.\n10.4 An Unbounded T otal Queue\nWe now describe a different kind of queue that can hold an unbounded number\nof items. The enq() method always enqueues its item, and deq() throws\nEmptyException if there is no item to dequeue. The representation is the same\nas the bounded queue, except there is no need to count the number of items in\nthe queue, or to provide conditions on which to wait. As illustrated in Figs. 10.7\nand 10.8, this algorithm is simpler than the bounded algorithm.\n1 public void enq(T x) {\n2 enqLock.lock();\n3 try {\n4 Node e = new Node(x);\n5 tail.next = e;\n6 tail = e;\n7 }finally {\n8 enqLock.unlock();\n9 }\n10 }\nFigure 10.7 TheUnboundedQueue<T> class: the enq()method.\n11 public T deq() throws EmptyException {\n12 T result;\n13 deqLock.lock();\n14 try {\n15 if(head.next == null ) {\n16 throw new EmptyException();\n17 }\n18 result = head.next.value;\n19 head = head.next;\n20 }finally {\n21 deqLock.unlock();\n22 }\n23 return result;\n24 }\nFigure 10.8 TheUnboundedQueue<T> class: the deq()method.\n230 Chapter 10 Concurrent Queues and the ABA Problem\nThis queue cannot deadlock, because each method acquires only one lock,\neither enqLock ordeqLock . A sentinel node alone in the queue will never be\ndeleted, so each enq() call will succeed as soon as it acquires the lock. Of course,\nadeq() method may fail if the queue is empty (i.e., if head.next isnull).", "doc_id": "d441f992-cbc7-422b-a40f-5bd54074a78d", "embedding": null, "doc_hash": "9b6ad0ac1c7a2b52f16603fa067887983dab239714c38b2d2099e14ec5ee7b4b", "extra_info": null, "node_info": {"start": 595051, "end": 598197}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a6cb6435-905f-4067-9322-cbb82796d1a8", "3": "8297ed42-65a4-499a-91a6-3ac0d3101021"}}, "__type__": "1"}, "8297ed42-65a4-499a-91a6-3ac0d3101021": {"__data__": {"text": "}finally {\n21 deqLock.unlock();\n22 }\n23 return result;\n24 }\nFigure 10.8 TheUnboundedQueue<T> class: the deq()method.\n230 Chapter 10 Concurrent Queues and the ABA Problem\nThis queue cannot deadlock, because each method acquires only one lock,\neither enqLock ordeqLock . A sentinel node alone in the queue will never be\ndeleted, so each enq() call will succeed as soon as it acquires the lock. Of course,\nadeq() method may fail if the queue is empty (i.e., if head.next isnull). As in\nthe earlier queue implementations, an item is actually enqueued when the enq()\ncall sets the last node\u2019s next \ufb01eld to the new node, even before enq() resets tail\nto refer to the new node. After that instant, the new item is reachable along a\nchain of the next references. As usual, the queue\u2019s actual head and tail are not\nnecessarily the items referenced by head andtail . Instead, the actual head is\nthe successor of the node referenced by head , and the actual tail is the last item\nreachable from the head. Both the enq() and deq() methods are total as they do\nnot wait for the queue to become empty or full.\n10.5 An Unbounded Lock-Free Queue\nWe now describe the LockFreeQueue<T> class, an unbounded lock-free queue\nimplementation. This class, depicted in Figs. 10.9 through 10.11 , is a natural\n1public class Node {\n2 public T value;\n3 public AtomicReference<Node> next;\n4 public Node(T value) {\n5 this .value = value;\n6 next = new AtomicReference<Node>( null );\n7 }\n8 }\nFigure 10.9 TheLockFreeQueue<T> class: list node.\n9 public void enq(T value) {\n10 Node node = new Node(value);\n11 while (true ) {\n12 Node last = tail.get();\n13 Node next = last.next.get();\n14 if(last == tail.get()) {\n15 if(next == null ) {\n16 if(last.next.compareAndSet(next, node)) {\n17 tail.compareAndSet(last, node);\n18 return ;\n19 }\n20 }else {\n21 tail.compareAndSet(last, next);\n22 }\n23 }\n24 }\n25 }\nFigure 10.10 TheLockFreeQueue<T> class: the enq()method.\n10.5 An Unbounded Lock-Free Queue 231\n26 public T deq() throws EmptyException {\n27 while (true ) {\n28 Node first = head.get();\n29 Node last = tail.get();\n30 Node next = first.next.get();\n31 if(first == head.get()) {\n32 if(first == last) {\n33 if(next == null ) {\n34 throw new EmptyException();\n35 }\n36 tail.compareAndSet(last, next);\n37 }else {\n38 T value = next.value;\n39 if(head.compareAndSet(first, next))\n40 return value;\n41 }\n42 }\n43 }\n44 }\nFigure 10.11 TheLockFreeQueue<T> class: the deq()method.\nextension of the unbounded total queue of Section 10.4 . Its implementation\nprevents method calls from starving by having the quicker threads help the slower\nthreads.\nAs done earlier, we represent the queue as a list of nodes. However, as shown\ninFig. 10.9 , each node\u2019s next \ufb01eld is an AtomicReference<Node> that refers to\nthe next node in the list. As can be seen in Fig. 10.12 , the queue itself consists of\ntwoAtomicReference<Node> \ufb01elds: head refers to the \ufb01rst node in the queue,\nandtail to the last. Again, the \ufb01rst node in the queue is a sentinel node, whose\nvalue is meaningless. The queue constructor sets", "doc_id": "8297ed42-65a4-499a-91a6-3ac0d3101021", "embedding": null, "doc_hash": "eaeddb901535b7769b47ed17091e3e668572756712155de19aa4cc300a4acdd3", "extra_info": null, "node_info": {"start": 598219, "end": 601255}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d441f992-cbc7-422b-a40f-5bd54074a78d", "3": "df7454f7-4d6f-4d97-8b57-d81a9226e184"}}, "__type__": "1"}, "df7454f7-4d6f-4d97-8b57-d81a9226e184": {"__data__": {"text": "done earlier, we represent the queue as a list of nodes. However, as shown\ninFig. 10.9 , each node\u2019s next \ufb01eld is an AtomicReference<Node> that refers to\nthe next node in the list. As can be seen in Fig. 10.12 , the queue itself consists of\ntwoAtomicReference<Node> \ufb01elds: head refers to the \ufb01rst node in the queue,\nandtail to the last. Again, the \ufb01rst node in the queue is a sentinel node, whose\nvalue is meaningless. The queue constructor sets both head andtail to refer to\nthe sentinel.\nAn interesting aspect of the enq() method is that it is lazy: it takes place in\ntwo distinct steps. T o make this method lock-free, threads may need to help one\nanother. Fig. 10.12 illustrates these steps.\nIn the following description the line numbers refer to Figs. 10.9 through 10.11 .\nNormally, the enq() method creates a new node (Line 10), locates the last node\nin the queue (Lines 12\u201313), and performs the following two steps:\n1.It calls compareAndSet() to append the new node (Line 16), and\n2.calls compareAndSet() to change the queue\u2019s tail \ufb01eld from the prior last\nnode to the current last node (Line 17).\nBecause these two steps are not executed atomically, every other method call must\nbe prepared to encounter a half-\ufb01nished enq() call, and to \ufb01nish the job. This is\na real-world example of the \u201chelping\u201d technique we \ufb01rst saw in the universal\nconstruction of Chapter 6 .\n232 Chapter 10 Concurrent Queues and the ABA Problem\nnext\nreleased node\nCAS nextCAS tailtail head\n2\nvalue sentinel\nnew\n1read value1CAS head2\nFigure 10.12 The lazy lock-free enq()anddeq()methods of the LockFreeQueue . A node\nis inserted into the queue in two steps. First, a compareAndSet() call changes the next\n\ufb01eld of the node referenced by the queue\u2019s tail from null to the new node. Then a\ncompareAndSet() call advances tail itself to refer to the new node. An item is removed\nfrom the queue in two steps. A compareAndSet() call reads the item from the node referred\nto by the sentinel node, and then redirects head from the current sentinel to the sentinel\u2019s\nnext node, making the latter the new sentinel. Both enq()anddeq()methods help complete\nun\ufb01nished tail updates.\nWe now review all the steps in detail. An enqueuer creates a new node with the\nnew value to be enqueued (Line 10), reads tail , and \ufb01nds the node that appears\nto be last (Lines 12\u201313). T o verify that node is indeed last, it checks whether that\nnode has a successor (Line 15). If so, the thread attempts to append the new\nnode by calling compareAndSet() (Line 16). (A compareAndSet() is required\nbecause other threads may be trying the same thing.) If the compareAndSet()\nsucceeds, the thread uses a second compareAndSet() to advance tail to the\nnew node (Line 17). Even if this second compareAndSet() call fails, the thread\ncan still return successfully because, as we will see, the call fails only if some\nother thread \u201chelped\u201d it by advancing tail . If the tail node has a successor (Line\n20), then the method tries to \u201chelp\u201d other threads by advancing tail to refer\ndirectly to the successor (Line 21) before trying again to insert its own node.\nThis enq() is total, meaning that it never waits for a dequeuer. A successful enq()\nis linearized at the instant where the executing thread (or a concurrent help-\ning", "doc_id": "df7454f7-4d6f-4d97-8b57-d81a9226e184", "embedding": null, "doc_hash": "ed770b4a938c6c9691b706db7c612f9a14f8e142b865f011f19ea7173c620680", "extra_info": null, "node_info": {"start": 601292, "end": 604557}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8297ed42-65a4-499a-91a6-3ac0d3101021", "3": "328a6eae-0eab-4dbb-962c-bd7a6071a93a"}}, "__type__": "1"}, "328a6eae-0eab-4dbb-962c-bd7a6071a93a": {"__data__": {"text": "this second compareAndSet() call fails, the thread\ncan still return successfully because, as we will see, the call fails only if some\nother thread \u201chelped\u201d it by advancing tail . If the tail node has a successor (Line\n20), then the method tries to \u201chelp\u201d other threads by advancing tail to refer\ndirectly to the successor (Line 21) before trying again to insert its own node.\nThis enq() is total, meaning that it never waits for a dequeuer. A successful enq()\nis linearized at the instant where the executing thread (or a concurrent help-\ning thread) calls compareAndSet() to redirect the tail \ufb01eld to the new node at\nLine 21.\nThedeq() method is similar to its total counterpart from the UnboundedQueue .\nIf the queue is nonempty, the dequeuer calls compareAndSet() to change head\nfrom the sentinel node to its successor, making the successor the new sentinel\nnode. The deq() method makes sure that the queue is not empty in the same way\nas before: by checking that the next \ufb01eld of the head node is not null.\nThere is, however, a subtle issue in the lock-free case, depicted in Fig. 10.13 :\nbefore advancing head one must make sure that tail is not left referring to\nthe sentinel node which is about to be removed from the queue. T o avoid this\nproblem we add a test: if head equals tail (Line 32) and the (sentinel) node\nthey refer to has a non- null next \ufb01eld (Line 33), then the tail is deemed to\n10.6 Memory Reclamation and the ABA Problem 233\nreleased\nnodetail head\nsentinel\nnewa\nb\nFigure 10.13 Why dequeuers must help advance tail in Line 36ofFig. 10.11 . Consider the\nscenario in which a thread enqueuing node bhas redirected a\u2019snext \ufb01eld to b, but has yet\nto redirect tail from atob. If another thread starts dequeuing, it will read b\u2019s value and\nredirect head from atob, effectively removing awhile tail still refers to it. T o avoid this\nproblem, the dequeuing thread must help advance tail from atobbefore redirecting head.\nbe lagging behind. As in the enq() method, deq() then attempts to help make\ntail consistent by swinging it to the sentinel node\u2019s successor (Line 36), and\nonly then updates head to remove the sentinel (Line 39). As in the partial queue,\nthe value is read from the successor of the sentinel node (Line 38). If this method\nreturns a value, then its linearization point occurs when it completes a successful\ncompareAndSet() call at Line 39, and otherwise it is linearized at Line 33.\nIt is easy to check that the resulting queue is lock-free. Every method call \ufb01rst\nchecks for an incomplete enq() call, and tries to complete it. In the worst case, all\nthreads are trying to advance the queue\u2019s tail \ufb01eld, and one of them must suc-\nceed. A thread fails to enqueue or dequeue a node only if another thread\u2019s method\ncall succeeds in changing the reference, so some method call always completes.\nAs it turns out, being lock-free substantially enhances the performance of queue\nimplementations, and the lock-free algorithms tend to outperform the most ef\ufb01-\ncient blocking ones.\n10.6 Memory Reclamation and the ABA Problem\nOur queue implementations so far rely on the Java garbage collector to recy-\ncle nodes after they have been dequeued. What happens if we choose to do our\nown memory management? There are several reasons we might want to do this.\nLanguages such as C or C++ do not provide garbage collection. Even if garbage\ncollection is available, it is often more ef\ufb01cient for a class to do its own", "doc_id": "328a6eae-0eab-4dbb-962c-bd7a6071a93a", "embedding": null, "doc_hash": "37b8aaa8b5fb95969207d35ad35758dc89a6226622e62b3fece3bfb92adcc341", "extra_info": null, "node_info": {"start": 604474, "end": 607907}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "df7454f7-4d6f-4d97-8b57-d81a9226e184", "3": "8195a1ca-b64b-4309-9aa6-74f36fba0e6c"}}, "__type__": "1"}, "8195a1ca-b64b-4309-9aa6-74f36fba0e6c": {"__data__": {"text": "the performance of queue\nimplementations, and the lock-free algorithms tend to outperform the most ef\ufb01-\ncient blocking ones.\n10.6 Memory Reclamation and the ABA Problem\nOur queue implementations so far rely on the Java garbage collector to recy-\ncle nodes after they have been dequeued. What happens if we choose to do our\nown memory management? There are several reasons we might want to do this.\nLanguages such as C or C++ do not provide garbage collection. Even if garbage\ncollection is available, it is often more ef\ufb01cient for a class to do its own memory\nmanagement, particularly if it creates and releases many small objects. Finally, if\nthe garbage collection process is not lock-free, we might want to supply our own\nlock-free memory reclamation.\nA natural way to recycle nodes in a lock-free manner is to have each thread\nmaintain its own private free list of unused queue entries.\n234 Chapter 10 Concurrent Queues and the ABA Problem\nThreadLocal<Node> freeList = new ThreadLocal<Node>() {\nprotected Node initialValue() { return null ; };\n};\nWhen an enqueuing thread needs a new node, it tries to remove one from the\nthread-local free list. If the free list is empty, it simply allocates a node using\nthenew operator. When a dequeuing thread is ready to retire a node, it links it\nback onto the thread-local list. Because the list is thread-local, there is no need\nfor expensive synchronization. This design works well, as long as each thread\ntail headThread A: about to CAS head from\n                 a to b1\nThreads B and C: deq a\nand b into local pools\nheadThread A: CAS succeeds, incorrectly\n                 pointing to b which is still\n                 in the local poola b\natail\nc\nbThreads B and C: enq a, c, and d\nd(a)\n(b)2\n3 4\nFigure 10.14 An ABA scenario: Assume that we use local pools of recycled nodes in our lock-free queue\nalgorithm. In Part (a), the dequeuer thread Aof Fig. 10.11 observes that the sentinel node is a, and next\nnode is b. (Step 1) It then prepares to update head by applying a compareAndSet() with old value aand\nnew value b. (Step 2) Suppose however, that before it takes another step, other threads dequeue b, then its\nsuccessor, placing both aandbin the free pool. In Part (b) (Step 3) node ais reused, and eventually reappears\nas the sentinel node in the queue. (Step 4) thread Anow wakes up, calls compareAndSet() , and succeeds\nin setting head tob, since the old value of head is indeed a. Now, head is incorrectly set to a recycled\nnode.\n10.6 Memory Reclamation and the ABA Problem 235\nperforms roughly the same number of enqueues and dequeues. If there is an\nimbalance, then there may be a need for more complex techniques, such as peri-\nodically stealing nodes from other threads.\nSurprisingly, perhaps, the lock-free queue will not work if nodes are recycled\nin the most straightforward way. Consider the scenario depicted in Fig. 10.14 .\nIn Part (a) of Fig. 10.14 , the dequeuing thread A observes the sentinel node\nisa, and the next node is b. It then prepares to update head by applying a\ncompareAndSet() with old value aand new value b. Before it takes another\nstep, other threads dequeue band its successor, placing both aandbin the\nfree pool. Node ais recycled, and eventually reappears as the sentinel node in\nthe queue,", "doc_id": "8195a1ca-b64b-4309-9aa6-74f36fba0e6c", "embedding": null, "doc_hash": "9e2efdbcc8cf904fb82db96f5dc5d7ab02be483a4f1fa8e7f71f6b965d1d9a29", "extra_info": null, "node_info": {"start": 607897, "end": 611176}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "328a6eae-0eab-4dbb-962c-bd7a6071a93a", "3": "cba9e15d-ae54-4a82-bd96-21e56409f321"}}, "__type__": "1"}, "cba9e15d-ae54-4a82-bd96-21e56409f321": {"__data__": {"text": "the lock-free queue will not work if nodes are recycled\nin the most straightforward way. Consider the scenario depicted in Fig. 10.14 .\nIn Part (a) of Fig. 10.14 , the dequeuing thread A observes the sentinel node\nisa, and the next node is b. It then prepares to update head by applying a\ncompareAndSet() with old value aand new value b. Before it takes another\nstep, other threads dequeue band its successor, placing both aandbin the\nfree pool. Node ais recycled, and eventually reappears as the sentinel node in\nthe queue, as depicted in Part (b) of Fig. 10.14 . The thread now wakes up, calls\ncompareAndSet() , and succeeds, since the old value of the head is indeeda.\nUnfortunately, it has redirected head to a recycled node!\nThis phenomenon is called the \u201cABA \u201d problem. It shows up often, especially\nin dynamic memory algorithms that use conditional synchronization operations\nsuch as compareAndSet() . Typically, a reference about to be modi\ufb01ed by a\ncompareAndSet() changes from a, tob, and back to aagain. As a result, the\ncompareAndSet() call succeeds even though its effect on the data structure has\nchanged, and no longer has the desired effect.\nOne straightforward way to \ufb01x this problem is to tag each atomic reference\nwith a unique stamp . As described in detail in Pragma 10.6.1 , an\nAtomicStampedReference<T> object encapsulates both a reference to an object\nof Type Tand an integer stamp . These \ufb01elds can be atomically updated either\ntogether or individually.\nPragma 10.6.1. The AtomicStampedReference<T> class encapsulates both\na reference to an object of Type Tand an integer stamp . It generalizes the\nAtomicMarkableReference<T> class , replacing the Boolean\nmark with an integer stamp.\nWe usually use this stamp to avoid the ABA problem, incrementing the\nvalue of the stamp each time we modify the object, although sometimes, as\nin the LockFreeExchanger class of Chapter 11 , we use the stamp to hold one\nof a \ufb01nite set of states.\nThe stamp and reference \ufb01elds can be updated atomically, either together\nor individually. For example, the compareAndSet() method tests expected\nreference and stamp values, and if both tests succeed, replaces them with\nupdated reference and stamp values. As shorthand, the attemptStamp ()\nmethod tests an expected reference value and if the test succeeds, replaces\nit with a new stamp value. The get() method has an unusual inter-\nface: it returns the object\u2019s reference value and stores the stamp value in\nan integer array argument. Fig. 10.15 illustrates the signatures for these\nmethods.(Pragma 9.8.1)\n236 Chapter 10 Concurrent Queues and the ABA Problem\n1public boolean compareAndSet(T expectedReference,\n2 T newReference,\n3 int expectedStamp,\n4 int newStamp);\n5public T get( int[] stampHolder);\n6public void set(T newReference, int newStamp);\nFigure 10.15 The AtomicStampedReference<T> class: the compareAndSet() and\nget()methods. The compareAndSet() method tests and updates both the stamp and\nreference \ufb01elds, the get()method returns the encapsulated reference and stores the\nstamp at position 0 in the argument array, and set()updates the encapsulated reference\nand the stamp.\nIn a language like C or C++, one could provide this functionality ef\ufb01-\nciently in a 64-bit architecture by \u201cstealing\u201d bits from pointers, although a\n32-bit architecture would probably require a level of indirection.\nAs shown in Fig. 10.16 , each time through the loop,", "doc_id": "cba9e15d-ae54-4a82-bd96-21e56409f321", "embedding": null, "doc_hash": "4b0530e2a1348fe9d0196e021d30d852ebc9e035750b1b2fe2d6f0690d871915", "extra_info": null, "node_info": {"start": 611203, "end": 614609}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8195a1ca-b64b-4309-9aa6-74f36fba0e6c", "3": "434c8543-9767-4ed9-a65e-da72f5e65914"}}, "__type__": "1"}, "434c8543-9767-4ed9-a65e-da72f5e65914": {"__data__": {"text": "The compareAndSet() method tests and updates both the stamp and\nreference \ufb01elds, the get()method returns the encapsulated reference and stores the\nstamp at position 0 in the argument array, and set()updates the encapsulated reference\nand the stamp.\nIn a language like C or C++, one could provide this functionality ef\ufb01-\nciently in a 64-bit architecture by \u201cstealing\u201d bits from pointers, although a\n32-bit architecture would probably require a level of indirection.\nAs shown in Fig. 10.16 , each time through the loop, deq() reads both the\nreference and stamp values for the \ufb01rst, next, and last nodes (Lines 6\u20138). It\nuses compareAndSet() to compare both the reference and the stamp (Line 17).\n1 public T deq() throws EmptyException {\n2 int[] lastStamp = new int [1];\n3 int[] firstStamp = new int [1];\n4 int[] nextStamp = new int [1];\n5 while (true ) {\n6 Node first = head.get(firstStamp);\n7 Node last = tail.get(lastStamp);\n8 Node next = first.next.get(nextStamp);\n9 if(first == last) {\n10 if(next == null ) {\n11 throw new EmptyException();\n12 }\n13 tail.compareAndSet(last, next,\n14 lastStamp[0], lastStamp[0]+1);\n15 }else {\n16 T value = next.value;\n17 if(head.compareAndSet(first, next, firstStamp[0],\nfirstStamp[0]+1)) {\n18 free(first);\n19 return value;\n20 }\n21 }\n22 }\n23 }\nFigure 10.16 The LockFreeQueueRecycle<T> class: the deq()method uses stamps to\navoid ABA.\n10.6 Memory Reclamation and the ABA Problem 237\nIt increments the stamp each time it uses compareAndSet() to update a reference\n(Lines 14and 17).1\nThe ABA problem can occur in many synchronization scenarios, not just those\ninvolving conditional synchronization. For example, it can occur when using\nonly loads and stores. Conditional synchronization operations such as load-\nlinked/store-conditional , available on some architectures (see Appendix B ), avoid\nABA by testing not whether a value is the same at two points in time, but whether\nthe value has ever changed between those points.\n10.6.1 A Na\u00a8 \u0131ve Synchronous Queue\nWe now turn our attention to an even tighter kind of synchronization. One or\nmore producer threads produce items to be removed, in \ufb01rst-in-\ufb01rst-out order,\nby one or more consumer threads. Here, however, producers and consumers ren-\ndezvous with one another: a producer that puts an item in the queue blocks until\nthat item is removed by a consumer, and vice versa. Such rendezvous synchro-\nnization is built into languages such as CSP and Ada.\nFig. 10.17 illustrates the SynchronousQueue<T> class, a straightforward\nmonitor-based synchronous queue implementation. It has the following \ufb01elds:\nitem is the \ufb01rst item waiting to be dequeued, enqueuing is a Boolean value\nused by enqueuers to synchronize among themselves, lock is the lock used for\nmutual exclusion, and condition is used to block partial methods. If the enq()\nmethod \ufb01nds enqueuing to be true (Line 10) then another enqueuer has sup-\nplied an item and is waiting to rendezvous with a dequeuer, so the enqueuer\nrepeatedly releases the lock, sleeps, and checks whether enqueuing has become\nfalse (Line 11). When this condition is satis\ufb01ed, the enqueuer sets enqueuing to\ntrue, which locks out other enqueuers until the current rendezvous is", "doc_id": "434c8543-9767-4ed9-a65e-da72f5e65914", "embedding": null, "doc_hash": "d02b3e1af8008ab62e0bada3fb616c53ed285ca76ea94081282303f2f3c422f7", "extra_info": null, "node_info": {"start": 614607, "end": 617799}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "cba9e15d-ae54-4a82-bd96-21e56409f321", "3": "ad43fe79-f688-4952-883a-1fdb0a5b8239"}}, "__type__": "1"}, "ad43fe79-f688-4952-883a-1fdb0a5b8239": {"__data__": {"text": "enqueuers to synchronize among themselves, lock is the lock used for\nmutual exclusion, and condition is used to block partial methods. If the enq()\nmethod \ufb01nds enqueuing to be true (Line 10) then another enqueuer has sup-\nplied an item and is waiting to rendezvous with a dequeuer, so the enqueuer\nrepeatedly releases the lock, sleeps, and checks whether enqueuing has become\nfalse (Line 11). When this condition is satis\ufb01ed, the enqueuer sets enqueuing to\ntrue, which locks out other enqueuers until the current rendezvous is complete,\nand sets item to refer to the new item (Lines 12\u201313). It then noti\ufb01es any waiting\nthreads (Line 14), and waits until item becomes null (Lines 15\u201316). When the\nwait is over, the rendezvous has occurred, so the enqueuer sets enqueuing to\nfalse, noti\ufb01es any waiting threads, and returns (Lines 17and 19).\nThedeq() method simply waits until item is non- null (Lines 26\u201327), records\nthe item, sets the item \ufb01eld to null, and noti\ufb01es any waiting threads before\nreturning the item (Lines 28\u201331).\nWhile the design of the queue is relatively simple, it incurs a high synchro-\nnization cost. At every point where one thread might wake up another, both\nenqueuers and dequeuers wake up all waiting threads, leading to a number of\nwakeups quadratic in the number of waiting threads. While it is possible to use\ncondition objects to reduce the number of wakeups, it is still necessary to block\non every call, which is expensive.\n1We ignore the remote possibility that the stamp could wrap around and cause an error.\n238 Chapter 10 Concurrent Queues and the ABA Problem\n1public class SynchronousQueue<T> {\n2 T item = null ;\n3 boolean enqueuing;\n4 Lock lock;\n5 Condition condition;\n6 ...\n7 public void enq(T value) {\n8 lock.lock();\n9 try {\n10 while (enqueuing)\n11 condition.await();\n12 enqueuing = true ;\n13 item = value;\n14 condition.signalAll();\n15 while (item != null )\n16 condition.await();\n17 enqueuing = false ;\n18 condition.signalAll();\n19 }finally {\n20 lock.unlock();\n21 }\n22 }\n23 public T deq() {\n24 lock.lock();\n25 try {\n26 while (item == null )\n27 condition.await();\n28 T t = item;\n29 item = null ;\n30 condition.signalAll();\n31 return t;\n32 }finally {\n33 lock.unlock();\n34 }\n35 }\n36 }\nFigure 10.17 TheSynchronousQueue<T> class.\n10.7 Dual Data Structures\nT o reduce the synchronization overheads of the synchronous queue, we consider\nan alternative synchronous queue implementation that splits enq() and deq()\nmethods into two steps. Here is how a dequeuer tries to remove an item from an\nempty queue. In the \ufb01rst step, it puts a reservation object in the queue, indicat-\ning that the dequeuer is waiting for an enqueuer with which to rendezvous. The\ndequeuer then spins on a \ufb02ag in the reservation. Later, when an enqueuer discov-\ners the reservation, it ful\ufb01lls the reservation by depositing an item and notifying\n10.7 Dual Data Structures 239\nthe dequeuer by setting the reservation\u2019s \ufb02ag. Similarly, an enqueuer can wait\nfor a rendezvous partner by creating its own reservation, and spinning on the\nreservation\u2019s \ufb02ag. At any time the queue itself contains either enq() reservations,\ndeq() reservations, or it is empty.\nThis structure", "doc_id": "ad43fe79-f688-4952-883a-1fdb0a5b8239", "embedding": null, "doc_hash": "274152a0951f0767ecb00354815f71f9315c5c48370b8f881311a72e7302d4fd", "extra_info": null, "node_info": {"start": 617796, "end": 620964}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "434c8543-9767-4ed9-a65e-da72f5e65914", "3": "950944e9-a7eb-4870-9e73-33d3f0013bc9"}}, "__type__": "1"}, "950944e9-a7eb-4870-9e73-33d3f0013bc9": {"__data__": {"text": "then spins on a \ufb02ag in the reservation. Later, when an enqueuer discov-\ners the reservation, it ful\ufb01lls the reservation by depositing an item and notifying\n10.7 Dual Data Structures 239\nthe dequeuer by setting the reservation\u2019s \ufb02ag. Similarly, an enqueuer can wait\nfor a rendezvous partner by creating its own reservation, and spinning on the\nreservation\u2019s \ufb02ag. At any time the queue itself contains either enq() reservations,\ndeq() reservations, or it is empty.\nThis structure is called a dual data structure , since the methods take effect\nin two stages, reservation and ful\ufb01llment. It has a number of nice properties.\nFirst, waiting threads can spin on a locally cached \ufb02ag, which we have seen is\nessential for scalability. Second, it ensures fairness in a natural way. Reservations\nare queued in the order they arrive, ensuring that requests are ful\ufb01lled in the same\norder. Note that this data structure is linearizable, since each partial method call\ncan be ordered when it is ful\ufb01lled.\nThe queue is implemented as a list of nodes, where a node represents either an\nitem waiting to be dequeued, or a reservation waiting to be ful\ufb01lled ( Fig. 10.18 ).\nA node\u2019s type \ufb01eld indicates which. At any time, all queue nodes have the same\ntype: either the queue consists entirely of items waiting to be dequeued, or\nentirely of reservations waiting to be ful\ufb01lled.\nWhen an item is enqueued, the node\u2019s item \ufb01eld holds the item, which is reset\ntonull when that item is dequeued. When a reservation is enqueued, the node\u2019s\nitem \ufb01eld is null, and is reset to an item when ful\ufb01lled by an enqueuer.\nFig. 10.19 shows the SynchronousDualQueue \u2019s constructor and enq() method.\n(The deq() method is symmetric.) Just like the earlier queues we have considered,\nthehead \ufb01eld always refers to a sentinel node that serves as a place-holder, and\nwhose actual value is unimportant. The queue is empty when head andtail\nagree. The constructor creates a sentinel node with an arbitrary value, referred to\nby both head andtail .\nTheenq() method \ufb01rst checks whether the queue is empty or whether it con-\ntains enqueued items waiting to be dequeued (Line 10). If so, then just as in the\nlock-free queue, the method reads the queue\u2019s tail \ufb01eld (Line 11), and checks\nthat the values read are consistent (Line 12). If the tail \ufb01eld does not refer to\nthe last node in the queue, then the method advances the tail \ufb01eld and starts\nover (Lines 13\u201314). Otherwise, the enq() method tries to append the new node\nto the end of the queue by resetting the tail node\u2019s next \ufb01eld to refer to the new\n1 private enum NodeType {ITEM, RESERVATION};\n2 private class Node {\n3 volatile NodeType type;\n4 volatile AtomicReference<T> item;\n5 volatile AtomicReference<Node> next;\n6 Node(T myItem, NodeType myType) {\n7 item = new AtomicReference<T>(myItem);\n8 next = new AtomicReference<Node>( null );\n9 type = myType;\n10 }\n11 }\nFigure 10.18 TheSynchronousDualQueue<T> class: queue node.\n240 Chapter 10 Concurrent Queues and the ABA Problem\n1 public SynchronousDualQueue() {\n2 Node sentinel = new Node( null , NodeType.ITEM);\n3 head = new AtomicReference<Node>(sentinel);\n4 tail = new", "doc_id": "950944e9-a7eb-4870-9e73-33d3f0013bc9", "embedding": null, "doc_hash": "627730e1465766e0b9a7777a7c264c73eea901e113028f7bc6c7f080b2f07c49", "extra_info": null, "node_info": {"start": 621003, "end": 624136}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ad43fe79-f688-4952-883a-1fdb0a5b8239", "3": "f45c243e-1497-466a-8108-8e3a6031f9fe"}}, "__type__": "1"}, "f45c243e-1497-466a-8108-8e3a6031f9fe": {"__data__": {"text": "item;\n5 volatile AtomicReference<Node> next;\n6 Node(T myItem, NodeType myType) {\n7 item = new AtomicReference<T>(myItem);\n8 next = new AtomicReference<Node>( null );\n9 type = myType;\n10 }\n11 }\nFigure 10.18 TheSynchronousDualQueue<T> class: queue node.\n240 Chapter 10 Concurrent Queues and the ABA Problem\n1 public SynchronousDualQueue() {\n2 Node sentinel = new Node( null , NodeType.ITEM);\n3 head = new AtomicReference<Node>(sentinel);\n4 tail = new AtomicReference<Node>(sentinel);\n5 }\n6 public void enq(T e) {\n7 Node offer = new Node(e, NodeType.ITEM);\n8 while (true ) {\n9 Node t = tail.get(), h = head.get();\n10 if(h == t || t.type == NodeType.ITEM) {\n11 Node n = t.next.get();\n12 if(t == tail.get()) {\n13 if(n != null ) {\n14 tail.compareAndSet(t, n);\n15 }else if (t.next.compareAndSet(n, offer)) {\n16 tail.compareAndSet(t, offer);\n17 while (offer.item.get() == e);\n18 h = head.get();\n19 if(offer == h.next.get())\n20 head.compareAndSet(h, offer);\n21 return ;\n22 }\n23 }\n24 }else {\n25 Node n = h.next.get();\n26 if(t != tail.get() || h != head.get() || n == null ) {\n27 continue ;\n28 }\n29 boolean success = n.item.compareAndSet( null , e);\n30 head.compareAndSet(h, n);\n31 if(success)\n32 return ;\n33 }\n34 }\n35 }\nFigure 10.19 TheSynchronousDualQueue<T> class: enq()method and constructor.\nnode (Line 15). If it succeeds, it tries to advance the tail to the newly appended\nnode (Line 16), and then spins, waiting for a dequeuer to announce that it has\ndequeued the item by setting the node\u2019s item \ufb01eld to null. Once the item is\ndequeued, the method tries to clean up by making its node the new sentinel.\nThis last step serves only to enhance performance, because the implementation\nremains correct, whether or not the method advances the head reference.\nIf, however, the enq() method discovers that the queue contains dequeuers\u2019\nreservations waiting to be ful\ufb01lled, then it tries to \ufb01nd a reservation to ful\ufb01ll.\nSince the queue\u2019s head node is a sentinel with no meaningful value, enq() reads\nthe head\u2019s successor (Line 25), checks that the values it has read are consistent\n(Lines 26\u201328), and tries to switch that node\u2019s item \ufb01eld from null to the item\nbeing enqueued. Whether or not this step succeeds, the method tries to advance\n10.9 Exercises 241\nhead (Line 30). If the compareAndSet() call succeeds (Line 29), the method\nreturns; otherwise it retries.\n10.8 Chapter Notes\nThe partial queue employs a mixture of techniques adapted from Doug Lea [ 98]\nand from an algorithm by Maged Michael and Michael Scott [ 115]. The lock-free\nqueue is a slightly simpli\ufb01ed version of a queue algorithm by Maged Michael and\nMichael Scott [ 115]. The synchronous queue implementations are adapted from\nalgorithms by Bill Scherer, Doug Lea, and Michael Scott [ 136].\n10.9 Exercises\nExercise 119. Change the SynchronousDualQueue<T> class to work correctly\nwith null items.\nExercise 120. Consider the simple lock-free queue for a single enqueuer and a sin-\ngle", "doc_id": "f45c243e-1497-466a-8108-8e3a6031f9fe", "embedding": null, "doc_hash": "265589e753708eb1c80d3b6116f49c07ab383d457cf7e5d000f2533dbc93d4f4", "extra_info": null, "node_info": {"start": 624152, "end": 627093}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "950944e9-a7eb-4870-9e73-33d3f0013bc9", "3": "1ca1cd9e-d7b1-4c94-9496-902c99c1d6fe"}}, "__type__": "1"}, "1ca1cd9e-d7b1-4c94-9496-902c99c1d6fe": {"__data__": {"text": "of techniques adapted from Doug Lea [ 98]\nand from an algorithm by Maged Michael and Michael Scott [ 115]. The lock-free\nqueue is a slightly simpli\ufb01ed version of a queue algorithm by Maged Michael and\nMichael Scott [ 115]. The synchronous queue implementations are adapted from\nalgorithms by Bill Scherer, Doug Lea, and Michael Scott [ 136].\n10.9 Exercises\nExercise 119. Change the SynchronousDualQueue<T> class to work correctly\nwith null items.\nExercise 120. Consider the simple lock-free queue for a single enqueuer and a sin-\ngle dequeuer, described earlier in Chapter 3 . The queue is presented in Fig. 10.20 .\n1class TwoThreadLockFreeQueue<T> {\n2 int head = 0, tail = 0;\n3 T[] items;\n4 public TwoThreadLockFreeQueue( int capacity) {\n5 head = 0; tail = 0;\n6 items = (T[]) new Object[capacity];\n7 }\n8 public void enq(T x) {\n9 while (tail - head == items.length) {};\n10 items[tail % items.length] = x;\n11 tail++;\n12 }\n13 public Object deq() {\n14 while (tail - head == 0) {};\n15 Object x = items[head % items.length];\n16 head++;\n17 return x;\n18 }\n19 }\nFigure 10.20 A Lock-free FIFO queue with blocking semantics for a single enqueuer and\nsingle dequeuer. The queue is implemented in an array. Initially the head andtail \ufb01elds are\nequal and the queue is empty. If the head andtail differ by capacity , then the queue is\nfull. The enq()method reads the head \ufb01eld, and if the queue is full, it repeatedly checks the\nhead until the queue is no longer full. It then stores the object in the array, and increments\nthetail \ufb01eld. The deq()method works in a symmetric way.\n242 Chapter 10 Concurrent Queues and the ABA Problem\nThis queue is blocking, that is, removing an item from an empty queue or\ninserting an item to a full one causes the threads to block (spin). The surpris-\ning thing about this queue is that it requires only loads and stores and not a more\npowerful read\u2013modify\u2013write synchronization operation. Does it however require\nthe use of a memory barrier? If not, explain, and if so, where in the code is such\na barrier needed and why?\nExercise 121. Design a bounded lock-based queue implementation using an array\ninstead of a linked list.\n1.Allow parallelism by using two separate locks for head andtail .\n2.Try to transform your algorithm to be lock-free. Where do you run into dif\ufb01-\nculty?\nExercise 122. Consider the unbounded lock-based queue\u2019s deq() method in\nFig. 10.8. Is it necessary to hold the lock when checking that the queue is not\nempty? Explain.\nExercise 123. In Dante\u2019s Inferno , he describes a visit to Hell. In a very recently\ndiscovered chapter, he encounters \ufb01ve people sitting at a table with a pot of stew\nin the middle. Although each one holds a spoon that reaches the pot, each spoon\u2019s\nhandle is much longer than each person\u2019s arm, so no one can feed him- or herself.\nThey are famished and desperate.\nDante then suggests \u201cwhy do not you feed one another?\u201d\nThe rest of the chapter is lost.\n1.Write an algorithm to allow these unfortunates to feed one another. Two or\nmore people may not feed the same person at the same time. Y our algorithm\nmust be, well, starvation-free.\n2.Discuss the advantages and disadvantages of your algorithm. Is it centralized,\ndecentralized, high or low in contention, deterministic or randomized?\nExercise 124. Consider the linearization points of the enq() and", "doc_id": "1ca1cd9e-d7b1-4c94-9496-902c99c1d6fe", "embedding": null, "doc_hash": "ab6d48c482f5e33df8d3f76ba5096666f7aa56366684c776c3f836332bb12162", "extra_info": null, "node_info": {"start": 627032, "end": 630353}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f45c243e-1497-466a-8108-8e3a6031f9fe", "3": "331d4155-85dd-40df-855f-abdfa676fac9"}}, "__type__": "1"}, "331d4155-85dd-40df-855f-abdfa676fac9": {"__data__": {"text": "him- or herself.\nThey are famished and desperate.\nDante then suggests \u201cwhy do not you feed one another?\u201d\nThe rest of the chapter is lost.\n1.Write an algorithm to allow these unfortunates to feed one another. Two or\nmore people may not feed the same person at the same time. Y our algorithm\nmust be, well, starvation-free.\n2.Discuss the advantages and disadvantages of your algorithm. Is it centralized,\ndecentralized, high or low in contention, deterministic or randomized?\nExercise 124. Consider the linearization points of the enq() and deq() methods\nof the lock-free queue:\n1.Can we choose the point at which the returned value is read from a node as\nthe linearization point of a successful deq()?\n2.Can we choose the linearization point of the enq() method to be the point\nat which the tail \ufb01eld is updated, possibly by other threads (consider if it is\nwithin the enq()\u2019s execution interval)? Argue your case.\nExercise 125. Consider the unbounded queue implementation shown in\nFig. 10.21. This queue is blocking, meaning that the deq() method does not\nreturn until it has found an item to dequeue.\n10.9 Exercises 243\n1public class HWQueue<T> {\n2 AtomicReference<T>[] items;\n3 AtomicInteger tail;\n4 ...\n5 public void enq(T x) {\n6 int i = tail.getAndIncrement();\n7 items[i].set(x);\n8 }\n9 public T deq() {\n10 while (true ) {\n11 int range = tail.get();\n12 for (int i = 0; i < range; i++) {\n13 T value = items[i].getAndSet( null );\n14 if(value != null ) {\n15 return value;\n16 }\n17 }\n18 }\n19 }\n20 }\nFigure 10.21 QueueusedinExercise125.\nThe queue has two \ufb01elds: items is a very large array, and tail is the index of\nthe next unused element in the array.\n1.Are the enq() and deq() methods wait-free? If not, are they lock-free? Explain.\n2.Identify the linearization points for enq() and deq(). (Careful! They may be\nexecution-dependent.)\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n11Concurrent Stacks\nand Elimination\n11.1 Introduction\nThe Stack<T> class is a collection of items (of type T) that provides push ()\nandpop() methods satisfying the last-in-\ufb01rst-out (LIFO) property: the last item\npushed is the \ufb01rst popped. This chapter considers how to implement concurrent\nstacks. At \ufb01rst glance, stacks seem to provide little opportunity for concurrency,\nbecause push () and pop() calls seem to need to synchronize at the top of the\nstack.\nSurprisingly, perhaps, stacks are not inherently sequential. In this chapter, we\nshow how to implement concurrent stacks that can achieve a high degree of par-\nallelism. As a \ufb01rst step, we consider how to build a lock-free stack in which pushes\nand pops synchronize at a single location.\n11.2 An Unbounded Lock-Free Stack\nFig. 11.1 shows a concurrent LockFreeStack class, whose code appears in\nFigs. 11.2 ,11.3 and 11.4 . The lock-free stack is a linked list, where the top \ufb01eld\npoints to the \ufb01rst node (or null if the stack is empty.) For simplicity, we usually\nassume it is illegal to add a null value to a stack.\nApop() call that tries to remove an item from an empty stack throws an excep-\ntion. A push ()", "doc_id": "331d4155-85dd-40df-855f-abdfa676fac9", "embedding": null, "doc_hash": "d4fd94ab3f65e656abd4fd32b0bb148430320dc5f8789dcef15c31f6f07a425c", "extra_info": null, "node_info": {"start": 630349, "end": 633452}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1ca1cd9e-d7b1-4c94-9496-902c99c1d6fe", "3": "e24ede6d-cf78-437b-9498-aea357fa47aa"}}, "__type__": "1"}, "e24ede6d-cf78-437b-9498-aea357fa47aa": {"__data__": {"text": "pops synchronize at a single location.\n11.2 An Unbounded Lock-Free Stack\nFig. 11.1 shows a concurrent LockFreeStack class, whose code appears in\nFigs. 11.2 ,11.3 and 11.4 . The lock-free stack is a linked list, where the top \ufb01eld\npoints to the \ufb01rst node (or null if the stack is empty.) For simplicity, we usually\nassume it is illegal to add a null value to a stack.\nApop() call that tries to remove an item from an empty stack throws an excep-\ntion. A push () method creates a new node (Line 13), and then calls tryPush ()\nto try to swing the top reference from the current top-of-stack to its succes-\nsor. If tryPush () succeeds, push () returns, and if not, the tryPush () attempt\nis repeated after backing off. The pop() method calls tryPop (), which uses\ncompareAndSet() to try to remove the \ufb01rst node from the stack. If it succeeds,\nit returns the node, otherwise it returns null. (It throws an exception if the stack\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00011-3\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.245\n246 Chapter 11 Concurrent Stacks and Elimination\nA:pop()(b)\nlstop\nvalue value valueA:push()(a) top\nvalue valuevaluea\na\nFigure 11.1 A Lock-free stack. In Part (a) a thread pushes value ainto the stack by applying\nacompareAndSet() to the top\ufb01eld. In Part (b) a thread pops value afrom the stack by\napplying a compareAndSet() to the top\ufb01eld.\n1public class LockFreeStack<T> {\n2 AtomicReference<Node> top = new AtomicReference<Node>( null );\n3 static final int MIN_DELAY = ...;\n4 static final int MAX_DELAY = ...;\n5 Backoff backoff = new Backoff(MIN_DELAY, MAX_DELAY);\n6\n7 protected boolean tryPush(Node node){\n8 Node oldTop = top.get();\n9 node.next = oldTop;\n10 return (top.compareAndSet(oldTop, node));\n11 }\n12 public void push(T value) {\n13 Node node = new Node(value);\n14 while (true ) {\n15 if(tryPush(node)) {\n16 return ;\n17 }else {\n18 backoff.backoff();\n19 }\n20 }\n21 }\nFigure 11.2 TheLockFreeStack<T> class: in the push ()method, threads alternate between\ntrying to alter the topreference by calling tryPush (), and backing off using the Backoff class\nfrom Fig. 7.5 of Chapter 7.\nis empty.) The tryPop () method is called until it succeeds, at which point pop()\nreturns the value from the removed node.\nAs we have seen in Chapter 7, one can signi\ufb01cantly reduce contention at the\ntop\ufb01eld using exponential backoff (see Fig. 7.5 of Chapter 7). Accordingly, both\n11.2 An Unbounded Lock-Free Stack 247\n1public class Node {\n2 public T value;\n3 public Node next;\n4 public Node(T value) {\n5 value = value;\n6 next = null ;\n7 }\n8}\nFigure 11.3 Lock-free stack list node.\n1 protected Node tryPop() throws EmptyException {\n2 Node oldTop = top.get();\n3 if(oldTop == null ) {\n4 throw new EmptyException();\n5 }\n6 Node newTop = oldTop.next;\n7 if(top.compareAndSet(oldTop, newTop)) {\n8 return oldTop;\n9 }else {\n10 return null ;\n11 }\n12 }\n13 public T pop() throws EmptyException {\n14 while (true ) {\n15", "doc_id": "e24ede6d-cf78-437b-9498-aea357fa47aa", "embedding": null, "doc_hash": "42fd4dba269da2dc64fb65f4ba44a422b35a12298a97bd16d6fae9afaa3a8a30", "extra_info": null, "node_info": {"start": 633532, "end": 636478}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "331d4155-85dd-40df-855f-abdfa676fac9", "3": "8c93c0be-7796-498c-aaf4-e7cd511b4447"}}, "__type__": "1"}, "8c93c0be-7796-498c-aaf4-e7cd511b4447": {"__data__": {"text": "Node(T value) {\n5 value = value;\n6 next = null ;\n7 }\n8}\nFigure 11.3 Lock-free stack list node.\n1 protected Node tryPop() throws EmptyException {\n2 Node oldTop = top.get();\n3 if(oldTop == null ) {\n4 throw new EmptyException();\n5 }\n6 Node newTop = oldTop.next;\n7 if(top.compareAndSet(oldTop, newTop)) {\n8 return oldTop;\n9 }else {\n10 return null ;\n11 }\n12 }\n13 public T pop() throws EmptyException {\n14 while (true ) {\n15 Node returnNode = tryPop();\n16 if(returnNode != null ) {\n17 return returnNode.value;\n18 }else {\n19 backoff.backoff();\n20 }\n21 }\n22 }\nFigure 11.4 The LockFreeStack<T> class: The pop()method alternates between trying to\nchange the top\ufb01eld and backing off.\nthepush () and pop() methods back off after an unsuccessful call to tryPush ()\nortryPop ().\nThis implementation is lock-free because a thread fails to complete a push () or\npop() method call only if there were in\ufb01nitely many successful calls that modi\ufb01ed\nthetop of the stack. The linearization point of both the push () and the pop()\nmethods is the successful compareAndSet() , or the throwing of the exception,\nin Line 3, in case of a pop() on an empty stack. Note that the compareAndSet()\ncall by pop() does not have an ABA problem (see Chapter 10 ) because the Java\ngarbage collector ensures that a node cannot be reused by one thread, as long as\nthat node is accessible to another thread. Designing a lock-free stack that avoids\nthe ABA problem without a garbage collector is left as an exercise.\n248 Chapter 11 Concurrent Stacks and Elimination\n11.3 Elimination\nThe LockFreeStack implementation scales poorly, not so much because the\nstack\u2019s top \ufb01eld is a source of contention , but primarily because it is a sequen-\ntial bottleneck : method calls can proceed only one after the other, ordered by\nsuccessful compareAndSet() calls applied to the stack\u2019s top \ufb01eld.\nAlthough exponential backoff can signi\ufb01cantly reduce contention, it does\nnothing to alleviate the sequential bottleneck. T o make the stack parallel, we\nexploit this simple observation: if a push () is immediately followed by a pop(),\nthe two operations cancel out, and the stack\u2019s state does not change. It is as if\nboth operations never happened. If one could somehow cause concurrent pairs\nof pushes and pops to cancel, then threads calling push () could exchange val-\nues with threads calling pop(), without ever modifying the stack itself. These two\ncalls would eliminate one another.\nAs depicted in Fig. 11.5, threads eliminate one another through an\nEliminationArray in which threads pick random array entries to try to meet\ncomplementary calls. Pairs of complementary push () and pop() calls exchange\nvalues and return. A thread whose call cannot be eliminated, either because it has\nfailed to \ufb01nd a partner, or found a partner with the wrong kind of method call\n(such as a push () meeting a push ()), can either try again to eliminate at a new\nlocation, or can access the shared LockFreeStack . The combined data structure,\narray, and shared stack, is linearizable because the shared stack is linearizable,\nand the eliminated calls can be ordered as if they happened at the point in which\nthey exchanged values.\nWe can use the EliminationArray as a backoff scheme on a shared\nLockFreeStack . Each thread \ufb01rst accesses the LockFreeStack , and if it fails\nA:pop()\ntop\ndef B:push(", "doc_id": "8c93c0be-7796-498c-aaf4-e7cd511b4447", "embedding": null, "doc_hash": "8b31ffa71bb1ec8b01f4623da372629face6f316cbe30fb6f4d63207178f53b7", "extra_info": null, "node_info": {"start": 636503, "end": 639830}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e24ede6d-cf78-437b-9498-aea357fa47aa", "3": "0a6964f2-bb1b-487b-b22b-9bd062c566d0"}}, "__type__": "1"}, "0a6964f2-bb1b-487b-b22b-9bd062c566d0": {"__data__": {"text": "wrong kind of method call\n(such as a push () meeting a push ()), can either try again to eliminate at a new\nlocation, or can access the shared LockFreeStack . The combined data structure,\narray, and shared stack, is linearizable because the shared stack is linearizable,\nand the eliminated calls can be ordered as if they happened at the point in which\nthey exchanged values.\nWe can use the EliminationArray as a backoff scheme on a shared\nLockFreeStack . Each thread \ufb01rst accesses the LockFreeStack , and if it fails\nA:pop()\ntop\ndef B:push( b)C:pop()\nB:return()A:return( b)\nC:return( d)\nFigure 11.5 The EliminationBackoffStack<T> class. Each thread selects a random loca-\ntion in the array. If thread A\u2019spop()andB\u2019spush ()calls arrive at the same location at about\nthe same time, then they exchange values without accessing the shared LockFreeStack .\nThread Cthat does not meet another thread eventually pops the shared LockFreeStack .\n11.4 The Elimination Backoff Stack 249\nto complete its call (that is, the compareAndSet() attempt fails), it attempts to\neliminate its call using the array instead of simply backing off. If it fails to elimi-\nnate itself, it calls the LockFreeStack again, and so on. We call this structure an\nEliminationBackoffStack .\n11.4 The Elimination Backoff Stack\nHere is how to construct an EliminationBackoffStack , a lock-free linearizable\nstack implementation.\nWe are reminded of a story about two friends who are discussing poli-\ntics on election day, each trying, to no avail, to convince the other to switch\nsides.\nFinally, one says to the other: \u201cLook, it\u2019s clear that we are unalterably opposed\non every political issue. Our votes will surely cancel out. Why not save ourselves\nsome time and both agree to not vote today?\u201d\nThe other agrees enthusiastically and they part.\nShortly after that, a friend of the \ufb01rst one who had heard the conversation says,\n\u201cThat was a sporting offer you made.\u201d\n\u201cNot really,\u201d says the second. \u201cThis is the third time I\u2019ve done this today.\u201d\nThe principle behind our construction is the same. We wish to allow threads\nwith pushes and pops to coordinate and cancel out, but must avoid a situation in\nwhich a thread can make a sporting offer to more than one other thread. We do\nso by implementing the EliminationArray using coordination structures called\nexchangers , objects that allow exactly two threads (and no more) to rendezvous\nand exchange values.\nWe already saw how to exchange values using locks in the synchronous queue\nof Chapter 10. Here, we need a lock-free exchange, one in which threads spin\nrather than block, as we expect them to wait only for very short durations.\n11.4.1 A Lock-Free Exchanger\nALockFreeExchanger<T> object permits two threads to exchange values of\ntype T. If threadAcalls the object\u2019s exchange () method with argument a, and\nBcalls the same object\u2019s exchange () method with argument b, thenA\u2019s call will\nreturn value band vice versa. On a high level, the exchanger works by having the\n\ufb01rst thread arrive to write its value, and spin until a second arrives. The second\nthen detects that the \ufb01rst is waiting, reads its value, and signals the exchange.\nThey each have now read the other\u2019s value, and can return. The \ufb01rst thread\u2019s call\nmay timeout if the second does not show up, allowing it to proceed and leave the\nexchanger, if it is unable to exchange a value within a reasonable", "doc_id": "0a6964f2-bb1b-487b-b22b-9bd062c566d0", "embedding": null, "doc_hash": "e7d8d78d6b5efd6811e96b68c9b9d6962e22e2d3cd9e7a856ffb8c9d4e852dea", "extra_info": null, "node_info": {"start": 639732, "end": 643110}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8c93c0be-7796-498c-aaf4-e7cd511b4447", "3": "f2735c18-bfc7-4488-9391-ea29ff474f54"}}, "__type__": "1"}, "f2735c18-bfc7-4488-9391-ea29ff474f54": {"__data__": {"text": "argument b, thenA\u2019s call will\nreturn value band vice versa. On a high level, the exchanger works by having the\n\ufb01rst thread arrive to write its value, and spin until a second arrives. The second\nthen detects that the \ufb01rst is waiting, reads its value, and signals the exchange.\nThey each have now read the other\u2019s value, and can return. The \ufb01rst thread\u2019s call\nmay timeout if the second does not show up, allowing it to proceed and leave the\nexchanger, if it is unable to exchange a value within a reasonable duration.\n250 Chapter 11 Concurrent Stacks and Elimination\n1public class LockFreeExchanger<T> {\n2 static final int EMPTY = ..., WAITING = ..., BUSY = ...;\n3 AtomicStampedReference<T> slot = new AtomicStampedReference<T>( null , 0);\n4 public T exchange(T myItem, long timeout, TimeUnit unit)\n5 throws TimeoutException {\n6 long nanos = unit.toNanos(timeout);\n7 long timeBound = System.nanoTime() + nanos;\n8 int[] stampHolder = {EMPTY};\n9 while (true ) {\n10 if(System.nanoTime() > timeBound)\n11 throw new TimeoutException();\n12 T yrItem = slot.get(stampHolder);\n13 int stamp = stampHolder[0];\n14 switch (stamp) {\n15 case EMPTY:\n16 if(slot.compareAndSet(yrItem, myItem, EMPTY, WAITING)) {\n17 while (System.nanoTime() < timeBound){\n18 yrItem = slot.get(stampHolder);\n19 if(stampHolder[0] == BUSY) {\n20 slot.set( null , EMPTY);\n21 return yrItem;\n22 }\n23 }\n24 if(slot.compareAndSet(myItem, null , WAITING, EMPTY)) {\n25 throw new TimeoutException();\n26 }else {\n27 yrItem = slot.get(stampHolder);\n28 slot.set( null , EMPTY);\n29 return yrItem;\n30 }\n31 }\n32 break ;\n33 case WAITING:\n34 if(slot.compareAndSet(yrItem, myItem, WAITING, BUSY))\n35 return yrItem;\n36 break ;\n37 case BUSY:\n38 break ;\n39 default :// impossible\n40 ...\n41 }\n42 }\n43 }\n44 }\nFigure 11.6 TheLockFreeExchanger<T> Class.\nThe LockFreeExchanger<T> class appears in Fig. 11.6 . It has a single\nAtomicStampedReference<T> \ufb01eld,1slot . The exchanger has three possible\nstates: EMPTY ,BUSY , orWAITING . The reference\u2019s stamp records the exchanger\u2019s\nstate (Line 14). The exchanger\u2019s main loop continues until the timeout limit\n1SeeChapter 10 ,Pragma 10.6.1 .\n11.4 The Elimination Backoff Stack 251\npasses, when it throws an exception (Line 10). In the meantime, a thread reads\nthe state of the slot (Line 12) and proceeds as follows:\n\u0004If the state is EMPTY , then the thread tries to place its item in the slot and set\nthe state to WAITING using a compareAndSet() (Line 16). If it fails, then some\nother thread succeeds and it retries. If it was successful (Line 17), then its item\nis in the slot and the state is WAITING , so it spins, waiting for another thread\nto complete the exchange. If another thread shows up, it will take the item in\nthe slot, replace it with its own, and set the state to BUSY (Line 19), indicat-\ning to the waiting thread that the exchange is complete. The waiting thread\nwill consume the item and reset the state to EMPTY . Resetting to EMPTY can be\ndone using a simple write because the", "doc_id": "f2735c18-bfc7-4488-9391-ea29ff474f54", "embedding": null, "doc_hash": "aff005b2a3d751952364adc96fbbeb80bc033e1ef45d4d67c58cb54e3cd0ac55", "extra_info": null, "node_info": {"start": 643148, "end": 646121}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0a6964f2-bb1b-487b-b22b-9bd062c566d0", "3": "3311c3c9-b112-4dc8-9661-4d62dfc6742e"}}, "__type__": "1"}, "3311c3c9-b112-4dc8-9661-4d62dfc6742e": {"__data__": {"text": "(Line 16). If it fails, then some\nother thread succeeds and it retries. If it was successful (Line 17), then its item\nis in the slot and the state is WAITING , so it spins, waiting for another thread\nto complete the exchange. If another thread shows up, it will take the item in\nthe slot, replace it with its own, and set the state to BUSY (Line 19), indicat-\ning to the waiting thread that the exchange is complete. The waiting thread\nwill consume the item and reset the state to EMPTY . Resetting to EMPTY can be\ndone using a simple write because the waiting thread is the only one that can\nchange the state from BUSY toEMPTY (Line 20). If no other thread shows up,\nthe waiting thread needs to reset the state of the slot to EMPTY . This change\nrequires a compareAndSet() because other threads might be attempting to\nexchange by setting the state from WAITING toBUSY (Line 24). If the call is suc-\ncessful, it raises a timeout exception. If, however, the call fails, some exchang-\ning thread must have shown up, so the waiting thread completes the exchange\n(Line 26).\n\u0004If the state is WAITING , then some thread is waiting and the slot contains its\nitem. The thread takes the item, and tries to replace it with its own by changing\nthe state from WAITING toBUSY using a compareAndSet() (Line 34). It may\nfail if another thread succeeds, or the other thread resets the state to EMPTY\nfollowing a timeout. If so, the thread must retry. If it does succeed changing\nthe state to BUSY , then it can return the item.\n\u0004If the state is BUSY then two other threads are currently using the slot for an\nexchange and the thread must retry (Line 37).\nNotice that the algorithm allows the inserted item to be null, something\nused later in the elimination array construction. There is no ABA problem\nbecause the compareAndSet() call that changes the state never inspects the item.\nA successful exchange\u2019s linearization point occurs when the second thread to\narrive changes the state from WAITING toBUSY (Line 34). At this point both\nexchange () calls overlap, and the exchange is committed to being successful. An\nunsuccessful exchange\u2019s linearization point occurs when the timeout exception is\nthrown.\nThe algorithm is lock-free because overlapping exchange () calls with suf\ufb01-\ncient time to exchange will fail only if other exchanges are repeatedly succeeding.\nClearly, too short an exchange time can cause a thread never to succeed, so care\nmust be taken when choosing timeout durations.\n11.4.2 The Elimination Array\nAnEliminationArray is implemented as an array of Exchanger objects of\nmaximal size capacity . A thread attempting to perform an exchange picks an\narray entry at random, and calls that entry\u2019s exchange () method, providing\n252 Chapter 11 Concurrent Stacks and Elimination\n1public class EliminationArray<T> {\n2 private static final int duration = ...;\n3 LockFreeExchanger<T>[] exchanger;\n4 Random random;\n5 public EliminationArray( int capacity) {\n6 exchanger = (LockFreeExchanger<T>[]) new LockFreeExchanger[capacity];\n7 for (int i = 0; i < capacity; i++) {\n8 exchanger[i] = new LockFreeExchanger<T>();\n9 }\n10 random = new Random();\n11 }\n12 public T visit(T value, int range) throws TimeoutException {\n13 int slot = random.nextInt(range);\n14 return (exchanger[slot].exchange(value, duration,\n15 TimeUnit.MILLISECONDS));\n16 }\n17 }\nFigure 11.7 TheEliminationArray<T> class: in each visit, a thread can choose dynamically\nthe sub-range of the array", "doc_id": "3311c3c9-b112-4dc8-9661-4d62dfc6742e", "embedding": null, "doc_hash": "4f7ccaf0b561d1a7c1c5910c7a312797a24608c64af56b85593ebd20bf2ee23f", "extra_info": null, "node_info": {"start": 646088, "end": 649538}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f2735c18-bfc7-4488-9391-ea29ff474f54", "3": "38b6021c-b637-4717-9d84-e49a38da29ee"}}, "__type__": "1"}, "38b6021c-b637-4717-9d84-e49a38da29ee": {"__data__": {"text": "for (int i = 0; i < capacity; i++) {\n8 exchanger[i] = new LockFreeExchanger<T>();\n9 }\n10 random = new Random();\n11 }\n12 public T visit(T value, int range) throws TimeoutException {\n13 int slot = random.nextInt(range);\n14 return (exchanger[slot].exchange(value, duration,\n15 TimeUnit.MILLISECONDS));\n16 }\n17 }\nFigure 11.7 TheEliminationArray<T> class: in each visit, a thread can choose dynamically\nthe sub-range of the array from which it will will randomly select a slot.\nits own input as an exchange value with another thread. The code for the\nEliminationArray appears in Fig. 11.7 . The constructor takes as an argu-\nment the capacity of the array (the number of distinct exchangers). The\nEliminationArray class provides a single method, visit (), which takes time-\nout arguments. (Following the conventions used in the java.util.concurrent pack-\nage, a timeout is expressed as a number and a time unit.) The visit () call takes\na value of type Tand either returns the value input by its exchange partner,\nor throws an exception if the timeout expires without exchanging a value with\nanother thread. At any point in time, each thread will select a random location in\na subrange of the array (Line 13). This subrange will be determined dynamically\nbased on the load on the data structure, and will be passed as a parameter to the\nvisit () method.\nThe EliminationBackoffStack is a subclass of LockFreeStack that over-\nrides the push () and pop() methods, and adds an EliminationArray \ufb01eld.\nFigs. 11.8 and 11.9 show the new push () and pop() methods. Upon failure of\natryPush () or tryPop () attempt, instead of simply backing off, these methods\ntry to use the EliminationArray to exchange values (Lines 15and34). Apush ()\ncall calls visit () with its input value as argument, and a pop() call with null as\nargument. Both push () and pop() have a thread-local RangePolicy object that\ndetermines the EliminationArray subrange to be used.\nWhen push () calls visit (), it selects a random array entry within its range\nand attempts to exchange a value with another thread. If the exchange is suc-\ncessful, the pushing thread checks whether the value was exchanged with a pop()\nmethod (Line 18) by testing if the value exchanged was null. (Recall that pop()\nalways offers null to the exchanger while push () always offers a non- null value.)\nSymmetrically, when pop() calls visit (), it attempts an exchange, and if the\n11.4 The Elimination Backoff Stack 253\n1public class EliminationBackoffStack<T> extends LockFreeStack<T> {\n2 static final int capacity = ...;\n3 EliminationArray<T> eliminationArray = new EliminationArray<T>(capacity);\n4 static ThreadLocal<RangePolicy> policy = new ThreadLocal<RangePolicy>() {\n5 protected synchronized RangePolicy initialValue() {\n6 return new RangePolicy();\n7 }\n8\n9 public void push(T value) {\n10 RangePolicy rangePolicy = policy.get();\n11 Node node = new Node(value);\n12 while (true ) {\n13 if(tryPush(node)) {\n14 return ;\n15 }else try {\n16 T otherValue = eliminationArray.visit\n17 (value, rangePolicy.getRange());\n18 if(otherValue == null ) {\n19 rangePolicy.recordEliminationSuccess();\n20 return ;// exchanged with pop\n21 }\n22 }catch (TimeoutException ex) {\n23 rangePolicy.recordEliminationTimeout();\n24 }\n25 }\n26 }\n27 }\nFigure 11.8 The EliminationBackoffStack<T> class: this push ()method overrides", "doc_id": "38b6021c-b637-4717-9d84-e49a38da29ee", "embedding": null, "doc_hash": "fa9f5cfa9ee0947838b82e3bc657679fbf4a1013df4dffad62b016158b500246", "extra_info": null, "node_info": {"start": 649622, "end": 652955}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3311c3c9-b112-4dc8-9661-4d62dfc6742e", "3": "73d71f49-50a3-49f3-b746-61ca9dfb8ab3"}}, "__type__": "1"}, "73d71f49-50a3-49f3-b746-61ca9dfb8ab3": {"__data__": {"text": "Node node = new Node(value);\n12 while (true ) {\n13 if(tryPush(node)) {\n14 return ;\n15 }else try {\n16 T otherValue = eliminationArray.visit\n17 (value, rangePolicy.getRange());\n18 if(otherValue == null ) {\n19 rangePolicy.recordEliminationSuccess();\n20 return ;// exchanged with pop\n21 }\n22 }catch (TimeoutException ex) {\n23 rangePolicy.recordEliminationTimeout();\n24 }\n25 }\n26 }\n27 }\nFigure 11.8 The EliminationBackoffStack<T> class: this push ()method overrides the\nLockFreeStack push ()method. Instead of using a simple Backoff class, it uses an\nEliminationArray and a dynamic RangePolicy to select the subrange of the array within\nwhich to eliminate.\n28 public T pop() throws EmptyException {\n29 RangePolicy rangePolicy = policy.get();\n30 while (true ) {\n31 Node returnNode = tryPop();\n32 if(returnNode != null ) {\n33 return returnNode.value;\n34 }else try {\n35 T otherValue = eliminationArray.visit( null , rangePolicy.getRange());\n36 if(otherValue != null ) {\n37 rangePolicy.recordEliminationSuccess();\n38 return otherValue;\n39 }\n40 }catch (TimeoutException ex) {\n41 rangePolicy.recordEliminationTimeout();\n42 }\n43 }\n44 }\nFigure 11.9 The EliminationBackoffStack<T> class: this pop()method overrides the\nLockFreeStack push ()method.\n254 Chapter 11 Concurrent Stacks and Elimination\nexchange is successful it checks (Line 36) whether the value was exchanged with\napush () call by checking whether it is not null.\nIt is possible that the exchange will be unsuccessful, either because no\nexchange took place (the call to visit () timed out) or because the exchange\nwas with the same type of operation (such as a pop() with a pop()). For brevity,\nwe choose a simple approach to deal with such cases: we retry the tryPush () or\ntryPop () calls (Lines 13and 31).\nOne important parameter is the range of the EliminationArray from which\na thread selects an Exchanger location. A smaller range will allow a greater\nchance of a successful collision when there are few threads, while a larger range\nwill lower the chances of threads waiting on a busy Exchanger (recall that an\nExchanger can only handle one exchange at a time). Thus, if few threads access\nthe array, they should choose smaller ranges, and as the number of threads\nincrease, so should the range. One can control the range dynamically using a\nRangePolicy object that records both successful exchanges (as in Line 37) and\ntimeout failures (Line 40). We ignore exchanges that fail because the operations\ndo not match (such as push () with push ()), because they account for a \ufb01xed frac-\ntion of the exchanges for any given distribution of push () and pop() calls. One\nsimple policy is to shrink the range as the number of failures increases and vice\nversa.\nThere are many other possible policies. For example, one can devise a more\nelaborate range selection policy, vary the delays on the exchangers dynamically,\nadd additional backoff delays before accessing the shared stack, and control\nwhether to access the shared stack or the array dynamically. We leave these as\nexercises.\nTheEliminationBackoffStack is a linearizable stack: any successful push ()\norpop() call that completes by accessing the LockFreeStack can be linearized at\nthe point of its LockFreeStack access. Any pair of eliminated push () and pop()\ncalls can be linearized when they collide. As noted earlier, the method calls com-\npleted through elimination do not affect the linearizability of those completed\nin the LockFreeStack , because they could have taken effect in any state of the\nLockFreeStack , and having taken effect,", "doc_id": "73d71f49-50a3-49f3-b746-61ca9dfb8ab3", "embedding": null, "doc_hash": "7f2376b41fc0cb94ee323b623a53cec72a896ee0aeb280b5a6da78ca69e40c95", "extra_info": null, "node_info": {"start": 652917, "end": 656468}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "38b6021c-b637-4717-9d84-e49a38da29ee", "3": "7ae1dcf6-b460-4d4a-acbf-5c423d68efda"}}, "__type__": "1"}, "7ae1dcf6-b460-4d4a-acbf-5c423d68efda": {"__data__": {"text": "stack or the array dynamically. We leave these as\nexercises.\nTheEliminationBackoffStack is a linearizable stack: any successful push ()\norpop() call that completes by accessing the LockFreeStack can be linearized at\nthe point of its LockFreeStack access. Any pair of eliminated push () and pop()\ncalls can be linearized when they collide. As noted earlier, the method calls com-\npleted through elimination do not affect the linearizability of those completed\nin the LockFreeStack , because they could have taken effect in any state of the\nLockFreeStack , and having taken effect, the state of the LockFreeStack would\nnot have changed.\nBecause the EliminationArray is effectively used as a backoff scheme, we\nexpect it to deliver performance comparable to the LockFreeStack at low loads.\nUnlike the LockFreeStack , it has the potential to scale. As the load increases, the\nnumber of successful eliminations will grow, allowing many operations to com-\nplete in parallel. Moreover, contention at the LockFreeStack is reduced because\neliminated operations never access the stack.\n11.5 Chapter Notes\nThe LockFreeStack is credited to Treiber [ 145]. Actually it predates Treiber\u2019s\nreport in 1986. It was probably invented in the early 1970s to motivate the\n11.6 Exercises 255\nCAS operation on the IBM 370. The EliminationBackoffStack is due to\nDanny Hendler, Nir Shavit, and Lena Y erushalmi [ 57]. An ef\ufb01cient exchanger,\nwhich quite interestingly uses an elimination array, was introduced by Doug Lea,\nMichael Scott, and Bill Scherer [ 136]. A variant of this exchanger appears in the\nJava Concurrency Package. The EliminationBackoffStack we present here is\nmodular, making use of exchangers, but somewhat inef\ufb01cient. Mark Moir, Daniel\nNussbaum, Ori Shalev, and Nir Shavit present a highly effective implementation\nof an EliminationArray [120].\n11.6 Exercises\nExercise 126. Design an unbounded lock-based Stack<T> implementation based\non a linked list.\nExercise 127. Design a bounded lock-based Stack<T> using an array.\n1.Use a single lock and a bounded array.\n2.Try to make your algorithm lock-free. Where do you run into dif\ufb01culty?\nExercise 128. Modify the unbounded lock-free stack of Section 11.2 to work\nin the absence of a garbage collector. Create a thread-local pool of preallo-\ncated nodes and recycle them. T o avoid the ABA problem, consider using the\nAtomicStampedReference<T> class from java.util.concurrent.atomic that encap-\nsulates both a reference and an integer stamp .\nExercise 129. Discuss the backoff policies used in our implementation. Does it\nmake sense to use the same shared Backoff object for both pushes and pops in\nourLockFreeStack<T> object? How else could we structure the backoff in space\nand time in the EliminationBackoffStack<T> ?\nExercise 130. Implement a stack algorithm assuming there is a bound, in any state\nof the execution, on the total difference between the number of pushes and pops\nto the stack.\nExercise 131. Consider the problem of implementing a bounded stack using an\narray indexed by a topcounter, initially zero. In the absence of concurrency, these\nmethods are almost trivial. T o push an item, increment top to reserve an array\nentry, and then store the item at that index. T o pop an item, decrement top, and\nreturn the item at the previous top index.\nClearly, this strategy does not work for concurrent implementations, because\none cannot make atomic changes to multiple memory locations. A single\nsynchronization operation can either increment or decrement the top counter,\nbut not both, and there is no way atomically to", "doc_id": "7ae1dcf6-b460-4d4a-acbf-5c423d68efda", "embedding": null, "doc_hash": "16b43285306a013344bbadef22cf6ed718396e19c00afc098106fae580a12316", "extra_info": null, "node_info": {"start": 656382, "end": 659955}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "73d71f49-50a3-49f3-b746-61ca9dfb8ab3", "3": "94c7b847-31c9-4f66-90c3-353a35d9828c"}}, "__type__": "1"}, "94c7b847-31c9-4f66-90c3-353a35d9828c": {"__data__": {"text": "a bounded stack using an\narray indexed by a topcounter, initially zero. In the absence of concurrency, these\nmethods are almost trivial. T o push an item, increment top to reserve an array\nentry, and then store the item at that index. T o pop an item, decrement top, and\nreturn the item at the previous top index.\nClearly, this strategy does not work for concurrent implementations, because\none cannot make atomic changes to multiple memory locations. A single\nsynchronization operation can either increment or decrement the top counter,\nbut not both, and there is no way atomically to increment the counter and store\na value.\n256 Chapter 11 Concurrent Stacks and Elimination\nNevertheless, Bob D. Hacker decides to solve this problem. He decides to adapt\nthe dual-data structure approach of Chapter 10 to implement a dual stack. His\nDualStack<T>class splits push () and pop() methods into reservation and ful-\n\ufb01llment steps. Bob\u2019s implementation appears in Fig. 11.10.\nThe stack\u2019s top is indexed by the top \ufb01eld, an AtomicInteger manipu-\nlated only by getAndIncrement () and getAndDecrement () calls. Bob\u2019s push ()\n1public class DualStack<T> {\n2 private class Slot {\n3 boolean full = false ;\n4 volatile T value = null ;\n5 }\n6 Slot[] stack;\n7 int capacity;\n8 private AtomicInteger top = new AtomicInteger(0); // array index\n9 public DualStack( int myCapacity) {\n10 capacity = myCapacity;\n11 stack = (Slot[]) new Object[capacity];\n12 for (int i = 0; i < capacity; i++) {\n13 stack[i] = new Slot();\n14 }\n15 }\n16 public T pop() throws EmptyException {\n17 while (true ) {\n18 int i = top.getAndDecrement();\n19 if(i <= 0) { // is stack empty?\n20 throw new EmptyException();\n21 }else if (i-1 < capacity){\n22 while (!stack [i-1].full ) {};\n23 T value = stack[i-1].value;\n24 stack[i-1].full = false ;\n25 return value ; //pop fulfilled\n26 }\n27 }\n28 }\n29 public T pop() throws EmptyException {\n30 while (true ) {\n31 int i = top.getAndDecrement();\n32 if(i < 0) { // is stack empty?\n33 throw new EmptyException();\n34 }else if (i < capacity - 1) {\n35 while (!stack[i].full){};\n36 T value = stack[i].value;\n37 stack[i].full = false ;\n38 return value; //pop fulfilled\n39 }\n40 }\n41 }\n42 }\nFigure 11.10 Bob\u2019s problematic dual stack.\n11.6 Exercises 257\nmethod\u2019s reservation step reserves a slot by applying getAndIncrement () to top.\nSuppose the call returns index i. Ifiis in the range 0 :::capacity\u00001, the reser-\nvation is complete. In the ful\ufb01llment phase, push (x) storesxat indexiin the\narray, and raises the full \ufb02ag to indicate that the value is ready to be read. The\nvalue \ufb01eld must be volatile to guarantee that once flag is raised, the value\nhas already been written to index iof the array.\nIf the index returned from push ()\u2019sgetAndIncrement () is less than 0,\nthepush () method repeatedly retries getAndIncrement () until it returns an\nindex greater than or equal to 0. The index could be less than 0 due to\ngetAndDecrement () calls of failed pop() calls to an empty stack. Each such failed\ngetAndDecrement () decrements the top by one more past the 0 array bound.\nIf the index returned is greater than capacity\u00001,push () throws an exception\nbecause the stack is full.\nThe", "doc_id": "94c7b847-31c9-4f66-90c3-353a35d9828c", "embedding": null, "doc_hash": "317535d88ce8874348aef013a7b121bc343cf203ec34f54d79b4be8fa9d3103a", "extra_info": null, "node_info": {"start": 659958, "end": 663121}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7ae1dcf6-b460-4d4a-acbf-5c423d68efda", "3": "9887d925-189c-4753-ab8c-54f476e29da3"}}, "__type__": "1"}, "9887d925-189c-4753-ab8c-54f476e29da3": {"__data__": {"text": "the value\nhas already been written to index iof the array.\nIf the index returned from push ()\u2019sgetAndIncrement () is less than 0,\nthepush () method repeatedly retries getAndIncrement () until it returns an\nindex greater than or equal to 0. The index could be less than 0 due to\ngetAndDecrement () calls of failed pop() calls to an empty stack. Each such failed\ngetAndDecrement () decrements the top by one more past the 0 array bound.\nIf the index returned is greater than capacity\u00001,push () throws an exception\nbecause the stack is full.\nThe situation is symmetric for pop(). It checks that the index is within the\nbounds and removes an item by applying getAndDecrement () to top, returning\nindexi. Ifiis in the range 0 :::capacity\u00001, the reservation is complete. For the\nful\ufb01llment phase, pop() spins on the full \ufb02ag of array slot i, until it detects that\nthe \ufb02ag is true, indicating that the push () call is successful.\nWhat is wrong with Bob\u2019s algorithm? Is this an inherent problem or can you\nthink of a way to \ufb01x it?\nExercise 132. In Exercise 97 we ask you to implement the Rooms interface, repro-\nduced in Fig. 11.11. The Rooms class manages a collection of rooms , indexed\nfrom 0 tom(wheremis a known constant). Threads can enter or exit any\nroom in that range. Each room can hold an arbitrary number of threads simul-\ntaneously, but only one room can be occupied at a time. The last thread to\nleave a room triggers an onEmpty () handler, which runs while all rooms are\nempty.\nFig. 11.12 shows an incorrect concurrent stack implementation.\n1.Explain why this stack implementation does not work.\n2.Fix it by adding calls to a two-room Rooms class: one room for pushing and\none for popping.\n1public interface Rooms {\n2 public interface Handler {\n3 void onEmpty();\n4 }\n5 void enter( int i);\n6 boolean exit();\n7 public void setExitHandler( int i, Rooms.Handler h) ;\n8}\nFigure 11.11 TheRooms interface.\n258 Chapter 11 Concurrent Stacks and Elimination\n1public class Stack<T> {\n2 private AtomicInteger top;\n3 private T[] items;\n4 public Stack( int capacity) {\n5 top = new AtomicInteger();\n6 items = (T[]) new Object[capacity];\n7 }\n8 public void push(T x) throws FullException {\n9 int i = top.getAndIncrement();\n10 if(i >= items.length) { // stack is full\n11 top.getAndDecrement(); // restore state\n12 throw new FullException();\n13 }\n14 items[i] = x;\n15 }\n16 public T pop() throws EmptyException {\n17 int i = top.getAndDecrement() - 1;\n18 if(i < 0) { // stack is empty\n19 top.getAndIncrement(); // restore state\n20 throw new EmptyException();\n21 }\n22 return items[i];\n23 }\n24 }\nFigure 11.12 Unsynchronized concurrent stack.\nExercise 133. This exercise is a follow-on to Exercise 132. Instead of having the\npush () method throw FullException , exploit the push room\u2019s exit handler to\nresize the array. Remember that no thread can be in any room when an exit han-\ndler is running, so (of course) only one exit handler can run at a time.\n12Counting, Sorting, and\nDistributed Coordination\n12.1 Introduction\nThis chapter shows how some important problems that seem inherently\nsequential can be made highly parallel by \u201cspreading out\u201d coordination tasks\namong multiple parties. What does this spreading out buy us?\nT o answer this question, we need to understand how to measure the perfor-\nmance of a concurrent data structure. There are two measures that come", "doc_id": "9887d925-189c-4753-ab8c-54f476e29da3", "embedding": null, "doc_hash": "5f149e9d5a6878cf52efbae3dc56f835bc6e8f40a03f7d69abcd48812d971607", "extra_info": null, "node_info": {"start": 663160, "end": 666515}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "94c7b847-31c9-4f66-90c3-353a35d9828c", "3": "2ae9ed2d-ba21-4988-b015-b4f022a1404b"}}, "__type__": "1"}, "2ae9ed2d-ba21-4988-b015-b4f022a1404b": {"__data__": {"text": "the array. Remember that no thread can be in any room when an exit han-\ndler is running, so (of course) only one exit handler can run at a time.\n12Counting, Sorting, and\nDistributed Coordination\n12.1 Introduction\nThis chapter shows how some important problems that seem inherently\nsequential can be made highly parallel by \u201cspreading out\u201d coordination tasks\namong multiple parties. What does this spreading out buy us?\nT o answer this question, we need to understand how to measure the perfor-\nmance of a concurrent data structure. There are two measures that come to mind:\nlatency , the time it takes an individual method call to complete, and throughput ,\nthe overall rate at which method calls complete. For example, real-time appli-\ncations might care more about latency, and databases might care more about\nthroughput.\nIn Chapter 11 we saw how to apply distributed coordination to the\nEliminationBackoffStack class. Here, we cover several useful patterns for dis-\ntributed coordination: combining, counting, diffraction, and sampling. Some\nare deterministic, while others use randomization. We also cover two basic struc-\ntures underlying these patterns: trees and combinatorial networks. Interestingly,\nfor some data structures based on distributed coordination, high throughput\ndoes not necessarily mean low latency.\n12.2 Shared Counting\nWe recall from Chapter 10 that a pool is a collection of items that provides put()\nandget() methods to insert and remove items (Fig. 10.1). Familiar classes such\nas stacks and queues can be viewed as pools that provide additional fairness guar-\nantees.\nOne way to implement a pool is to use coarse-grained locking, perhaps making\nboth put() and get()synchronized methods. The problem, of course, is that\ncoarse-grained locking is too heavy-handed, because the lock itself creates both a\nsequential bottleneck , forcing all method calls to synchronize, as well as a hot spot ,\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00012-5\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.259\n260 Chapter 12 Counting, Sorting, and Distributed Coordination\na source of memory contention. We would prefer to have Pool method calls work\nin parallel, with less synchronization and lower contention.\nLet us consider the following alternative. The pool\u2019s items reside in a cyclic\narray, where each array entry contains either an item or null. We route threads\nthrough two counters. Threads calling put() increment one counter to choose\nan array index into which the new item should be placed. (If that entry is full, the\nthread waits until it becomes empty.) Similarly, threads calling get() increment\nanother counter to choose an array index from which the new item should be\nremoved. (If that entry is empty, the thread waits until it becomes full.)\nThis approach replaces one bottleneck: the lock, with two: the counters. Nat-\nurally, two bottlenecks are better than one (think about that claim for a second).\nWe now explore the idea that shared counters need not be bottlenecks, and can\nbe effectively parallelized. We face two challenges.\n1.We must avoid memory contention , where too many threads try to access the\nsame memory location, stressing the underlying communication network and\ncache coherence protocols.\n2.We must achieve real parallelism. Is incrementing a counter an inherently\nsequential operation, or is it possible for nthreads to increment a counter\nfaster than it takes one thread to increment a counter ntimes?\nWe now look at several ways to build highly parallel counters through data struc-\ntures that coordinate the distribution of counter indexes.\n12.3 Software Combining\nHere is a linearizable shared counter class using a pattern called software com-\nbining .", "doc_id": "2ae9ed2d-ba21-4988-b015-b4f022a1404b", "embedding": null, "doc_hash": "a496c57ac96a10d758213424956e8efa39e4960b13cee022f6b0138d6611bd79", "extra_info": null, "node_info": {"start": 666493, "end": 670242}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9887d925-189c-4753-ab8c-54f476e29da3", "3": "82d0704b-adaf-45e5-9108-c7f7b555ca0a"}}, "__type__": "1"}, "82d0704b-adaf-45e5-9108-c7f7b555ca0a": {"__data__": {"text": "where too many threads try to access the\nsame memory location, stressing the underlying communication network and\ncache coherence protocols.\n2.We must achieve real parallelism. Is incrementing a counter an inherently\nsequential operation, or is it possible for nthreads to increment a counter\nfaster than it takes one thread to increment a counter ntimes?\nWe now look at several ways to build highly parallel counters through data struc-\ntures that coordinate the distribution of counter indexes.\n12.3 Software Combining\nHere is a linearizable shared counter class using a pattern called software com-\nbining . ACombiningTree is a binary tree of nodes , where each node contains\nbookkeeping information. The counter\u2019s value is stored at the root. Each thread\nis assigned a leaf, and at most two threads share a leaf, so if there are pphysical\nprocessors, then there are p=2 leaves. T o increment the counter, a thread starts\nat its leaf, and works its way up the tree to the root. If two threads reach a node\nat approximately the same time, then they combine their increments by adding\nthem together. One thread, the active thread, propagates their combined incre-\nments up the tree, while the other, the passive thread, waits for the active thread to\ncomplete their combined work. A thread may be active at one level and become\npassive at a higher level.\nFor example, suppose threads AandBshare a leaf node. They start at the\nsame time, and their increments are combined at their shared leaf. The \ufb01rst one,\nsay,B, actively continues up to the next level, with the mission of adding 2 to the\ncounter value, while the second, A, passively waits for Bto return from the root\nwith an acknowledgment that A\u2019s increment has occurred. At the next level in\nthe tree,Bmay combine with another thread C, and advance with the renewed\nintention of adding 3 to the counter value.\n12.3 Software Combining 261\nWhen a thread reaches the root, it adds the sum of its combined increments to\nthe counter\u2019s current value. The thread then moves back down the tree, notifying\neach waiting thread that the increments are now complete.\nCombining trees have an inherent disadvantage with respect to locks: each\nincrement has a higher latency, that is, the time it takes an individual method call\nto complete. With a lock, a getAndIncrement () call takesO(1) time, while with a\nCombiningTree , it takesO(logp) time. Nevertheless, a CombiningTree is attrac-\ntive because it promises far better throughput, that is, the overall rate at which\nmethod calls complete. For example, using a queue lock, pgetAndIncrement ()\ncalls complete in O(p) time, at best, while using a CombiningTree , under ideal\nconditions where all threads move up the tree together, pgetAndIncrement ()\ncalls complete in O(logp) time, an exponential improvement. Of course, the\nactual performance is often less than ideal, a subject examined in detail later\non. Still, the CombiningTree class, like other techniques we consider later, is\nintended to bene\ufb01t throughput, not latency.\nCombining trees are also attractive because they can be adapted to apply any\ncommutative function, not just increment, to the value maintained by the tree.\n12.3.1 Overview\nAlthough the idea behind a CombiningTree is quite simple, the implementation\nis not. T o keep the overall (simple) structure from being submerged in (not-so-\nsimple) detail, we split the data structure into two classes: the CombiningTree\nclass manages navigation within the tree, moving up and down the tree as\nneeded, while the Node class manages each visit to a node. As you go through the\nalgorithm\u2019s description, it might be a good idea to consult Fig. 12.3 that describes\nan example CombiningTree", "doc_id": "82d0704b-adaf-45e5-9108-c7f7b555ca0a", "embedding": null, "doc_hash": "14758698796dc100031f9289dd07234f8c878d7f94bcb5a93673777281431063", "extra_info": null, "node_info": {"start": 670196, "end": 673895}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2ae9ed2d-ba21-4988-b015-b4f022a1404b", "3": "fa3d94b7-1315-40bb-a269-37b1bb943b79"}}, "__type__": "1"}, "fa3d94b7-1315-40bb-a269-37b1bb943b79": {"__data__": {"text": "to the value maintained by the tree.\n12.3.1 Overview\nAlthough the idea behind a CombiningTree is quite simple, the implementation\nis not. T o keep the overall (simple) structure from being submerged in (not-so-\nsimple) detail, we split the data structure into two classes: the CombiningTree\nclass manages navigation within the tree, moving up and down the tree as\nneeded, while the Node class manages each visit to a node. As you go through the\nalgorithm\u2019s description, it might be a good idea to consult Fig. 12.3 that describes\nan example CombiningTree execution.\nThis algorithm uses two kinds of synchronization. Short-term synchronization\nis provided by synchronized methods of the Node class. Each method locks the\nnode for the duration of the call to ensure that it can read\u2013write node \ufb01elds\nwithout interference from other threads. The algorithm also requires excluding\nthreads from a node for durations longer than a single method call. Such\nlong-term synchronization is provided by a Boolean locked \ufb01eld. When this\n\ufb01eld is true, no other thread is allowed to access the node.\nEvery tree node has a combining status , which de\ufb01nes whether the node is in\nthe early, middle, or late stages of combining concurrent requests.\nenum CStatus{FIRST, SECOND, RESULT, IDLE, ROOT};\nThese values have the following meanings:\n\u0004IDLE : This node is not in use.\n\u0004FIRST : One active thread has visited this node, and will return to check\nwhether another passive thread has left a value with which to combine.\n\u0004SECOND : A second thread has visited this node and stored a value in the node\u2019s\nvalue \ufb01eld to be combined with the active thread\u2019s value, but the combined\noperation is not yet complete.\n262 Chapter 12 Counting, Sorting, and Distributed Coordination\n\u0004RESULT : Both threads\u2019 operations have been combined and completed, and\nthe second thread\u2019s result has been stored in the node\u2019s result \ufb01eld.\n\u0004ROOT : This value is a special case to indicate that the node is the root, and\nmust be treated specially.\nFig. 12.1 shows the Node class\u2019s other \ufb01elds.\nT o initialize the CombiningTree forpthreads, we create a width w>p=2 array\nofNode objects. The root is node [0], and for 0 <i<w , the parent of node [i] is\nnode [(i\u00001)=2]. The leaf nodes are the last ( w+ 1)=2 nodes in the array, where\nthreadiis assigned to leaf i=2. The root\u2019s initial combining state is ROOT and the\nother nodes combining state is IDLE .Fig. 12.2 shows the CombiningTree class\nconstructor.\nTheCombiningTree \u2019sgetAndIncrement () method, shown in Fig. 12.4 , has\nfour phases. In the precombining phase (Lines 16through 19), the CombiningTree\nclass\u2019s getAndIncrement () method moves up the tree applying precombine () to\n1public class Node {\n2 enum CStatus{IDLE, FIRST, SECOND, RESULT, ROOT};\n3 boolean locked;\n4 CStatus cStatus;\n5 int firstValue, secondValue;\n6 int result;\n7 Node parent;\n8 public Node() {\n9 cStatus = CStatus.ROOT;\n10 locked = false ;\n11 }\n12 public Node(Node myParent) {\n13 parent = myParent;\n14 cStatus = CStatus.IDLE;\n15 locked = false ;\n16 }\n17 ...\n18 }\nFigure 12.1 TheNode class: the constructors and \ufb01elds.\n1 public CombiningTree( int width) {\n2 Node[] nodes = new Node[width - 1];\n3 nodes[0] = new Node();\n4 for (int i = 1; i < nodes.length;", "doc_id": "fa3d94b7-1315-40bb-a269-37b1bb943b79", "embedding": null, "doc_hash": "4c8be2b31d47e72952211186a98816bd51314ce06598a45c126fcbda0aa85660", "extra_info": null, "node_info": {"start": 673952, "end": 677182}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "82d0704b-adaf-45e5-9108-c7f7b555ca0a", "3": "9ec77d71-1d72-4623-959b-4649866c15e4"}}, "__type__": "1"}, "9ec77d71-1d72-4623-959b-4649866c15e4": {"__data__": {"text": "int result;\n7 Node parent;\n8 public Node() {\n9 cStatus = CStatus.ROOT;\n10 locked = false ;\n11 }\n12 public Node(Node myParent) {\n13 parent = myParent;\n14 cStatus = CStatus.IDLE;\n15 locked = false ;\n16 }\n17 ...\n18 }\nFigure 12.1 TheNode class: the constructors and \ufb01elds.\n1 public CombiningTree( int width) {\n2 Node[] nodes = new Node[width - 1];\n3 nodes[0] = new Node();\n4 for (int i = 1; i < nodes.length; i++) {\n5 nodes[i] = new Node(nodes[(i-1)/2]);\n6 }\n7 leaf = new Node[(width + 1)/2];\n8 for (int i = 0; i < leaf.length; i++) {\n9 leaf[i] = nodes[nodes.length - i - 1];\n10 }\n11 }\nFigure 12.2 TheCombiningTree class: constructor.\n12.3 Software Combining 263\n(b)cstatus\nlocked\nD stopsresult\n0R\n1S\n0F\n1S\n0F\n0F\n0I\nA BC  stops DEB stopsA stops3\nThreads(a)result\nsecondlockedparent\ncstatusfirst\n0R\n0F\n0I\n0F\n0F\n0I\n0I\nA BC D E3\n(c)\n0R\n1S\n1F\n0S\n1F\n1F\n0F\nA B combines\nwith CC sets\nsecondreleases lock   DE  precombiningA waits\nfor BD updates result 4\n1 1112B sets\nsecond 1E missed\nprecombiningwaits for D (d)\n0R\n0S\n0F\n1S\n1F\n0F\n0F\nA B C waits\nfor resultD\nreturns  3 EA sets lock\ncombines\nwith BD decends 7\n1 1112B releases\nlock waitsfor result1D releases\nlock and E\ncontinues1A updates\nresult\n(e)\n0R\n0S\n0F\n0S\n0F\n0I\n0F\nA\nreturns 4 B\nreturns 5  C\nreturns 6 D\nreturned  3 E A decends\nwith value4 7\n1 112B decends\nwith value5E continues\nprecombining1A decends\nwith value 4\n5\n6\nFigure 12.3 The concurrent traversal of a width 8 combining tree by 5 threads. The structure is initialized with all nodes\nunlocked, the root node having the CStatus ROOT and all other nodes having the CStatus IDLE .\n264 Chapter 12 Counting, Sorting, and Distributed Coordination\n12 public int getAndIncrement() {\n13 Stack<Node> stack = new Stack<Node>();\n14 Node myLeaf = leaf[ThreadID.get()/2];\n15 Node node = myLeaf;\n16 // precombining phase\n17 while (node.precombine()) {\n18 node = node.parent;\n19 }\n20 Node stop = node;\n21 // combining phase\n22 node = myLeaf;\n23 int combined = 1;\n24 while (node != stop) {\n25 combined = node.combine(combined);\n26 stack.push(node);\n27 node = node.parent;\n28 }\n29 // operation phase\n30 int prior = stop.op(combined);\n31 // distribution phase\n32 while (!stack.empty()) {\n33 node = stack.pop();\n34 node.distribute(prior);\n35 }\n36 return prior;\n37 }\nFigure 12.4 TheCombiningTree class: the getAndIncrement ()method.\neach node. The precombine () method returns a Boolean indicating whether the\nthread was the \ufb01rst to arrive at the node. If so, the getAndIncrement () method\ncontinues moving up the tree. The stop variable is set to the last node visited,\nwhich is either the last node at which the thread arrived", "doc_id": "9ec77d71-1d72-4623-959b-4649866c15e4", "embedding": null, "doc_hash": "e64632fb43b5035c9a5fc49d54ecbdf0cc4db372538d3c77570174135597474b", "extra_info": null, "node_info": {"start": 677312, "end": 679922}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "fa3d94b7-1315-40bb-a269-37b1bb943b79", "3": "1227d61e-6885-49f5-8ed7-2ed4533d3631"}}, "__type__": "1"}, "1227d61e-6885-49f5-8ed7-2ed4533d3631": {"__data__": {"text": "// distribution phase\n32 while (!stack.empty()) {\n33 node = stack.pop();\n34 node.distribute(prior);\n35 }\n36 return prior;\n37 }\nFigure 12.4 TheCombiningTree class: the getAndIncrement ()method.\neach node. The precombine () method returns a Boolean indicating whether the\nthread was the \ufb01rst to arrive at the node. If so, the getAndIncrement () method\ncontinues moving up the tree. The stop variable is set to the last node visited,\nwhich is either the last node at which the thread arrived second, or the root.\nFor example, Part (a) of Fig. 12.3 shows a precombining phase example. Thread\nA, which is fastest, stops at the root, while Bstops in the middle-level node where\nit arrived after A, andCstops at the leaf where it arrived after B.\nFig. 12.5 shows the Node \u2019sprecombine () method. In Line 20, the thread waits\nuntil the locked \ufb01eld is false. In Line 21, it tests the combining status.\nIDLE\nThe thread sets the node\u2019s status to FIRST to indicate that it will return to look\nfor a value for combining. If it \ufb01nds such a value, it proceeds as the active thread,\nand the thread that provided that value is passive. The call then returns true,\ninstructing the thread to move up the tree.\nFIRST\nAn earlier thread has recently visited this node, and will return to look for a\nvalue to combine. The thread instructs the thread to stop moving up the tree (by\n12.3 Software Combining 265\n19 synchronized boolean precombine() {\n20 while (locked) wait();\n21 switch (cStatus) {\n22 case IDLE:\n23 cStatus = CStatus.FIRST;\n24 return true ;\n25 case FIRST:\n26 locked = true ;\n27 cStatus = CStatus.SECOND;\n28 return false ;\n29 case ROOT:\n30 return false ;\n31 default :\n32 throw new PanicException(\"unexpected Node state\" + cStatus);\n33 }\n34 }\nFigure 12.5 TheNode class: the precombining phase.\nreturning false ), and to start the next phase, computing the value to combine.\nBefore it returns, the thread places a long-term lock on the node (by setting\nlocked totrue) to prevent the earlier visiting thread from proceeding without\ncombining with the thread\u2019s value.\nROOT\nIf the thread has reached the root node, it instructs the thread to start the next\nphase.\nLine 31is a default case that is executed only if an unexpected status is encoun-\ntered.\nPragma 12.3.1. It is good programming practice always to provide an arm\nfor every possible enumeration value, even if we know it cannot happen. If\nwe are wrong, the program is easier to debug, and if we are right, the program\nmay later be changed even by someone who does not know as much as we\ndo. Always program defensively.\nIn the combining phase , (Fig. 12.4 , Lines 21\u201328), the thread revisits the nodes\nit visited in the precombining phase, combining its value with values left by other\nthreads. It stops when it arrives at the node stop where the precombining phase\nended. Later on, we traverse these nodes in reverse order, so as we go we push the\nnodes we visit onto a stack.\nTheNode class\u2019s combine () method, shown in Fig. 12.6 , adds any values left\nby a recently arrived passive process to the values combined so far. As before, the\nthread \ufb01rst waits until the locked \ufb01eld is false . It then sets a long-term lock on\nthe node, to ensure that late-arriving threads do not expect to combine with it.\nIf the status is SECOND , it adds the other thread\u2019s value to the accumulated", "doc_id": "1227d61e-6885-49f5-8ed7-2ed4533d3631", "embedding": null, "doc_hash": "2bd636ab8337d4e4e525d0f184875f94170fb55e4bdc82191a424744967a7f18", "extra_info": null, "node_info": {"start": 679845, "end": 683171}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9ec77d71-1d72-4623-959b-4649866c15e4", "3": "abb402c9-f17d-46b8-b6af-05498460f07a"}}, "__type__": "1"}, "abb402c9-f17d-46b8-b6af-05498460f07a": {"__data__": {"text": "these nodes in reverse order, so as we go we push the\nnodes we visit onto a stack.\nTheNode class\u2019s combine () method, shown in Fig. 12.6 , adds any values left\nby a recently arrived passive process to the values combined so far. As before, the\nthread \ufb01rst waits until the locked \ufb01eld is false . It then sets a long-term lock on\nthe node, to ensure that late-arriving threads do not expect to combine with it.\nIf the status is SECOND , it adds the other thread\u2019s value to the accumulated value,\notherwise it returns the value unchanged. In Part (c) of Fig. 12.3 , threadAstarts\n266 Chapter 12 Counting, Sorting, and Distributed Coordination\n35 synchronized int combine( int combined) {\n36 while (locked) wait();\n37 locked = true ;\n38 firstValue = combined;\n39 switch (cStatus) {\n40 case FIRST:\n41 return firstValue;\n42 case SECOND:\n43 return firstValue + secondValue;\n44 default :\n45 throw new PanicException(\"unexpected Node state \" + cStatus);\n46 }\n47 }\nFigure 12.6 The Node class: the combining phase. This method applies addition to\nFirstValue andSecondValue , but any other commutative operation would work just as well.\n48 synchronized int op(int combined) {\n49 switch (cStatus) {\n50 case ROOT:\n51 int prior = result;\n52 result += combined;\n53 return prior;\n54 case SECOND:\n55 secondValue = combined;\n56 locked = false ;\n57 notifyAll(); // wake up waiting threads\n58 while (cStatus != CStatus.RESULT) wait();\n59 locked = false ;\n60 notifyAll();\n61 cStatus = CStatus.IDLE;\n62 return result;\n63 default :\n64 throw new PanicException(\"unexpected Node state\");\n65 }\n66 }\nFigure 12.7 TheNode class: applying the operation.\nascending the tree in the combining phase. It reaches the second level node locked\nby threadBand waits. In Part (d), Breleases the lock on the second level node,\nandA, seeing that the node is in a SECOND combining state, locks the node and\nmoves to the root with the combined value 3, the sum of the FirstValue and\nSecondValue \ufb01elds written respectively by AandB.\nAt the start of the operation phase (Lines 29and 30), the thread has now\ncombined all method calls from lower-level nodes, and now examines the node\nwhere it stopped at the end of the precombining phase ( Fig. 12.7 ). If the node\nis the root, as in Part (d) of Fig. 12.3 , then the thread, in this case A, carries\nout the combined getAndIncrement () operations: it adds its accumulated value\n(3 in the example) to the result and returns the prior value. Otherwise, the\nthread unlocks the node, noti\ufb01es any blocked thread, deposits its value as the\n12.3 Software Combining 267\n67 synchronized void distribute( int prior) {\n68 switch (cStatus) {\n69 case FIRST:\n70 cStatus = CStatus.IDLE;\n71 locked = false ;\n72 break ;\n73 case SECOND:\n74 result = prior + firstValue;\n75 cStatus = CStatus.RESULT;\n76 break ;\n77 default :\n78 throw new PanicException(\"unexpected Node state\");\n79 }\n80 notifyAll();\n81 }\nFigure 12.8 TheNode class: the distribution phase.\nSecondValue , and waits for the other thread to return a result after propagating\nthe combined operations toward the root. For example, this is the sequence of\nactions taken by thread Bin Parts (c) and (d) of Fig. 12.3 .\nWhen the result arrives, Aenters the distribution phase , propagating the result\ndown the tree. In this phase", "doc_id": "abb402c9-f17d-46b8-b6af-05498460f07a", "embedding": null, "doc_hash": "1a935b1d42151414478812e3556d432640c1dd77035b0c8aefd37a3fc31cf168", "extra_info": null, "node_info": {"start": 683192, "end": 686458}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1227d61e-6885-49f5-8ed7-2ed4533d3631", "3": "27345450-8ee9-4cc4-91b9-9b0890494381"}}, "__type__": "1"}, "27345450-8ee9-4cc4-91b9-9b0890494381": {"__data__": {"text": "cStatus = CStatus.RESULT;\n76 break ;\n77 default :\n78 throw new PanicException(\"unexpected Node state\");\n79 }\n80 notifyAll();\n81 }\nFigure 12.8 TheNode class: the distribution phase.\nSecondValue , and waits for the other thread to return a result after propagating\nthe combined operations toward the root. For example, this is the sequence of\nactions taken by thread Bin Parts (c) and (d) of Fig. 12.3 .\nWhen the result arrives, Aenters the distribution phase , propagating the result\ndown the tree. In this phase (Lines 31\u201336), the thread moves down the tree,\nreleasing locks, and informing passive partners of the values they should report to\ntheir own passive partners, or to the caller (at the lowest level). The distribute\nmethod is shown in Fig. 12.8 . If the state of the node is FIRST , no thread com-\nbines with the distributing thread, and it can reset the node to its initial state by\nreleasing the lock and setting the state to IDLE . If, on the other hand, the state\nisSECOND , the distributing thread updates the result to be the sum of the prior\nvalue brought from higher up the tree, and the FIRST value. This re\ufb02ects a situ-\nation in which the active thread at the node managed to perform its increment\nbefore the passive one. The passive thread waiting to get a value reads the result\nonce the distributing thread sets the status to RESULT . For example, in Part (e)\nofFig. 12.3 , the active thread Aexecutes its distribution phase in the middle\nlevel node, setting the result to 5, changing the state to RESULT , and descending\ndown to the leaf, returning the value 4 as its output. The passive thread Bawakes\nand sees that the middle-level node\u2019s state has changed, and reads result 5.\n12.3.2 An Extended Example\nFig. 12.3 describes the various phases of a CombiningTree execution. There are\n\ufb01ve threads labeled AthroughE. Each node has six \ufb01elds, as shown in Fig. 12.1 .\nInitially, all nodes are unlocked and all but the root are in an IDLE combin-\ning state. The counter value in the initial state in Part (a) is 3, the result of an\nearlier computation. In Part (a), to perform a getAndIncrement (), threadsA\nandBstart the precombining phase. Aascends the tree, changing the nodes it\nvisits from IDLE toFIRST , indicating that it will be the active thread in combining\nthe values up the tree. Thread Bis the active thread at its leaf node, but has not\n268 Chapter 12 Counting, Sorting, and Distributed Coordination\nyet arrived at the second-level node shared with A. In Part (b), Barrives at the\nsecond-level node and stops, changing it from FIRST toSECOND , indicating that\nit will collect its combined values and wait here for Ato proceed with them to the\nroot.Blocks the node (changing the locked \ufb01eld from false totrue), preventing\nAfrom proceeding with the combining phase without B\u2019s combined value. But\nBhas not combined the values. Before it does so, Cstarts precombining, arrives\nat the leaf node, stops, and changes its state to SECOND . It also locks the node to\npreventBfrom ascending without its input to the combining phase. Similarly,\nDstarts precombining and successfully reaches the root node. Neither AnorD\nchanges the root node state, and in fact it never changes. They simply mark it as\nthe node where they stopped precombining. In Part (c) Astarts up the tree in\nthe combining phase. It locks the leaf so that any later thread will not be able to\nproceed in its precombining phase, and will wait until Acompletes its combin-\ning and distribution phases. It reaches the second-level", "doc_id": "27345450-8ee9-4cc4-91b9-9b0890494381", "embedding": null, "doc_hash": "16d8340cf0c418db86a3c0664727f4cf460e6815577b9bb4cd9a4c984bb3cd3e", "extra_info": null, "node_info": {"start": 686421, "end": 689943}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "abb402c9-f17d-46b8-b6af-05498460f07a", "3": "0a5a6a44-63c2-4161-8a78-3e47b255f0a2"}}, "__type__": "1"}, "0a5a6a44-63c2-4161-8a78-3e47b255f0a2": {"__data__": {"text": "changes its state to SECOND . It also locks the node to\npreventBfrom ascending without its input to the combining phase. Similarly,\nDstarts precombining and successfully reaches the root node. Neither AnorD\nchanges the root node state, and in fact it never changes. They simply mark it as\nthe node where they stopped precombining. In Part (c) Astarts up the tree in\nthe combining phase. It locks the leaf so that any later thread will not be able to\nproceed in its precombining phase, and will wait until Acompletes its combin-\ning and distribution phases. It reaches the second-level node, locked by B, and\nwaits. In the meantime, Cstarts combining, but since it stopped at the leaf node,\nit executes the op() method on this node, setting SecondValue to 1 and then\nreleasing the lock. When Bstarts its combining phase, the leaf node is unlocked\nand marked SECOND , soBwrites 1 to FirstValue and ascends to the second-\nlevel node with a combined value of 2, the result of adding the FirstValue and\nSecondValue \ufb01elds.\nWhen it reaches the second level node, the one at which it stopped in the\nprecombining phase, it calls the op() method on this node, setting SecondValue\nto 2.Amust wait until it releases the lock. Meanwhile, in the right-hand side of\nthe tree,Dexecutes its combining phase, locking nodes as it ascends. Because\nit meets no other threads with which to combine, it reads 3 in the result \ufb01eld\nin the root and updates it to 4. Thread Ethen starts precombining, but is late\nin meetingD. It cannot continue precombining as long as Dlocks the second-\nlevel node. In Part (d), Breleases the lock on the second-level node, and A,\nseeing that the node is in state SECOND , locks the node and moves to the root\nwith the combined value 3, the sum of the FirstValue andSecondValue \ufb01elds\nwritten, respectively, by AandB.Ais delayed while Dcompletes updating the\nroot. OnceDis done,Areads 4 in the root\u2019s result \ufb01eld and updates it to 7.\nDdescends the tree (by popping its local Stack ), releasing the locks and return-\ning the value 3 that it originally read in the root\u2019s result \ufb01eld.Enow continues\nits ascent in the precombining phase. Finally, in Part (e), Aexecutes its distribu-\ntion phase. It returns to the middle-level node, setting result to 5, changing the\nstate to RESULT , and descending to the leaf, returning the value 4 as its output.\nBawakens and sees the state of the middle-level node has changed, reads 5 as the\nresult , and descends to its leaf where it sets the result \ufb01eld to 6 and the state\ntoRESULT .Bthen returns 5 as its output. Finally, Cawakens and observes that\nthe leaf node state has changed, reads 6 as the result , which it returns as its\noutput value. Threads AthroughDreturn values 3 to 6 which \ufb01t the root\u2019s\nresult \ufb01eld value of 7. The linearization order of the getAndIncrement ()\nmethod calls by the different threads is determined by their order in the tree\nduring the precombining phase.\n12.4 Quiescently Consistent Pools and Counters 269\n12.3.3 Performance and Robustness\nLike all the algorithms described in this chapter, CombiningTree throughput\ndepends in complex ways on the characteristics both of the application and of\nthe underlying architecture. Nevertheless, it is worthwhile to review, in qualita-\ntive terms, some experimental results from the literature. Readers interested in\ndetailed experimental results (mostly for obsolete architectures) may consult the\nchapter notes.\nAs a thought experiment, a CombiningTree should provide high through-\nput under ideal circumstances when each thread can", "doc_id": "0a5a6a44-63c2-4161-8a78-3e47b255f0a2", "embedding": null, "doc_hash": "5f4b58af7ad0095aa302463dea4c49c9b61d5f48bcd5114aceac55426788885b", "extra_info": null, "node_info": {"start": 689889, "end": 693440}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "27345450-8ee9-4cc4-91b9-9b0890494381", "3": "5b42a20a-7a10-49aa-90a3-45c05d397e2f"}}, "__type__": "1"}, "5b42a20a-7a10-49aa-90a3-45c05d397e2f": {"__data__": {"text": "Quiescently Consistent Pools and Counters 269\n12.3.3 Performance and Robustness\nLike all the algorithms described in this chapter, CombiningTree throughput\ndepends in complex ways on the characteristics both of the application and of\nthe underlying architecture. Nevertheless, it is worthwhile to review, in qualita-\ntive terms, some experimental results from the literature. Readers interested in\ndetailed experimental results (mostly for obsolete architectures) may consult the\nchapter notes.\nAs a thought experiment, a CombiningTree should provide high through-\nput under ideal circumstances when each thread can combine its increment with\nanother\u2019s. But it may provide poor throughput under worst-case circumstances,\nwhere many threads arrive late at a locked node, missing the chance to combine,\nand are forced to wait for the earlier request to ascend and descend the tree.\nIn practice, experimental evidence supports this informal analysis. The higher\nthe contention, the greater the observed rate of combining, and the greater the\nobserved speed-up. Worse is better. Combining trees are less attractive when con-\ncurrency is low. The combining rate decreases rapidly as the arrival rate of incre-\nment requests is reduced. Throughput is sensitive to the arrival rate of requests.\nBecause combining increases throughput, and failure to combine does not,\nit makes sense for a request arriving at a node to wait for a reasonable dura-\ntion for another thread to arrive with a increment with which to combine. Not\nsurprisingly, it makes sense to wait for a short time when the contention is\nlow, and longer when contention is high. When contention is suf\ufb01ciently high,\nunbounded waiting works very well.\nAn algorithm is robust if it performs well in the presence of large \ufb02uctuations\nin request arrival times. The literature suggests that the CombiningTree algo-\nrithm with a \ufb01xed waiting time is not robust, because high variance in request\narrival rates seems to reduce the combining rate.\n12.4 Quiescently Consistent Pools and Counters\nFirst shalt thou take out the Holy Pin. Then shalt thou count to three, no more,\nno less. Three shall be the number thou shalt count, and the number of the count-\ning shall be three :::. Once the number three, being the third number, be reached,\nthen lobbest thou thy Holy Hand Grenade of Antioch towards thy foe, who, being\nnaughty in my sight, shall snuff it.\nFrom Monty Python and the Holy Grail .\nNot all applications require linearizable counting. Indeed, counter-based Pool\nimplementations require only quiescently consistent1counting: all that matters\nis that the counters produce no duplicates and no omissions. It is enough that\n1See Chapter 3 for a detailed de\ufb01nition of quiescent consistency.\n270 Chapter 12 Counting, Sorting, and Distributed Coordination\nfor every item placed by a put() in an array entry, another thread eventually\nexecutes a get() that accesses that entry, eventually matching put() and get()\ncalls. (Wrap-around may still cause multiple put() calls or get() calls to compete\nfor the same array entry.)\n12.5 Counting Networks\nStudents of Tango know that the partners must be tightly coordinated: if they do\nnot move together, the dance does not work, no matter how skilled the dancers\nmay be as individuals. In the same way, combining trees must be tightly coordi-\nnated: if requests do not arrive together, the algorithm does not work ef\ufb01ciently,\nno matter how fast the individual processes.\nIn this chapter, we consider counting networks , which look less like Tango and\nmore like a Rave: each participant moves at its own pace, but collectively the\ncounter delivers a quiescently consistent set", "doc_id": "5b42a20a-7a10-49aa-90a3-45c05d397e2f", "embedding": null, "doc_hash": "a44d20788d9654906b07b6ff7f31f6b7fd927a301fff6adfb9df285fce9b54c5", "extra_info": null, "node_info": {"start": 693394, "end": 697065}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0a5a6a44-63c2-4161-8a78-3e47b255f0a2", "3": "2290e198-9fb4-4b7a-9355-8d32c6ceb6c0"}}, "__type__": "1"}, "2290e198-9fb4-4b7a-9355-8d32c6ceb6c0": {"__data__": {"text": "of Tango know that the partners must be tightly coordinated: if they do\nnot move together, the dance does not work, no matter how skilled the dancers\nmay be as individuals. In the same way, combining trees must be tightly coordi-\nnated: if requests do not arrive together, the algorithm does not work ef\ufb01ciently,\nno matter how fast the individual processes.\nIn this chapter, we consider counting networks , which look less like Tango and\nmore like a Rave: each participant moves at its own pace, but collectively the\ncounter delivers a quiescently consistent set of indexes with high throughput.\nLet us imagine that we replace the combining tree\u2019s single counter with mul-\ntiple counters, each of which distributes a subset of indexes (see Fig. 12.9). We\nallocatewcounters (in the \ufb01gure w= 4), each of which distributes a set of unique\nindexes modulo w(in the \ufb01gure, for example, the second counter distributes 2,\n6, 10,:::i\u0001w+ 2 for increasing i). The challenge is how to distribute the threads\namong the counters so that there are no duplications or omissions, and how to\ndo so in a distributed and loosely coordinated style.\n12.5.1 Networks That Count\nAbalancer is a simple switch with two input wires and two output wires, called\nthetopandbottom wires (or sometimes the north andsouth wires). T okens arrive\non the balancer\u2019s input wires at arbitrary times, and emerge on their output wires,\nat some later time. A balancer can be viewed as a toggle: given a stream of input\ntokens, it sends one token to the top output wire, and the next to the bottom,\nand so on, effectively balancing the number of tokens between the two wires\n(see Fig. 12.10). More precisely, a balancer has two states: upand down . If\nw shared\ncounters n threadsthreads\nreturn indexes...\n......\n...\n43215\n6 width w\ncountingnetwork\ni * w14i * w12i * w11\nFigure 12.9 A quiescently consistent shared counter based on w= 4 counters preceded by a counting net-\nwork. Threads traverse the counting network to choose which counters to access.\n12.5 Counting Networks 271\nx0\nx1y0\ny1balancer\nFigure 12.10 A balancer. T okens arrive at arbitrary times on arbitrary input lines and are\nredirected to ensure that when all tokens have exited the balancer, there is at most one\nmore token on the top wire than on the bottom one.\nthe state is up, the next token exits on the top wire, otherwise it exits on the\nbottom wire.\nWe usex0andx1to denote the number of tokens that respectively arrive on a\nbalancer\u2019s top and bottom input wires, and y0andy1to denote the number that\nexit on the top and bottom output wires. A balancer never creates tokens: at all\ntimes.\nx0+x1>y0+y1:\nA balancer is said to be quiescent if every token that arrived on an input wire has\nemerged on an output wire:\nx0+x1=y0+y1:\nAbalancing network is constructed by connecting some balancers\u2019 output wires\nto other balancers\u2019 input wires. A balancing network of width whas input\nwiresx0,x1,:::,xw\u00001(not connected to output wires of balancers), and woutput\nwiresy0,y1,:::,yw\u00001(similarly unconnected). The balancing network\u2019s depth is\nthe maximum number of balancers one can traverse starting from any input wire.\nWe consider only balancing networks of \ufb01nite depth (meaning the wires do not\nform a loop). Like balancers, balancing networks", "doc_id": "2290e198-9fb4-4b7a-9355-8d32c6ceb6c0", "embedding": null, "doc_hash": "61c2c78e1c57daab06eb3ceaa7d4ea8634490d4e3d714aa085e5cda8e6abeb22", "extra_info": null, "node_info": {"start": 697131, "end": 700397}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5b42a20a-7a10-49aa-90a3-45c05d397e2f", "3": "7f5ffab3-4e83-4e60-bb03-52dee15f52a3"}}, "__type__": "1"}, "7f5ffab3-4e83-4e60-bb03-52dee15f52a3": {"__data__": {"text": "network is constructed by connecting some balancers\u2019 output wires\nto other balancers\u2019 input wires. A balancing network of width whas input\nwiresx0,x1,:::,xw\u00001(not connected to output wires of balancers), and woutput\nwiresy0,y1,:::,yw\u00001(similarly unconnected). The balancing network\u2019s depth is\nthe maximum number of balancers one can traverse starting from any input wire.\nWe consider only balancing networks of \ufb01nite depth (meaning the wires do not\nform a loop). Like balancers, balancing networks do not create tokens:\nX\nxi>X\nyi:\n(We usually drop indexes from summations when we sum over every element in\na sequence.) A balancing network is quiescent if every token that arrived on an\ninput wire has emerged on an output wire:\nX\nxi=X\nyi:\nSo far, we have described balancing networks as if they were switches in a net-\nwork. On a shared-memory multiprocessor, however, a balancing network can\nbe implemented as an object in memory. Each balancer is an object, whose wires\nare references from one balancer to another. Each thread repeatedly traverses the\nobject, starting on some input wire, and emerging at some output wire, effec-\ntively shepherding a token through the network.\nSome balancing networks have interesting properties. The network shown in\nFig. 12.11 has four input wires and four output wires. Initially, all balancers are\nup. We can check for ourselves that if any number of tokens enter the network, in\nany order, on any set of input wires, then they emerge in a regular pattern on the\noutput wires. Informally, no matter how token arrivals are distributed among the\n272 Chapter 12 Counting, Sorting, and Distributed Coordination\n3515\n26\n434\n2\n61x0\nx1\nx2\nx3y0\ny1\ny2\ny3342\n32\n41\n4321\n5\n155\n66\n6\nFigure 12.11 A sequential execution of a B ITONIC [4] counting network. Each vertical line\nrepresents a balancer, and each balancer\u2019s two input and output wires are the horizontal lines\nit connects to at the dots. In this sequential execution, tokens pass through the network, one\ncompletely after the other in the order speci\ufb01ed by the numbers on the tokens. We track\nevery token as it passes through the balancers on the way to an output wire. For example,\ntoken number 3 enters on wire 2, goes down to wire 3, and ends up on wire 2. Notice how\nthe step property is maintained in every balancer, and also in the network as a whole.\ninput wires, the output distribution is balanced across the output wires, where\nthe top output wires are \ufb01lled \ufb01rst. If the number of tokens nis a multiple of four\n(the network width), then the same number of tokens emerges from each wire.\nIf there is one excess token, it emerges on output wire 0, if there are two, they\nemerge on output wires 0 and 1, and so on. In general,\nif\nn=X\nxi\nthen\nyi= (n=w) + (imodw):\nWe call this property the step property .\nAny balancing network that satis\ufb01es the step property is called a counting net-\nwork , because it can easily be adapted to count the number of tokens that have\ntraversed the network. Counting is done, as we described earlier in Fig. 12.9, by\nadding a local counter to each output wire i, so that tokens emerging on that wire\nare assigned consecutive numbers i,i+w,:::,i+ (yi\u00001)w.\nThe step property can be de\ufb01ned in a number of ways which we use inter-\nchangeably.\nLemma 12.5.1.", "doc_id": "7f5ffab3-4e83-4e60-bb03-52dee15f52a3", "embedding": null, "doc_hash": "a60ab734ab66d3bc158139e4fc277810cc4c30325ee3d43fe45bfa79c64b1726", "extra_info": null, "node_info": {"start": 700439, "end": 703716}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2290e198-9fb4-4b7a-9355-8d32c6ceb6c0", "3": "877e08cf-023b-48a9-a38c-4e9dd2fe855f"}}, "__type__": "1"}, "877e08cf-023b-48a9-a38c-4e9dd2fe855f": {"__data__": {"text": "balancing network that satis\ufb01es the step property is called a counting net-\nwork , because it can easily be adapted to count the number of tokens that have\ntraversed the network. Counting is done, as we described earlier in Fig. 12.9, by\nadding a local counter to each output wire i, so that tokens emerging on that wire\nare assigned consecutive numbers i,i+w,:::,i+ (yi\u00001)w.\nThe step property can be de\ufb01ned in a number of ways which we use inter-\nchangeably.\nLemma 12.5.1. Ify0,:::,yw\u00001is a sequence of nonnegative integers, the following\nstatements are all equivalent:\n1.For anyi<j , 06yi\u0000yj61.\n2.Eitheryi=yjfor alli,j, or there exists some csuch that for any i<c and\nj>c,yi\u0000yj= 1.\n3.Ifm=Pyi,yi=\u0006m\u0000i\nw\u0007.\n12.5 Counting Networks 273\n12.5.2 The Bitonic Counting Network\nIn this section we describe how to generalize the counting network of Fig. 12.11\nto a counting network whose width is any power of 2. We give an inductive con-\nstruction.\nWhen describing counting networks, we do not care about when tokens arrive,\nwe care only that when the network is quiescent, the number of tokens exiting on\nthe output wires satis\ufb01es the step property. De\ufb01ne a width wsequence of inputs\nor outputsx=x0,:::,xw\u00001to be a collection of tokens, partitioned into wsub-\nsetsxi. Thexiare the input tokens that arrive or leave on wire i.\nWe de\ufb01ne the width-2 kbalancing network M ERGER [2k] as follows. It has two\ninput sequences of width k,xandx0, and a single output sequence yof width\n2k. In any quiescent state, if xandx0both have the step property, then so does\ny. The M ERGER [2k] network is de\ufb01ned inductively (see Fig. 12.12). When kis\nequal to 1, the M ERGER [2k] network is a single balancer. For k>1, we construct\nthe M ERGER [2k] network with input sequences xandx0from two M ERGER [k]\nnetworks and kbalancers. Using a M ERGER [k] network, we merge the even sub-\nsequencex0,x2,:::,xk\u00002ofxwith the odd subsequence x0\n1,x0\n3,:::,x0\nk\u00001ofx0\n(that is, the sequence x0,:::,xk\u00002,x0\n1,:::,x0\nk\u00001is the input to the M ERGER [k]\nnetwork), while with a second M ERGER [k] network we merge the odd subse-\nquence ofxwith the even subsequence of x0. We call the outputs of these two\nMERGER [k] networkszandz0. The \ufb01nal stage of the network combines zand\nz0by sending each pair of wires ziandz0\niinto a balancer whose outputs yield y2i\nandy2i+1.\nThe M ERGER [2k] network consists of log 2 klayers ofkbalancers each. It\nprovides the step property for its outputs only when its two input sequences also\nhave the step property, which we ensure by \ufb01ltering the inputs through smaller\nbalancing", "doc_id": "877e08cf-023b-48a9-a38c-4e9dd2fe855f", "embedding": null, "doc_hash": "ba0dcac86b850c37b59e74441d0bfca8da4cc64b06882625f57f29cde9e775f3", "extra_info": null, "node_info": {"start": 703752, "end": 706319}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7f5ffab3-4e83-4e60-bb03-52dee15f52a3", "3": "ca80b4df-50a5-4571-818c-46b04778d6a9"}}, "__type__": "1"}, "ca80b4df-50a5-4571-818c-46b04778d6a9": {"__data__": {"text": "ofxwith the even subsequence of x0. We call the outputs of these two\nMERGER [k] networkszandz0. The \ufb01nal stage of the network combines zand\nz0by sending each pair of wires ziandz0\niinto a balancer whose outputs yield y2i\nandy2i+1.\nThe M ERGER [2k] network consists of log 2 klayers ofkbalancers each. It\nprovides the step property for its outputs only when its two input sequences also\nhave the step property, which we ensure by \ufb01ltering the inputs through smaller\nbalancing networks.\nx0\nx1\nx2\nx3\nx4\nx5\nx6\nx7x0\nx1\nx2\nx3\nx4\nx5\nx6\nx7y0\ny1\ny2\ny3\ny4\ny5\ny6\ny7y0\ny1\ny2\ny3\ny4\ny5\ny6\ny7Merger[4]Merger[4]\nFigure 12.12 On the left-hand side we see the logical structure of a M ERGER [8] network,\ninto which feed two B ITONIC [4] networks, as depicted in Fig. 12.11. The gray M ERGER [4] net-\nwork has as inputs the even wires coming out of the top B ITONIC [4] network, and the odd\nones from the lower B ITONIC [4] network. In the lower M ERGER [4] the situation is reversed.\nOnce the wires exit the two M ERGER [4] networks, each pair of identically numbered wires is\ncombined by a balancer. On the right-hand side we see the physical layout of a M ERGER [8] net-\nwork. The different balancers are color coded to match the logical structure in the left-hand\n\ufb01gure.\n274 Chapter 12 Counting, Sorting, and Distributed Coordination\nBitonic[ k]\nMerger[2 k]\nBitonic[ k]\nFigure 12.13 The recursive structure of a B ITONIC [2k] Counting Network. T wo B ITONIC [k]\ncounting networks feed into a M ERGER [2k] balancing network.\nThe B ITONIC [2k] network is constructed by passing the outputs from two\nBITONIC [k] networks into a M ERGER [2k] network, where the induction is\ngrounded in the B ITONIC [2] network consisting of a single balancer, as depicted\nin Fig. 12.13. This construction gives us a network consisting of\u0010\nlog 2k+1\n2\u0011\nlayers\neach consisting of kbalancers.\nA Software Bitonic Counting Network\nSo far, we have described counting networks as if they were switches in a net-\nwork. On a shared-memory multiprocessor however, a balancing network can be\nimplemented as an object in memory. Each balancer is an object, whose wires\nare references from one balancer to another. Each thread repeatedly traverses the\nobject, starting on some input wire and emerging at some output wire, effectively\nshepherding a token through the network. Here, we show how to implement a\nBITONIC [2] network as a shared-memory data structure.\nTheBalancer class (Fig. 12.14) has a single Boolean \ufb01eld: toggle . The syn-\nchronized traverse () method complements the toggle \ufb01eld and returns as out-\nput wire, either 0 or 1. The Balancer class\u2019s traverse () method does not need\nan argument because the wire on which a token exits a balancer does not depend\non the wire on which it enters.\nTheMerger class (Fig. 12.15) has three \ufb01elds: the width \ufb01eld must be a power\nof 2, half [] is a two-element array of half-width Merger objects (empty if the\nnetwork has width 2), and layer [] is an array", "doc_id": "ca80b4df-50a5-4571-818c-46b04778d6a9", "embedding": null, "doc_hash": "d3179b5f5f4771087995e462441ba086591444055977572d86b22ccf83df8b10", "extra_info": null, "node_info": {"start": 706316, "end": 709276}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "877e08cf-023b-48a9-a38c-4e9dd2fe855f", "3": "741d0bb6-6711-46b2-8469-ee30d206318b"}}, "__type__": "1"}, "741d0bb6-6711-46b2-8469-ee30d206318b": {"__data__": {"text": "toggle . The syn-\nchronized traverse () method complements the toggle \ufb01eld and returns as out-\nput wire, either 0 or 1. The Balancer class\u2019s traverse () method does not need\nan argument because the wire on which a token exits a balancer does not depend\non the wire on which it enters.\nTheMerger class (Fig. 12.15) has three \ufb01elds: the width \ufb01eld must be a power\nof 2, half [] is a two-element array of half-width Merger objects (empty if the\nnetwork has width 2), and layer [] is an array of width/2 balancers implement-\ning the \ufb01nal network layer.\nThe class provides a traverse (i) method, where iis the wire on which the\ntoken enters. (For merger networks, unlike balancers, a token\u2019s path depends on\nits input wire.) If the token entered on the lower width /2 wires, then it passes\nthrough half [0], otherwise half [1]. No matter which half-width merger net-\nwork it traverses, a balancer that emerges on wire iis fed to the ithbalancer at\nlayer [i].\nThe Bitonic class (Fig. 12.16) also has three \ufb01elds: width is the width\n(a power of 2), half [] is a two-element array of half-width Bitonic [] objects,\n12.5 Counting Networks 275\n1public class Balancer {\n2 boolean toggle = true ;\n3 public synchronized int traverse() {\n4 try {\n5 if(toggle) {\n6 return 0;\n7 }else {\n8 return 1;\n9 }\n10 }finally {\n11 toggle = !toggle;\n12 }\n13 }\n14 }\nFigure 12.14 TheBalancer class: a synchronized implementation.\n1public class Merger {\n2 Merger[] half; // two half-width merger networks\n3 Balancer[] layer; // final layer\n4 final int width;\n5 public Merger( int myWidth) {\n6 width = myWidth;\n7 layer = new Balancer[width / 2];\n8 for (int i = 0; i < width / 2; i++) {\n9 layer[i] = new Balancer();\n10 }\n11 if(width > 2) {\n12 half = new Merger[]{ new Merger(width/2), new Merger(width/2)};\n13 }\n14 }\n15 public int traverse( int input) {\n16 int output = 0;\n17 if(input < width / 2) {\n18 output = half[input % 2].traverse(input / 2);\n19 }else {\n20 output = half[1 - (input % 2)].traverse(input / 2);\n21 return (2*output) + layer[output].traverse();\n22 }\n23 }\nFigure 12.15 TheMerger class.\nandmerger is a full width Merger network width . If the network has width 2,\nthehalf [] array is uninitialized. Otherwise, each element of half [] is initialized\nto a half-width Bitonic [] network.\nThe class provides a traverse (i) method. If the token entered on the lower\nwidth /2 wires, then it passes through half [0], otherwise half [1]. A token that\n276 Chapter 12 Counting, Sorting, and Distributed Coordination\n1public class Bitonic {\n2 Bitonic[] half; // two half-width bitonic networks\n3 Merger merger; // final merger layer\n4 final int width; // network width\n5 public Bitonic( int myWidth) {\n6 width = myWidth;\n7 merger = new Merger(width);\n8 if(width > 2) {\n9 half = new Bitonic[]{ new Bitonic(width/2), new Bitonic(width/2)};\n10 }\n11 }\n12 public int traverse( int input) {\n13 int output = 0;\n14 if(width > 2) {\n15 output = half[input / (width / 2)].traverse(input / 2);\n16 }\n17 return merger.traverse((input >=", "doc_id": "741d0bb6-6711-46b2-8469-ee30d206318b", "embedding": null, "doc_hash": "be1b11477f20e929d576133a3a1f01f79f1b755ba25c7f65b17304d7d33230a5", "extra_info": null, "node_info": {"start": 709276, "end": 712269}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ca80b4df-50a5-4571-818c-46b04778d6a9", "3": "93e549c0-5e21-4536-9ef6-470c45da509b"}}, "__type__": "1"}, "93e549c0-5e21-4536-9ef6-470c45da509b": {"__data__": {"text": "merger; // final merger layer\n4 final int width; // network width\n5 public Bitonic( int myWidth) {\n6 width = myWidth;\n7 merger = new Merger(width);\n8 if(width > 2) {\n9 half = new Bitonic[]{ new Bitonic(width/2), new Bitonic(width/2)};\n10 }\n11 }\n12 public int traverse( int input) {\n13 int output = 0;\n14 if(width > 2) {\n15 output = half[input / (width / 2)].traverse(input / 2);\n16 }\n17 return merger.traverse((input >= (size / 2) ? (size / 2) : 0) + output);\n18 }\n19 }\nFigure 12.16 TheBitonic []class.\nemerges from the half-merger subnetwork on wire ithen traverses the \ufb01nal\nmerger network from input wire i.\nNotice that this class uses a simple synchronized Balancer implementation,\nbut that if the Balancer implementation were lock-free (or wait-free) the net-\nwork implementation as a whole would be lock-free (or wait-free).\nProof of Correctness\nWe now show that B ITONIC [w] is a counting network. The proof proceeds as\na progression of arguments about the token sequences passing through the net-\nwork. Before examining the network itself, here are some simple lemmas about\nsequences with the step property.\nLemma 12.5.2. If a sequence has the step property, then so do all its subse-\nquences.\nLemma 12.5.3. Ifx0,:::,xk\u00001has the step property, then its even and odd sub-\nsequences satisfy:\nk=2\u00001X\ni=0x2i=&k\u00001X\ni=0xi=2'\nandk=2\u00001X\ni=0x2i+1=$k\u00001X\ni=0xi=2%\n:\nProof: Eitherx2i=x2i+1for 06i<k= 2, or by Lemma 12.5.1, there exists a\nuniquejsuch thatx2j=x2j+1+ 1 andx2i=x2i+1for alli6=j, 06i<k= 2. In\nthe \ufb01rst case,Px2i=Px2i+1=Pxi=2, and in the second casePx2i=\u0006Pxi=2\u0007\nandPx2i+1=\u0004Pxi=2\u0005. 2\n12.5 Counting Networks 277\nLemma 12.5.4. Letx0,:::,xk\u00001andy0,:::,yk\u00001be arbitrary sequences having\nthe step property. IfPxi=Pyi, thenxi=yifor all 0 6i<k .\nProof: Letm=Pxi=Pyi. By Lemma 12.5.1, xi=yi=\u0006m\u0000i\nk\u0007. 2\nLemma 12.5.5. Letx0,:::,xk\u00001andy0,:::,yk\u00001be arbitrary sequences having\nthe step property. IfPxi=Pyi+ 1, then there exists a unique j, 06j<k , such\nthatxj=yj+ 1, andxi=yifori6=j, 06i<k .\nProof: Letm=Pxi=Pyi+ 1. By Lemma 12.5.1, xi=\u0006m\u00001\nk\u0007andyi=\u0006m\u00001\u0000i\nk\u0007.\nThese two terms agree for all i, 06i<k , except for the unique isuch thati=\nm\u00001 (modk). 2\nWe now show that the M ERGER [w] network preserves the step property.\nLemma 12.5.6. If M ERGER [2k] is quiescent, and its inputs x0,:::,xk\u00001and\nx0\n0,:::,x0\nk\u00001both have the step property, then its outputs y0,:::,y2k\u00001also", "doc_id": "93e549c0-5e21-4536-9ef6-470c45da509b", "embedding": null, "doc_hash": "ff27cd269f1d88c000988d444cc7d8124bb89e577d3655164872bacf76ff2ae8", "extra_info": null, "node_info": {"start": 712318, "end": 714683}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "741d0bb6-6711-46b2-8469-ee30d206318b", "3": "35144928-0a0c-4ea1-971b-abe3e790e50b"}}, "__type__": "1"}, "35144928-0a0c-4ea1-971b-abe3e790e50b": {"__data__": {"text": "xi=\u0006m\u00001\nk\u0007andyi=\u0006m\u00001\u0000i\nk\u0007.\nThese two terms agree for all i, 06i<k , except for the unique isuch thati=\nm\u00001 (modk). 2\nWe now show that the M ERGER [w] network preserves the step property.\nLemma 12.5.6. If M ERGER [2k] is quiescent, and its inputs x0,:::,xk\u00001and\nx0\n0,:::,x0\nk\u00001both have the step property, then its outputs y0,:::,y2k\u00001also have\nthe step property.\nProof: We argue by induction on log k. It may be worthwhile to consult Fig. 12.17\nwhich shows an example of the proof structure for a M ERGER [8] network.\n1210\nyx\n13z 11\nb\nb\nb\nbeven\nodd\nodd\nevenMerger[4]\nMerger[4]\nz\u2032x\u2032\nFigure 12.17 The inductive proof that a M ERGER [8] network correctly merges two width 4 sequences x\nandx\u2019 that have the step property into a single width 8 sequence ythat has the step property. The\nodd and even width 2 subsequences of xandx\u2019 all have the step property. Moreover, the difference\nin the number of tokens between the even sequence from one and the odd sequence from the other\nis at most 1 (in this example, 11 and 12 tokens, respectively). It follows from the induction hypothe-\nsis that the outputs zandz\u2019 of the two M ERGER [4] networks have the step property, with at most 1\nextra token in one of them. This extra token must fall on a speci\ufb01c numbered wire (wire 3 in this\ncase) leading into the same balancer. In this \ufb01gure, these tokens are darkened. They are passed to the\nsouthern-most balancer, and the extra token is pushed north, ensuring the \ufb01nal output has the step\nproperty.\n278 Chapter 12 Counting, Sorting, and Distributed Coordination\nIf 2k= 2, M ERGER [2k] is just a balancer, and its outputs are guaranteed to\nhave the step property by the de\ufb01nition of a balancer.\nIf 2k>2, letz0,:::,zk\u00001be the outputs of the \ufb01rst M ERGER [k] subnetwork\nwhich merges the even subsequence of xwith the odd subsequence of x0. Let\nz0\n0,:::,z0\nk\u00001be the outputs of the second M ERGER [k] subnetwork. Since xand\nx0have the step property by assumption, so do their even and odd subsequences\n(Lemma 12.5.2), and hence so do zandz0(induction hypothesis). Furthermore,Pzi=\u0006Pxi=2\u0007+\u0004Px0\ni=2\u0005andPz0\ni=\u0004Pxi=2\u0005+\u0006Px0\ni=2\u0007(Lemma 12.5.3).\nA straightforward case analysis shows thatPziandPz0\nican differ by at most 1.\nWe claim that 0 6yi\u0000yj61 for anyi<j . IfPzi=Pz0\ni, then Lemma 12.5.4\nimplies that zi=z0\nifor 06i<k= 2. After the \ufb01nal layer of balancers,\nyi\u0000yj=zbi=2c\u0000zbj=2c,\nand the result follows because zhas the step property.\nSimilarly, ifPziandPz0\nidiffer by one, Lemma 12.5.5 implies that zi=z0\ni\nfor 06i<k= 2, except for a unique `such thatz`andz0\n`differ by one. Let\nmax(z`,z0\n`) =x+ 1 and min( z`,z0\n`) =xfor some nonnegative integer x. From\nthe step property for zandz0we have, for all", "doc_id": "35144928-0a0c-4ea1-971b-abe3e790e50b", "embedding": null, "doc_hash": "f0019bf83a0508f923baa91d888e4138ab55164647afccea848208732ba3049d", "extra_info": null, "node_info": {"start": 714749, "end": 717427}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "93e549c0-5e21-4536-9ef6-470c45da509b", "3": "5f1fa700-e632-4d02-9ec2-3d21b18bc11d"}}, "__type__": "1"}, "5f1fa700-e632-4d02-9ec2-3d21b18bc11d": {"__data__": {"text": "layer of balancers,\nyi\u0000yj=zbi=2c\u0000zbj=2c,\nand the result follows because zhas the step property.\nSimilarly, ifPziandPz0\nidiffer by one, Lemma 12.5.5 implies that zi=z0\ni\nfor 06i<k= 2, except for a unique `such thatz`andz0\n`differ by one. Let\nmax(z`,z0\n`) =x+ 1 and min( z`,z0\n`) =xfor some nonnegative integer x. From\nthe step property for zandz0we have, for all i<` ,zi=z0\ni=x+ 1 and for all i>`\nzi=z0\ni=x. Sincez`andz0\n`are joined by a balancer with outputs y2`andy2`+1,\nit follows that y2`=x+ 1 andy2`+1=x. Similarly,ziandz0\nifori6=`are joined\nby the same balancer. Thus, for any i<` ,y2i=y2i+1=x+ 1 and for any i>` ,\ny2i=y2i+1=x. The step property follows by choosing c= 2`+ 1 and applying\nLemma 12.5.1. 2\nThe proof of the following theorem is now immediate.\nTheorem 12.5.1. In any quiescent state, the outputs of B ITONIC [w] have the step\nproperty.\nA Periodic Counting Network\nIn this section, we show that the Bitonic network is not the only counting network\nwith depthO(log2w). We introduce a new counting network with the remark-\nable property that it is periodic , consisting of a sequence of identical subnetworks,\nas depicted in Fig. 12.18. We de\ufb01ne the network B LOCK [k] as follows. When kis\nequal to 2, the B LOCK [k] network consists of a single balancer. The B LOCK [2k]\nnetwork for larger kis constructed recursively. We start with two B LOCK [k] net-\nworksAandB. Given an input sequence x, the input to AisxA, and the input to\nBisxB. Letybe the output sequence for the two subnetworks, where yAis the\noutput sequence for AandyBthe output sequence for B. The \ufb01nal stage of the\nnetwork combines each yA\niandyB\niin a single balancer, yielding \ufb01nal outputs z2i\nandz2i+1.\nFig. 12.19 describes the recursive construction of a B LOCK [8] network. The\nPERIODIC [2k] network consists of log kBLOCK [2k] networks joined so that\n12.5 Counting Networks 279\ny0\ny1\ny2\ny3\ny4\ny5\ny6\ny7x0\nx1\nx2\nx3\nx4\nx5\nx6\nx7\n1st Block[8] 2nd Block[8] 3rd Block[8]Periodic[8]\nFigure 12.18 A P ERIODIC [8] counting network constructed from 3 identical B LOCK [8]\nnetworks.\nBlock[4]Block[4]x0\nx1\nx2\nx3\nx4\nx5\nx6\nx7x0\nx1\nx2\nx3\nx4\nx5\nx6\nx7y0y1\ny2\ny3\ny4\ny5\ny6\ny7y0y1\ny2\ny3\ny4\ny5\ny6\ny7\nFigure 12.19 The left-hand side illustrates a B LOCK [8] network, into which feed two\nPERIODIC [4] networks. The right-hand illustrates the physical layout of a M ERGER [8] network.\nThe balancers are color-coded to match the logical structure in the left-hand \ufb01gure.\ntheithoutput wire of one is the", "doc_id": "5f1fa700-e632-4d02-9ec2-3d21b18bc11d", "embedding": null, "doc_hash": "eb46d0b595c3193c4eea1262bfca7ca2d60af7bf7921663995a9038059b116e5", "extra_info": null, "node_info": {"start": 717405, "end": 719866}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "35144928-0a0c-4ea1-971b-abe3e790e50b", "3": "c6894a5c-68f8-464f-96d4-563b110a7233"}}, "__type__": "1"}, "c6894a5c-68f8-464f-96d4-563b110a7233": {"__data__": {"text": "12.19 The left-hand side illustrates a B LOCK [8] network, into which feed two\nPERIODIC [4] networks. The right-hand illustrates the physical layout of a M ERGER [8] network.\nThe balancers are color-coded to match the logical structure in the left-hand \ufb01gure.\ntheithoutput wire of one is the ithwire of the next. Fig. 12.18 is a P ERIODIC [8]\ncounting network.2\nA Software Periodic Counting Network\nHere is how to implement the Periodic network in software. We reuse the\nBalancer class in Fig. 12.14. A single layer of a B LOCK [w] network is imple-\nmented by the L AYER [w] network (Fig. 12.20). A L AYER [w] network joins input\nwiresiandw\u0000i\u00001 to the same balancer.\nIn the B LOCK [w] class (Fig. 12.21), after the token emerges from the initial\nLAYER [w] network, it passes through one of two half-width B LOCK [w=2] net-\nworks (called north and south ).\nThe P ERIODIC [w] network (Fig. 12.22) is implemented as an array of log w\nBLOCK [w] networks. Each token traverses each block in sequence, where the\noutput wire taken on each block is the input wire for its successor. (The chapter\nnotes cite the proof that the P ERIODIC [w] is a counting network.)\n2While the B LOCK [2k] and M ERGER [2k] networks may look the same, they are not: there is no\npermutation of wires that yields one from the other.\n280 Chapter 12 Counting, Sorting, and Distributed Coordination\n1public class Layer {\n2 int width;\n3 Balancer[] layer;\n4 public Layer( int width) {\n5 this .width = width;\n6 layer = new Balancer[width];\n7 for (int i = 0; i < width / 2; i++) {\n8 layer[i] = layer[width-i-1] = new Balancer();\n9 }\n10 }\n11 public int traverse( int input) {\n12 int toggle = layer[input].traverse();\n13 int hi, lo;\n14 if(input < width / 2) {\n15 lo = input;\n16 hi = width - input - 1;\n17 }else {\n18 lo = width - input - 1;\n19 hi = input;\n20 }\n21 if(toggle == 0) {\n22 return lo;\n23 }else {\n24 return hi;\n25 }\n26 }\n27 }\nFigure 12.20 TheLayer network.\n12.5.3 Performance and Pipelining\nHow does counting network throughput vary as a function of the number of\nthreads and the network width? For a \ufb01xed network width, throughput rises\nwith the number of threads up to a point, and then the network saturates , and\nthroughput remains constant or declines. T o understand these results, let us think\nof a counting network as a pipeline.\n\u0004If the number of tokens concurrently traversing the network is less than\nthe number of balancers, then the pipeline is partly empty, and throughput\nsuffers.\n\u0004If the number of concurrent tokens is greater than the number of balancers,\nthen the pipeline becomes clogged because too many tokens arrive at each\nbalancer at the same time, resulting in per-balancer contention.\n\u0004Throughput is maximized when the number of tokens is roughly equal to the\nnumber of balancers.\nIf an application needs a counting network, then the best size network to choose\nis one that ensures that the number of tokens traversing the balancer at any time\nis roughly equal to the number of balancers.\n12.5 Counting Networks 281\n1public class Block {\n2 Block north;\n3 Block south;\n4 Layer layer;\n5 int width;\n6 public Block( int width) {\n7 this", "doc_id": "c6894a5c-68f8-464f-96d4-563b110a7233", "embedding": null, "doc_hash": "aaad91b0fd8ec8b4dbec50e6fffeba887601026ca6e93a45ba1e88026aa6a886", "extra_info": null, "node_info": {"start": 719934, "end": 723061}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5f1fa700-e632-4d02-9ec2-3d21b18bc11d", "3": "a2f3d8f2-ebfd-48f3-b735-534d1f6fd42a"}}, "__type__": "1"}, "a2f3d8f2-ebfd-48f3-b735-534d1f6fd42a": {"__data__": {"text": "arrive at each\nbalancer at the same time, resulting in per-balancer contention.\n\u0004Throughput is maximized when the number of tokens is roughly equal to the\nnumber of balancers.\nIf an application needs a counting network, then the best size network to choose\nis one that ensures that the number of tokens traversing the balancer at any time\nis roughly equal to the number of balancers.\n12.5 Counting Networks 281\n1public class Block {\n2 Block north;\n3 Block south;\n4 Layer layer;\n5 int width;\n6 public Block( int width) {\n7 this .width = width;\n8 if(width > 2) {\n9 north = new Block(width / 2);\n10 south = new Block(width / 2);\n11 }\n12 layer = new Layer(width);\n13 }\n14 public int traverse( int input) {\n15 int wire = layer.traverse(input);\n16 if(width > 2) {\n17 if(wire < width / 2) {\n18 return north.traverse(wire);\n19 }else {\n20 return (width / 2) + south.traverse(wire - (width / 2));\n21 }\n22 }else {\n23 return wire;\n24 }\n25 }\n26 }\nFigure 12.21 The B LOCK [w] network.\n1public class Periodic {\n2 Block[] block;\n3 public Periodic( int width) {\n4 int logSize = 0;\n5 int myWidth = width;\n6 while (myWidth > 1) {\n7 logSize++;\n8 myWidth = myWidth / 2;\n9 }\n10 block = new Block[logSize];\n11 for (int i = 0; i < logSize; i++) {\n12 block[i] = new Block(width);\n13 }\n14 }\n15 public int traverse( int input) {\n16 int wire = input;\n17 for (Block b : block) {\n18 wire = b.traverse(wire);\n19 }\n20 return wire;\n21 }\n22 }\nFigure 12.22 ThePeriodic network.\n282 Chapter 12 Counting, Sorting, and Distributed Coordination\n12.6 Diffracting Trees\nCounting networks provide a high degree of pipelining, so throughput is largely\nindependent of network depth. Latency, however, does depend on network\ndepth. Of the counting networks we have seen, the most shallow has depth\n\u00ca(log2w). Can we design a logarithmic-depth counting network? The good news\nis yes, such networks exist, but the bad news is that for all known constructions,\nthe constant factors involved render these constructions impractical.\nHere is an alternative approach. Consider a set of balancers with a single input\nwire and two output wires, with the top and bottom labeled 0 and 1, respectively.\nThe T REE[w] network (depicted in Fig. 12.23) is a binary tree structured as fol-\nlows. Letwbe a power of two, and de\ufb01ne T REE[2k] inductively. When kis equal\nto 1, T REE[2k] consists of a single balancer with output wires y0andy1. For\nk>1, construct T REE[2k] from two T REE[k] trees and one additional balancer.\nMake the input wire xof the single balancer the root of the tree and connect each\nof its output wires to the input wire of a tree of width k. Redesignate output wires\ny0,y1,:::,yk\u00001of the T REE[k] subtree extending from the \u201c0\u201d output wire as the\neven output wires y0,y2,:::,y2k\u00002of the \ufb01nal T REE[2k] network and the wires\ny0,y1,:::,yk\u00001of the T REE[k] subtree extending from the balancer\u2019s \u201c1\u201d output\nwire as the odd output wires y1,y3,:::,y2k\u00001of \ufb01nal T REE[2k] network.\nT", "doc_id": "a2f3d8f2-ebfd-48f3-b735-534d1f6fd42a", "embedding": null, "doc_hash": "9f17ef519e0f42c60f7246a65f4d199ffce9c28f3539f54e9d95ed298a2fc44e", "extra_info": null, "node_info": {"start": 722861, "end": 725795}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "c6894a5c-68f8-464f-96d4-563b110a7233", "3": "3e7f5741-bf18-48a4-a5bd-130265d966a9"}}, "__type__": "1"}, "3e7f5741-bf18-48a4-a5bd-130265d966a9": {"__data__": {"text": "output wires\ny0,y1,:::,yk\u00001of the T REE[k] subtree extending from the \u201c0\u201d output wire as the\neven output wires y0,y2,:::,y2k\u00002of the \ufb01nal T REE[2k] network and the wires\ny0,y1,:::,yk\u00001of the T REE[k] subtree extending from the balancer\u2019s \u201c1\u201d output\nwire as the odd output wires y1,y3,:::,y2k\u00001of \ufb01nal T REE[2k] network.\nT o understand why the T REE[2k] network has the step property in a quiescent\nstate, let us assume inductively that a quiescent T REE[k] has the step property.\nThe root balancer passes at most one token more to the T REE[k] subtree on its\n\u201c0\u201d (top) wire than on its\u201c1\u201d (bottom) wire. The tokens exiting the top T REE[k]\nsubtree have a step property differing from that of the bottom subtree at exactly\none wirejamong their koutput wires. The T REE[2k] outputs are a perfect shuf-\n\ufb02e of the wires leaving the two subtrees, and it follows that the two step-shaped\ntoken sequences of width kform a new step of width 2 kwhere the possible single\n11\n1\n12\n222\n3\n33 3\nbbbb\nbb\nb\nFigure 12.23 The T REE[8] class: a tree that counts. Notice how the network maintains the\nstep property.\n12.6 Diffracting Trees 283\nexcess token appears at the higher of the two wires j, that is, the one from the top\nTREE[k] tree.\nThe T REE[w] network may be a counting network, but is it a good counting\nnetwork? The good news is that it has shallow depth: while a B ITONIC [w] net-\nwork has depth log2w, the T REE[w] network depth is just log w. The bad news\nis contention: every token that enters the network passes through the same root\nbalancer, causing that balancer to become a bottleneck. In general, the higher the\nbalancer in the tree, the higher the contention.\nWe can reduce contention by exploiting a simple observation similar to one\nwe made about the EliminationBackoffStack ofChapter 11 :\nIf an even number of tokens pass through a balancer, the outputs are evenly bal-\nanced on the top and bottom wires, but the balancer\u2019s state remains unchanged.\nThe basic idea behind diffracting trees is to place a Prism at each balancer, an\nout-of-band mechanism similar to the EliminationArray which allowed tokens\n(threads) accessing a stack to exchange items. The Prism allows tokens to pair off\nat random array locations and agree to diffract in different directions, that is, to\nexit on different wires without traversing the balancer\u2019s toggle bit or changing\nits state. A token traverses the balancer\u2019s toggle bit only if it is unable to pair off\nwith another token within a reasonable period of time. If it did not manage to\ndiffract, the token toggles the bit to determine which way to go. It follows that\nwe can avoid excessive contention at balancers if the prism can pair off enough\ntokens without introducing too much contention.\nAPrism is an array of Exchanger<Integer> objects, like the\nEliminationArray . An Exchanger<T> object permits two threads to exchange\nTvalues. If thread Acalls the object\u2019s exchange () method with argument a, and\nBcalls the same object\u2019s exchange () method with argument b, thenA\u2019s call\nreturns value band vice versa. The \ufb01rst thread to arrive is blocked until the sec-\nond arrives. The call includes a timeout argument allowing a", "doc_id": "3e7f5741-bf18-48a4-a5bd-130265d966a9", "embedding": null, "doc_hash": "91be65ef6b414e9123037d96b4d092ac8029ce6be2ea474a138ba024893499ae", "extra_info": null, "node_info": {"start": 725967, "end": 729134}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a2f3d8f2-ebfd-48f3-b735-534d1f6fd42a", "3": "03c25c38-5f6c-40ef-8671-78e8ec47cbf4"}}, "__type__": "1"}, "03c25c38-5f6c-40ef-8671-78e8ec47cbf4": {"__data__": {"text": "balancers if the prism can pair off enough\ntokens without introducing too much contention.\nAPrism is an array of Exchanger<Integer> objects, like the\nEliminationArray . An Exchanger<T> object permits two threads to exchange\nTvalues. If thread Acalls the object\u2019s exchange () method with argument a, and\nBcalls the same object\u2019s exchange () method with argument b, thenA\u2019s call\nreturns value band vice versa. The \ufb01rst thread to arrive is blocked until the sec-\nond arrives. The call includes a timeout argument allowing a thread to proceed if\nit is unable to exchange a value within a reasonable duration.\nThePrism implementation appears in Fig. 12.24 . Before thread Avisits the\nbalancer\u2019s toggle bit, it visits associated Prism . In the Prism , it picks an array\nentry at random, and calls that slot\u2019s exchange () method, providing its own\nthread ID as an exchange value. If it succeeds in exchanging ids with another\nthread, then the lower thread ID exits on wire 0, and the higher on wire 1.\nFig. 12.24 shows a Prism implementation. The constructor takes as an argu-\nment the capacity of the prism (the maximal number of distinct exchangers).\nThe Prism class provides a single method, visit (), that chooses the random\nexchanger entry. The visit () call returns true if the caller should exit on the top\nwire, false if the bottom wire, and it throws a TimeoutException if the timeout\nexpires without exchanging a value. The caller acquires its thread ID (Line 13),\nchooses a random entry in the array (Line 14), and tries to exchange its own ID\nwith its partner\u2019s (Line 15). If it succeeds, it returns a Boolean value, and if it\ntimes out, it rethrows TimeoutException .\n284 Chapter 12 Counting, Sorting, and Distributed Coordination\n1public class Prism {\n2 private static final int duration = 100;\n3 Exchanger<Integer>[] exchanger;\n4 Random random;\n5 public Prism( int capacity) {\n6 exchanger = (Exchanger<Integer>[]) new Exchanger[capacity];\n7 for (int i = 0; i < capacity; i++) {\n8 exchanger[i] = new Exchanger<Integer>();\n9 }\n10 random = new Random();\n11 }\n12 public boolean visit() throws TimeoutException, InterruptedException {\n13 int me = ThreadID.get();\n14 int slot = random.nextInt(exchanger.length);\n15 int other = exchanger[slot].exchange(me, duration, TimeUnit.MILLISECONDS);\n16 return (me < other);\n17 }\n18 }\nFigure 12.24 ThePrism class.\n1public class DiffractingBalancer {\n2 Prism prism;\n3 Balancer toggle;\n4 public DiffractingBalancer( int capacity) {\n5 prism = new Prism(capacity);\n6 toggle = new Balancer();\n7 }\n8 public int traverse() {\n9 boolean direction = false ;\n10 try{\n11 if(prism.visit())\n12 return 0;\n13 else\n14 return 1;\n15 }catch (TimeoutException ex) {\n16 return toggle.traverse();\n17 }\n18 }\n19 }\nFigure 12.25 TheDiffractingBalancer class: if the caller pairs up with a concurrent caller\nthrough the prism, it does not need to traverse the balancer.\nADiffractingBalancer (Fig. 12.25), like a regular Balancer , provides a\ntraverse () method whose return value alternates between 0 and 1. This class\nhas two \ufb01elds: prism is aPrism , and toggle is aBalancer . When a thread calls\ntraverse (), it tries to \ufb01nd a partner through the prism . If it succeeds, then the\npartners return with distinct values, without creating contention", "doc_id": "03c25c38-5f6c-40ef-8671-78e8ec47cbf4", "embedding": null, "doc_hash": "b3b9ce96525038f02d6f99354624019c64a2dbbe6dbca3603130f5d512cce043", "extra_info": null, "node_info": {"start": 728967, "end": 732227}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3e7f5741-bf18-48a4-a5bd-130265d966a9", "3": "a3a151f9-6353-434d-ae9e-7b40bd691597"}}, "__type__": "1"}, "a3a151f9-6353-434d-ae9e-7b40bd691597": {"__data__": {"text": "class: if the caller pairs up with a concurrent caller\nthrough the prism, it does not need to traverse the balancer.\nADiffractingBalancer (Fig. 12.25), like a regular Balancer , provides a\ntraverse () method whose return value alternates between 0 and 1. This class\nhas two \ufb01elds: prism is aPrism , and toggle is aBalancer . When a thread calls\ntraverse (), it tries to \ufb01nd a partner through the prism . If it succeeds, then the\npartners return with distinct values, without creating contention at the toggle\n12.6 Diffracting Trees 285\n1public class DiffractingTree {\n2 DiffractingBalancer root;\n3 DiffractingTree[] child;\n4 int size;\n5 public DiffractingTree( int mySize) {\n6 size = mySize;\n7 root = new DiffractingBalancer(size);\n8 if(size > 2) {\n9 child = new DiffractingTree[]{\n10 new DiffractingTree(size/2),\n11 new DiffractingTree(size/2)};\n12 }\n13 }\n14 public int traverse() {\n15 int half = root.traverse();\n16 if(size > 2) {\n17 return (2*(child[half].traverse()) + half);\n18 }else {\n19 return half;\n20 }\n21 }\n22 }\nFigure 12.26 TheDiffractingTree class: \ufb01elds, constructor, and traverse ()method.\n(Line 11). Otherwise, if the thread is unable to \ufb01nd a partner, it traverses (Line 16)\nthetoggle (implemented as a balancer).\nThe DiffractingTree class ( Fig. 12.26 ) has two \ufb01elds. The child array is\na two-element array of child trees. The root \ufb01eld is a DiffractingBalancer\nthat alternates between forwarding calls to the left or right subtree. Each\nDiffractingBalancer has a capacity, which is actually the capacity of its inter-\nnal prism. Initially this capacity is the size of the tree, and the capacity shrinks by\nhalf at each level.\nAs with the EliminationBackoffStack ,DiffractingTree performance\ndepends on two parameters: prism capacities and timeouts. If the prisms are too\nbig, threads miss one another, causing excessive contention at the balancer. If the\narrays are too small, then too many threads concurrently access each exchanger\nin a prism, resulting in excessive contention at the exchangers. If prism timeouts\nare too short, threads miss one another, and if they are too long, threads may be\ndelayed unnecessarily. There are no hard-and-fast rules for choosing these val-\nues, since the optimal values depend on the load and the characteristics of the\nunderlying multiprocessor architecture.\nNevertheless, experimental evidence suggests that it is sometimes possible\nto choose these values to outperform both the CombiningTree and\nCountingNetwork classes. Here are some heuristics that work well in practice.\nBecause balancers higher in the tree have more contention, we use larger prisms\nnear the top of the tree, and add the ability to dynamically shrink and grow the\n286 Chapter 12 Counting, Sorting, and Distributed Coordination\nrandom range chosen. The best timeout interval choice depends on the load:\nif only a few threads are accessing the tree, then time spent waiting is mostly\nwasted, while if there are many threads, then time spent waiting pays off. Adap-\ntive schemes are promising: lengthen the timeout while threads succeed in pair-\ning off, and shorten it otherwise.\n12.7 Parallel Sorting\nSorting is one of the most important computational tasks, dating back to\nHollerith\u2019s Nineteenth-Century sorting machine, through the \ufb01rst electronic\ncomputer systems in the 1940s, and culminating today, when a", "doc_id": "a3a151f9-6353-434d-ae9e-7b40bd691597", "embedding": null, "doc_hash": "e2ddf77631fa141022a97e2aecb938996fdd7c352082a8f8895b1179a2d04ae7", "extra_info": null, "node_info": {"start": 732257, "end": 735598}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "03c25c38-5f6c-40ef-8671-78e8ec47cbf4", "3": "a40a8b7e-8f72-4482-b7f1-a7f8a008c607"}}, "__type__": "1"}, "a40a8b7e-8f72-4482-b7f1-a7f8a008c607": {"__data__": {"text": "only a few threads are accessing the tree, then time spent waiting is mostly\nwasted, while if there are many threads, then time spent waiting pays off. Adap-\ntive schemes are promising: lengthen the timeout while threads succeed in pair-\ning off, and shorten it otherwise.\n12.7 Parallel Sorting\nSorting is one of the most important computational tasks, dating back to\nHollerith\u2019s Nineteenth-Century sorting machine, through the \ufb01rst electronic\ncomputer systems in the 1940s, and culminating today, when a high fraction\nof programs use sorting in some form or another. As most Computer Science\nundergraduates learn early on, the choice of sorting algorithm depends crucially\non the number of items being sorted, the numerical properties of their keys, and\nwhether the items reside in memory or in an external storage device. Parallel\nsorting algorithms can be classi\ufb01ed in the same way.\nWe present two classes of sorting algorithms: sorting networks , which typically\nwork well for small in-memory data sets, and sample sorting algorithms , which\nwork well for large data sets in external memory. In our presentation, we sacri\ufb01ce\nperformance for simplicity. More complex techniques are cited in the chapter\nnotes.\n12.8 Sorting Networks\nIn much the same way that a counting network is a network of balancers , a sorting\nnetwork is a network of comparators .3A comparator is a computing element with\ntwo input wires and two output wires, called the topand bottom wires. It receives\ntwo numbers on its input wires, and forwards the larger to its top wire and the\nsmaller to its bottom wire. A comparator, unlike a balancer, is synchronous : it\noutputs values only when both inputs have arrived (see Fig. 12.27).\ny0 5 max( x0,x1)\ny1 5 min( x0,x1)x0\nx1comparator\nFigure 12.27 A comparator.\n3Historically sorting networks predate counting networks by several decades.\n12.8 Sorting Networks 287\nAcomparison network , like a balancing network, is an acyclic network of com-\nparators. An input value is placed on each of its winput lines. These values pass\nthrough each layer of comparators synchronously, \ufb01nally leaving together on the\nnetwork output wires.\nA comparison network with input values xiand output values yi,i2f0:::1g,\neach on wire i, is a valid sorting network if its output values are the input values\nsorted in descending order, that is, yi\u00001>yi.\nThe following classic theorem simpli\ufb01es the process of proving that a given\nnetwork sorts.\nTheorem 12.8.1 (0-1-principle). If a sorting network sorts every input sequence\nof 0s and 1s, then it sorts any sequence of input values.\n12.8.1 Designing a Sorting Network\nThere is no need to design sorting networks, because we can recycle counting net-\nwork layouts. A balancing network and a comparison network are isomorphic if\none can be constructed from the other by replacing balancers with comparators,\nor vice versa.\nTheorem 12.8.2. If a balancing network counts, then its isomorphic comparison\nnetwork sorts.\nProof: We construct a mapping from comparison network transitions to isomor-\nphic balancing network transitions.\nBy Theorem 12.8.1, a comparison network which sorts all sequences of 0s and\n1s is a sorting network. Take any arbitrary sequence of 0s and 1s as inputs to the\ncomparison network, and for the balancing network place a token on each 1 input\nwire and no token on each 0 input wire. If we run both networks in lock-step, the\nbalancing network simulates", "doc_id": "a40a8b7e-8f72-4482-b7f1-a7f8a008c607", "embedding": null, "doc_hash": "60e9feeb65f6c95f58c2ee2152c2d394dd213bb7b1c3e4d110be37f589e51cb0", "extra_info": null, "node_info": {"start": 735582, "end": 739006}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a3a151f9-6353-434d-ae9e-7b40bd691597", "3": "f82214ba-a8fd-4347-b32c-db1e927b153a"}}, "__type__": "1"}, "f82214ba-a8fd-4347-b32c-db1e927b153a": {"__data__": {"text": "If a balancing network counts, then its isomorphic comparison\nnetwork sorts.\nProof: We construct a mapping from comparison network transitions to isomor-\nphic balancing network transitions.\nBy Theorem 12.8.1, a comparison network which sorts all sequences of 0s and\n1s is a sorting network. Take any arbitrary sequence of 0s and 1s as inputs to the\ncomparison network, and for the balancing network place a token on each 1 input\nwire and no token on each 0 input wire. If we run both networks in lock-step, the\nbalancing network simulates the comparison network.\nThe proof is by induction on the depth of the network. For level 0 the claim\nholds by construction. Assuming it holds for wires of a given level k, let us prove\nit holds for level k+1. On every comparator where two 1s meet in the comparison\nnetwork, two tokens meet in the balancing network, so one 1 leaves on each wire\nin the comparison network on level k+ 1, and one token leaves on each wire in\nthe balancing network on level k+ 1. On every comparator where two 0s meet in\nthe comparison network, no tokens meet in the balancing network, so a 0 leaves\non each level k+ 1 wire in the comparison network, and no tokens leave in the\nbalancing network. On every comparator where a 0 and 1 meet in the comparison\nnetwork, the 1 leaves on the north (upper) wire and the 1 on the south (lower)\nwire on level k+ 1, while in the balancing network the token leaves on the north\nwire, and no token leaves on the south wire.\nIf the balancing network is a counting network, that is, it has the step property\non its output level wires, then the comparison network must have sorted the\ninput sequence of 0s and 1s. 2\n288 Chapter 12 Counting, Sorting, and Distributed Coordination\n1\n4\n322\n4\n314\n3\n12\n14\n23inputs outputs\n4\n2\n13\nFigure 12.28 TheOddEven sorting network.\nThe converse is false: not all sorting networks are counting networks. We leave it\nas an exercise to verify that the OddEven network in Fig. 12.28 is a sorting network\nbut not a counting network.\nCorollary 12.8.1. Comparison networks isomorphic to B ITONIC [] and P ERI-\nODIC [] networks are sorting networks.\nSorting a set of size wby comparisons requires \u00d2(wlogw) comparisons.\nA sorting network with winput wires has at most O(w) comparators in each\nlevel, so its depth can be no smaller than \u00d2(logw).\nCorollary 12.8.2. The depth of any counting network is at least \u00d2(logw).\nA Bitonic Sorting Algorithm\nWe can represent any width- wsorting network, such as B ITONIC [w], as a col-\nlection ofdlayers ofw=2 balancers each. We can represent a sorting network\nlayout as a table, where each entry is a pair that describes which two wires meet\nat that balancer at that layer. (E.g., in the B ITONIC [4] network of Fig. 12.11,\nwires 0 and 1 meet at the \ufb01rst balancer in the \ufb01rst layer, and wires 0 and 3 meet\nat the \ufb01rst balancer of the second layer.) Let us assume, for simplicity, that we are\ngiven an unbounded table bitonicTable [i][d][j], where each array entry con-\ntains the index of the associated north (0) or south (1) input wire to balancer iat\ndepthd.\nAnin-place array-based sorting algorithm takes as input an array of items to\nbe sorted (here we assume these items have unique integer keys) and returns\nthe same array with the items sorted by key. Here is how we", "doc_id": "f82214ba-a8fd-4347-b32c-db1e927b153a", "embedding": null, "doc_hash": "0b44981168493a42afecb1423bbe4ce150bdf34879aa82659f23b90bface8d21", "extra_info": null, "node_info": {"start": 738983, "end": 742273}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a40a8b7e-8f72-4482-b7f1-a7f8a008c607", "3": "a5b6b26f-87e2-41f8-abac-d549d49f61b4"}}, "__type__": "1"}, "a5b6b26f-87e2-41f8-abac-d549d49f61b4": {"__data__": {"text": "in the \ufb01rst layer, and wires 0 and 3 meet\nat the \ufb01rst balancer of the second layer.) Let us assume, for simplicity, that we are\ngiven an unbounded table bitonicTable [i][d][j], where each array entry con-\ntains the index of the associated north (0) or south (1) input wire to balancer iat\ndepthd.\nAnin-place array-based sorting algorithm takes as input an array of items to\nbe sorted (here we assume these items have unique integer keys) and returns\nthe same array with the items sorted by key. Here is how we implement\nBitonicSort , an in-place array-based sorting algorithm based on a Bitonic\n12.8 Sorting Networks 289\nsorting network. Let us assume that we wish to sort an array of 2 \u0001p\u0001selements,\nwherepis the number of threads (and typically also the maximal number of\navailable processors on which the threads run) and p\u0001sis a power of 2. The\nnetwork has p\u0001scomparators at every layer.\nEach of the pthreads emulates the work of scomparators. Unlike counting\nnetworks, which act like uncoordinated raves, sorting networks are synchronous:\nall inputs to a comparator must arrive before it can compute the outputs. The\nalgorithm proceeds in rounds. In each round, a thread performs scomparisons\nin a layer of the network, switching the array entries of items if necessary, so that\nthey are properly ordered. In each network layer, the comparators join different\nwires, so no two threads attempt to exchange the items of the same entry, avoid-\ning the need to synchronize operations at any given layer.\nT o ensure that the comparisons of a given round (layer) are complete before\nproceeding to the next one, we use a synchronization construct called a Barrier\n(studied in more detail in Chapter 17 ). A barrier for pthreads provides an\nawait () method, whose call does not return until all pthreads have called\nawait (). The BitonicSort implementation appears in Fig. 12.29 . Each thread\nproceeds through the layers of the network round by round. In each round, it\nawaits the arrival of the other threads (Line 12), ensuring that the items array\ncontains the prior round\u2019s results. It then emulates the behavior of sbalancers\nat that layer by comparing the items at the array positions corresponding to the\n1public class BitonicSort {\n2 static final int [][][] bitonicTable = ...;\n3 static final int width = ...; // counting network width\n4 static final int depth = ...; // counting network depth\n5 static final int p = ...; // number of threads\n6 static final int s = ...; // a power of 2\n7 Barrier barrier;\n8 ...\n9 public <T> void sort(Item<T>[] items) {\n10 int i = ThreadID.get();\n11 for (int d = 0; d < depth; d++) {\n12 barrier.await();\n13 for (int j = 0; j < s; j++) {\n14 int north = bitonicTable[(i *s)+j][d][0];\n15 int south = bitonicTable[(i *s)+j][d][1];\n16 if(items[north].key < items[south].key) {\n17 Item<T> temp = items[north];\n18 items[north] = items[south];\n19 items[south] = temp;\n20 }\n21 }\n22 }\n23 }\nFigure 12.29 TheBitonicSort class.\n290 Chapter 12 Counting, Sorting, and Distributed Coordination\ncomparator\u2019s wires, and exchanging them if their keys are out of order (Lines 14\nthrough 19).\nTheBitonicSort takesO(slog2p) time forpthreads running on pproces-\nsors, which, if sis constant, is O(log2p) time.\n12.9 Sample", "doc_id": "a5b6b26f-87e2-41f8-abac-d549d49f61b4", "embedding": null, "doc_hash": "769c76576986eff2fabe9dd915be3c46741edf5e0d4b260b9ddbb099ddcb0fd2", "extra_info": null, "node_info": {"start": 742306, "end": 745543}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f82214ba-a8fd-4347-b32c-db1e927b153a", "3": "08625520-6815-491b-a696-514ec14bb470"}}, "__type__": "1"}, "08625520-6815-491b-a696-514ec14bb470": {"__data__": {"text": "Item<T> temp = items[north];\n18 items[north] = items[south];\n19 items[south] = temp;\n20 }\n21 }\n22 }\n23 }\nFigure 12.29 TheBitonicSort class.\n290 Chapter 12 Counting, Sorting, and Distributed Coordination\ncomparator\u2019s wires, and exchanging them if their keys are out of order (Lines 14\nthrough 19).\nTheBitonicSort takesO(slog2p) time forpthreads running on pproces-\nsors, which, if sis constant, is O(log2p) time.\n12.9 Sample Sorting\nTheBitonicSort is appropriate for small data sets that reside in memory. For\nlarger data sets (where n, the number of items, is much larger than p, the number\nof threads), especially ones that reside on out-of-memory storage devices, we\nneed a different approach. Because accessing a data item is expensive, we must\nmaintain as much locality-of-reference as possible, so having a single thread sort\nitems sequentially is cost-effective. A parallel sort like BitonicSort , where an\nitem is accessed by multiple threads, is simply too expensive.\nWe attempt to minimize the number of threads that access a given item through\nrandomization. Thisuseof randomnessdiffersfrom thatinthe DiffractingTree ,\nwhere it was used to distribute memory accesses. Here we use randomness to guess\nthe distribution of items in the data set to be sorted.\nSince the data set to be sorted is large, we split it into buckets, throwing into\neach bucket the items that have keys within a given range. Each thread then sorts\nthe items in one of the buckets using a sequential sorting algorithm, and the result\nis a sorted set (when viewed in the appropriate bucket order). This algorithm is\na generalization of the well-known quicksort algorithm, but instead of having a\nsingle splitter key to divide the items into two subsets, we have p\u00001 splitter keys\nthat split the input set into psubsets.\nThe algorithm for nitems andpthreads involves three phases:\n1.Threads choose p\u00001 splitter keys to partition the data set into pbuckets. The\nsplitters are published so all threads can read them.\n2.Each thread sequentially processes n=p items, moving each item to its bucket,\nwhere the appropriate bucket is determined by performing a binary search\nwith the item\u2019s key among the splitter keys.\n3.Each thread sequentially sorts the items in its bucket.\nBarriers between the phases ensure that all threads have completed one phase\nbefore the next starts.\nBefore we consider Phase one, we look at the second and third phases.\nThe second phase\u2019s time complexity is ( n=p) logp, consisting of reading each\nitem from memory, disk, or tape, followed by a binary search among psplitters\ncached locally, and \ufb01nally adding the item into the appropriate bucket. The buc-\nkets into which the items are moved could be in memory, on disk, or on tape, so\nthe dominating cost is that of the n=p accesses to the stored data items.\n12.10 Distributed Coordination 291\nLetbbe the number of items in a bucket. The time complexity of the third\nphase for a given thread is O(blogb), to sort the items using a sequential version\nof, say, quicksort .4This part has the highest cost because it consists of read\u2013write\nphases that access relatively slow memory, such as disk or tape.\nThe time complexity of the algorithm is dominated by the thread with the\nmost items in its bucket in the third phase. It is therefore important to choose the\nsplitters to be as evenly distributed as possible, so each bucket receives approxi-\nmatelyn\u0000pitems in the second phase.\nThe key to choosing good splitters is to have each thread pick a set of sam-\nplesplitters that represent its own n\u0000psize data", "doc_id": "08625520-6815-491b-a696-514ec14bb470", "embedding": null, "doc_hash": "58e4cb05667fd93e15b0a5288da06bb9b8b60fa969a6687dadf487fc79a21807", "extra_info": null, "node_info": {"start": 745597, "end": 749154}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a5b6b26f-87e2-41f8-abac-d549d49f61b4", "3": "0cdbb62a-5cc7-424e-9890-a5e28b69e03c"}}, "__type__": "1"}, "0cdbb62a-5cc7-424e-9890-a5e28b69e03c": {"__data__": {"text": "version\nof, say, quicksort .4This part has the highest cost because it consists of read\u2013write\nphases that access relatively slow memory, such as disk or tape.\nThe time complexity of the algorithm is dominated by the thread with the\nmost items in its bucket in the third phase. It is therefore important to choose the\nsplitters to be as evenly distributed as possible, so each bucket receives approxi-\nmatelyn\u0000pitems in the second phase.\nThe key to choosing good splitters is to have each thread pick a set of sam-\nplesplitters that represent its own n\u0000psize data set, and choose the \ufb01nal p\u00001\nsplitters from among all the sample splitter sets of all threads. Each thread selects\nuniformly at random skeys from its data set of size n\u0000p. (In practice, it suf\ufb01ces\nto choosesto be 32 or 64 keys.) Each thread then participates in running the par-\nallelBitonicSort (Fig. 12.29) on the s\u0001psample keys selected by the pthreads.\nFinally, each thread reads the p\u00001 splitter keys in positions s, 2s,:::, (p\u00001)sin\nthe sorted set of splitters, and uses these as the splitters in the second phase. This\nchoice ofssamples, and the later choice of the \ufb01nal splitters from the sorted set\nof all samples, reduces the effects of an uneven key distribution among the n\u0000p\nsize data sets accessed by the threads.\nFor example, a sample sort algorithm could choose to have each thread pick\np\u00001 splitters for its second phase from within its own n=p size data set, without\never communicating with other threads. The problem with this approach is that\nif the distribution of the data is uneven, the size of the buckets may differ greatly,\nand performance would suffer. For example, if the number of items in the largest\nbucket is doubled, so is the worst-case time complexity of sorting algorithm.\nThe \ufb01rst phase\u2019s complexity is s(a constant) to perform the random sampling,\nandO(log2p) for the parallel Bitonic sort. The overall time complexity of sample\nsort with a good splitter set (where every bucket gets O(n=p) of the items) is\nO(log2p) +O((n=p) logp) +O((n=p) log(n=p))\nwhich overall is O((n=p) log(n=p)).\n12.10 Distributed Coordination\nThis chapter covered several distributed coordination patterns. Some, such as\ncombining trees, sorting networks, and sample sorting, have high parallelism and\nlow overheads. All these algorithms contain synchronization bottlenecks, that is,\npoints in the computation where threads must wait to rendezvous with others. In\nthe combining trees, threads must synchronize to combine, and in sorting, when\nthreads wait at barriers.\n4If the item\u2019s key size is known and \ufb01xed, one could use algorithms like Radixsort .\n292 Chapter 12 Counting, Sorting, and Distributed Coordination\nIn other schemes, such as counting networks and diffracting trees, threads\nnever wait for one another. (Although we implement balancers using\nsynchronized methods, they could be implemented in a lock-free manner using\ncompareAndSet() .) Here, the distributed structures pass information from one\nthread to another, and while a rendezvous could prove advantageous (as in the\nPrism array), it is not necessary.\nRandomization, which is useful in many places, helps to distribute work\nevenly. For diffracting trees, randomization distributes work over multiple mem-\nory locations, reducing the chance that too many threads simultaneously access\nthe same location. For sample sort, randomization helps distribute work evenly\namong buckets, which threads later sort in", "doc_id": "0cdbb62a-5cc7-424e-9890-a5e28b69e03c", "embedding": null, "doc_hash": "840b726bb44d5f624eac2651ade5766d10aa7172ad548fc5a0bb6a4a198eba23", "extra_info": null, "node_info": {"start": 749052, "end": 752507}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "08625520-6815-491b-a696-514ec14bb470", "3": "1e26ae6c-6131-4bd7-974e-d26805e86455"}}, "__type__": "1"}, "1e26ae6c-6131-4bd7-974e-d26805e86455": {"__data__": {"text": "methods, they could be implemented in a lock-free manner using\ncompareAndSet() .) Here, the distributed structures pass information from one\nthread to another, and while a rendezvous could prove advantageous (as in the\nPrism array), it is not necessary.\nRandomization, which is useful in many places, helps to distribute work\nevenly. For diffracting trees, randomization distributes work over multiple mem-\nory locations, reducing the chance that too many threads simultaneously access\nthe same location. For sample sort, randomization helps distribute work evenly\namong buckets, which threads later sort in parallel.\nFinally, we saw that pipelining can ensure that some data structures can have\nhigh throughput, even though they have high latency.\nAlthough we focus on shared-memory multiprocessors, it is worth mention-\ning that the distributed algorithms and structures considered in this chapter also\nwork in message-passing architectures. The message-passing model might be\nimplemented directly in hardware, as in a network of processors, or it could be\nprovided on top of a shared-memory architecture through a software layer such\nas MPI.\nIn shared-memory architectures, switches (such as combining tree nodes or\nbalancers) are naturally implemented as shared-memory counters. In message-\npassing architectures, switches are naturally implemented as processor-local\ndata structures, where wires that link one processor to another also link one\nswitch to another. When a processor receives a message, it atomically updates\nits local data structure and forwards messages to the processors managing other\nswitches.\n12.11 Chapter Notes\nThe idea behind combining trees is due to Allan Gottlieb, Ralph Grishman, Clyde\nKruskal, Kevin McAuliffe, Larry Rudolph, and Marc Snir [47]. The software\nCombiningTree presented here is a adapted from an algorithm by PenChung\nY ew, Nian-Feng Tzeng, and Duncan Lawrie [151] with modi\ufb01cations by Maurice\nHerlihy, Beng-Hong Lim, and Nir Shavit [65], all based on an original proposal\nby James Goodman, Mary Vernon, and Philip Woest [45].\nCounting networks were invented by Jim Aspnes, Maurice Herlihy, and Nir\nShavit [16]. Counting networks are related to sorting networks , including the\nground breaking Bitonic network of Kenneth Batcher [18], and the periodic net-\nwork of Martin Dowd, Y ehoshua Perl, Larry Rudolph, and Mike Saks [35]. Mikl \u00b4os\nAjtai, J \u00b4anos Koml \u00b4os, and Endre Szemer \u00b4edi discovered the AKS sorting network,\nanO(logw) depth sorting network [8]. (This asymptotic expression hides large\nconstants which make networks based on AKS impractical.)\n12.12 Exercises 293\nMike Klugerman and Greg Plaxton [84, 85] were the \ufb01rst to provide an AKS-\nbased counting network construction with O(logw) depth. The 0-1 principle for\nsorting networks is by Donald Knuth [86]. A similar set of rules for balancing\nnetworks is provided by Costas Busch and Marios Mavronicolas [25]. Diffracting\ntrees were invented by Nir Shavit and Asaph Zemach [143].\nSample sorting was suggested by John Reif and Leslie Valiant [132] and by\nHuang and Chow [73]. The sequential Quicksort algorithm to which all sample\nsorting algorithms relate is due to T ony Hoare [70]. There are numerous par-\nallel radix sort algorithms in the literature such as the one by Daniel Jim \u00b4enez-\nGonz \u00b4alez, Joseph Larriba-Pey, and Juan Navarro [82] or the one by", "doc_id": "1e26ae6c-6131-4bd7-974e-d26805e86455", "embedding": null, "doc_hash": "61163f4ff397dc2e146607722312b8e2dce269eeca94484d2cab69ce3732a0e1", "extra_info": null, "node_info": {"start": 752456, "end": 755823}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0cdbb62a-5cc7-424e-9890-a5e28b69e03c", "3": "e28e61ed-b96f-4fa4-8c75-94607aaf9381"}}, "__type__": "1"}, "e28e61ed-b96f-4fa4-8c75-94607aaf9381": {"__data__": {"text": "Marios Mavronicolas [25]. Diffracting\ntrees were invented by Nir Shavit and Asaph Zemach [143].\nSample sorting was suggested by John Reif and Leslie Valiant [132] and by\nHuang and Chow [73]. The sequential Quicksort algorithm to which all sample\nsorting algorithms relate is due to T ony Hoare [70]. There are numerous par-\nallel radix sort algorithms in the literature such as the one by Daniel Jim \u00b4enez-\nGonz \u00b4alez, Joseph Larriba-Pey, and Juan Navarro [82] or the one by Shin-Jae Lee\nand Minsoo Jeon and Dongseung Kim and Andrew Sohn [101].\nMonty Python and the Holy Grail was written by Graham Chapman, John\nCleese, T erry Gilliam, Eric Idle, T erry Jones, and Michael Palin and co-directed\nby T erry Gilliam and T erry Jones [27].\n12.12 Exercises\nExercise 134. Prove Lemma 12.5.1.\nExercise 135. Implement a trinary CombiningTree , that is, one that allows up to\nthree threads coming from three subtrees to combine at a given node. Can you\nestimate the advantages and disadvantages of such a tree when compared to a\nbinary combining tree?\nExercise 136. Implement a CombiningTree using Exchanger objects to per-\nform the coordination among threads ascending and descending the tree. What\nare the possible disadvantages of your construction when compared to the\nCombiningTree class presented in Section 12.3?\nExercise 137. Implement the cyclic array based shared pool described in\nSection 12.2 using two simple counters and a ReentrantLock per array entry.\nExercise 138. Provide an ef\ufb01cient lock-free implementation of a Balancer .\nExercise 139. (Hard) Provide an ef\ufb01cient wait-free implementation of a Balancer\n(i.e. not by using the universal construction).\nExercise 140. Prove that the T REE[2k] balancing network constructed in Sec-\ntion 12.6 is a counting network, that is, that in any quiescent state, the sequences\nof tokens on its output wires have the step property.\n294 Chapter 12 Counting, Sorting, and Distributed Coordination\nExercise 141. LetBbe a width-wbalancing network of depth din a quiescent\nstates. Letn= 2d. Prove that if ntokens enter the network on the same wire, pass\nthrough the network, and exit, then Bwill have the same state after the tokens\nexit as it did before they entered.\nIn the following exercises, a k-smooth sequence is a sequence y0,:::,yw\u00001that\nsatis\ufb01es\nifi<j thenjyi\u0000yjj6k:\nExercise 142. LetXandYbek-smooth sequences of length w. Amatching layer\nof balancers for XandYis one where each element of Xis joined by a balancer\nto an element of Yin a one-to-one correspondence.\nProve that if XandYare eachk-smooth, and Zis the result of matching X\nandY, thenZis (k+ 1)-smooth.\nExercise 143. Consider a B LOCK [k] network in which each balancer has been\ninitialized to an arbitrary state (either upordown ). Show that no matter what\nthe input distribution is, the output distribution is (log k)-smooth.\nHint: you may use the claim in Exercise 142.\nExercise 144. Asmoothing network is a balancing network that ensures that in any\nquiescent state, the output sequence is 1-smooth.\nCounting networks are smoothing networks, but not vice versa.\nA Boolean sorting network is one in which all inputs are guaranteed to be\nBoolean. De\ufb01ne a pseudo-sorting balancing network to be a balancing network\nwith a layout isomorphic to a Boolean sorting", "doc_id": "e28e61ed-b96f-4fa4-8c75-94607aaf9381", "embedding": null, "doc_hash": "fb4ffd4173360dcf62a4529a53a331a601374655dafb11523925cadc7ccd3bdc", "extra_info": null, "node_info": {"start": 755950, "end": 759224}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1e26ae6c-6131-4bd7-974e-d26805e86455", "3": "d3d21100-97ce-405b-9675-ead70847721c"}}, "__type__": "1"}, "d3d21100-97ce-405b-9675-ead70847721c": {"__data__": {"text": "). Show that no matter what\nthe input distribution is, the output distribution is (log k)-smooth.\nHint: you may use the claim in Exercise 142.\nExercise 144. Asmoothing network is a balancing network that ensures that in any\nquiescent state, the output sequence is 1-smooth.\nCounting networks are smoothing networks, but not vice versa.\nA Boolean sorting network is one in which all inputs are guaranteed to be\nBoolean. De\ufb01ne a pseudo-sorting balancing network to be a balancing network\nwith a layout isomorphic to a Boolean sorting network.\nLetNbe the balancing network constructed by taking a smoothing network\nSof widthw, a pseudo-sorting balancing network Palso of width w, and joining\ntheithoutput wire ofSto theithinput wire ofP.\nShow thatNis a counting network.\nExercise 145. A3-balancer is a balancer with three input lines and three output\nlines. Like its 2-line relative, its output sequences have the step property in any\nquiescent state. Construct a depth-3 counting network with 6 input and output\nlines from 2-balancers and 3-balancers. Explain why it works.\nExercise 146. Suggest ways to modify the BitonicSort class so that it will sort an\ninput array of width wwherewis not a power of 2.\nExercise 147. Consider the following w-thread counting algorithm. Each thread\n\ufb01rst uses a bitonic counting network of width wto take a counter value v. It then\ngoes through a waiting \ufb01lter , in which each thread waits for threads with lesser\nvalues to catch up.\n12.12 Exercises 295\nThe waiting \ufb01lter is an array filter [] ofwBoolean values. De\ufb01ne the phase\nfunction\n\u001e(v) =b(v=w)cmod 2:\nA thread that exits with value vspins on filter [(v\u00001) modn] until that value\nis set to\u001e(v\u00001). The thread responds by setting filter [vmodw] to\u001e(v), and\nthen returns v.\n1.Explain why this counter implementation is linearizable.\n2.An exercise here shows that any linearizable counting network has depth at\nleastw. Explain why the filter [] construction does not contradict this claim.\n3.On a bus-based multiprocessor, would this filter [] construction have better\nthroughput than a single variable protected by a spin lock? Explain.\nExercise 148. If a sequence X=x0,:::xw\u00001isk-smooth, then the result of passing\nXthrough a balancing network is k-smooth.\nExercise 149. Prove that the Bitonic [w] network has depth (log w)(1 + logw)=2\nand uses (wlogw)(1 + logw)=4 balancers.\nExercise 150. (Hard) Provide an implementation of a DiffractingBalancer\nthat is lock-free.\nExercise 151. Add an adaptive timeout mechanism to the Prism of the\nDiffractingBalancer .\nExercise 152. Show that the OddEven network in Fig. 12.28 is a sorting network\nbut not a counting network.\nExercise 153. Can counting networks do anything besides increments? Consider\na new kind of token, called an antitoken , which we use for decrements. Recall\nthat when a token visits a balancer, it executes a getAndComplement (): it atomi-\ncally reads the toggle value and complements it, and then departs on the output\nwire indicated by the old toggle value. Instead, an antitoken complements the\ntoggle value, and then departs on the output wire indicated by the new toggle\nvalue. Informally, an antitoken \u201ccancels\u201d the effect of the most recent token on\nthe balancer\u2019s toggle state, and vice versa.\nInstead of simply balancing the number of tokens that emerge on each wire,\nwe assign a weight of +1", "doc_id": "d3d21100-97ce-405b-9675-ead70847721c", "embedding": null, "doc_hash": "2257006632ee628493d611d681f560eaccd3f2aa28b6ea3d06b96780743eb183", "extra_info": null, "node_info": {"start": 759173, "end": 762522}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e28e61ed-b96f-4fa4-8c75-94607aaf9381", "3": "130e407d-a072-4509-9554-e299f1841c0f"}}, "__type__": "1"}, "130e407d-a072-4509-9554-e299f1841c0f": {"__data__": {"text": "a token visits a balancer, it executes a getAndComplement (): it atomi-\ncally reads the toggle value and complements it, and then departs on the output\nwire indicated by the old toggle value. Instead, an antitoken complements the\ntoggle value, and then departs on the output wire indicated by the new toggle\nvalue. Informally, an antitoken \u201ccancels\u201d the effect of the most recent token on\nthe balancer\u2019s toggle state, and vice versa.\nInstead of simply balancing the number of tokens that emerge on each wire,\nwe assign a weight of +1 to each token and \u00001 to each antitoken. We generalize\nthe step property to require that the sums of the weights of the tokens and anti-\ntokens that emerge on each wire have the step property. We call this property the\nweighted step property .\n296 Chapter 12 Counting, Sorting, and Distributed Coordination\n1 public synchronized int antiTraverse() {\n2 try {\n3 if(toggle) {\n4 return 1;\n5 }else {\n6 return 0;\n7 }\n8 }finally {\n9 toggle = !toggle;\n10 }\n11 }\nFigure 12.30 TheantiTraverse ()method.\nFig. 12.30 shows how to implement an antiTraverse () method that moves\nan antitoken though a balancer. Adding an antiTraverse () method to the other\nnetworks is left as an exercise.\nLetBbe a width-wbalancing network of depth din a quiescent state s. Let\nn= 2d. Show that if ntokens enter the network on the same wire, pass through\nthe network, and exit, then Bwill have the same state after the tokens exit as it\ndid before they entered.\nExercise 154. LetBbe a balancing network in a quiescent state s, and suppose a\ntoken enters on wire iand passes through the network, leaving the network in\nstates0. Show that if an antitoken now enters on wire iand passes through the\nnetwork, then the network goes back to state s.\nExercise 155. Show that if balancing network Bis a counting network for tokens\nalone, then it is also a balancing network for tokens and antitokens.\nExercise 156. Aswitching network is a directed graph, where edges are called wires\nand node are called switches . Each thread shepherds a token through the network.\nSwitches and tokens are allowed to have internal states. A token arrives at a switch\nvia an input wire. In one atomic step, the switch absorbs the token, changes its\nstate and possibly the token\u2019s state, and emits the token on an output wire. Here,\nfor simplicity, switches have two input and output wires. Note that switching\nnetworks are more powerful than balancing networks, since switches can have\narbitrary state (instead of a single bit) and tokens also have state.\nAnadding network is a switching network that allows threads to add (or sub-\ntract) arbitrary values.\nWe say that a token is in front of a switch if it is on one of the switch\u2019s input\nwires. Start with the network in a quiescent state q0, where the next token to\nrun will take value 0. Imagine we have one token tof weightaandn\u20131 tokens\nt1,:::,tn\u00001all of weight b, whereb>a , each on a distinct input wire. Denote by\nSthe set of switches that ttraverses if it traverses the network by starting in q0.\n12.12 Exercises 297\nProve that if we run the t1,:::,tn\u00001one at a time though the network, we can\nhalt eachtiin front of a switch of S.\nAt the end of this construction, n\u00001 tokens are in front of switches of S.\nSince switches have two input wires, it follows that t\u2019s path through the network\nencompasses at least n\u00001 switches, so any adding network must have depth", "doc_id": "130e407d-a072-4509-9554-e299f1841c0f", "embedding": null, "doc_hash": "006478635991cd71e7e4de588d35e3ef827d7bef071b0648d68bf35851955eb5", "extra_info": null, "node_info": {"start": 762526, "end": 765923}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d3d21100-97ce-405b-9675-ead70847721c", "3": "d53b67ee-f0e2-49b6-aa20-955b0ca96b2c"}}, "__type__": "1"}, "d53b67ee-f0e2-49b6-aa20-955b0ca96b2c": {"__data__": {"text": "on a distinct input wire. Denote by\nSthe set of switches that ttraverses if it traverses the network by starting in q0.\n12.12 Exercises 297\nProve that if we run the t1,:::,tn\u00001one at a time though the network, we can\nhalt eachtiin front of a switch of S.\nAt the end of this construction, n\u00001 tokens are in front of switches of S.\nSince switches have two input wires, it follows that t\u2019s path through the network\nencompasses at least n\u00001 switches, so any adding network must have depth at\nleastn\u00001, wherenis the maximum number of concurrent tokens. This bound\nis discouraging because it implies that the size of the network depends on the\nnumber of threads (also true for CombiningTree s, but not counting networks),\nand that the network has inherently high latency.\nExercise 157. Extend the proof of Exercise 156 to show that a linearizable count-\ning network has depth at least n.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n13Concurrent Hashing\nand Natural Parallelism\n13.1 Introduction\nIn earlier chapters, we studied how to extract parallelism from data struc-\ntures like queues, stacks, and counters, that seemed to provide few opportu-\nnities for parallelism. In this chapter we take the opposite approach. We study\nconcurrent hashing , a problem that seems to be \u201cnaturally parallelizable\u201d or, using\na more technical term, disjoint\u2013access\u2013parallel , meaning that concurrent method\ncalls are likely to access disjoint locations, implying that there is little need for\nsynchronization.\nHashing is a technique commonly used in sequential Set implementations to\nensure that contains (),add(), and remove () calls take constant average time.\nThe concurrent Set implementations studied in Chapter 9 required time linear\nin the size of the set. In this chapter, we study ways to make hashing concurrent,\nsometimes using locks and sometimes not. Even though hashing seems natu-\nrally parallelizable, devising an effective concurrent hash algorithm is far from\ntrivial.\nAs in earlier chapters, the Setinterface provides the following methods, which\nreturn Boolean values:\n\u0004add(x) addsxto the set. Returns true ifxwas absent, and false otherwise,\n\u0004remove (x) removesxfrom the set. Returns true ifxwas present, and false\notherwise, and\n\u0004contains (x) returns true ifxis present, and false otherwise.\nWhen designing set implementations, we need to keep the following principle in\nmind: we can buy more memory, but we cannot buy more time. Given a choice\nbetween an algorithm that runs faster but consumes more memory, and a slower\nalgorithm that consumes less memory, we tend to prefer the faster algorithm\n(within reason).\nAhash set (sometimes called a hash table ) is an ef\ufb01cient way to implement a\nset. A hash set is typically implemented as an array, called the table . Each table\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00013-7\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.299\n300 Chapter 13 Concurrent Hashing and Natural Parallelism\nentry is a reference to one or more items . Ahash function maps items to integers\nso that distinct items usually map to distinct values. (Java provides each object\nwith a hashCode () method that serves this purpose.) T o add, remove, or test an\nitem for membership, apply the hash function to the item (modulo the table size)\nto identify the table entry associated with that item. (We call this step hashing the\nitem.)\nIn some hash-based set algorithms, each", "doc_id": "d53b67ee-f0e2-49b6-aa20-955b0ca96b2c", "embedding": null, "doc_hash": "27071a24d83aa0a0899ece3860c5a6029119b171a07c7373703e960a1c63d452", "extra_info": null, "node_info": {"start": 765971, "end": 769461}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "130e407d-a072-4509-9554-e299f1841c0f", "3": "15b6aeb1-fbeb-437b-b807-f4aebe2ee102"}}, "__type__": "1"}, "15b6aeb1-fbeb-437b-b807-f4aebe2ee102": {"__data__": {"text": "\u00a9 2012 by Elsevier Inc. All rights reserved.299\n300 Chapter 13 Concurrent Hashing and Natural Parallelism\nentry is a reference to one or more items . Ahash function maps items to integers\nso that distinct items usually map to distinct values. (Java provides each object\nwith a hashCode () method that serves this purpose.) T o add, remove, or test an\nitem for membership, apply the hash function to the item (modulo the table size)\nto identify the table entry associated with that item. (We call this step hashing the\nitem.)\nIn some hash-based set algorithms, each table entry refers to a single item, an\napproach known as open addressing . In others, each table entry refers to a set of\nitems, traditionally called a bucket , an approach known as closed addressing .\nAny hash set algorithm must deal with collisions : what to do when two distinct\nitems hash to the same table entry. Open-addressing algorithms typically resolve\ncollisions by applying alternative hash functions to test alternative table entries.\nClosed-addressing algorithms place colliding items in the same bucket, until that\nbucket becomes too full. In both kinds of algorithms, it is sometimes necessary to\nresize the table. In open-addressing algorithms, the table may become too full to\n\ufb01nd alternative table entries, and in closed-addressing algorithms, buckets may\nbecome too large to search ef\ufb01ciently.\nAnecdotal evidence suggests that in most applications, sets are subject to\nthe following distribution of method calls: 90% contains (), 9% add(), and\n1%remove () calls. As a practical matter, sets are more likely to grow than to\nshrink, so we focus here on extensible hashing in which hash sets only grow\n(shrinking them is a problem for the exercises).\nIt is easier to make closed-addressing hash set algorithms parallel, so we con-\nsider them \ufb01rst.\n13.2 Closed-Address Hash Sets\nPragma 13.2.1. Here and elsewhere, we use the standard Java List<T> inter-\nface (in package java.util.List ). A List<T> is an ordered collection of T\nobjects, where Tis a type. Here, we make use of the following List methods:\nadd(x) appendsxto the end of the list, get(i) returns (but does not remove)\nthe item at position i,contains (x) returns true if the list contains x. There\nare many more.\nTheList interface can be implemented by a number of classes. Here, it\nis convenient to use the ArrayList class.\nWe start by de\ufb01ning a base hash set implementation common to all the con-\ncurrent closed-addressing hash sets we consider here. The BaseHashSet<T> class\nis an abstract class, that is, it does not implement all its methods. Later, we look\nat three alternative synchronization techniques: one using a single coarse-grained\nlock, one using a \ufb01xed-size array of locks, and one using a resizable array of locks.\n13.2 Closed-Address Hash Sets 301\n1public abstract class BaseHashSet<T> {\n2 protected List<T>[] table;\n3 protected int setSize;\n4 public BaseHashSet( int capacity) {\n5 setSize = 0;\n6 table = (List<T>[]) new List[capacity];\n7 for (int i = 0; i < capacity; i++) {\n8 table[i] = new ArrayList<T>();\n9 }\n10 }\n11 ...\n12 }\nFigure 13.1 BaseHashSet<T> class: \ufb01elds and constructor.\nFig. 13.1 shows the base hash set\u2019s \ufb01elds and constructor. The table [] \ufb01eld is an\narray of buckets, each of which is a set implemented as a list (Line 2). We use\nArrayList<T> lists for convenience, supporting the standard", "doc_id": "15b6aeb1-fbeb-437b-b807-f4aebe2ee102", "embedding": null, "doc_hash": "39772efdad79faa25e2408f77630b5101b5c1f397f739eca5545cb14c4f3c40a", "extra_info": null, "node_info": {"start": 769389, "end": 772760}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d53b67ee-f0e2-49b6-aa20-955b0ca96b2c", "3": "9fe57de2-5dee-4eca-83b0-daa46a76283f"}}, "__type__": "1"}, "9fe57de2-5dee-4eca-83b0-daa46a76283f": {"__data__": {"text": "new List[capacity];\n7 for (int i = 0; i < capacity; i++) {\n8 table[i] = new ArrayList<T>();\n9 }\n10 }\n11 ...\n12 }\nFigure 13.1 BaseHashSet<T> class: \ufb01elds and constructor.\nFig. 13.1 shows the base hash set\u2019s \ufb01elds and constructor. The table [] \ufb01eld is an\narray of buckets, each of which is a set implemented as a list (Line 2). We use\nArrayList<T> lists for convenience, supporting the standard sequential add(),\nremove (), and contains () methods. The setSize \ufb01eld is the number of items\nin the table (Line 3). We sometimes refer to the length of the table [] array, that\nis, the number of buckets in it, as its capacity .\nThe BaseHashSet<T> class does not implement the following abstract\nmethods: acquire (x) acquires the locks necessary to manipulate item x,\nrelease (x) releases them, policy () decides whether to resize the set, and\nresize () doubles the capacity of the table [] array. The acquire (x) method\nmust be reentrant (Chapter 8 ,Section 8.4 ), meaning that if a thread that has\nalready called acquire (x) makes the same call, then it will proceed without\ndeadlocking with itself.\nFig. 13.2 shows the contains (x) and add(x) methods of the\nBaseHashSet<T> class. Each method \ufb01rst calls acquire (x) to perform the\nnecessary synchronization, then enters a try block whose finally block calls\nrelease (x). The contains (x) method simply tests whether xis present in the\nassociated bucket (Line 17), while add(x) addsxto the list if it is not already\npresent (Line 26).\nHow big should the bucket array be to ensure that method calls take constant\nexpected time? Consider an add(x) call. The \ufb01rst step, hashing x, takes constant\ntime. The second step, adding the item to the bucket, requires traversing a linked\nlist. This traversal takes constant expected time only if the lists have constant\nexpected length, so the table capacity should be proportional to the number of\nitems in the table. This number may vary unpredictably over time, so to ensure\nthat method call times remain (more-or-less) constant, we must resize the table\nevery now and then to ensure that list lengths remain (more-or-less) constant.\nWe still need to decide when to resize the hash set, and how the resize ()\nmethod synchronizes with the others. There are many reasonable alternatives.\nFor closed-addressing algorithms, one simple strategy is to resize the set when\nthe average bucket size exceeds a \ufb01xed threshold. An alternative policy employs\ntwo \ufb01xed integer quantities: the bucket threshold and the global threshold .\n302 Chapter 13 Concurrent Hashing and Natural Parallelism\n13 public boolean contains(T x) {\n14 acquire(x);\n15 try {\n16 int myBucket = x.hashCode() % table.length;\n17 return table[myBucket].contains(x);\n18 }finally {\n19 release(x);\n20 }\n21 }\n22 public boolean add(T x) {\n23 boolean result = false ;\n24 acquire(x);\n25 try {\n26 int myBucket = Math.abs(x.hashCode() % table.length);\n27 if(! table[myBucket].contains(x)) {\n28 table[myBucket].add(x);\n29 result = true ;\n30 size++;\n31 }\n32 }finally {\n33 release(x);\n34 }\n35 if(policy())\n36 resize();\n37 return result;\n38 }\nFigure 13.2 BaseHashSet<T> class: the contains() andadd() methods hash the", "doc_id": "9fe57de2-5dee-4eca-83b0-daa46a76283f", "embedding": null, "doc_hash": "73f122be7782ac17b6b1ffc8191fcbe0aaae7fb71f142d0736c9ede1ae790827", "extra_info": null, "node_info": {"start": 772907, "end": 776055}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "15b6aeb1-fbeb-437b-b807-f4aebe2ee102", "3": "144ba5ce-e885-41f9-a124-3c0061eeafcb"}}, "__type__": "1"}, "144ba5ce-e885-41f9-a124-3c0061eeafcb": {"__data__": {"text": "public boolean add(T x) {\n23 boolean result = false ;\n24 acquire(x);\n25 try {\n26 int myBucket = Math.abs(x.hashCode() % table.length);\n27 if(! table[myBucket].contains(x)) {\n28 table[myBucket].add(x);\n29 result = true ;\n30 size++;\n31 }\n32 }finally {\n33 release(x);\n34 }\n35 if(policy())\n36 resize();\n37 return result;\n38 }\nFigure 13.2 BaseHashSet<T> class: the contains() andadd() methods hash the item to\nchoose a bucket.\n\u0004If more than, say, 1 =4 of the buckets exceed the bucket threshold, then double\nthe table capacity, or\n\u0004If any single bucket exceeds the global threshold, then double the table\ncapacity.\nBoth these strategies work well in practice, as do others. Open-addressing\nalgorithms are slightly more complicated, and are discussed later.\n13.2.1 A Coarse-Grained Hash Set\nFig. 13.3 shows the CoarseHashSet<T> class\u2019s \ufb01elds, constructor, acquire (x),\nandrelease (x) methods. The constructor \ufb01rst initializes its superclass (Line 4).\nSynchronization is provided by a single reentrant lock (Line 2), acquired by\nacquire (x) (Line 8) and released by release (x) (Line 11).\nFig. 13.4 shows the CoarseHashSet<T> class\u2019s policy () and resize ()\nmethods. We use a simple policy: we resize when the average bucket length\nexceeds 4 (Line 16). The resize () method locks the set (Line 20), and checks\nthat no other thread has resized the table in the meantime (Line 23). It then\nallocates and initializes a new table with double the capacity (Lines 25\u201329) and\ntransfers items from the old to the new buckets (Lines 30\u201334). Finally, it unlocks\nthe set (Line 36).\n13.2 Closed-Address Hash Sets 303\n1public class CoarseHashSet<T> extends BaseHashSet<T>{\n2 final Lock lock;\n3 CoarseHashSet( int capacity) {\n4 super (capacity);\n5 lock = new ReentrantLock();\n6 }\n7 public final void acquire(T x) {\n8 lock.lock();\n9 }\n10 public void release(T x) {\n11 lock.unlock();\n12 }\n13 ...\n14 }\nFigure 13.3 CoarseHashSet<T> class: \ufb01elds, constructor, acquire() , and release()\nmethods.\n15 public boolean policy() {\n16 return setSize / table.length > 4;\n17 }\n18 public void resize() {\n19 int oldCapacity = table.length;\n20 lock.lock();\n21 try {\n22 if(oldCapacity != table.length) {\n23 return ;// someone beat us to it\n24 }\n25 int newCapacity = 2 *oldCapacity;\n26 List<T>[] oldTable = table;\n27 table = (List<T>[]) new List[newCapacity];\n28 for (int i = 0; i < newCapacity; i++)\n29 table[i] = new ArrayList<T>();\n30 for (List<T> bucket : oldTable) {\n31 for (T x : bucket) {\n32 table[x.hashCode() % table.length].add(x);\n33 }\n34 }\n35 }finally {\n36 lock.unlock();\n37 }\n38 }\nFigure 13.4 CoarseHashSet<T> class: the policy() andresize() methods.\n13.2.2 A Striped Hash Set\nLike the coarse-grained list studied in Chapter 9, the coarse-grained hash\nset shown in the last section is easy to understand and easy to implement.\nUnfortunately, it is also a sequential bottleneck. Method calls take effect in", "doc_id": "144ba5ce-e885-41f9-a124-3c0061eeafcb", "embedding": null, "doc_hash": "e4f7e3f436eac5afb9a2bd3e2cebfd5ba85654f28e4ded7bc8fe39492be413c0", "extra_info": null, "node_info": {"start": 776036, "end": 778913}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9fe57de2-5dee-4eca-83b0-daa46a76283f", "3": "e3f5c41f-9d4f-4465-964f-a03f80ba13cd"}}, "__type__": "1"}, "e3f5c41f-9d4f-4465-964f-a03f80ba13cd": {"__data__": {"text": "{\n31 for (T x : bucket) {\n32 table[x.hashCode() % table.length].add(x);\n33 }\n34 }\n35 }finally {\n36 lock.unlock();\n37 }\n38 }\nFigure 13.4 CoarseHashSet<T> class: the policy() andresize() methods.\n13.2.2 A Striped Hash Set\nLike the coarse-grained list studied in Chapter 9, the coarse-grained hash\nset shown in the last section is easy to understand and easy to implement.\nUnfortunately, it is also a sequential bottleneck. Method calls take effect in a\none-at-a-time order, even when there is no logical reason for them to do so.\n304 Chapter 13 Concurrent Hashing and Natural Parallelism\nWe now present a closed address hash table with greater parallelism and less\nlock contention. Instead of using a single lock to synchronize the entire set, we\nsplit the set into independently synchronized pieces. We introduce a technique\ncalled lock striping , which will be useful for other data structures as well. Fig. 13.5\nshows the \ufb01elds and constructor for the StripedHashSet<T> class. The set is ini-\ntialized with an array locks [] ofLlocks, and an array table [] ofN=Lbuckets,\nwhere each bucket is an unsynchronized List<T> . Although these arrays are ini-\ntially of the same capacity, table [] will grow when the set is resized, but lock []\nwill not. Every now and then, we double the table capacity Nwithout chang-\ning the lock array size L, so that lock ieventually protects each table entry j,\nwherej=i(modL). The acquire (x) and release (x) methods use x\u2019s hash\ncode to pick which lock to acquire or release. An example illustrating how a\nStripedHashSet<T> is resized appears in Fig. 13.6 .\nThere are two reasons not to grow the lock array every time we grow the table:\n\u0004Associating a lock with every table entry could consume too much space, espe-\ncially when tables are large and contention is low.\n\u0004While resizing the table is straightforward, resizing the lock array (while in\nuse) is more complex, as discussed in Section 13.2.3 .\nResizing a StripedHashSet (Fig. 13.7 ) is almost identical to resizing a\nCoarseHashSet . One difference is that resize () acquires the locks in lock []\nin ascending order (Lines 18\u201320). It cannot deadlock with a contains (),add(),\norremove () call because these methods acquire only a single lock. A resize ()\ncall cannot deadlock with another resize () call because both calls start without\nholding any locks, and acquire the locks in the same order. What if two or more\nthreads try to resize at the same time? As in the CoarseHashSet<T> , when a\nthread starts to resize the table, it records the current table capacity. If, after it has\nacquired all the locks, it discovers that some other thread has changed the table\n1public class StripedHashSet<T> extends BaseHashSet<T>{\n2 final ReentrantLock[] locks;\n3 public StripedHashSet( int capacity) {\n4 super (capacity);\n5 locks = new Lock[capacity];\n6 for (int j = 0; j < locks.length; j++) {\n7 locks[j] = new ReentrantLock();\n8 }\n9 }\n10 public final void acquire(T x) {\n11 locks[x.hashCode() % locks.length].lock();\n12 }\n13 public void release(T x) {\n14 locks[x.hashCode() % locks.length].unlock();\n15 }\nFigure 13.5 StripedHashSet<T> class: \ufb01elds, constructor, acquire() , and release()\nmethods.\n13.2 Closed-Address Hash Sets", "doc_id": "e3f5c41f-9d4f-4465-964f-a03f80ba13cd", "embedding": null, "doc_hash": "33d857d81e47216ad43089e8f28caa26dd1e4f58f22cda7cd7654478b0ba6deb", "extra_info": null, "node_info": {"start": 778877, "end": 782088}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "144ba5ce-e885-41f9-a124-3c0061eeafcb", "3": "58383262-46e9-4fd9-9f9c-e07d86eed030"}}, "__type__": "1"}, "58383262-46e9-4fd9-9f9c-e07d86eed030": {"__data__": {"text": "Lock[capacity];\n6 for (int j = 0; j < locks.length; j++) {\n7 locks[j] = new ReentrantLock();\n8 }\n9 }\n10 public final void acquire(T x) {\n11 locks[x.hashCode() % locks.length].lock();\n12 }\n13 public void release(T x) {\n14 locks[x.hashCode() % locks.length].unlock();\n15 }\nFigure 13.5 StripedHashSet<T> class: \ufb01elds, constructor, acquire() , and release()\nmethods.\n13.2 Closed-Address Hash Sets 305\n0\n123456\n78\n9\n10\n1112\n13\n14\n150\n1\n2\n3\n4\n5\n6\n75 (mod 8) 5 i\n13 (mod 8) 5 ii lockstable\nFigure 13.6 Resizing a StripedHashSet lock-based hash table. As the table grows, the\nstriping is adjusted to ensure that each lock covers 2N/Lentries. In the \ufb01gure above, N=16\nandL= 8. When Nis doubled from 8 to 16, the memory is striped so that lock i= 5 for\nexample covers both locations that are equal to 5 modulo L.\ncapacity (Line 23), then it releases the locks and gives up. (It could just double\nthe table size anyway, since it already holds all the locks.)\nOtherwise, it creates a new table [] array with twice the capacity (Line 25),\nand transfer items from the old table to the new (Line 30). Finally, it releases\nthe locks (Line 36). Because the initializeFrom () method calls add(), it may\ntrigger nested calls to resize (). We leave it as an exercise to check that nested\nresizing works correctly in this and later hash set implementations.\nT o summarize, striped locking permits more concurrency than a single coarse-\ngrained lock because method calls whose items hash to different locks can proceed\nin parallel. The add(),contains (), and remove () methods take constant expected\ntime, but resize () takes linear time and is a \u201cstop-the-world\u201d operation: it halts\nall concurrent method calls while it increases the table\u2019s capacity.\n13.2.3 A Re\ufb01nable Hash Set\nWhat if we want to re\ufb01ne the granularity of locking as the table size grows, so\nthat the number of locations in a stripe does not continuously grow? Clearly, if\nwe want to resize the lock array, then we need to rely on another form of syn-\nchronization. Resizing is rare, so our principal goal is to devise a way to permit\nthe lock array to be resized without substantially increasing the cost of normal\nmethod calls.\n306 Chapter 13 Concurrent Hashing and Natural Parallelism\n16 public void resize() {\n17 int oldCapacity = table.length;\n18 for (Lock lock : locks) {\n19 lock.lock();\n20 }\n21 try {\n22 if(oldCapacity != table.length) {\n23 return ;// someone beat us to it\n24 }\n25 int newCapacity = 2 *oldCapacity;\n26 List<T>[] oldTable = table;\n27 table = (List<T>[]) new List[newCapacity];\n28 for (int i = 0; i < newCapacity; i++)\n29 table[i] = new ArrayList<T>();\n30 for (List<T> bucket : oldTable) {\n31 for (T x : bucket) {\n32 table[x.hashCode() % table.length].add(x);\n33 }\n34 }\n35 }finally {\n36 for (Lock lock : locks) {\n37 lock.unlock();\n38 }\n39 }\n40 }\nFigure 13.7 StripedHashSet<T> class: to resize the set, lock each lock in order, then check\nthat no other thread has resized the table in the meantime.\n1public class RefinableHashSet<T> extends", "doc_id": "58383262-46e9-4fd9-9f9c-e07d86eed030", "embedding": null, "doc_hash": "9c4a2c9ec8f8e45c53f9e2f06ca52e639f583f7da91ab4aa1f9e87d951ddf454", "extra_info": null, "node_info": {"start": 782131, "end": 785138}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e3f5c41f-9d4f-4465-964f-a03f80ba13cd", "3": "cf01ef30-db47-4095-96e4-846b421d6994"}}, "__type__": "1"}, "cf01ef30-db47-4095-96e4-846b421d6994": {"__data__": {"text": "= new ArrayList<T>();\n30 for (List<T> bucket : oldTable) {\n31 for (T x : bucket) {\n32 table[x.hashCode() % table.length].add(x);\n33 }\n34 }\n35 }finally {\n36 for (Lock lock : locks) {\n37 lock.unlock();\n38 }\n39 }\n40 }\nFigure 13.7 StripedHashSet<T> class: to resize the set, lock each lock in order, then check\nthat no other thread has resized the table in the meantime.\n1public class RefinableHashSet<T> extends BaseHashSet<T>{\n2 AtomicMarkableReference<Thread> owner;\n3 volatile ReentrantLock[] locks;\n4 public RefinableHashSet( int capacity) {\n5 super (capacity);\n6 locks = new ReentrantLock[capacity];\n7 for (int i = 0; i < capacity; i++) {\n8 locks[i] = new ReentrantLock();\n9 }\n10 owner = new AtomicMarkableReference<Thread>( null ,false );\n11 }\n12 ...\n13 }\nFigure 13.8 RefinableHashSet<T> class: \ufb01elds and constructor.\nFig. 13.8 shows the \ufb01elds and constructor for the RefinableHashSet<T> class.\nT o add a higher level of synchronization, we introduce a globally shared owner\n\ufb01eld that combines a Boolean value with a reference to a thread. Normally, the\nBoolean value is false , meaning that the set is not in the middle of resizing.\nWhile a resizing is in progress, however, the Boolean value is true, and the\nassociated reference indicates the thread that is in charge of resizing. These\n13.2 Closed-Address Hash Sets 307\ntwo values are combined in an AtomicMarkableReference<Thread> to allow\nthem to be modi\ufb01ed atomically (see Pragma 9.8.1 inChapter 9 ). We use the\nowner as a mutual exclusion \ufb02ag between the resize () method and any of the\nadd() methods, so that while resizing, there will be no successful updates, and\nwhile updating, there will be no successful resizes. Every add() call must read\ntheowner \ufb01eld. Because resizing is rare, the value of owner should usually be\ncached.\nEach method locks the bucket for xby calling acquire (x), shown in Fig. 13.9 .\nIt spins until no other thread is resizing the set (Lines 19\u201321), and then reads\nthe lock array (Line 22). It then acquires the item\u2019s lock (Line 24), and checks\nagain, this time while holding the locks (Line 26), to make sure no other thread\nis resizing, and that no resizing took place between Lines 21and 26.\nIf it passes this test, the thread can proceed. Otherwise, the locks it has\nacquired could be out-of-date because of an ongoing update, so it releases\nthem and starts over. When starting over, it will \ufb01rst spin until the current\nresize completes (Lines 19\u201321) before attempting to acquire the locks again. The\nrelease (x) method releases the locks acquired by acquire (x).\nThe resize () method is almost identical to the resize () method for the\nStripedHashSet class. The one difference appears on Line 46: instead of acquir-\ning all the locks in lock [], the method calls quiesce () (Fig. 13.10 ) to ensure that\nno other thread is in the middle of an add(),remove (), or contains () call. The\nquiesce () method visits each lock and waits until it is unlocked.\n14 public void acquire(T x) {\n15 boolean [] mark = { true };\n16 Thread me = Thread.currentThread();\n17 Thread who;\n18 while (true ) {\n19 do{\n20 who = owner.get(mark);\n21 }while (mark[0] && who != me);\n22", "doc_id": "cf01ef30-db47-4095-96e4-846b421d6994", "embedding": null, "doc_hash": "12b4e307b1ae076e6f0e45383668d630425e0957189973910897b2328cd91ea8", "extra_info": null, "node_info": {"start": 785134, "end": 788283}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "58383262-46e9-4fd9-9f9c-e07d86eed030", "3": "68f3c2f7-cb24-4c9f-9011-a09f2578a70c"}}, "__type__": "1"}, "68f3c2f7-cb24-4c9f-9011-a09f2578a70c": {"__data__": {"text": "46: instead of acquir-\ning all the locks in lock [], the method calls quiesce () (Fig. 13.10 ) to ensure that\nno other thread is in the middle of an add(),remove (), or contains () call. The\nquiesce () method visits each lock and waits until it is unlocked.\n14 public void acquire(T x) {\n15 boolean [] mark = { true };\n16 Thread me = Thread.currentThread();\n17 Thread who;\n18 while (true ) {\n19 do{\n20 who = owner.get(mark);\n21 }while (mark[0] && who != me);\n22 ReentrantLock[] oldLocks = locks;\n23 ReentrantLock oldLock = oldLocks[x.hashCode() % oldLocks.length];\n24 oldLock.lock();\n25 who = owner.get(mark);\n26 if((!mark[0] || who == me) && locks == oldLocks) {\n27 return ;\n28 }else {\n29 oldLock.unlock();\n30 }\n31 }\n32 }\n33 public void release(T x) {\n34 locks[x.hashCode() % locks.length].unlock();\n35 }\nFigure 13.9 RefinableHashSet<T> class: acquire() andrelease() methods.\n308 Chapter 13 Concurrent Hashing and Natural Parallelism\n36 public void resize() {\n37 int oldCapacity = table.length;\n38 boolean [] mark = { false };\n39 int newCapacity = 2 *oldCapacity;\n40 Thread me = Thread.currentThread();\n41 if(owner.compareAndSet( null , me, false ,true )) {\n42 try {\n43 if(table.length != oldCapacity) { // someone else resized first\n44 return ;\n45 }\n46 quiesce();\n47 List<T>[] oldTable = table;\n48 table = (List<T>[]) new List[newCapacity];\n49 for (int i = 0; i < newCapacity; i++)\n50 table[i] = new ArrayList<T>();\n51 locks = new ReentrantLock[newCapacity];\n52 for (int j = 0; j < locks.length; j++) {\n53 locks[j] = new ReentrantLock();\n54 }\n55 initializeFrom(oldTable);\n56 }finally {\n57 owner.set( null ,false );\n58 }\n59 }\n60 }\nFigure 13.10 RefinableHashSet<T> class: resize() method.\n61 protected void quiesce() {\n62 for (ReentrantLock lock : locks) {\n63 while (lock.isLocked()) {}\n64 }\n65 }\nFigure 13.11 RefinableHashSet<T> class: quiesce() method.\nTheacquire () and the resize () methods guarantee mutually exclusive access\nvia the \ufb02ag principle using the mark \ufb01eld of the owner \ufb02ag and the table\u2019s locks\narray: acquire () \ufb01rst acquires its locks and then reads the mark \ufb01eld, while\nresize () \ufb01rst sets mark and then reads the locks during the quiesce () call. This\nordering ensures that any thread that acquires the locks after quiesce () has com-\npleted will see that the set is in the processes of being resized, and will back off\nuntil the resizing is complete. Similarly, resize () will \ufb01rst set the mark \ufb01eld, then\nread the locks, and will not proceed while any add(),remove (), or contains ()\ncall\u2019s lock is set.\nT o summarize, we have seen that one can design a hash table in which both\nthe number of buckets and the number of locks can be continuously resized.\nOne limitation of this algorithm is that threads cannot access the items in the\ntable during a resize.\n13.3 A Lock-Free Hash Set 309\n13.3 A Lock-Free Hash Set\nThe next step is to make the hash set implementation lock-free, and to make\nresizing incremental , meaning that each add() method call performs a small", "doc_id": "68f3c2f7-cb24-4c9f-9011-a09f2578a70c", "embedding": null, "doc_hash": "e48a5aadb5864b36fe0b8349dc82ff3b2d5e50ff6b8f9f955d1db1273f43f5aa", "extra_info": null, "node_info": {"start": 788251, "end": 791238}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "cf01ef30-db47-4095-96e4-846b421d6994", "3": "fde55601-f138-436d-89f2-4d814eb5d60a"}}, "__type__": "1"}, "fde55601-f138-436d-89f2-4d814eb5d60a": {"__data__": {"text": "the locks, and will not proceed while any add(),remove (), or contains ()\ncall\u2019s lock is set.\nT o summarize, we have seen that one can design a hash table in which both\nthe number of buckets and the number of locks can be continuously resized.\nOne limitation of this algorithm is that threads cannot access the items in the\ntable during a resize.\n13.3 A Lock-Free Hash Set 309\n13.3 A Lock-Free Hash Set\nThe next step is to make the hash set implementation lock-free, and to make\nresizing incremental , meaning that each add() method call performs a small frac-\ntion of the work associated with resizing. This way, we do not need to \u201cstop-the-\nworld\u201d to resize the table. Each of the contains (),add(), and remove () methods\ntakes constant expected time.\nT o make resizable hashing lock-free, it is not enough to make the individual\nbuckets lock-free, because resizing the table requires atomically moving entries\nfrom old buckets to new buckets. If the table doubles in capacity, then we must\nsplit the items in the old bucket between two new buckets. If this move is not\ndone atomically, entries might be temporarily lost or duplicated.\nWithout locks, we must synchronize using atomic methods such as\ncompareAndSet() . Unfortunately, these methods operate only on a single\nmemory location, which makes it dif\ufb01cult to move a node atomically from one\nlinked list to another.\n13.3.1 Recursive Split-Ordering\nWe now describe a hash set implementation that works by \ufb02ipping the conven-\ntional hashing structure on its head:\nInstead of moving the items among the buckets, move the buckets among the\nitems.\nMore speci\ufb01cally, keep all items in a single lock-free linked list, similar to the\nLockFreeList class studied in Chapter 9. A bucket is just a reference into the\nlist. As the list grows, we introduce additional bucket references so that no object\nis ever too far from the start of a bucket. This algorithm ensures that once an item\nis placed in the list, it is never moved, but it does require that items be inserted\naccording to a recursive split-order algorithm that we describe shortly.\nPart (b) of Fig. 13.12 illustrates a lock-free hash set implementation. It shows\ntwo components: a lock-free linked list, and an expanding array of references\ninto the list. These references are logical buckets. Any item in the hash set can\nbe reached by traversing the list from its head, while the bucket references pro-\nvide short-cuts into the list to minimize the number of list nodes traversed when\nsearching. The principal challenge is ensuring that the bucket references into the\nlist remain well-distributed as the number of items in the set grows. Bucket ref-\nerences should be spaced evenly enough to allow constant-time access to any\nnode. It follows that new buckets must be created and assigned to sparsely cov-\nered regions in the list.\nAs before, the capacity Nof the hash set is always a power of two. The bucket\narray initially has Capacity 2 and all bucket references are null, except for the\nbucket at index 0, which refers to an empty list. We use the variable bucketSize\nto denote this changing capacity of the bucket structure. Each entry in the bucket\n310 Chapter 13 Concurrent Hashing and Natural Parallelism\n0\n1\n2\n30\n1\n2\n3(a) (b)\n000 001 100 011 101 110 111\n0 4 1 3 6 7 5 2010 000 001 100 011 101 111\n0 4 1 6 7 5\nFigure 13.12 This \ufb01gure explains the recursive nature of the split ordering. Part (a) shows a split-ordered list\nconsisting of two buckets. The array of buckets refer into a single linked list. The split-ordered keys (above\neach node) are the reverse of", "doc_id": "fde55601-f138-436d-89f2-4d814eb5d60a", "embedding": null, "doc_hash": "537457ef1bc1a1c4c37d7c6ad12884bcb42201f36e4a7c3cf4faec65279dbb61", "extra_info": null, "node_info": {"start": 791159, "end": 794742}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "68f3c2f7-cb24-4c9f-9011-a09f2578a70c", "3": "8df6acac-b47d-4c83-a66e-9ce5415d1354"}}, "__type__": "1"}, "8df6acac-b47d-4c83-a66e-9ce5415d1354": {"__data__": {"text": "denote this changing capacity of the bucket structure. Each entry in the bucket\n310 Chapter 13 Concurrent Hashing and Natural Parallelism\n0\n1\n2\n30\n1\n2\n3(a) (b)\n000 001 100 011 101 110 111\n0 4 1 3 6 7 5 2010 000 001 100 011 101 111\n0 4 1 6 7 5\nFigure 13.12 This \ufb01gure explains the recursive nature of the split ordering. Part (a) shows a split-ordered list\nconsisting of two buckets. The array of buckets refer into a single linked list. The split-ordered keys (above\neach node) are the reverse of the bitwise representation of the items\u2019 keys. The active bucket array entries\n0 and 1 have special sentinel nodes within the list (square nodes), while other (ordinary) nodes are round.\nItems 4 (whose reverse bit order is \u201c001\u201d) and 6 (whose reverse bit order is \u201c011\u201d) are in Bucket 0 since\nthe LSB of the original key, is \u201c0.\u201d Items 5 and 7 (whose reverse bit orders are \u201c101\u201d and \u201c111\u201d respectively)\nare in Bucket 1, since the LSB of their original key is 1. Part (b) shows how each of the two buckets is split in\nhalf once the table capacity grows from 2 buckets to four. The reverse bit values of the two added Buckets\n2 and 3 happen to perfectly split the Buckets 0 and 1.\narray is initialized when \ufb01rst accessed, and subsequently refers to a node in\nthe list.\nWhen an item with hash code kis inserted, removed, or searched for, the hash\nset uses bucket index k(modN). As with earlier hash set implementations, we\ndecide when to double the table capacity by consulting a policy () method. Here,\nhowever, the table is resized incrementally by the methods that modify it, so there\nis no explicit resize () method. If the table capacity is 2i, then the bucket index is\nthe integer represented by the key\u2019s ileast signi\ufb01cant bits (LSBs); in other words,\neach bucketbcontains items each of whose hash code ksatis\ufb01esk=b(mod 2i).\nBecause the hash function depends on the table capacity, we must be careful\nwhen the table capacity changes. An item inserted before the table was resized\nmust be accessible afterwards from both its previous and current buckets. When\nthe capacity grows to 2i+1, the items in bucket bare split between two buck-\nets: those for which k=b(mod 2i+1) remain in bucket b, while those for which\nk=b+ 2i(mod 2i+1) migrate to bucket b+ 2i. Here is the key idea behind the\nalgorithm: we ensure that these two groups of items are positioned one after the\nother in the list, so that splitting bucket bis achieved by simply setting bucket\nb+ 2iafter the \ufb01rst group of items and before the second. This organization keeps\neach item in the second group accessible from bucket b.\nAs depicted in Fig. 13.12, items in the two groups are distinguished by their\nithbinary digits (counting backwards, from least-signi\ufb01cant to most-signi\ufb01cant).\nThose with digit 0 belong to the \ufb01rst group, and those with 1 to the second. The\nnext hash table doubling will cause each group to split again into two groups\ndifferentiated by the ( i+ 1)stbit, and so on. For example, the items 4 (\u201c100\u201d\nbinary) and 6 (\u201c110\u201d) share the same least signi\ufb01cant bit. When the table capacity\nis 21, they are in the same bucket, but when it grows to 22, they will be in distinct\nbuckets because their second bits differ.\n13.3 A Lock-Free Hash Set 311\nThis process induces a total order on items, which we call recursive", "doc_id": "8df6acac-b47d-4c83-a66e-9ce5415d1354", "embedding": null, "doc_hash": "f1337ec14c359f615d69ffcf23ccbed73c9ac0763a8122f4df6b8fb54ae3bc42", "extra_info": null, "node_info": {"start": 794793, "end": 798090}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "fde55601-f138-436d-89f2-4d814eb5d60a", "3": "49b9fad6-8e87-4b79-9bb5-e59356e9cabc"}}, "__type__": "1"}, "49b9fad6-8e87-4b79-9bb5-e59356e9cabc": {"__data__": {"text": "and those with 1 to the second. The\nnext hash table doubling will cause each group to split again into two groups\ndifferentiated by the ( i+ 1)stbit, and so on. For example, the items 4 (\u201c100\u201d\nbinary) and 6 (\u201c110\u201d) share the same least signi\ufb01cant bit. When the table capacity\nis 21, they are in the same bucket, but when it grows to 22, they will be in distinct\nbuckets because their second bits differ.\n13.3 A Lock-Free Hash Set 311\nThis process induces a total order on items, which we call recursive split-\nordering , as can be seen in Fig. 13.12. Given a key\u2019s hash code, its order is de\ufb01ned\nby its bit-reversed value.\nT o recapitulate: a split-ordered hash set is an array of buckets, where each\nbucket is a reference into a lock-free list where nodes are sorted by their\nbit-reversed hash codes. The number of buckets grows dynamically, and each\nnew bucket is initialized when accessed for the \ufb01rst time.\nT o avoid an awkward \u201ccorner case\u201d that arises when deleting a node referenced\nby a bucket reference, we add a sentinel node, which is never deleted, to the start\nof each bucket. Speci\ufb01cally, suppose the table capacity is 2i+1. The \ufb01rst time that\nbucketb+ 2iis accessed, a sentinel node is created with key b+ 2i. This node is\ninserted in the list via bucket b, the parent bucket ofb+ 2i. Under split-ordering,\nb+ 2iprecedes all items of bucket b+ 2i, since those items must end with ( i+ 1)\nbits forming the value b+ 2i. This value also comes after all the items of bucket b\nthat do not belong to b+ 2i: they have identical LSBs, but their ithbit is 0. There-\nfore, the new sentinel node is positioned in the exact list location that separates\nthe items of the new bucket from the remaining items of bucket b. T o distin-\nguish sentinel items from ordinary items, we set the most signi\ufb01cant bit (MSB)\nof ordinary items to 1, and leave the sentinel items with 0 at the MSB. Fig. 13.17\nillustrates two methods: makeOrdinaryKey() , which generates a split-ordered\nkey for an object, and makeSentinelKey() , which generates a split-ordered key\nfor a bucket index.\nFig. 13.13 illustrates how inserting a new key into the set can cause a bucket\nto be initialized. The split-order key values are written above the nodes using\n8-bit words. For instance, the split-order value of 3 is the bit-reverse of its binary\nrepresentation, which is 11000000. The square nodes are the sentinel nodes cor-\nresponding to buckets with original keys that are 0,1, and 3 modulo 4 with their\nMSB being 0. The split-order keys of ordinary (round) nodes are exactly the bit-\nreversed images of the original keys after turning on their MSB. For example,\nitems 9 and 13 are in the \u201c1 mod 4\u201d bucket, which can be recursively split in\ntwo by inserting a new node between them. The sequence of \ufb01gures describes an\nobject with hash code 10 being added when the table capacity is 4 and Buckets 0,\n1, and 3 are already initialized.\nThe table is grown incrementally, that is, there is no explicit resize operation.\nRecall that each bucket is a linked list, with nodes ordered based on the split-\nordered hash values. As mentioned earlier, the table resizing mechanism is\nindependent of the policy used to decide when to resize. T o keep the example\nconcrete, we implement the following policy: we use a shared counter to allow\nadd() calls to track the average bucket load. When the", "doc_id": "49b9fad6-8e87-4b79-9bb5-e59356e9cabc", "embedding": null, "doc_hash": "0da1f762271437e806e06e8c97456d6172967710ca755898ebf7a05bb5795910", "extra_info": null, "node_info": {"start": 798088, "end": 801445}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8df6acac-b47d-4c83-a66e-9ce5415d1354", "3": "5c28eb19-4c81-479e-ae1e-58d3c7c7484f"}}, "__type__": "1"}, "5c28eb19-4c81-479e-ae1e-58d3c7c7484f": {"__data__": {"text": "describes an\nobject with hash code 10 being added when the table capacity is 4 and Buckets 0,\n1, and 3 are already initialized.\nThe table is grown incrementally, that is, there is no explicit resize operation.\nRecall that each bucket is a linked list, with nodes ordered based on the split-\nordered hash values. As mentioned earlier, the table resizing mechanism is\nindependent of the policy used to decide when to resize. T o keep the example\nconcrete, we implement the following policy: we use a shared counter to allow\nadd() calls to track the average bucket load. When the average load crosses a\nthreshold, we double the table capacity.\nT o avoid technical distractions, we keep the array of buckets in a large,\n\ufb01xed-size array. We start out using only the \ufb01rst array entry, and use progres-\nsively more of the array as the set grows. When the add() method accesses an\nuninitialized bucket that should have been initialized given the current table\ncapacity, it initializes it. While conceptually simple, this design is far from ideal,\n312 Chapter 13 Concurrent Hashing and Natural Parallelism\n0\n1\n2\n300000000\n0001000110000000 10010001\n1011000111000000\n11100001\n010100010 8 1 3 9 7 13\n10(c)(b) (a)\n(d)0\n1\n2\n300000000\n00010001\n0100000010000000 10010001\n1011000111000000\n20 8 1 3 9 7 13\n11100001\n0\n1\n2\n300000000\n00010001\n0100000010000000 10010001\n1011000111000000\n20 8 1 3 9 7 13\n11100001\n0\n1\n2\n300000000\n00010001\n010000001000000010010001\n1011000111000000\n20 8 1 3 9 7 13\n11100001\nFigure 13.13 How the add()method places key 10 to the lock-free table. As in earlier \ufb01gures, the split-order\nkey values, expressed as 8-bit binary words, appear above the nodes. For example, the split-order value of 1\nis the bit-wise reversal of its binary representation. In Step (a) Buckets 0, 1, and 3 are initialized, but Bucket\n2 is uninitialized. In Step (b) an item with hash value 10 is inserted, causing Bucket 2 to be initialized. A new\nsentinel is inserted with split-order key 2. In Step (c) Bucket 2 is assigned a new sentinel. Finally, in Step (d),\nthe split-order ordinary key 10 is added to Bucket 2.\nsince the \ufb01xed array size limits the ultimate number of buckets. In practice, it\nwould be better to represent the buckets as a multilevel tree structure which\nwould cover the machine\u2019s full memory size, a task we leave as an exercise.\n13.3.2 The BucketList Class\nFig. 13.14 shows the \ufb01elds, constructor, and some utility methods of the\nBucketList class that implements the lock-free list used by the split-ordered\nhash set. Although this class is essentially the same as the LockFreeList\nclass, there are two important differences. The \ufb01rst is that items are sorted in\nrecursive-split order, not simply by hash code. The makeOrdinaryKey() and\nmakeSentinelKey() methods (Lines 10and 14) show how we compute these\nsplit-ordered keys. (T o ensure that reversed keys are positive, we use only the\nlower three bytes of the hash code.) Fig. 13.15 shows how the contains () method\nis modi\ufb01ed to use the split-ordered key. (As in the LockFreeList class, the\nfind (x) method returns a record containing the x\u2019s node, if it exists, along with\nthe immediately preceding and subsequent nodes.)\nThe second difference is that while the LockFreeList class uses only two\nsentinels, one at each end of the list, the BucketList<T> class places a", "doc_id": "5c28eb19-4c81-479e-ae1e-58d3c7c7484f", "embedding": null, "doc_hash": "2573b5c0d1f9f5ac13b76f2dd6b329b3d79c789e93a71ece92c0a6fe10a7111c", "extra_info": null, "node_info": {"start": 801378, "end": 804704}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "49b9fad6-8e87-4b79-9bb5-e59356e9cabc", "3": "45e4abbd-648d-4c90-915e-e93affc4596d"}}, "__type__": "1"}, "45e4abbd-648d-4c90-915e-e93affc4596d": {"__data__": {"text": "these\nsplit-ordered keys. (T o ensure that reversed keys are positive, we use only the\nlower three bytes of the hash code.) Fig. 13.15 shows how the contains () method\nis modi\ufb01ed to use the split-ordered key. (As in the LockFreeList class, the\nfind (x) method returns a record containing the x\u2019s node, if it exists, along with\nthe immediately preceding and subsequent nodes.)\nThe second difference is that while the LockFreeList class uses only two\nsentinels, one at each end of the list, the BucketList<T> class places a sentinel\n13.3 A Lock-Free Hash Set 313\n1public class BucketList<T> implements Set<T> {\n2 static final int HI_MASK = 0x80000000;\n3 static final int MASK = 0x00FFFFFF;\n4 Node head;\n5 public BucketList() {\n6 head = new Node(0);\n7 head.next =\n8 new AtomicMarkableReference<Node>( new Node(Integer.MAX_VALUE), false );\n9 }\n10 public int makeOrdinaryKey(T x) {\n11 int code = x.hashCode() & MASK; // take 3 lowest bytes\n12 return reverse(code | HI_MASK);\n13 }\n14 private static int makeSentinelKey( int key) {\n15 return reverse(key & MASK);\n16 }\n17 ...\n18 }\nFigure 13.14 BucketList<T> class: \ufb01elds, constructor, and utilities.\n19 public boolean contains(T x) {\n20 int key = makeOrdinaryKey(x);\n21 Window window = find(head, key);\n22 Node curr = window.curr;\n23 return (curr.key == key);\n24 }\nFigure 13.15 BucketList<T> class: the contains() method.\nat the start of each new bucket whenever the table is resized. It requires the\nability to insert sentinels at intermediate positions within the list, and to tra-\nverse the list starting from such sentinels. The BucketList<T> class provides a\ngetSentinel (x) method ( Fig. 13.16 ) that takes a bucket index, \ufb01nds the associ-\nated sentinel (inserting it if absent), and returns the tail of the BucketList<T>\nstarting from that sentinel.\n13.3.3 The LockFreeHashSet<T> Class\nFig. 13.17 shows the \ufb01elds and constructor for the LockFreeHashSet<T>\nclass. The set has the following mutable \ufb01elds: bucket is an array of\nLockFreeHashSet<T> references into the list of items, bucketSize is an atomic\ninteger that tracks how much of the bucket array is currently in use, and setSize\nis an atomic integer that tracks how many objects are in the set, used to decide\nwhen to resize.\nFig. 13.18 shows the LockFreeHashSet<T> class\u2019s add() method. If xhas hash\ncodek,add(x) retrieves bucket k(modN), whereNis the current table size,\ninitializing it if necessary (Line 15). It then calls the BucketList<T> \u2019sadd(x)\nmethod. Ifxwas not already present (Line 18) it increments setSize , and checks\n314 Chapter 13 Concurrent Hashing and Natural Parallelism\n25 public BucketList<T> getSentinel( int index) {\n26 int key = makeSentinelKey(index);\n27 boolean splice;\n28 while (true ) {\n29 Window window = find(head, key);\n30 Node pred = window.pred;\n31 Node curr = window.curr;\n32 if(curr.key == key) {\n33 return new BucketList<T>(curr);\n34 }else {\n35 Node node = new Node(key);\n36 node.next.set(pred.next.getReference(), false );\n37 splice = pred.next.compareAndSet(curr, node, false", "doc_id": "45e4abbd-648d-4c90-915e-e93affc4596d", "embedding": null, "doc_hash": "8a83bf7df19c79903024be30f3d0dee0e46de792ab6389883bb3aad57b55ce64", "extra_info": null, "node_info": {"start": 804749, "end": 807770}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5c28eb19-4c81-479e-ae1e-58d3c7c7484f", "3": "3fcf8f87-4641-4a96-adc0-67b7a6b17f0f"}}, "__type__": "1"}, "3fcf8f87-4641-4a96-adc0-67b7a6b17f0f": {"__data__": {"text": "public BucketList<T> getSentinel( int index) {\n26 int key = makeSentinelKey(index);\n27 boolean splice;\n28 while (true ) {\n29 Window window = find(head, key);\n30 Node pred = window.pred;\n31 Node curr = window.curr;\n32 if(curr.key == key) {\n33 return new BucketList<T>(curr);\n34 }else {\n35 Node node = new Node(key);\n36 node.next.set(pred.next.getReference(), false );\n37 splice = pred.next.compareAndSet(curr, node, false ,false );\n38 if(splice)\n39 return new BucketList<T>(node);\n40 else\n41 continue ;\n42 }\n43 }\n44 }\nFigure 13.16 BucketList<T> class: getSentinel() method.\n1public class LockFreeHashSet<T> {\n2 protected BucketList<T>[] bucket;\n3 protected AtomicInteger bucketSize;\n4 protected AtomicInteger setSize;\n5 public LockFreeHashSet( int capacity) {\n6 bucket = (BucketList<T>[]) new BucketList[capacity];\n7 bucket[0] = new BucketList<T>();\n8 bucketSize = new AtomicInteger(2);\n9 setSize = new AtomicInteger(0);\n10 }\n11 ...\n12 }\nFigure 13.17 LockFreeHashSet<T> class: \ufb01elds and constructor.\n13 public boolean add(T x) {\n14 int myBucket = BucketList.hashCode(x) % bucketSize.get();\n15 BucketList<T> b = getBucketList(myBucket);\n16 if(!b.add(x))\n17 return false ;\n18 int setSizeNow = setSize.getAndIncrement();\n19 int bucketSizeNow = bucketSize.get();\n20 if(setSizeNow / bucketSizeNow > THRESHOLD)\n21 bucketSize.compareAndSet(bucketSizeNow, 2 *bucketSizeNow);\n22 return true ;\n23 }\nFigure 13.18 LockFreeHashSet<T> class: add() method.\n13.3 A Lock-Free Hash Set 315\n24 private BucketList<T> getBucketList( int myBucket) {\n25 if(bucket[myBucket] == null )\n26 initializeBucket(myBucket);\n27 return bucket[myBucket];\n28 }\n29 private void initializeBucket( int myBucket) {\n30 int parent = getParent(myBucket);\n31 if(bucket[parent] == null )\n32 initializeBucket(parent);\n33 BucketList<T> b = bucket[parent].getSentinel(myBucket);\n34 if(b != null )\n35 bucket[myBucket] = b;\n36 }\n37 private int getParent( int myBucket){\n38 int parent = bucketSize.get();\n39 do{\n40 parent = parent >> 1;\n41 }while (parent > myBucket);\n42 parent = myBucket - parent;\n43 return parent;\n44 }\nFigure 13.19 LockFreeHashSet<T> class: if a bucket is uninitialized, initialize it by adding a\nnew sentinel. Initializing a bucket may require initializing its parent.\nwhether to increase bucketSize , the number of active buckets. The contains (x)\nandremove (x) methods work in much the same way.\nFig. 13.19 shows the InitializeBucket() method, whose role is to initialize\nthebucket array entry at a particular index, setting that entry to refer to a new\nsentinel node. The sentinel node is \ufb01rst created and added to an existing par-\nentbucket, and then the array entry is assigned a reference to the sentinel. If the\nparent bucket is not initialized (Line 31),InitializeBucket() is applied recur-\nsively to the parent. T o control the recursion we maintain the invariant that the\nparent index is less than the new bucket", "doc_id": "3fcf8f87-4641-4a96-adc0-67b7a6b17f0f", "embedding": null, "doc_hash": "469a174ee0d02209bc67e439a9c12f5b7eb3b6ca51cee547774837eff1f8c4ea", "extra_info": null, "node_info": {"start": 807841, "end": 810732}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "45e4abbd-648d-4c90-915e-e93affc4596d", "3": "34b7f21f-a181-419e-9039-20176af27117"}}, "__type__": "1"}, "34b7f21f-a181-419e-9039-20176af27117": {"__data__": {"text": "work in much the same way.\nFig. 13.19 shows the InitializeBucket() method, whose role is to initialize\nthebucket array entry at a particular index, setting that entry to refer to a new\nsentinel node. The sentinel node is \ufb01rst created and added to an existing par-\nentbucket, and then the array entry is assigned a reference to the sentinel. If the\nparent bucket is not initialized (Line 31),InitializeBucket() is applied recur-\nsively to the parent. T o control the recursion we maintain the invariant that the\nparent index is less than the new bucket index. It is also prudent to choose the\nparent index as close as possible to the new bucket index, but still preceding it.\nWe compute this index by unsetting the bucket index\u2019s most signi\ufb01cant nonzero\nbit (Line 39).\nTheadd(),remove (), and contains () methods require a constant expected\nnumber of steps to \ufb01nd a key (or determine that the key is absent). T o initialize\na bucket in a table of bucketSize N, the InitializeBucket() method may\nneed to recursively initialize (i.e., split) as many as O(logN) of its parent buckets\nto allow the insertion of a new bucket. An example of this recursive initialization\nis shown in Fig. 13.20 . In Part (a) the table has four buckets; only Bucket 0 is\ninitialized. In Part (b) the item with key 7 is inserted. Bucket 3 now requires\ninitialization, further requiring recursive initialization of Bucket 1. In Part (c)\nBucket 1 is initialized. Finally, in Part (d), Bucket 3 is initialized. Although the\ntotal complexity in such a case is logarithmic, not constant, it can be shown that\ntheexpected length of any such recursive sequence of splits is constant, making\nthe overall expected complexity of all the hash set operations constant.\n316 Chapter 13 Concurrent Hashing and Natural Parallelism\n3 01 81 2 70\n1\n2\n30\n1\n2\n3\n0\n1\n2\n30\n1\n2\n3(a) (b)\n(c) (d)08 1 2 0 8 12 7\n0 1 81 2 7\nFigure 13.20 Recursive initialization of lock-free hash table buckets. (a) Table has four buck-\nets; only bucket 0 is initialized. (b) We wish to insert the item with key 7. Bucket 3\nnow requires initialization, which in turn requires recursive initialization of Bucket 1.\n(c) Bucket 1 is initialized by \ufb01rst adding the 1 sentinel to the list, then setting the bucket\nto this sentinel. (d) Then Bucket 3 is initialized in a similar fashion, and \ufb01nally 7 is added to\nthe list. In the worst case, insertion of an item may require recursively initializing a number\nof buckets logarithmic in the table size, but it can be shown that the expected length of such\na recursive sequence is constant.\n13.4 An Open-Addressed Hash Set\nWe now turn our attention to a concurrent open hashing algorithm. Open\nhashing, in which each table entry holds a single item rather than a set, seems\nharder to make concurrent than closed hashing. We base our concurrent algo-\nrithm on a sequential algorithm known as Cuckoo Hashing.\n13.4.1 Cuckoo Hashing\nCuckoo hashing is a (sequential) hashing algorithm in which a newly added item\ndisplaces any earlier item occupying the same slot.1For brevity, a table is a\nk-entry array of items. For a hash set of size N= 2kwe use a two-entry array\ntable [] of tables,2and two independent hash functions,\nh0,h1:KeyRange!0,:::,k\u00001:\n1Cuckoos are a family of birds (not clocks) found in North America and Europe. Most species\nare nest parasites: they lay their", "doc_id": "34b7f21f-a181-419e-9039-20176af27117", "embedding": null, "doc_hash": "8f0320401415e630135e819972c856c843b287281391b4c23ef125033e1a1424", "extra_info": null, "node_info": {"start": 810638, "end": 813980}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3fcf8f87-4641-4a96-adc0-67b7a6b17f0f", "3": "501d2b30-d7ce-4183-b3ec-c8363f53eb21"}}, "__type__": "1"}, "501d2b30-d7ce-4183-b3ec-c8363f53eb21": {"__data__": {"text": "Cuckoo Hashing\nCuckoo hashing is a (sequential) hashing algorithm in which a newly added item\ndisplaces any earlier item occupying the same slot.1For brevity, a table is a\nk-entry array of items. For a hash set of size N= 2kwe use a two-entry array\ntable [] of tables,2and two independent hash functions,\nh0,h1:KeyRange!0,:::,k\u00001:\n1Cuckoos are a family of birds (not clocks) found in North America and Europe. Most species\nare nest parasites: they lay their eggs in other birds\u2019 nests. Cuckoo chicks hatch early, and quickly\npush the other eggs out of the nest.\n2This division of the table into two arrays will help in presenting the concurrent algorithm. There\nare sequential Cuckoo hashing algorithms that use, for the same number of hashed items, only a\nsingle array of size 2 k.\n13.4 An Open-Addressed Hash Set 317\n1public boolean add(T x) {\n2 if(contains(x)) {\n3 return false ;\n4 }\n5 for (int i = 0; i < LIMIT; i++) {\n6 if((x = swap(0, hash0(x), x)) == null ) {\n7 return true ;\n8 }else if ((x = swap(1, hash1(x), x)) == null ) {\n9 return true ;\n10 }\n11 }\n12 resize();\n13 add(x);\n14 }\nFigure 13.21 Sequential Cuckoo Hashing: the add() method.\ntable [1] table [0]\n14h0(x)5x (mod 9) (mod 8) 0\n1234\n6\n75\n2312\n390\n1234\n6\n7 5h1(x)5x (mod 11) (mod 8)\n3\nFigure 13.22 A sequence of displacements started when an item with key 14 \ufb01nds both\nlocations Table[0][ h0(14)] and Table[1][ h1(14)] taken by the values 3 and 23, and ends when\nthe item with key 39 is successfully placed in Table[1][ h1(39)].\n(denoted as hash0() and hash1() in the code) mapping the set of possible\nkeys to entries in the array. T o test whether a value xis in the set, contains (x)\ntests whether either table [0][h0(x)] or table [1][h1(x)] is equal to x. Similarly,\nremove (x) checks whether xis in either table [0][h0(x)] or table [1][h1(x)],\nand removes it if found.\nTheadd(x) method ( Fig. 13.21 ) is the most interesting. It successively \u201ckicks\nout\u201d con\ufb02icting items until every key has a slot. T o add x, the method swaps x\nwithy, the current occupant of table [0][h0(x)] (Line 6). If the prior value y\nwas null, it is done (Line 7). Otherwise, it swaps the newly nest-less value yfor\nthe current occupant of table [1][h1(y)] in the same way (Line 8). As before, if\nthe prior value was null, it is done. Otherwise, the method continues swapping\nentries (alternating tables) until it \ufb01nds an empty slot. An example of such a\nsequence of displacements appears in Fig. 13.22 .\nWe might not \ufb01nd an empty slot, either because the table is full, or because\nthe sequence of displacements forms a cycle. We therefore need an upper limit\non the number of successive displacements we are willing to undertake (Line 5).\nWhen this limit is exceeded, we resize the hash table, choose new hash functions\n(Line 12), and start over (Line 13).\n318 Chapter 13 Concurrent Hashing and Natural Parallelism\nSequential cuckoo hashing is attractive for its simplicity. It provides constant-\ntime", "doc_id": "501d2b30-d7ce-4183-b3ec-c8363f53eb21", "embedding": null, "doc_hash": "f1c5d6ba193cdf10be4cbfcd8ad3489c4ba42d6e41a61714b9aee51863451ebb", "extra_info": null, "node_info": {"start": 814056, "end": 817002}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "34b7f21f-a181-419e-9039-20176af27117", "3": "2c8ccbc7-ab94-4837-9bb6-d2c1a949fc1c"}}, "__type__": "1"}, "2c8ccbc7-ab94-4837-9bb6-d2c1a949fc1c": {"__data__": {"text": "such a\nsequence of displacements appears in Fig. 13.22 .\nWe might not \ufb01nd an empty slot, either because the table is full, or because\nthe sequence of displacements forms a cycle. We therefore need an upper limit\non the number of successive displacements we are willing to undertake (Line 5).\nWhen this limit is exceeded, we resize the hash table, choose new hash functions\n(Line 12), and start over (Line 13).\n318 Chapter 13 Concurrent Hashing and Natural Parallelism\nSequential cuckoo hashing is attractive for its simplicity. It provides constant-\ntime contains () and remove () methods, and it can be shown that over time,\nthe average number of displacements caused by each add() call will be constant.\nExperimental evidence shows that sequential Cuckoo hashing works well in\npractice.\n13.4.2 Concurrent Cuckoo Hashing\nThe principal obstacle to making the sequential Cuckoo hashing algorithm\nconcurrent is the add() method\u2019s need to perform a long sequence of swaps.\nT o address this problem, we now de\ufb01ne an alternative Cuckoo hashing algo-\nrithm, the PhasedCuckooHashSet<T> class. We break up each method call into a\nsequence of phases , where each phase adds, removes, or displaces a single item x.\nRather than organizing the set as a two-dimensional table of items, we use\na two-dimensional table of probe sets , where a probe set is a constant-sized set\nof items with the same hash code. Each probe set holds at most PROBE_SIZE\nitems, but the algorithm tries to ensure that when the set is quiescent (i.e., no\nmethod calls are in progress) each probe set holds no more than THRESHOLD\n<PROBE_SIZE items. An example of the PhasedCuckooHashSet structure\nappears in Fig. 13.24, where the PROBE_SIZE is 4 and the THRESHOLD is 2.\nWhile method calls are in-\ufb02ight, a probe set may temporarily hold more than\nTHRESHOLD but never more than PROBE_SIZE items. (In our examples, it is con-\nvenient to implement each probe set as a \ufb01xed-size List<T> .) Fig. 13.23 shows\nthePhasedCuckooHashSet<T> \u2019s \ufb01elds and constructor.\nT o postpone our discussion of synchronization, the PhasedCuckooHashSet<T>\nclass is de\ufb01ned to be abstract , that is, it does not implement all its methods.\nThe PhasedCuckooHashSet<T> class has the same abstract methods as the\nBaseHashSet<T> class: The acquire (x) method acquires all the locks neces-\nsary to manipulate item x,release (x) releases them, and resize () resizes the\nset. (As before, we require acquire (x) to be reentrant).\nFrom a bird\u2019s eye view, the PhasedCuckooHashSet<T> works as follows. It\nadds and removes items by \ufb01rst locking the associated probe sets in both tables.\n1public abstract class PhasedCuckooHashSet<T> {\n2 volatile int capacity;\n3 volatile List<T>[][] table;\n4 public PhasedCuckooHashSet( int size) {\n5 capacity = size;\n6 table = (List<T>[][]) new java.util.ArrayList[2][capacity];\n7 for (int i = 0; i < 2; i++) {\n8 for (int j = 0; j < capacity; j++) {\n9 table[i][j] = new ArrayList<T>(PROBE_SIZE);\n10 }\n11 }\n12 }\n13 ...\n14 }\nFigure 13.23 PhasedCuckooHashSet<T> class: \ufb01elds and constructor.\n13.4 An", "doc_id": "2c8ccbc7-ab94-4837-9bb6-d2c1a949fc1c", "embedding": null, "doc_hash": "221d88c87ef1804504c42a4d834e6275a2ac8068689681b9120ebfc83bdebc58", "extra_info": null, "node_info": {"start": 816921, "end": 819971}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "501d2b30-d7ce-4183-b3ec-c8363f53eb21", "3": "b807df28-b842-452a-908d-12415dab3b5f"}}, "__type__": "1"}, "b807df28-b842-452a-908d-12415dab3b5f": {"__data__": {"text": "table;\n4 public PhasedCuckooHashSet( int size) {\n5 capacity = size;\n6 table = (List<T>[][]) new java.util.ArrayList[2][capacity];\n7 for (int i = 0; i < 2; i++) {\n8 for (int j = 0; j < capacity; j++) {\n9 table[i][j] = new ArrayList<T>(PROBE_SIZE);\n10 }\n11 }\n12 }\n13 ...\n14 }\nFigure 13.23 PhasedCuckooHashSet<T> class: \ufb01elds and constructor.\n13.4 An Open-Addressed Hash Set 319\ntable [1]\nh0(x)=x (mod 9) (mod 8)1\n230\n2\n3 1\nh1(x)5x (mod 11) (mod 8) h0(x)=x (mod 9) (mod 8) h1(x)5x (mod 11) (mod 8)0\n1\n23\n0\n2\n3 10table [0]\n1\n24\n5threshold\nthresholdthreshold\nthreshold13\n14 145234\n40\n2412\n13\n??table [1] table[ 0]\n?(a) (b)\n1\n230\n2\n3 10\n1\n23\n0\n2\n3 101\n24\n5threshold\nthresholdthreshold\nthreshold13\n145234\n40\n2412\nFigure 13.24 The PhasedCuckooHashSet<T> class: add()andrelocate ()methods. The \ufb01gure shows the\narray segments consisting of 8 probe sets of size 4 each, with a threshold of 2. Shown are probe sets 4 and 5\nof Table[0][] and 1 and 2 of Table[1][]. In Part (a) an item with key 13 \ufb01nds Table[0][4] above threshold and\nTable[1][2] below threshold so it adds the item to the probe set Table[1][2]. The item with key 14 on the\nother hand \ufb01nds that both of its probe sets are above threshold, so it adds its item to Table[0][5] and signals\nthat the item should be relocated. In Part (b), the method tries to relocate the item with key 23, the oldest\nitem in Table[0][5]. Since Table[1][1] is below threshold, the item is successfully relocated. If Table[1][1] were\nabove threshold, the algorithm would attempt to relocate item 12 from Table[1][1], and if Table[1][1] were\nat the probe set\u2019s size limit of 4 items, it would attempt to relocate the item with key 5, the next oldest item,\nfrom Table[0][5].\nT o remove an item, it proceeds as in the sequential algorithm, checking if it is\nin one of the probe sets and removing it if so. T o add an item, it attempts to\nadd it to one of the probe sets. An item\u2019s probe sets serves as temporary over-\n\ufb02ow buffer for long sequences of consecutive displacements that might occur\nwhen adding an item to the table. The THRESHOLD value is essentially the size of\nthe probe sets in a sequential algorithm. If the probe sets already has this many\nitems, the item is added anyway to one of the PROBE_SIZE \u2013THRESHOLD over\ufb02ow\nslots. The algorithm then tries to relocate another item from the probe set. There\nare various policies one can use to choose which item to relocate. Here, we move\nthe oldest items out \ufb01rst, until the probe set is below threshold. As in the sequen-\ntial cuckoo hashing algorithm, one relocation may trigger another, and so on.\nFig. 13.24 shows an example execution of the PhasedCuckooHashSet<T> .\nFig. 13.25 shows the PhasedCuckooHashSet<T> class\u2019s remove (x) method. It\ncalls the abstract acquire", "doc_id": "b807df28-b842-452a-908d-12415dab3b5f", "embedding": null, "doc_hash": "23c6ff992ae319c857446fa4bdcb0a20a2b794ceffb8237abe20d832a3cf2c58", "extra_info": null, "node_info": {"start": 820146, "end": 822903}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2c8ccbc7-ab94-4837-9bb6-d2c1a949fc1c", "3": "ee695332-dfce-4236-bbfb-9d4472e63323"}}, "__type__": "1"}, "ee695332-dfce-4236-bbfb-9d4472e63323": {"__data__": {"text": "then tries to relocate another item from the probe set. There\nare various policies one can use to choose which item to relocate. Here, we move\nthe oldest items out \ufb01rst, until the probe set is below threshold. As in the sequen-\ntial cuckoo hashing algorithm, one relocation may trigger another, and so on.\nFig. 13.24 shows an example execution of the PhasedCuckooHashSet<T> .\nFig. 13.25 shows the PhasedCuckooHashSet<T> class\u2019s remove (x) method. It\ncalls the abstract acquire (x) method to acquire the necessary locks, then enters\natryblock whose finally block calls release (x). In the tryblock, the method\nsimply checks whether xis present in Table[0][ h0(x)] or Table[1][ h1(x)]. If so,\nit removesxand returns true, and otherwise returns false . The contains (x)\nmethod works in a similar way.\nFig. 13.26 illustrates the add(x) method. Like remove (), it calls acquire (x) to\nacquire the necessary locks, then enters a try block whose finally block calls\nrelease (x). It returns false if the item is already present (Line 41). If either of\n320 Chapter 13 Concurrent Hashing and Natural Parallelism\n15 public boolean remove(T x) {\n16 acquire(x);\n17 try {\n18 List<T> set0 = table[0][hash0(x) % capacity];\n19 if(set0.contains(x)) {\n20 set0.remove(x);\n21 return true ;\n22 }else {\n23 List<T> set1 = table[1][hash1(x) % capacity];\n24 if(set1.contains(x)) {\n25 set1.remove(x);\n26 return true ;\n27 }\n28 }\n29 return false ;\n30 }finally {\n31 release(x);\n32 }\n33 }\nFigure 13.25 PhasedCuckooHashSet<T> class: the remove() method.\n34 public boolean add(T x) {\n35 T y = null ;\n36 acquire(x);\n37 int h0 = hash0(x) % capacity, h1 = hash1(x) % capacity;\n38 int i = -1, h = -1;\n39 boolean mustResize = false ;\n40 try {\n41 if(present(x)) return false ;\n42 List<T> set0 = table[0][h0];\n43 List<T> set1 = table[1][h1];\n44 if(set0.size() < THRESHOLD) {\n45 set0.add(x); return true ;\n46 }else if (set1.size() < THRESHOLD) {\n47 set1.add(x); return true ;\n48 }else if (set0.size() < PROBE_SIZE) {\n49 set0.add(x); i = 0; h = h0;\n50 }else if (set1.size() < PROBE_SIZE) {\n51 set1.add(x); i = 1; h = h1;\n52 }else {\n53 mustResize = true ;\n54 }\n55 }finally {\n56 release(x);\n57 }\n58 if(mustResize) {\n59 resize(); add(x);\n60 }else if (!relocate(i, h)) {\n61 resize();\n62 }\n63 return true ;// x must have been present\n64 }\nFigure 13.26 PhasedCuckooHashSet<T> class: the add() method.\n13.4 An Open-Addressed Hash Set 321\nthe item\u2019s probe sets is below threshold (Lines 44and 46), it adds the item and\nreturns. Otherwise, if either of the item\u2019s probe sets is above threshold but not full\n(Lines 48and 50), it adds the item and makes a note to rebalance the probe set\nlater. Finally, if both sets are full, it makes a note to resize the entire set (Line", "doc_id": "ee695332-dfce-4236-bbfb-9d4472e63323", "embedding": null, "doc_hash": "0f80a6a4af7567de213d180e496210a9e532d3a0f31be92a3cb62b858c41c69a", "extra_info": null, "node_info": {"start": 822797, "end": 825517}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b807df28-b842-452a-908d-12415dab3b5f", "3": "2aac5abf-978b-4787-99c1-95386b55bfd7"}}, "__type__": "1"}, "2aac5abf-978b-4787-99c1-95386b55bfd7": {"__data__": {"text": "}\n63 return true ;// x must have been present\n64 }\nFigure 13.26 PhasedCuckooHashSet<T> class: the add() method.\n13.4 An Open-Addressed Hash Set 321\nthe item\u2019s probe sets is below threshold (Lines 44and 46), it adds the item and\nreturns. Otherwise, if either of the item\u2019s probe sets is above threshold but not full\n(Lines 48and 50), it adds the item and makes a note to rebalance the probe set\nlater. Finally, if both sets are full, it makes a note to resize the entire set (Line 53).\nIt then releases the lock on x(Line 56).\nIf the method was unable to add xbecause both its probe sets were full, it\nresizes the hash set and tries again (Line 58). If the probe set at row rand column\ncwas above threshold, it calls relocate (r,c) (described later) to rebalance probe\nset sizes. If the call returns false , indicating that it failed to rebalance the probe\nsets, then add() resizes the table.\nTherelocate () method appears in Fig. 13.27 . It takes the row and column\ncoordinates of a probe set observed to have more than THRESHOLD items, and\n65 protected boolean relocate( int i,int hi) {\n66 int hj = 0;\n67 int j = 1 - i;\n68 for (int round = 0; round < LIMIT; round++) {\n69 List<T> iSet = table[i][hi];\n70 T y = iSet.get(0);\n71 switch (i) {\n72 case 0: hj = hash1(y) % capacity; break ;\n73 case 1: hj = hash0(y) % capacity; break ;\n74 }\n75 acquire(y);\n76 List<T> jSet = table[j][hj];\n77 try {\n78 if(iSet.remove(y)) {\n79 if(jSet.size() < THRESHOLD) {\n80 jSet.add(y);\n81 return true ;\n82 }else if (jSet.size() < PROBE_SIZE) {\n83 jSet.add(y);\n84 i = 1 - i;\n85 hi = hj;\n86 j = 1 - j;\n87 }else {\n88 iSet.add(y);\n89 return false ;\n90 }\n91 }else if (iSet.size() >= THRESHOLD) {\n92 continue ;\n93 }else {\n94 return true ;\n95 }\n96 }finally {\n97 release(y);\n98 }\n99 }\n100 return false ;\n101 }\nFigure 13.27 PhasedCuckooHashSet<T> class: the relocate() method.\n322 Chapter 13 Concurrent Hashing and Natural Parallelism\ntries to reduce its size below threshold by moving items from this probe set to\nalternative probe sets.\nThis method makes a \ufb01xed number( LIMIT ) of attempts before giving up. Each\ntime around the loop, the following invariants hold: iSet is the probe set we are\ntrying to shrink, yis the oldest item in iSet , and jSet is the other probe set\nwhereycould be. The loop identi\ufb01es y(Line 70), locks both probe sets to which\nycould belong (Line 75), and tries to remove yfrom the probe set (Line 78).\nIf it succeeds (another thread could have removed ybetween Lines 70and 78),\nthen it prepares to add ytojSet . IfjSet is below threshold (Line 79), then\nthe method adds ytojSet and returns true (no need to resize). If jSet is above\nthreshold but not full (Line 82), then it tries to shrink jSet by swapping iSet\nandjSet (Lines 82\u201386) and resuming the loop. If jSet is full (Line 87), the\nmethod puts yback in iSet and returns false (triggering a resize). Otherwise it\ntries to shrink", "doc_id": "2aac5abf-978b-4787-99c1-95386b55bfd7", "embedding": null, "doc_hash": "b22326d31ff8e654c8f645f13d64b1ffc1c6d1b29a6f490bcfcb38b2da5e3cad", "extra_info": null, "node_info": {"start": 825523, "end": 828405}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ee695332-dfce-4236-bbfb-9d4472e63323", "3": "9505c2f8-555e-46cb-b118-f216f191810b"}}, "__type__": "1"}, "9505c2f8-555e-46cb-b118-f216f191810b": {"__data__": {"text": "it succeeds (another thread could have removed ybetween Lines 70and 78),\nthen it prepares to add ytojSet . IfjSet is below threshold (Line 79), then\nthe method adds ytojSet and returns true (no need to resize). If jSet is above\nthreshold but not full (Line 82), then it tries to shrink jSet by swapping iSet\nandjSet (Lines 82\u201386) and resuming the loop. If jSet is full (Line 87), the\nmethod puts yback in iSet and returns false (triggering a resize). Otherwise it\ntries to shrink jSet by swapping iSet andjSet (Lines 82\u201386). If the method\ndoes not succeed in removing yat Line 78, then it rechecks the size of iSet . If\nit is still over threshold (Line 91), then the method resumes the loop and tries\nagain to remove an item. Otherwise, iSet is below threshold, and the method\nreturns true (no resize needed). Fig. 13.24 shows an example execution of the\nPhasedCuckooHashSet<T> where the item with key 14 causes a relocation of the\noldest item 23 from the probe set table [0][5].\n13.4.3 Striped Concurrent Cuckoo Hashing\nWe \ufb01rst consider a concurrent Cuckoo hash set implementation using lock strip-\ning ( Chapter 13 ,Section 13.2.2 ). The StripedCuckooHashSet class extends\nPhasedCuckooHashSet , providing a \ufb01xed 2-by- Larray of reentrant locks. As\nusual, lock [i][j] protects table [i][k], wherek(modL) =j.Fig. 13.28 shows\ntheStripedCuckooHashSet class\u2019s \ufb01elds and constructor. The constructor calls\nthePhasedCuckooHashSet<T> constructor (Line 4) and then initializes the lock\narray.\n1public class StripedCuckooHashSet<T> extends PhasedCuckooHashSet<T>{\n2 final ReentrantLock[][] lock;\n3 public StripedCuckooHashSet( int capacity) {\n4 super (capacity);\n5 lock = new ReentrantLock[2][capacity];\n6 for (int i = 0; i < 2; i++) {\n7 for (int j = 0; j < capacity; j++) {\n8 lock[i][j] = new ReentrantLock();\n9 }\n10 }\n11 }\n12 ...\n13 }\nFigure 13.28 StripedCuckooHashSet class: \ufb01elds and constructor.\n13.4 An Open-Addressed Hash Set 323\nThe StripedCuckooHashSet class\u2019s acquire (x) method (Fig. 13.29) locks\nlock [0][h0(x)] and lock [1][h1(x)] in that order, to avoid deadlock. The\nrelease (x) method unlocks those locks.\nThe only difference between the resize ()methods of StripedCuckooHashSet\n(Fig. 13.30) and StripedHashSet is that the latter acquires the locks in lock [0]\n14 public final void acquire(T x) {\n15 lock[0][hash0(x) % lock[0].length].lock();\n16 lock[1][hash1(x) % lock[1].length].lock();\n17 }\n18 public final void release(T x) {\n19 lock[0][hash0(x) % lock[0].length].unlock();\n20 lock[1][hash1(x) % lock[1].length].unlock();\n21 }\nFigure 13.29 StripedCuckooHashSet class: acquire() andrelease() .\n22 public void resize() {\n23 int oldCapacity = capacity;\n24 for (Lock aLock : lock[0]) {\n25 aLock.lock();\n26 }\n27 try {\n28 if(capacity != oldCapacity) {\n29 return ;\n30 }\n31 List<T>[][]", "doc_id": "9505c2f8-555e-46cb-b118-f216f191810b", "embedding": null, "doc_hash": "62753c438c15ce378323a23ca1978b103fb7551f8581b5e3d52cab464b7d43ce", "extra_info": null, "node_info": {"start": 828405, "end": 831194}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2aac5abf-978b-4787-99c1-95386b55bfd7", "3": "2b8cfdd1-78d7-479b-8abd-a7333ac3682f"}}, "__type__": "1"}, "2b8cfdd1-78d7-479b-8abd-a7333ac3682f": {"__data__": {"text": "public final void release(T x) {\n19 lock[0][hash0(x) % lock[0].length].unlock();\n20 lock[1][hash1(x) % lock[1].length].unlock();\n21 }\nFigure 13.29 StripedCuckooHashSet class: acquire() andrelease() .\n22 public void resize() {\n23 int oldCapacity = capacity;\n24 for (Lock aLock : lock[0]) {\n25 aLock.lock();\n26 }\n27 try {\n28 if(capacity != oldCapacity) {\n29 return ;\n30 }\n31 List<T>[][] oldTable = table;\n32 capacity = 2 *capacity;\n33 table = (List<T>[][]) new List[2][capacity];\n34 for (List<T>[] row : table) {\n35 for (int i = 0; i < row.length; i++) {\n36 row[i] = new ArrayList<T>(PROBE_SIZE);\n37 }\n38 }\n39 for (List<T>[] row : oldTable) {\n40 for (List<T> set : row) {\n41 for (T z : set) {\n42 add(z);\n43 }\n44 }\n45 }\n46 }finally {\n47 for (Lock aLock : lock[0]) {\n48 aLock.unlock();\n49 }\n50 }\n51 }\nFigure 13.30 StripedCuckooHashSet class: the resize() method.\n324 Chapter 13 Concurrent Hashing and Natural Parallelism\nin ascending order (Line 24). Acquiring these locks in this order ensures that\nno other thread is in the middle of an add(),remove (), or contains () call, and\navoids deadlocks with other concurrent resize () calls.\n13.4.4 A Re\ufb01nable Concurrent Cuckoo Hash Set\nWe can use the methods of Chapter 13 ,Section 13.2.3 to resize the lock arrays\nas well. This section introduces the RefinableCuckooHashSet class ( Fig. 13.31 ).\nJust as for the RefinableHashSet class, we introduce an owner \ufb01eld of type\nAtomicMarkableReference<Thread> that combines a Boolean value with a\nreference to a thread. If the Boolean value is true, the set is resizing, and the\nreference indicates which thread is in charge of resizing.\nEach phase locks the buckets for xby calling acquire (x), shown in Fig. 13.32 .\nIt reads the lock array (Line 24), and then spins until no other thread is resizing\nthe set (Lines 21\u201323). It then acquires the item\u2019s two locks (Lines 27and 28),\nand checks if the lock array is unchanged (Line 30). If the lock array has not\nchanged between Lines 24and30, then the thread has acquired the locks it needs\nto proceed. Otherwise, the locks it has acquired are out of date, so it releases\nthem and starts over. The release (x) method releases the locks acquired by\nacquire (x).\nThe resize () method in ( Fig. 13.33 ) is almost identical to the resize ()\nmethod for the StripedCuckooHashSet class. One difference is that the locks []\narray has two dimensions.\nThequiesce () method, like its counterpart in the RefinableHashSet class,\nvisits each lock and waits until it is unlocked. The only difference is that it visits\nonly the locks in locks [0].\n1public class RefinableCuckooHashSet<T> extends PhasedCuckooHashSet<T>{\n2 AtomicMarkableReference<Thread> owner;\n3 volatile ReentrantLock[][] locks;\n4 public RefinableCuckooHashSet( int capacity) {\n5 super (capacity);\n6 locks = new ReentrantLock[2][capacity];\n7 for (int i = 0; i < 2; i++) {\n8 for (int j = 0; j < capacity; j++) {\n9 locks[i][j] = new ReentrantLock();\n10 }\n11 }\n12 owner = new", "doc_id": "2b8cfdd1-78d7-479b-8abd-a7333ac3682f", "embedding": null, "doc_hash": "5d691e82703ec4d672ac7a005a3afc1314b9969bf0d45e8c773137b0698ba16f", "extra_info": null, "node_info": {"start": 831256, "end": 834214}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9505c2f8-555e-46cb-b118-f216f191810b", "3": "f68ddd63-d84f-41a2-9c3d-9882f5d33575"}}, "__type__": "1"}, "f68ddd63-d84f-41a2-9c3d-9882f5d33575": {"__data__": {"text": "class RefinableCuckooHashSet<T> extends PhasedCuckooHashSet<T>{\n2 AtomicMarkableReference<Thread> owner;\n3 volatile ReentrantLock[][] locks;\n4 public RefinableCuckooHashSet( int capacity) {\n5 super (capacity);\n6 locks = new ReentrantLock[2][capacity];\n7 for (int i = 0; i < 2; i++) {\n8 for (int j = 0; j < capacity; j++) {\n9 locks[i][j] = new ReentrantLock();\n10 }\n11 }\n12 owner = new AtomicMarkableReference<Thread>( null ,false );\n13 }\n14 ...\n15 }\nFigure 13.31 RefinableCuckooHashSet<T> : \ufb01elds and constructor.\n13.5 Chapter Notes 325\n16 public void acquire(T x) {\n17 boolean [] mark = { true };\n18 Thread me = Thread.currentThread();\n19 Thread who;\n20 while (true ) {\n21 do{// wait until not resizing\n22 who = owner.get(mark);\n23 }while (mark[0] && who != me);\n24 ReentrantLock[][] oldLocks = locks;\n25 ReentrantLock oldLock0 = oldLocks[0][hash0(x) % oldLocks[0].length];\n26 ReentrantLock oldLock1 = oldLocks[1][hash1(x) % oldLocks[1].length];\n27 oldLock0.lock();\n28 oldLock1.lock();\n29 who = owner.get(mark);\n30 if((!mark[0] || who == me) && locks == oldLocks) {\n31 return ;\n32 }else {\n33 oldLock0.unlock();\n34 oldLock1.unlock();\n35 }\n36 }\n37 }\n38 public void release(T x) {\n39 locks[0][hash0(x)].unlock();\n40 locks[1][hash1(x)].unlock();\n41 }\nFigure 13.32 RefinableCuckooHashSet<T> :acquire ()andrelease ()methods.\n13.5 Chapter Notes\nThe term disjoint-access-parallelism was coined by Amos Israeli and Lihu\nRappoport [76]. Maged Michael [114] has shown that simple algorithms\nusing a reader-writer lock [113] per bucket have reasonable performance\nwithout resizing. The lock-free hash set based on split-ordering described in\nSection 13.3.1 is by Ori Shalev and Nir Shavit [141]. The optimistic and \ufb01ne-\ngrained hash sets are adapted from a hash set implementation by Doug Lea [99],\nused in java.util.concurrent .\nOther concurrent closed-addressing schemes include Meichun Hsu and\nWei-Pang Y ang [72], Vijay Kumar [87], Carla Schlatter Ellis [38], and Michael\nGreenwald [48]. Hui Gao, Jan Friso Groote, and Wim Hesselink [44] propose\nan almost wait-free extensible open-addressing hashing algorithm and Chris\nPurcell and Tim Harris [129] propose a concurrent non-blocking hash table\nwith open addressing. Cuckoo hashing is credited to Rasmus Pagh and Flemming\nRodler [122], and the concurrent version is by Maurice Herlihy, Nir Shavit, and\nMoran Tzafrir [68].\n326 Chapter 13 Concurrent Hashing and Natural Parallelism\n42 public void resize() {\n43 int oldCapacity = capacity;\n44 Thread me = Thread.currentThread();\n45 if(owner.compareAndSet( null , me, false ,true )) {\n46 try {\n47 if(capacity != oldCapacity) { // someone else resized first\n48 return ;\n49 }\n50 quiesce();\n51 capacity = 2 *capacity;\n52 List<T>[][] oldTable = table;\n53 table = (List<T>[][]) new List[2][capacity];\n54 locks = new", "doc_id": "f68ddd63-d84f-41a2-9c3d-9882f5d33575", "embedding": null, "doc_hash": "a4b345c80e7f2521eb24fee900b5bd35cc75b033aca1964e907100065e88c0ef", "extra_info": null, "node_info": {"start": 834219, "end": 837018}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2b8cfdd1-78d7-479b-8abd-a7333ac3682f", "3": "8a5ff00a-9e8f-450c-b442-27013f5a9f66"}}, "__type__": "1"}, "8a5ff00a-9e8f-450c-b442-27013f5a9f66": {"__data__": {"text": "Tzafrir [68].\n326 Chapter 13 Concurrent Hashing and Natural Parallelism\n42 public void resize() {\n43 int oldCapacity = capacity;\n44 Thread me = Thread.currentThread();\n45 if(owner.compareAndSet( null , me, false ,true )) {\n46 try {\n47 if(capacity != oldCapacity) { // someone else resized first\n48 return ;\n49 }\n50 quiesce();\n51 capacity = 2 *capacity;\n52 List<T>[][] oldTable = table;\n53 table = (List<T>[][]) new List[2][capacity];\n54 locks = new ReentrantLock[2][capacity];\n55 for (int i = 0; i < 2; i++) {\n56 for (int j = 0; j < capacity; j++) {\n57 locks[i][j] = new ReentrantLock();\n58 }\n59 }\n60 for (List<T>[] row : table) {\n61 for (int i = 0; i < row.length; i++) {\n62 row[i] = new ArrayList<T>(PROBE_SIZE);\n63 }\n64 }\n65 for (List<T>[] row : oldTable) {\n66 for (List<T> set : row) {\n67 for (T z : set) {\n68 add(z);\n69 }\n70 }\n71 }\n72 }finally {\n73 owner.set( null ,false );\n74 }\n75 }\n76 }\nFigure 13.33 RefinableCuckooHashSet<T> : the resize ()method.\n78 protected void quiesce() {\n79 for (ReentrantLock lock : locks[0]) {\n80 while (lock.isLocked()) {}\n81 }\n82 }\nFigure 13.34 RefinableCuckooHashSet<T> : the quiesce ()method.\n13.6 Exercises\nExercise 158. Modify the StripedHashSet to allow resizing of the range lock\narray using read/write locks.\n13.6 Exercises 327\nExercise 159. For the LockFreeHashSet , show an example of the problem that\narises when deleting an entry pointed to by a bucket reference, if we do not add a\nsentinel entry, which is never deleted, to the start of each bucket.\nExercise 160. For the LockFreeHashSet , when an uninitialized bucket is accessed\nin a table of size N, it might be necessary to recursively initialize (i.e., split) as\nmany asO(logN) of its parent buckets to allow the insertion of a new bucket.\nShow an example of such a scenario. Explain why the expected length of any such\nrecursive sequence of splits is constant.\nExercise 161. For the LockFreeHashSet , design a lock-free data structure to\nreplace the \ufb01xed-size bucket array. Y our data structure should allow an arbitrary\nnumber of buckets.\nExercise 162. Outline correctness arguments for LockFreeHashSet \u2019sadd(),\nremove (), and contains () methods.\nHint: you may assume the LockFreeList algorithm\u2019s methods are correct.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n14Skiplists and Balanced Search\n14.1 Introduction\nWe have seen several concurrent implementations of sets based on linked lists\nand on hash tables. We now turn our attention to concurrent search structures\nwith logarithmic depth. There are many concurrent logarithmic search structures\nin the literature. Here, we are interested in search structures intended for in-\nmemory data, as opposed to data residing on outside storage such as disks.\nMany popular sequential search structures, such as red-black trees or A VL-\ntrees, require periodic rebalancing to maintain the structure\u2019s logarithmic\ndepth. Rebalancing works well for sequential tree-based search structures, but\nfor concurrent structures, rebalancing may cause bottlenecks and", "doc_id": "8a5ff00a-9e8f-450c-b442-27013f5a9f66", "embedding": null, "doc_hash": "33f92de32953e4708a7b24143da3873a201d1e1c69eec2b8b48fbc36233bdb0b", "extra_info": null, "node_info": {"start": 836966, "end": 840030}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f68ddd63-d84f-41a2-9c3d-9882f5d33575", "3": "97992d48-0f29-4e1c-896b-9b135936d969"}}, "__type__": "1"}, "97992d48-0f29-4e1c-896b-9b135936d969": {"__data__": {"text": "now turn our attention to concurrent search structures\nwith logarithmic depth. There are many concurrent logarithmic search structures\nin the literature. Here, we are interested in search structures intended for in-\nmemory data, as opposed to data residing on outside storage such as disks.\nMany popular sequential search structures, such as red-black trees or A VL-\ntrees, require periodic rebalancing to maintain the structure\u2019s logarithmic\ndepth. Rebalancing works well for sequential tree-based search structures, but\nfor concurrent structures, rebalancing may cause bottlenecks and contention.\nInstead, we focus here on concurrent implementations of a proven data structure\nthat provides expected logarithmic time search without the need to rebalance:\ntheSkipList . In the following sections we present two SkipList implemen-\ntations. The LazySkipList class is a lock-based implementation, while the\nLockFreeSkipList class is not. In both algorithms, the typically most frequent\nmethod, contains (), which searches for an item, is wait-free. These construc-\ntions follow the design patterns outlined earlier in Chapter 9.\n14.2 Sequential Skiplists\nFor simplicity we treat the list as a set, meaning that keys are unique. A SkipList\nis a collection of sorted linked lists, which mimics, in a subtle way, a balanced\nsearch tree. Nodes in a SkipList are ordered by key. Each node is linked into\na subset of the lists. Each list has a level, ranging from 0 to a maximum. The\nbottom-level list contains all the nodes, and each higher-level list is a sublist of the\nlower-level lists. Fig. 14.1 shows a SkipList with integer keys. The higher-level\nlists are shortcuts into the lower-level lists, because, roughly speaking, each link\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00014-9\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.329\n330 Chapter 14 Skiplists and Balanced Search\n9 51 8 11 25 15 8 223\n2222level\n3\n21\n0head tail\nkeys 2` 1`\nFigure 14.1 The SkipList class: this example has four levels of lists. Each node has a key,\nand the head andtail sentinels have\u00061keys. The list at level iis a shortcut where each\nreference skips over 2inodes of the next lower level list. For example, at level 3, references\nskip 23nodes, at level 2, 22nodes, and so on.\nat leveliskips over about 2inodes in next lower-level list, (e.g., in the SkipList\nshown in Fig. 14.1, each reference at level 3 skips over 23nodes.) Between any\ntwo nodes at a given level, the number of nodes in the level immediately below it\nis effectively constant, so the total height of the SkipList is roughly logarithmic\nin the number of nodes. One can \ufb01nd a node with a given key by searching \ufb01rst\nthrough the lists in higher levels, skipping over large numbers of lower nodes,\nand progressively descending until a node with the target key is found (or not) at\nthe bottom level.\nTheSkipList is aprobabilistic data structure. (No one knows how to provide\nthis kind of performance without randomization.) Each node is created with a\nrandom top level ( topLevel ), and belongs to all lists up to that level. T op levels\nare chosen so that the expected number of nodes in each level\u2019s list decreases\nexponentially. Let 0 <p< 1 be the conditional probability that a node at level i\nalso appears at level i+ 1. All nodes appear at level 0. The probability that a node\nat level 0 also appears at level i>0 ispi. For example, with p= 1=2, 1=2 of the\nnodes are expected to appear at level 1, 1 =4 at level", "doc_id": "97992d48-0f29-4e1c-896b-9b135936d969", "embedding": null, "doc_hash": "8a0d81c477f98b877d2085fe123f0d3fc2d1bd05e3857fa7b101eb73f0a9fa2e", "extra_info": null, "node_info": {"start": 839909, "end": 843406}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8a5ff00a-9e8f-450c-b442-27013f5a9f66", "3": "63bdab34-408b-4f57-a3b2-80e23070634e"}}, "__type__": "1"}, "63bdab34-408b-4f57-a3b2-80e23070634e": {"__data__": {"text": "Each node is created with a\nrandom top level ( topLevel ), and belongs to all lists up to that level. T op levels\nare chosen so that the expected number of nodes in each level\u2019s list decreases\nexponentially. Let 0 <p< 1 be the conditional probability that a node at level i\nalso appears at level i+ 1. All nodes appear at level 0. The probability that a node\nat level 0 also appears at level i>0 ispi. For example, with p= 1=2, 1=2 of the\nnodes are expected to appear at level 1, 1 =4 at level 2 and so on, providing a bal-\nancing property like the classical sequential tree-based search structures, except\nwithout the need for complex global restructuring.\nWe put head andtail sentinel nodes at the beginning and end of the lists\nwith the maximum allowed height. Initially, when the SkipList is empty, the\nhead (left sentinel) is the predecessor of the tail (right sentinel) at every level.\nThehead \u2019s key is less than any key that may be added to the set, and the tail \u2019s\nkey is greater.\nEach SkipList node\u2019s next \ufb01eld is an array of references, one for each list to\nwhich it belongs and so \ufb01nding a node means \ufb01nding its predecessors and suc-\ncessors. Searching the SkipList always begins at the head . The find () method\nproceeds down the levels one after the other, and traverses each level as in the\nLazyList using references to a predecessor node pred and a current node curr .\nWhenever it \ufb01nds a node with a greater or matching key, it records the pred and\ncurr as the predecessor and successor of a node in arrays called preds [] and\nsuccs [], and continues to the next lower level. The traversal ends at the bottom\nlevel. Fig. 14.2 (Part a) shows a sequential find () call.\n14.3 A Lock-Based Concurrent Skiplist 331\n9 51 8 11 25 15 8 23level\n2\n1\n0\n2` 1`A: find(12)\npreds[0] succs[0] preds[1]\nandpreds[2]succs[1] succs[2]\nandsuccs[3]preds[3](a)\n9 58 23level\n2\n1\n0\n2` 18 11 25 15121`A: add(12)(b)\nFigure 14.2 The SkipList class: add()andfind ()methods. In Part (a), find ()traverses at each level,\nstarting at the highest level, for as long as curr is less than or equal to the target key 12. Otherwise, it\nstores pred andcurr in the preds []andsuccs []arrays at each level and descends to the next level. For\nexample, the node with key 9 is preds [2]andpreds [1], while tail issuccs [2]and the node with key 18\nissuccs [1]. Here, find ()returns falsesince the node with key 12 was not found in the lowest-level list and\nso an add(12)call in Part (b) can proceed. In Part (b) a new node is created with a random topLevel = 2.\nThe new node\u2019s next references are redirected to the corresponding succs []nodes, and each predecessor\nnode\u2019s next reference is redirected to the new node.\nT o add a node to a skiplist, a find () call \ufb01lls in the preds [] and succs [] arrays.\nThe new node is created and linked between its predecessors and successors.\nFig. 14.2, Part (b) shows an add(12) call.\nT o remove a victim node from the skiplist, the find () method initializes the\nvictim\u2019s preds [] and succs [] arrays. The victim is then removed from the\nlist at all levels by redirecting each", "doc_id": "63bdab34-408b-4f57-a3b2-80e23070634e", "embedding": null, "doc_hash": "c9d0e91099b3a5612ed72e4af40ad165fbc60fd4956c3223377d96839375a3d0", "extra_info": null, "node_info": {"start": 843517, "end": 846605}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "97992d48-0f29-4e1c-896b-9b135936d969", "3": "28f7d8e4-7af8-400a-a33d-ad8703f6d564"}}, "__type__": "1"}, "28f7d8e4-7af8-400a-a33d-ad8703f6d564": {"__data__": {"text": "and each predecessor\nnode\u2019s next reference is redirected to the new node.\nT o add a node to a skiplist, a find () call \ufb01lls in the preds [] and succs [] arrays.\nThe new node is created and linked between its predecessors and successors.\nFig. 14.2, Part (b) shows an add(12) call.\nT o remove a victim node from the skiplist, the find () method initializes the\nvictim\u2019s preds [] and succs [] arrays. The victim is then removed from the\nlist at all levels by redirecting each predecessor\u2019s next reference to the victim\u2019s\nsuccessor.\n14.3 A Lock-Based Concurrent Skiplist\nWe now describe the \ufb01rst concurrent skiplist design, the LazySkipList class.\nThis class builds on the LazyList algorithm of Chapter 9: each level of the\nSkipList structure is a LazyList , and as in the LazyList algorithm, the add()\nandremove () methods use optimistic \ufb01ne-grained locking, while the contains ()\nmethod is wait-free.\n14.3.1 A Bird\u2019s-Eye View\nHere is a bird\u2019s-eye view of the LazySkipList class. Start with Fig. 14.3. As in\ntheLazyList class, each node has its own lock and a marked \ufb01eld indicating\nwhether it is in the abstract set, or has been logically removed. All along, the\n332 Chapter 14 Skiplists and Balanced Search\nB: remove(8)\nwill succeed0\n1(a)\n2level\n3\n2\n10\nA: add(18)\nwill fail0\n150\n180\n190\n1110\n10\n1180\n0250\n10\n1key\nfullyLinkedmarkedlock\n0 00 0 0 00 0 0 0\nC: remove(18)\nfails2` 1`\nB: remove(8)\nsucceeds(b)\nlevel\n3\n2\n10\nA: add(18)\nfailskeyfullyLinkedmarkedlock\n0 0 0 1 0 0 0 0 0 0\nC: remove(18)\nsuceeds2`0\n120\n151\n181\n190\n1110\n10\n1180\n1250\n10\n11`\nFigure 14.3 TheLazySkipList class: failed and successful add()andremove ()calls. In Part (a) the add(18)\ncall \ufb01nds the node with key 18 unmarked but not yet fullyLinked . It spins waiting for the node to become\nfullyLinked in Part (b), at which point it returns false. In Part (a) the remove (8)call \ufb01nds the node with key\n8 unmarked and fully linked, which means that it can acquire the node\u2019s lock in Part (b). It then sets the mark\nbit, and proceeds to lock the node\u2019s predecessors, in this case the node with key 5. Once the predecessor\nis locked, it physically removes the node from the list by redirecting the bottom-level reference of the node\nwith key 5, completing the successful remove (). In Part (a) a remove (18)fails, because it found the node not\nfully linked. The same remove (18)call succeeds in Part (b) because it found that the node is fully linked.\nalgorithm maintains the skiplist property : higher-level lists are always contained\nin lower-level lists.\nThe skiplist property is maintained using locks to prevent structural changes\nin the vicinity of a node while it is being added or removed, and by delaying any\naccess to a node until it has been inserted into all levels of the list.\nT o add a node, it must be linked into the list at several levels. Every add()\ncall calls find (), which traverses the skiplist and returns the node\u2019s predecessors\nand successors at all levels. T o prevent changes to the node\u2019s predecessors while\nthe node is being added, add() locks", "doc_id": "28f7d8e4-7af8-400a-a33d-ad8703f6d564", "embedding": null, "doc_hash": "0d632572af8f128f1cc7fa1144522179762f6328970758bc3d3fc4a0f096e1b1", "extra_info": null, "node_info": {"start": 846616, "end": 849650}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "63bdab34-408b-4f57-a3b2-80e23070634e", "3": "a7b58e55-9e55-4714-8337-acc55ffc2c20"}}, "__type__": "1"}, "a7b58e55-9e55-4714-8337-acc55ffc2c20": {"__data__": {"text": "lists are always contained\nin lower-level lists.\nThe skiplist property is maintained using locks to prevent structural changes\nin the vicinity of a node while it is being added or removed, and by delaying any\naccess to a node until it has been inserted into all levels of the list.\nT o add a node, it must be linked into the list at several levels. Every add()\ncall calls find (), which traverses the skiplist and returns the node\u2019s predecessors\nand successors at all levels. T o prevent changes to the node\u2019s predecessors while\nthe node is being added, add() locks the predecessors, validates that the locked\npredecessors still refer to their successors, then adds the node in a manner similar\nto the sequential add() shown in Fig. 14.2. T o maintain the skiplist property, a\nnode is not considered to be logically in the set until all references to it at all\nlevels have been properly set. Each node has an additional \ufb02ag, fullyLinked ,\nset to true once it has been linked in all its levels. We do not allow access to\na node until it is fully linked, so for example, the add() method, when trying\nto determine whether the node it wishes to add is already in the list, must spin\nwaiting for it to become fully linked. Fig. 14.3 shows a call to add(18) that spins\nwaiting until the node with key 18 becomes fully linked.\nT o remove a node from the list, remove () uses find () to check whether a vic-\ntim node with the target key is already in the list. If so, it checks whether the\nvictim is ready to be deleted, that is, is fully linked and unmarked. In Part (a) of\nFig. 14.3, remove (8) \ufb01nds the node with key 8 unmarked and fully linked, which\nmeans that it can remove it. The remove (18) call fails, because it found that the\n14.3 A Lock-Based Concurrent Skiplist 333\nvictim is not fully linked. The same remove (18) call succeeds in Part (b) because\nit found that the victim is fully linked.\nIf the victim can be removed, remove () logically removes it by setting its mark\nbit. It completes the physical deletion of the victim by locking its predecessors\nat all levels and then the victim node itself, validating that the predecessors are\nunmarked and still refer to the victim, and then splicing out the victim node one\nlevel at a time. T o maintain the skiplist property, the victim is spliced out from\ntop to bottom.\nFor example, in Part (b) of Fig. 14.3 ,remove (8) locks the predecessor node\nwith key 5. Once this predecessor is locked, remove () physically removes the node\nfrom the list by redirecting the bottom-level reference of the node with key 5 to\nrefer to the node with key 9.\nIn both the add() and remove () methods, if validation fails, find () is called\nagain to \ufb01nd the newly changed set of predecessors, and the attempt to complete\nthe method resumes.\nThe wait-free contains () method calls find () to locate the node contain-\ning the target key. If it \ufb01nds a node, it determines whether the node is in the\nset by checking whether it is unmarked and fully linked. This method, like the\nLazyList class\u2019s contains (), is wait-free because it ignores any locks or concur-\nrent changes in the SkipList structure.\nT o summarize, the LazySkipList class uses a technique familiar from earlier\nalgorithms: it holds lock on all locations to be modi\ufb01ed, validates that nothing\nimportant has changed, completes the modi\ufb01cations, and releases the locks (in\nthis context, the fullyLinked \ufb02ag acts like a lock).\n14.3.2 The Algorithm\nFig. 14.4", "doc_id": "a7b58e55-9e55-4714-8337-acc55ffc2c20", "embedding": null, "doc_hash": "2d38b71f7c3bcd40c3ac702298f202c0b519808a89c4385e66f9124dbe0da410", "extra_info": null, "node_info": {"start": 849569, "end": 853023}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "28f7d8e4-7af8-400a-a33d-ad8703f6d564", "3": "656fe6a6-f9a1-44af-a953-1d9a2a3d9b8d"}}, "__type__": "1"}, "656fe6a6-f9a1-44af-a953-1d9a2a3d9b8d": {"__data__": {"text": "and fully linked. This method, like the\nLazyList class\u2019s contains (), is wait-free because it ignores any locks or concur-\nrent changes in the SkipList structure.\nT o summarize, the LazySkipList class uses a technique familiar from earlier\nalgorithms: it holds lock on all locations to be modi\ufb01ed, validates that nothing\nimportant has changed, completes the modi\ufb01cations, and releases the locks (in\nthis context, the fullyLinked \ufb02ag acts like a lock).\n14.3.2 The Algorithm\nFig. 14.4 shows the LazySkipList \u2019sNode class. A key is in the set if, and only if\nthe list contains an unmarked, fully linked node with that key. The key 8 in Part\n(a) of Fig. 14.3 , is an example of such a key.\nFig. 14.5 shows the skiplist find () method. (The same method works in both\nthe sequential and concurrent algorithms). The find () method returns \u00001 if the\nitem is not found. It traverses the SkipList using pred and curr references\nstarting at the head and at the highest level.1This highest level can be main-\ntained dynamically to re\ufb02ect the highest level actually in the SkipList , but for\nbrevity, we do not do so here. The find () method goes down the levels one after\nthe other. At each level it sets curr to be the pred node\u2019s successor. If it \ufb01nds a\nnode with a matching key, it records the level (Line 48). If it does not \ufb01nd a node\nwith a matching key, then find () records the pred andcurr as the predeces-\nsor and successor at that level in the preds [] and succs [] arrays (Lines 51\u201352),\ncontinuing to the next lower level starting from the current pred node. Part (a)\n1InFig. 14.5 we make the curr \ufb01eld volatile to prevent compiler optimizations of the loop in\nLine 45. Recall that making the node array volatile does not make the array entries volatile.\n334 Chapter 14 Skiplists and Balanced Search\n1public final class LazySkipList<T> {\n2 static final int MAX_LEVEL = ...;\n3 final Node<T> head = new Node<T>(Integer.MIN_VALUE);\n4 final Node<T> tail = new Node<T>(Integer.MAX_VALUE);\n5 public LazySkipList() {\n6 for (int i = 0; i < head.next.length; i++) {\n7 head.next[i] = tail;\n8 }\n9 }\n10 ...\n11 private static final class Node<T> {\n12 final Lock lock = new ReentrantLock();\n13 final T item;\n14 final int key;\n15 final Node<T>[] next;\n16 volatile boolean marked = false ;\n17 volatile boolean fullyLinked = false ;\n18 private int topLevel;\n19 public Node( int key) { // sentinel node constructor\n20 this .item = null ;\n21 this .key = key;\n22 next = new Node[MAX_LEVEL + 1];\n23 topLevel = MAX_LEVEL;\n24 }\n25 public Node(T x, int height) {\n26 item = x;\n27 key = x.hashCode();\n28 next = new Node[height + 1];\n29 topLevel = height;\n30 }\n31 public void lock() {\n32 lock.lock();\n33 }\n34 public void unlock() {\n35 lock.unlock();\n36 }\n37 }\n38 }\nFigure 14.4 TheLazySkipList class: constructor, \ufb01elds, and Node class.\nofFig. 14.2 shows how find () traverses a SkipList . Part (b) shows how find ()\nresults would be used to add() a new item to a SkipList .\nBecause we start with pred at the head sentinel node and always advance the\nwindow only if curr is less than the target key, pred is always a predecessor of\nthe target", "doc_id": "656fe6a6-f9a1-44af-a953-1d9a2a3d9b8d", "embedding": null, "doc_hash": "240dedba578edd7ff98b8d0b69b3a8b0a593f2e41a0f5be952867598e1e79c22", "extra_info": null, "node_info": {"start": 853082, "end": 856196}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a7b58e55-9e55-4714-8337-acc55ffc2c20", "3": "54f8fd33-4d34-44e1-b2b5-d3a246fa6ca0"}}, "__type__": "1"}, "54f8fd33-4d34-44e1-b2b5-d3a246fa6ca0": {"__data__": {"text": "public void lock() {\n32 lock.lock();\n33 }\n34 public void unlock() {\n35 lock.unlock();\n36 }\n37 }\n38 }\nFigure 14.4 TheLazySkipList class: constructor, \ufb01elds, and Node class.\nofFig. 14.2 shows how find () traverses a SkipList . Part (b) shows how find ()\nresults would be used to add() a new item to a SkipList .\nBecause we start with pred at the head sentinel node and always advance the\nwindow only if curr is less than the target key, pred is always a predecessor of\nthe target key, and never refers to the node with the key itself. The find () method\nreturns the preds [] and succs [] arrays as well as the level at which the node with\na matching key was found.\nThe add(k) method, shown in Fig. 14.6 , uses find () (Fig. 14.5 ) to deter-\nmine whether a node with the target key kis already in the list (Line 42). If an\nunmarked node with the key is found (Lines 62\u201367) then add(k) returns false ,\n14.3 A Lock-Based Concurrent Skiplist 335\n39 int find(T x, Node<T>[] preds, Node<T>[] succs) {\n40 int key = x.hashCode();\n41 int lFound = -1;\n42 Node<T> pred = head;\n43 for (int level = MAX_LEVEL; level >= 0; level--) {\n44 volatile Node<T> curr = pred.next[level];\n45 while (key > curr.key) {\n46 pred = curr; curr = pred.next[level];\n47 }\n48 if(lFound == -1 && key == curr.key) {\n49 lFound = level;\n50 }\n51 preds[level] = pred;\n52 succs[level] = curr;\n53 }\n54 return lFound;\n55 }\nFigure 14.5 The LazySkipList class: the wait-free find ()method. This algorithm is the\nsame as in the sequential SkipList implementation. The preds []andsuccs []arrays are\n\ufb01lled from the maximum level to level 0 with the predecessor and successor references for\nthe given key.\nindicating that the key kis already in the set. However, if that node is not yet\nfully linked (indicated by the fullyLinked \ufb01eld), then the thread waits until it is\nlinked (because the key kis not in the abstract set until the node is fully linked).\nIf the node found is marked, then some other thread is in the process of delet-\ning it, so the add() call simply retries. Otherwise, it checks whether the node is\nunmarked and fully linked, indicating that the add() call should return false . It\nis safe to check if the node is unmarked before the node is fully linked, because\nremove () methods do not mark nodes unless they are fully linked. If a node is\nunmarked and not yet fully linked, it must become unmarked and fully linked\nbefore it can become marked (see Fig. 14.7 ). This step is the linearization point\n(Line 66) of an unsuccessful add() method call.\nThe add() method calls find () to initialize the preds [] and succs [] arrays\nto hold the ostensible predecessor and successor nodes of the node to be added.\nThese references are unreliable, because they may no longer be accurate by the\ntime the nodes are accessed. If no unmarked fully linked node was found with key\nk, then the thread proceeds to lock and validate each of the predecessors returned\nbyfind () from level 0 up to the topLevel of the new node (Lines 74\u201380). T o\navoid deadlocks, both add() and remove () acquire locks in ascending order. The\ntopLevel value is determined at the very beginning of the add() method using\ntherandomLevel () method.2The validation (Line 79) at each level checks that\nthe predecessor is still adjacent to", "doc_id": "54f8fd33-4d34-44e1-b2b5-d3a246fa6ca0", "embedding": null, "doc_hash": "5f85744ffc289d55baa3afc460979400b5d28808385a1bf6a41ace461d4d7557", "extra_info": null, "node_info": {"start": 856212, "end": 859479}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "656fe6a6-f9a1-44af-a953-1d9a2a3d9b8d", "3": "2541c835-8ddc-42aa-83c2-2ed338203a96"}}, "__type__": "1"}, "2541c835-8ddc-42aa-83c2-2ed338203a96": {"__data__": {"text": "because they may no longer be accurate by the\ntime the nodes are accessed. If no unmarked fully linked node was found with key\nk, then the thread proceeds to lock and validate each of the predecessors returned\nbyfind () from level 0 up to the topLevel of the new node (Lines 74\u201380). T o\navoid deadlocks, both add() and remove () acquire locks in ascending order. The\ntopLevel value is determined at the very beginning of the add() method using\ntherandomLevel () method.2The validation (Line 79) at each level checks that\nthe predecessor is still adjacent to the successor and that neither is marked. If\n2The randomLevel () method is designed based on empirical measurements to maintain the\nSkipList property. For example, in the Java concurrency package, for a maximal SkipList\nlevel of 31, randomLevel () returns 0 with probability3\n4,iwith probability 2\u0000(i+2)fori2[1, 30],\nand 31 with probability 2\u000032.\n336 Chapter 14 Skiplists and Balanced Search\n56 boolean add(T x) {\n57 int topLevel = randomLevel();\n58 Node<T>[] preds = (Node<T>[]) new Node[MAX_LEVEL + 1];\n59 Node<T>[] succs = (Node<T>[]) new Node[MAX_LEVEL + 1];\n60 while (true ) {\n61 int lFound = find(x, preds, succs);\n62 if(lFound != -1) {\n63 Node<T> nodeFound = succs[lFound];\n64 if(!nodeFound.marked) {\n65 while (!nodeFound.fullyLinked) {}\n66 return false ;\n67 }\n68 continue ;\n69 }\n70 int highestLocked = -1;\n71 try {\n72 Node<T> pred, succ;\n73 boolean valid = true ;\n74 for (int level = 0; valid && (level <= topLevel); level++) {\n75 pred = preds[level];\n76 succ = succs[level];\n77 pred.lock.lock();\n78 highestLocked = level;\n79 valid = !pred.marked && !succ.marked && pred.next[level]==succ;\n80 }\n81 if(!valid) continue ;\n82 Node<T> newNode = new Node(x, topLevel);\n83 for (int level = 0; level <= topLevel; level++)\n84 newNode.next[level] = succs[level];\n85 for (int level = 0; level <= topLevel; level++)\n86 preds[level].next[level] = newNode;\n87 newNode.fullyLinked = true ;// successful add linearization point\n88 return true ;\n89 }finally {\n90 for (int level = 0; level <= highestLocked; level++)\n91 preds[level].unlock();\n92 }\n93 }\n94 }\nFigure 14.6 TheLazySkipList class: the add()method.\nvalidation fails, the thread must have encountered the effects of a con\ufb02icting\nmethod, so it releases (in the finally block at Line 87) the locks it acquired\nand retries.\nIf the thread successfully locks and validates the results of find () up to the\ntopLevel of the new node, then the add() call will succeed because the thread\nholds all the locks it needs. The thread then allocates a new node with the appro-\npriate key and randomly chosen topLevel , links it in, and sets the new node\u2019s\nfullyLinked \ufb02ag. Setting this \ufb02ag is the linearization point of a successful add()\nmethod (Line 87). It then releases all its locks and returns true (Lines 89). The\nonly time a thread modi\ufb01es an unlocked node\u2019s next \ufb01eld is when it initializes the\n14.3 A Lock-Based Concurrent Skiplist 337\nnew node\u2019s next references (Line 83). This initialization is safe because it occurs\nbefore the new node is", "doc_id": "2541c835-8ddc-42aa-83c2-2ed338203a96", "embedding": null, "doc_hash": "b3108337f630e2d1363fec155d89aa6deb553b9606ff28a7038ed9c73bc37803", "extra_info": null, "node_info": {"start": 859413, "end": 862459}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "54f8fd33-4d34-44e1-b2b5-d3a246fa6ca0", "3": "e7732b8d-f617-4069-9e3a-db7f24ca8381"}}, "__type__": "1"}, "e7732b8d-f617-4069-9e3a-db7f24ca8381": {"__data__": {"text": "with the appro-\npriate key and randomly chosen topLevel , links it in, and sets the new node\u2019s\nfullyLinked \ufb02ag. Setting this \ufb02ag is the linearization point of a successful add()\nmethod (Line 87). It then releases all its locks and returns true (Lines 89). The\nonly time a thread modi\ufb01es an unlocked node\u2019s next \ufb01eld is when it initializes the\n14.3 A Lock-Based Concurrent Skiplist 337\nnew node\u2019s next references (Line 83). This initialization is safe because it occurs\nbefore the new node is accessible.\nTheremove () method appears in Fig. 14.7 . It calls find () to determine whether\na node with the appropriate key is in the list. If so, the thread checks whether the\n95 boolean remove(T x) {\n96 Node<T> victim = null ;boolean isMarked = false ;int topLevel = -1;\n97 Node<T>[] preds = (Node<T>[]) new Node[MAX_LEVEL + 1];\n98 Node<T>[] succs = (Node<T>[]) new Node[MAX_LEVEL + 1];\n99 while (true ) {\n100 int lFound = find(x, preds, succs);\n101 if(lFound != -1) victim = succs[lFound];\n102 if(isMarked ||\n103 (lFound != -1 &&\n104 (victim.fullyLinked\n105 && victim.topLevel == lFound\n106 && !victim.marked))) {\n107 if(!isMarked) {\n108 topLevel = victim.topLevel;\n109 victim.lock.lock();\n110 if(victim.marked) {\n111 victim.lock.unlock();\n112 return false ;\n113 }\n114 victim.marked = true ;\n115 isMarked = true ;\n116 }\n117 int highestLocked = -1;\n118 try {\n119 Node<T> pred, succ; boolean valid = true ;\n120 for (int level = 0; valid && (level <= topLevel); level++) {\n121 pred = preds[level];\n122 pred.lock.lock();\n123 highestLocked = level;\n124 valid = !pred.marked && pred.next[level]==victim;\n125 }\n126 if(!valid) continue ;\n127 for (int level = topLevel; level >= 0; level--) {\n128 preds[level].next[level] = victim.next[level];\n129 }\n130 victim.lock.unlock();\n131 return true ;\n132 }finally {\n133 for (int i = 0; i <= highestLocked; i++) {\n134 preds[i].unlock();\n135 }\n136 }\n137 }else return false ;\n138 }\n139 }\nFigure 14.7 TheLazySkipList class: the remove ()method.\n338 Chapter 14 Skiplists and Balanced Search\nnode is ready to be deleted (Line 104): meaning it is fully linked, unmarked, and\nat its top level. A node found below its top level was either not yet fully linked\n(see the node with key 18 in Part (a) of Fig. 14.3 ), or marked and already partially\nunlinked by a concurrent remove () method call. (The remove () method could\ncontinue, but the subsequent validation would fail.)\nIf the node is ready to be deleted, the thread locks the node (Line 109 ) and\nveri\ufb01es that it is still not marked. If it is still not marked, the thread marks the\nnode, logically deleting that item. This step (Line 114), is the linearization point\nof a successful remove () call. If the node was marked, then the thread returns\nfalse since the node was already deleted. This step is one linearization point of an\nunsuccessful remove (). Another occurs when find () does not \ufb01nd a node with a\nmatching key, or when the node with the matching key was marked, or not fully\nlinked, or not found at its top level (Line 104).\nThe rest of the method completes the physical", "doc_id": "e7732b8d-f617-4069-9e3a-db7f24ca8381", "embedding": null, "doc_hash": "e44e12f0feafef68b3f54f145bd37db66a9799342974cf6b474eb471478f73fc", "extra_info": null, "node_info": {"start": 862513, "end": 865574}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2541c835-8ddc-42aa-83c2-2ed338203a96", "3": "9a28a8e8-b4b1-4767-856f-f579579bab7a"}}, "__type__": "1"}, "9a28a8e8-b4b1-4767-856f-f579579bab7a": {"__data__": {"text": "that it is still not marked. If it is still not marked, the thread marks the\nnode, logically deleting that item. This step (Line 114), is the linearization point\nof a successful remove () call. If the node was marked, then the thread returns\nfalse since the node was already deleted. This step is one linearization point of an\nunsuccessful remove (). Another occurs when find () does not \ufb01nd a node with a\nmatching key, or when the node with the matching key was marked, or not fully\nlinked, or not found at its top level (Line 104).\nThe rest of the method completes the physical deletion of the victim node.\nT o remove the victim from the list, the remove () method \ufb01rst locks (in ascending\norder, to avoid deadlock) the victim\u2019s predecessors at all levels up to the victim\u2019s\ntopLevel (Lines 120 \u2013124). After locking each predecessor, it validates that the\npredecessor is still unmarked and still refers to the victim. It then splices out the\nvictim one level at a time (Line 128 ). T o maintain the SkipList property, that\nany node reachable at a given level is reachable at lower levels, the victim is spliced\nout from top to bottom. If the validation fails at any level, then the thread releases\nthe locks for the predecessors (but not the victim) and calls find () to acquire the\nnew set of predecessors. Because it has already set the victim\u2019s isMarked \ufb01eld, it\ndoes not try to mark the node again. After successfully removing the victim node\nfrom the list, the thread releases all its locks and returns true.\nFinally, we recall that if no node was found, or the node found was marked,\nor not fully linked, or not found at its top level, then the method simply returns\nfalse . It is easy to see that it is correct to return false if the node is not marked,\nbecause for any key, there can at any time be at most one node with this key in the\nSkipList (i.e., reachable from the head ). Moreover, once a node is entered into\nthe list, (which must have occurred before it is found by find ()), it cannot be\nremoved until it is marked. It follows that if the node is not marked, and not all\nits links are in place, it must be in the process of being added into the SkipList ,\nbut the adding method has not reached the linearization point (see the node with\nkey 18 in Part (a) of Fig. 14.3 ).\nIf the node is marked at the time it is found, it might not be in the list, and\nsome unmarked node with the same key may be in the list. However, in that case,\njust like for the LazyList remove () method, there must have been some point\nduring the remove () call when the key was not in the abstract set.\nThe wait-free contains () method ( Fig. 14.8 ) calls find () to locate the node\ncontaining the target key. If it \ufb01nds a node it checks whether it is unmarked\nand fully linked. This method, like that of the LazyList class of Chapter 9 , is\nwait-free, ignoring any locks or concurrent changes in the SkipList list struc-\nture. A successful contains () call\u2019s linearization point occurs when the prede-\ncessor\u2019s next reference is traversed, having been observed to be unmarked and\n14.4 A Lock-Free Concurrent Skiplist 339\n140 boolean contains(T x) {\n141 Node<T>[] preds = (Node<T>[]) new Node[MAX_LEVEL + 1];\n142 Node<T>[] succs = (Node<T>[]) new Node[MAX_LEVEL + 1];\n143 int lFound = find(x, preds, succs);\n144 return (lFound != -1\n145 && succs[lFound].fullyLinked\n146 && !succs[lFound].marked);\n147 }\nFigure 14.8 TheLazySkipList class: the", "doc_id": "9a28a8e8-b4b1-4767-856f-f579579bab7a", "embedding": null, "doc_hash": "70c8fbe813d1cb1b643100c3bacc6526098d227edd60506433c443d59be54270", "extra_info": null, "node_info": {"start": 865509, "end": 868945}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e7732b8d-f617-4069-9e3a-db7f24ca8381", "3": "81b75af8-dbab-4f15-a7d3-fd2cc0ca81fe"}}, "__type__": "1"}, "81b75af8-dbab-4f15-a7d3-fd2cc0ca81fe": {"__data__": {"text": "and\n14.4 A Lock-Free Concurrent Skiplist 339\n140 boolean contains(T x) {\n141 Node<T>[] preds = (Node<T>[]) new Node[MAX_LEVEL + 1];\n142 Node<T>[] succs = (Node<T>[]) new Node[MAX_LEVEL + 1];\n143 int lFound = find(x, preds, succs);\n144 return (lFound != -1\n145 && succs[lFound].fullyLinked\n146 && !succs[lFound].marked);\n147 }\nFigure 14.8 TheLazySkipList class: the wait-free contains ()method.\nfully linked. An unsuccessful contains () call, like the remove () call, occurs if the\nmethod \ufb01nds a node that is marked. Care is needed, because at the time the node\nis found, it might not be in the list, while an unmarked node with the same key\nmay be in the list. As with remove (), however, there must have been some point\nduring the contains () call when the key was not in the abstract set.\n14.4 A Lock-Free Concurrent Skiplist\nThe basis of our LockFreeSkipList implementation is the LockFreeList\nalgorithm of Chapter 9: each level of the SkipList structure is a LockFreeList ,\neach next reference in a node is an AtomicMarkableReference<Node> , and\nlist manipulations are performed using compareAndSet() .\n14.4.1 A Bird\u2019s-Eye View\nHere is a bird\u2019s-eye view of the of the LockFreeSkipList class.\nBecause we cannot use locks to manipulate references at all levels at the same\ntime, the LockFreeSkipList cannot maintain the SkipList property that each\nlist is a sublist of the list at levels below it.\nSince we cannot maintain the skiplist property, we take the approach that the\nabstract set is de\ufb01ned by the bottom-level list: a key is in the set if there is a node\nwith that key whose next reference is unmarked in the bottom-level list. Nodes in\nhigher-level lists in the skiplist serve only as shortcuts to the bottom level. There\nis no need for a fullyLinked \ufb02ag as in the LazySkipList .\nHow do we add or remove a node? We treat each level of the list as a\nLockFreeList . We use compareAndSet() to insert a node at a given level, and\nwe mark the next references of a node to remove it.\nAs in the LockFreeList , the find () method cleans up marked nodes. The\nmethod traverses the skiplist, proceeding down each list at each level. As in the\nLockFreeList class\u2019s find () method, it repeatedly snips out marked nodes as\nthey are encountered, so that it never looks at a marked node\u2019s key. Unfortu-\nnately, this means that a node may be physically removed while it is in the process\n340 Chapter 14 Skiplists and Balanced Search\nof being linked at the higher levels. A find () call that passes through a node\u2019s\nmiddle-level references may remove these references, so, as noted earlier, the\nSkipList property is not maintained.\nTheadd() method calls find () to determine whether a node is already in the\nlist, and to \ufb01nd its set of predecessors and successors. A new node is prepared with\na randomly chosen topLevel , and its next references are directed to the potential\nsuccessors returned by the find () call. The next step is to try to logically add the\nnew node to the abstract set by linking it into the bottom-level list, using the same\napproach as in the LockFreeList . If the addition succeeds, the item is logically in\nthe set. The add() call then links the node in at higher levels (up to its top level).\nFig. 14.9 shows the LockFreeSkipList class. In Part (a) add(12) calls\nfind (12) while there are three", "doc_id": "81b75af8-dbab-4f15-a7d3-fd2cc0ca81fe", "embedding": null, "doc_hash": "ae7e49ba11f82206bf5f0607500f3cc372d63b3f73bad1f4d16cf236557be3e7", "extra_info": null, "node_info": {"start": 869106, "end": 872421}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9a28a8e8-b4b1-4767-856f-f579579bab7a", "3": "485d2b29-db88-412c-92d5-a863a54758e9"}}, "__type__": "1"}, "485d2b29-db88-412c-92d5-a863a54758e9": {"__data__": {"text": "prepared with\na randomly chosen topLevel , and its next references are directed to the potential\nsuccessors returned by the find () call. The next step is to try to logically add the\nnew node to the abstract set by linking it into the bottom-level list, using the same\napproach as in the LockFreeList . If the addition succeeds, the item is logically in\nthe set. The add() call then links the node in at higher levels (up to its top level).\nFig. 14.9 shows the LockFreeSkipList class. In Part (a) add(12) calls\nfind (12) while there are three ongoing remove () calls. Part (b) shows the results\n9 51 8 11 25 15 8 23\n2\n10\n/H11546/H11557 /H11545/H11557(a)   A: add(12)\nA: find(12) level\n0\n0\n0\n00\n0\n0\n0 011\n00\n10\n0 0 0 1 0\nB: remove(2) C: remove(9)  D: remove(15)9 51 8 11 25 8 23\n2\n1\n0\n/H11546/H11557 /H11545/H11557(b)   A: add(12)\nA: find(12) level\n0\n0\n0\n00\n0\n0\n0 011\n00\n10\n0 0 0 0\nB: remove(2) C: remove(9) D: remove(15)\nB: remove(2) C: remove(9)A: insert(12)9 51 8 11 25 8 23\n2\n1\n0\n/H11546/H11557 /H11545/H11557(c)   A: add(12)\nlevel\n0\n0\n0\n00\n0\n0\n0 011\n00\n10\n0\n120\n00 0 0\nC: remove(9)A: insert(12)\nB: remove(11)9 51 8 2 5 83\n2\n1\n0\n/H11546/H11557 /H11545/H11557(d)   A: add(12) after remove(11)\nlevel\n0\n0\n0\n00\n0\n0\n0 011\n00 0\n0\n120\n00 0\nFigure 14.9 TheLockFreeSkipList class: an add()call. Each node consists of links that are unmarked (a 0)\nor marked (a 1). In Part (a), add(12)calls find (12)while there are three ongoing remove ()calls. The find ()\nmethod \u201ccleans\u201d the marked links (denoted by 1s) as it traverses the skiplist. The traversal is not the same\nas a sequential find (12), because marked nodes are unlinked whenever they are encountered. The path in\nthe \ufb01gure shows the nodes traversed by the pred reference, which always refers to unmarked nodes with\nkeys less than the target key. Part (b) shows the result of redirecting the dashed links. We denote bypassing a\nnode by placing the link in front of it. Node 15, whose bottom-level next reference was marked, is removed\nfrom the skiplist. Part (c) shows the subsequent addition of the new node with key 12. Part (d) shows an\nalternate addition scenario which would occur if the node with key 11 were removed before the addition of\nthe node with key 12. The bottom-level next reference of the node with key 9 is not yet marked, and so the\nbottom-level predecessor node, whose next reference is marked, is redirected by the add()method to the\nnew node. Once thread Ccompletes marking this reference, the node with key 9 is removed and the node\nwith key 5 becomes the immediate predecessor of the newly added node.\n14.4 A Lock-Free Concurrent Skiplist 341\nof redirecting the dashed links. Part (c) shows the subsequent addition of the new\nnode with key 12. Part (d) shows an alternate addition scenario which would occur\nif the node with key 11 were removed before the", "doc_id": "485d2b29-db88-412c-92d5-a863a54758e9", "embedding": null, "doc_hash": "925c052fa43e2620e1b4ce095af0120bdc303ea9fa73046db018131853442ee0", "extra_info": null, "node_info": {"start": 872291, "end": 875121}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "81b75af8-dbab-4f15-a7d3-fd2cc0ca81fe", "3": "dd456587-ab8e-41dc-bfc6-03cf9489796a"}}, "__type__": "1"}, "dd456587-ab8e-41dc-bfc6-03cf9489796a": {"__data__": {"text": "yet marked, and so the\nbottom-level predecessor node, whose next reference is marked, is redirected by the add()method to the\nnew node. Once thread Ccompletes marking this reference, the node with key 9 is removed and the node\nwith key 5 becomes the immediate predecessor of the newly added node.\n14.4 A Lock-Free Concurrent Skiplist 341\nof redirecting the dashed links. Part (c) shows the subsequent addition of the new\nnode with key 12. Part (d) shows an alternate addition scenario which would occur\nif the node with key 11 were removed before the addition of the node with key 12.\nTheremove () method calls find () to determine whether an unmarked node\nwith the target key is in the bottom-level list. If an unmarked node is found, it is\nmarked starting from the topLevel . All next references up to, but not includ-\ning the bottom-level reference are logically removed from their appropriate level\nlist by marking them. Once all levels but the bottom one have been marked,\nthe method marks the bottom-level\u2019s next reference. This marking, if successful,\nremoves the item from the abstract set. The physical removal of the node is the\nresult of its physical removal from the lists at all levels by the remove () method\nitself and the find () methods of other threads that access it while traversing the\nskiplist. In both add() and remove (), if at any point a compareAndSet() fails,\nthe set of predecessors and successors might have changed, and so find () must\nbe called again.\nThe key to the interaction between the add(),remove (), and find () methods\nis the order in which list manipulations take place. The add() method sets its\nnext references to the successors before it links the node into the bottom-level\nlist, meaning that a node is ready to be removed from the moment it is logi-\ncally added to the list. Similarly, the remove () method marks the next references\ntop-down, so that once a node is logically removed, it is not traversed by a find ()\nmethod call.\nAs noted, in most applications, calls to contains () usually outnumber calls\nto other methods. As a result contains () should not call find (). While it may\nbe effective to have individual find () calls physically remove logically deleted\nnodes, contention results if too many concurrent find () calls try to clean up the\nsame nodes at the same time. This kind of contention is much more likely with\nfrequent contains () calls than with calls to the other methods.\nHowever, contains () cannot use the approach taken by the LockFreeList \u2019s\nwait-free contains (): look at the keys of all reachable nodes independently of\nwhether they are marked or not. The problem is that add() and remove () may\nviolate the skiplist property. It is possible for a marked node to be reachable in a\nhigher-level list after being physically deleted from the lowest-level list. Ignoring\nthe mark could lead to skipping over nodes reachable in the lowest level.\nNotice, however, that the find () method of the LockFreeSkipList is not\nsubject to this problem because it never looks at keys of marked nodes, removing\nthem instead. We will have the contains () method mimic this behavior, but\nwithout cleaning up marked nodes. Instead, contains () traverses the skiplist,\nignoring the keys of marked nodes, and skipping over them instead of physically\nremoving them. Avoiding the physical removal allows the method to be wait-free.\n14.4.2 The Algorithm in Detail\nAs we present the algorithmic details, the reader should keep in mind that the\nabstract set is de\ufb01ned only by the bottom-level list. Nodes in the higher-level\n342 Chapter 14 Skiplists and Balanced Search\nlists are used only as shortcuts into the bottom-level list. Fig. 14.10 shows the\nstructure of the list\u2019s nodes.\nTheadd()", "doc_id": "dd456587-ab8e-41dc-bfc6-03cf9489796a", "embedding": null, "doc_hash": "5266bc24ddbcf5f3a096af46656f0c86f56145ccedcabde9ed30d025331d8c5b", "extra_info": null, "node_info": {"start": 875109, "end": 878849}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "485d2b29-db88-412c-92d5-a863a54758e9", "3": "e439006d-71c5-4ca6-a7f0-7f97c7b2973a"}}, "__type__": "1"}, "e439006d-71c5-4ca6-a7f0-7f97c7b2973a": {"__data__": {"text": "the skiplist,\nignoring the keys of marked nodes, and skipping over them instead of physically\nremoving them. Avoiding the physical removal allows the method to be wait-free.\n14.4.2 The Algorithm in Detail\nAs we present the algorithmic details, the reader should keep in mind that the\nabstract set is de\ufb01ned only by the bottom-level list. Nodes in the higher-level\n342 Chapter 14 Skiplists and Balanced Search\nlists are used only as shortcuts into the bottom-level list. Fig. 14.10 shows the\nstructure of the list\u2019s nodes.\nTheadd() method, shown in Fig. 14.11 , uses find (), shown in Fig. 14.13 , to\ndetermine whether a node with key kis already in the list (Line 61). As in the\nLazySkipList ,add() calls find () to initialize the preds [] and succs [] arrays to\nhold the new node\u2019s ostensible predecessors and successors.\nIf an unmarked node with the target key is found in the bottom-level list,\nfind () returns true and the add() method returns false , indicating that the key is\nalready in the set. The unsuccessful add()\u2019s linearization point is the same as the\nsuccessful find ()\u2019s (Line 42). If no node is found, then the next step is to try to\nadd a new node with the key into the structure.\n1public final class LockFreeSkipList<T> {\n2 static final int MAX_LEVEL = ...;\n3 final Node<T> head = new Node<T>(Integer.MIN_VALUE);\n4 final Node<T> tail = new Node<T>(Integer.MAX_VALUE);\n5 public LockFreeSkipList() {\n6 for (int i = 0; i < head.next.length; i++) {\n7 head.next[i]\n8 =new AtomicMarkableReference<LockFreeSkipList.Node<T>>(tail, false );\n9 }\n10 }\n11 public static final class Node<T> {\n12 final T value; final int key;\n13 final AtomicMarkableReference<Node<T>>[] next;\n14 private int topLevel;\n15 // constructor for sentinel nodes\n16 public Node( int key) {\n17 value = null ; key = key;\n18 next = (AtomicMarkableReference<Node<T>>[])\n19 new AtomicMarkableReference[MAX_LEVEL + 1];\n20 for (int i = 0; i < next.length; i++) {\n21 next[i] = new AtomicMarkableReference<Node<T>>( null ,false );\n22 }\n23 topLevel = MAX_LEVEL;\n24 }\n25 // constructor for ordinary nodes\n26 public Node(T x, int height) {\n27 value = x;\n28 key = x.hashCode();\n29 next = (AtomicMarkableReference<Node<T>>[])\nnew AtomicMarkableReference[height + 1];\n30 for (int i = 0; i < next.length; i++) {\n31 next[i] = new AtomicMarkableReference<Node<T>>( null ,false );\n32 }\n33 topLevel = height;\n34 }\n35 }\nFigure 14.10 TheLockFreeSkipList class: \ufb01elds and constructor.\n14.4 A Lock-Free Concurrent Skiplist 343\nA new node is created with a randomly chosen topLevel . The node\u2019s next\nreferences are unmarked and set to the successors returned by the find () method\n(Lines 46\u201349).\nThe next step is to try to add the new node by linking it into the bottom-level\nlist between the preds [0] and succs [0] nodes returned by find (). As in the\nLockFreeList , we use compareAndSet() to set the reference while validating\nthat these nodes still refer one to the other and have not been removed from the\nlist (Line 54). If the compareAndSet() fails, something has changed and the call\nrestarts. If the compareAndSet() succeeds, the item is added, and Line", "doc_id": "e439006d-71c5-4ca6-a7f0-7f97c7b2973a", "embedding": null, "doc_hash": "92b69ccba6f97ca2a3407e39d4cd3dd932feb9a5d19e6335b31ab5a8c296dd5c", "extra_info": null, "node_info": {"start": 878860, "end": 881979}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "dd456587-ab8e-41dc-bfc6-03cf9489796a", "3": "ef844671-48f6-427c-82be-112a825ae0fd"}}, "__type__": "1"}, "ef844671-48f6-427c-82be-112a825ae0fd": {"__data__": {"text": "the successors returned by the find () method\n(Lines 46\u201349).\nThe next step is to try to add the new node by linking it into the bottom-level\nlist between the preds [0] and succs [0] nodes returned by find (). As in the\nLockFreeList , we use compareAndSet() to set the reference while validating\nthat these nodes still refer one to the other and have not been removed from the\nlist (Line 54). If the compareAndSet() fails, something has changed and the call\nrestarts. If the compareAndSet() succeeds, the item is added, and Line 54is the\ncall\u2019s linearization point.\nTheadd() then links the node in at higher levels (Line 57). For each level, it\nattempts to splice the node in by setting the predecessor, if it refers to the valid\n36 boolean add(T x) {\n37 int topLevel = randomLevel();\n38 int bottomLevel = 0;\n39 Node<T>[] preds = (Node<T>[]) new Node[MAX_LEVEL + 1];\n40 Node<T>[] succs = (Node<T>[]) new Node[MAX_LEVEL + 1];\n41 while (true ) {\n42 boolean found = find(x, preds, succs);\n43 if(found) {\n44 return false ;\n45 }else {\n46 Node<T> newNode = new Node(x, topLevel);\n47 for (int level = bottomLevel; level <= topLevel; level++) {\n48 Node<T> succ = succs[level];\n49 newNode.next[level].set(succ, false );\n50 }\n51 Node<T> pred = preds[bottomLevel];\n52 Node<T> succ = succs[bottomLevel];\n53 if(!pred.next[bottomLevel].compareAndSet(succ, newNode,\n54 false ,false )) {\n55 continue ;\n56 }\n57 for (int level = bottomLevel+1; level <= topLevel; level++) {\n58 while (true ) {\n59 pred = preds[level];\n60 succ = succs[level];\n61 if(pred.next[level].compareAndSet(succ, newNode, false ,false ))\n62 break ;\n63 find(x, preds, succs);\n64 }\n65 }\n66 return true ;\n67 }\n68 }\n69 }\nFigure 14.11 TheLockFreeSkipList class: the add()method.\n344 Chapter 14 Skiplists and Balanced Search\nsuccessor, to the new node (Line 61). If successful, it breaks and moves on to the\nnext level. If unsuccessful, then the node referenced by the predecessor must have\nchanged, and find () is called again to \ufb01nd a new valid set of predecessors and\nsuccessors. We discard the result of calling find () (Line 63) because we care only\nabout recomputing the ostensible predecessors and successors on the remaining\nunlinked levels. Once all levels are linked, the method returns true (Line 66).\nTheremove () method, shown in Fig. 14.12 , calls find () to determine whether\nan unmarked node with a matching key is in the bottom-level list. If no node is\nfound in the bottom-level list, or the node with a matching key is marked, the\nmethod returns false. The linearization point of the unsuccessful remove () is that\nof the find () method called in Line 76. If an unmarked node is found, then the\nmethod logically removes the associated key from the abstract set, and prepares\n70 boolean remove(T x) {\n71 int bottomLevel = 0;\n72 Node<T>[] preds = (Node<T>[]) new Node[MAX_LEVEL + 1];\n73 Node<T>[] succs = (Node<T>[]) new Node[MAX_LEVEL + 1];\n74 Node<T> succ;\n75 while (true ) {\n76 boolean found = find(x, preds, succs);\n77 if(!found) {\n78 return false ;\n79 }else", "doc_id": "ef844671-48f6-427c-82be-112a825ae0fd", "embedding": null, "doc_hash": "4d8bd774df4da8bb53373c76222f37a9827fe12071c51172787c7e2e16f0da97", "extra_info": null, "node_info": {"start": 881993, "end": 885018}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e439006d-71c5-4ca6-a7f0-7f97c7b2973a", "3": "32a546d9-9e1a-4aab-a42b-68c284303937"}}, "__type__": "1"}, "32a546d9-9e1a-4aab-a42b-68c284303937": {"__data__": {"text": "If an unmarked node is found, then the\nmethod logically removes the associated key from the abstract set, and prepares\n70 boolean remove(T x) {\n71 int bottomLevel = 0;\n72 Node<T>[] preds = (Node<T>[]) new Node[MAX_LEVEL + 1];\n73 Node<T>[] succs = (Node<T>[]) new Node[MAX_LEVEL + 1];\n74 Node<T> succ;\n75 while (true ) {\n76 boolean found = find(x, preds, succs);\n77 if(!found) {\n78 return false ;\n79 }else {\n80 Node<T> nodeToRemove = succs[bottomLevel];\n81 for (int level = nodeToRemove.topLevel;\n82 level >= bottomLevel+1; level--) {\n83 boolean [] marked = { false };\n84 succ = nodeToRemove.next[level].get(marked);\n85 while (!marked[0]) {\n86 nodeToRemove.next[level].compareAndSet(succ, succ, false ,true );\n87 succ = nodeToRemove.next[level].get(marked);\n88 }\n89 }\n90 boolean [] marked = { false };\n91 succ = nodeToRemove.next[bottomLevel].get(marked);\n92 while (true ) {\n93 boolean iMarkedIt =\n94 nodeToRemove.next[bottomLevel].compareAndSet(succ, succ,\n95 false ,true );\n96 succ = succs[bottomLevel].next[bottomLevel].get(marked);\n97 if(iMarkedIt) {\n98 find(x, preds, succs);\n99 return true ;\n100 }\n101 else if (marked[0]) return false ;\n102 }\n103 }\n104 }\n105 }\nFigure 14.12 TheLockFreeSkipList class: the remove ()method.\n14.4 A Lock-Free Concurrent Skiplist 345\nit for physical removal. This step uses the set of ostensible predecessors (stored by\nfind () in preds []) and the nodeToRemove (returned from find () in succs []).\nFirst, starting from the topLevel , all links up to and not including the bottom-\nlevel link are marked (Lines 82\u201388) by repeatedly reading next and its mark and\napplying a compareAndSet() . If the link is found to be marked (either because\nit was already marked or because the attempt succeeded) the method moves on\nto the next-level link. Otherwise, the current level\u2019s link is reread since it must\nhave been changed by another concurrent thread, so the marking attempt must\nbe repeated. Once all levels but the bottom one have been marked, the method\nmarks the bottom-level\u2019s next reference. This marking (Line 95), if successful, is\nthe linearization point of a successful remove (). The remove () method tries to\nmark the next \ufb01eld using compareAndSet() . If successful, it can determine that\nit was the thread that changed the mark from false totrue. Before returning true,\nthefind () method is called again. This call is an optimization: as a side effect,\nfind () physically removes all links to the node it is searching for if that node is\nalready logically removed.\nOn the other hand, if the compareAndSet() call failed, but the next reference\nis marked, then another thread must have concurrently removed it, so remove ()\nreturns false . The linearization point of this unsuccessful remove () is the lin-\nearization point of the remove () method by the thread that successfully marked\nthenext \ufb01eld. Notice that this linearization point must occur during the remove ()\ncall because the find () call found the node unmarked before it found it marked.\nFinally, if the compareAndSet() fails and the node is unmarked, then the next\nnode must have changed concurrently. Since the nodeToRemove is known, there\nis no need to call find () again, and remove () simply uses the new value read\nfrom next to retry the marking.\nAs noted, both the add()", "doc_id": "32a546d9-9e1a-4aab-a42b-68c284303937", "embedding": null, "doc_hash": "e1ee58967561d583b8839134d7829d79005cea08a087d11893659245d73b9812", "extra_info": null, "node_info": {"start": 885116, "end": 888397}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ef844671-48f6-427c-82be-112a825ae0fd", "3": "47b5abf6-5d3e-440f-98c4-b582c4ad3b5c"}}, "__type__": "1"}, "47b5abf6-5d3e-440f-98c4-b582c4ad3b5c": {"__data__": {"text": "false . The linearization point of this unsuccessful remove () is the lin-\nearization point of the remove () method by the thread that successfully marked\nthenext \ufb01eld. Notice that this linearization point must occur during the remove ()\ncall because the find () call found the node unmarked before it found it marked.\nFinally, if the compareAndSet() fails and the node is unmarked, then the next\nnode must have changed concurrently. Since the nodeToRemove is known, there\nis no need to call find () again, and remove () simply uses the new value read\nfrom next to retry the marking.\nAs noted, both the add() and remove () methods rely on find (). This method\nsearches the LockFreeSkipList , returning true if and only if a node with the\ntarget key is in the set. It \ufb01lls in the preds [] and succs [] arrays with the tar-\nget node\u2019s ostensible predecessors and successors at each level. It maintains the\nfollowing two properties:\n\u0004It never traverses a marked link. Instead, it removes the node referred to by a\nmarked link from the list at that level.\n\u0004Every preds [] reference is to a node with a key strictly less than the target.\nThe find () method in Fig. 14.13 proceeds as follows. It starts traversing\ntheSkipList from the topLevel of the head sentinel, which has the maximal\nallowed node level. It then proceeds in each level down the list, \ufb01lling in preds\nandsuccs nodes that are repeatedly advanced until pred refers to a node with\nthe largest value on that level that is strictly less than the target key (Lines 117\u2013\n131). As in the LockFreeList , it repeatedly snips out marked nodes from the\ngiven level as they are encountered (Lines 119\u2013125) using a compareAndSet() .\nNotice that the compareAndSet() validates that the next \ufb01eld of the predecessor\nreferences the current node. Once an unmarked curr is found (Line 126 ), it is\ntested to see if its key is less than the target key. If so, pred is advanced to curr .\n346 Chapter 14 Skiplists and Balanced Search\n106 boolean find(T x, Node<T>[] preds, Node<T>[] succs) {\n107 int bottomLevel = 0;\n108 int key = x.hashCode();\n109 boolean [] marked = { false };\n110 boolean snip;\n111 Node<T> pred = null , curr = null , succ = null ;\n112 retry:\n113 while (true ) {\n114 pred = head;\n115 for (int level = MAX_LEVEL; level >= bottomLevel; level--) {\n116 curr = pred.next[level].getReference();\n117 while (true ) {\n118 succ = curr.next[level].get(marked);\n119 while (marked[0]) {\n120 snip = pred.next[level].compareAndSet(curr, succ,\n121 false ,false );\n122 if(!snip) continue retry;\n123 curr = pred.next[level].getReference();\n124 succ = curr.next[level].get(marked);\n125 }\n126 if(curr.key < key){\n127 pred = curr; curr = succ;\n128 }else {\n129 break ;\n130 }\n131 }\n132 preds[level] = pred;\n133 succs[level] = curr;\n134 }\n135 return (curr.key == key);\n136 }\n137 }\nFigure 14.13 TheLockFreeSkipList class: a more complex find ()than in LazySkipList .\nOtherwise, curr \u2019s key is greater than or equal to the target\u2019s, so the current value\nofpred is the target node\u2019s immediate predecessor. The find () method breaks\nout of the current level search loop, saving the current values of pred andcurr\n(Line 132).\nThefind () method proceeds this way until it reaches the bottom level. Here\nis an important", "doc_id": "47b5abf6-5d3e-440f-98c4-b582c4ad3b5c", "embedding": null, "doc_hash": "249a12294f3d40bb2f09492ce6a3a89340f4db74459d277e515e316d17811e01", "extra_info": null, "node_info": {"start": 888231, "end": 891479}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "32a546d9-9e1a-4aab-a42b-68c284303937", "3": "348ba703-76fa-45a2-8662-02b9a83eaf04"}}, "__type__": "1"}, "348ba703-76fa-45a2-8662-02b9a83eaf04": {"__data__": {"text": "succs[level] = curr;\n134 }\n135 return (curr.key == key);\n136 }\n137 }\nFigure 14.13 TheLockFreeSkipList class: a more complex find ()than in LazySkipList .\nOtherwise, curr \u2019s key is greater than or equal to the target\u2019s, so the current value\nofpred is the target node\u2019s immediate predecessor. The find () method breaks\nout of the current level search loop, saving the current values of pred andcurr\n(Line 132).\nThefind () method proceeds this way until it reaches the bottom level. Here\nis an important point: the traversal at each level maintains the two properties\ndescribed earlier. In particular, if a node with the target key is in the list, it\nwill be found at the bottom level even if traversed nodes are removed at higher\nlevels. When the traversal stops, pred refers to a predecessor of the target node.\nThe method descends to each next lower level without skipping over the target\nnode. If the node is in the list, it will be found at the bottom level. Moreover, if the\nnode is found, it cannot be marked because if it were marked, it would have been\nsnipped out in Lines 119\u2013125. Therefore, the test in Line 135need only check if\nthe key of curr is equal to the target key to determine if the target is in the set.\nThe linearization points of both successful and unsuccessful calls to the find ()\nmethods occur when the curr reference at the bottom-level list is set, at either\n14.4 A Lock-Free Concurrent Skiplist 347\nLine 116 or123 , for the last time before the find () call\u2019s success or failure is\ndetermined in Line 135.Fig. 14.9 shows how a node is successfully added to the\nLockFreeSkipList .\nThe wait-free contains () method appears in Fig. 14.14 . It traverses the\nSkipList in the same way as the find () method, descending level-by-level from\nthehead . Like find (),contains () ignores keys of marked nodes. Unlike find (),\nit does not try to remove marked nodes. Instead, it simply jumps over them\n(Line 147\u2013150). For an example execution, see Fig. 14.15 .\nThe method is correct because contains () preserves the same properties as\nfind (), among them, that pred , in any level, never refers to an unmarked node\nwhose key is greater than or equal to the target key. The pred variable arrives at the\nbottom-level list at a node before, and never after, the target node. If the node is\nadded before the contains () method call starts, then it will be found. Moreover,\nrecall that add() calls find (), which unlinks marked nodes from the bottom-level\nlist before adding the new node. It follows that if contains () does not \ufb01nd the\ndesired node, or \ufb01nds the desired node at the bottom level but marked, then any\nconcurrently added node that was not found must have been added to the bottom\nlevel after the start of the contains () call, so it is correct to return false in Line 159.\nFig. 14.16 shows an execution of the contains () method. In Part (a), a\ncontains (18) call traverses the list starting from the top level of the head node.\nIn Part (b) the contains (18) call traverses the list after the node with key 18 has\nbeen logically removed.\n138 boolean contains(T x) {\n139 int bottomLevel = 0;\n140 int v = x.hashCode();\n141 boolean [] marked = { false };\n142 Node<T> pred = head, curr = null , succ = null ;\n143 for (int level = MAX_LEVEL; level >= bottomLevel; level--) {\n144 curr = curr.next[level].getReference();\n145 while (true ) {\n146 succ = curr.next[level].get(marked);\n147 while (marked[0]) {\n148 curr = pred.next[level].getReference();\n149 succ =", "doc_id": "348ba703-76fa-45a2-8662-02b9a83eaf04", "embedding": null, "doc_hash": "6acb9e81af061d635549ab4523b9aa6ab5db1a58eaec0cb06ff88977e1c696e1", "extra_info": null, "node_info": {"start": 891565, "end": 895045}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "47b5abf6-5d3e-440f-98c4-b582c4ad3b5c", "3": "f1de361f-3c94-41f0-970f-80376e5ab2cb"}}, "__type__": "1"}, "f1de361f-3c94-41f0-970f-80376e5ab2cb": {"__data__": {"text": "boolean contains(T x) {\n139 int bottomLevel = 0;\n140 int v = x.hashCode();\n141 boolean [] marked = { false };\n142 Node<T> pred = head, curr = null , succ = null ;\n143 for (int level = MAX_LEVEL; level >= bottomLevel; level--) {\n144 curr = curr.next[level].getReference();\n145 while (true ) {\n146 succ = curr.next[level].get(marked);\n147 while (marked[0]) {\n148 curr = pred.next[level].getReference();\n149 succ = curr.next[level].get(marked);\n150 }\n151 if(curr.key < v){\n152 pred = curr;\n153 curr = succ;\n154 }else {\n155 break ;\n156 }\n157 }\n158 }\n159 return (curr.key == v);\n160 }\nFigure 14.14 TheLockFreeSkipList class: the wait-free contains ()method.\n348 Chapter 14 Skiplists and Balanced Search\n9 51 8 11 25 15 8 23\n2\n1\n0\n2` 1`A: contains(18) returns true\nlevel\n0\n0\n0\n00\n0\n0\n0 111\n00\n00\n0 0 0 1 0\nB: remove(9) C: remove(15)curr\npredcurr\ncurr\ncurr\nFigure 14.15 Thread Acalls contains (18), which traverses the list starting from the top\nlevel of the head node. The dotted line marks the traversal by the pred \ufb01eld, and the sparse\ndotted line marks the path of the curr \ufb01eld. The curr \ufb01eld is advanced to tail on level 3.\nSince its key is greater than 18, pred descends to level 2. The curr \ufb01eld advances past the\nmarked reference in the node with key 9, again reaching tail which is greater than 18, so\npred descends to level 1. Here pred is advanced to the unmarked node with key 5, and\ncurr advances past the marked node with key 9 to reach the unmarked node with key 18, at\nwhich point curr is no longer advanced. Though 18 is the target key, the method continues\nto descend with pred to the bottom level, advancing pred to the node with key 8. From this\npoint, curr traverses past marked Nodes 9 and 15 and Node 11 whose key is smaller than\n18. Eventually curr reaches the unmarked node with key 18, returning true.\n14.5 Concurrent Skiplists\nWe have seen two highly concurrent SkipList implementations, each providing\nlogarithmic search without the need to rebalance. In the LazySkipList class,\ntheadd() and remove () methods use optimistic \ufb01ne-grained locking, meaning\nthat the method searches for its target node without locking, and acquires locks\nand validates only when it discovers the target. The contains () method, usu-\nally the most common, is wait-free. In the LockFreeSkipList class, the add()\nand remove () methods are lock-free, building on the LockFreeList class of\nChapter 9. In this class too, the contains () method is wait-free.\nIn Chapter 15 we will see how one can build highly concurrent priority queues\nbased on the concurrent SkipList we presented here.\n14.6 Chapter Notes\nBill Pugh invented skiplists, both sequential [128] and concurrent [127]. The\nLazySkipList is by Y ossi Lev, Maurice Herlihy, Victor Luchangco, and Nir Shavit\n[103]. The LockFreeSkipList presented here is credited to Maurice Herlihy,\n14.7 Exercises 349\n9 51 8 11 25 15 8 23\n210\n2` 1`(a)   A: contains(18) traversing\nlevel\n0\n0\n0\n00\n0\n0\n0 111\n00\n01\n1 0 0 1 0\nB: remove(15)B: remove(9) D:", "doc_id": "f1de361f-3c94-41f0-970f-80376e5ab2cb", "embedding": null, "doc_hash": "681c9d262458516c48b7590af3fdeecec0031c153ad19ce42b65e313786a244a", "extra_info": null, "node_info": {"start": 895119, "end": 898107}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "348ba703-76fa-45a2-8662-02b9a83eaf04", "3": "71e89d55-b7ba-4159-89fd-ce13d910c59b"}}, "__type__": "1"}, "71e89d55-b7ba-4159-89fd-ce13d910c59b": {"__data__": {"text": "[128] and concurrent [127]. The\nLazySkipList is by Y ossi Lev, Maurice Herlihy, Victor Luchangco, and Nir Shavit\n[103]. The LockFreeSkipList presented here is credited to Maurice Herlihy,\n14.7 Exercises 349\n9 51 8 11 25 15 8 23\n210\n2` 1`(a)   A: contains(18) traversing\nlevel\n0\n0\n0\n00\n0\n0\n0 111\n00\n01\n1 0 0 1 0\nB: remove(15)B: remove(9) D: remove(18)9 5\n1811 25 8 23\n21\n0\n2` 1`(b)   A: contains(18) returns false\nlevel\n0\n0\n0\n00\n0\n0\n01\n11\n18000\n00\n11\n100\n1510\nB: remove(9)pred pred\ncurrE: add(18) \ncurr\nFigure 14.16 The LockFreeSkipList class: a contains ()call. In Part (a), contains (18)traverses the list\nstarting from the top level of the head node. The dotted line marks the traversal by the pred \ufb01eld. The pred\n\ufb01eld eventually reaches Node 8 at the bottom level and we show the path of curr from that point on using\na sparser dotted line. The curr traverses past Node 9 and reaches the marked Node 15. In Part (b) a new\nnode with key 18 is added to the list by a thread E. Thread E, as part of its find (18)call, physically removes\nthe old nodes with keys 9, 15, and 18. Now thread Acontinues its traversal with the curr \ufb01eld from the\nremoved node with key 15 (the nodes with keys 15 and 18 are not recycled since they are reachable by\nthread A). Thread Areaches the node with key 25 which is greater than 18, returning false. Even though at\nthis point there is an unmarked node with key 18 in the LockFreeSkipList , this node was inserted by E\nconcurrently with A\u2019s traversal and is linearized after A\u2019sadd(18).\nY ossi Lev, and Nir Shavit [64]. It is partly based on an earlier lock-free SkipList\nalgorithm developed by to Kier Fraser [42], a variant of which was incorporated\ninto the Java Concurrency Package by Doug Lea [100].\n14.7 Exercises\nExercise 163. Recall that a skiplist is a probabilistic data structure. Although the\nexpected peformance of a contains () call isO(logn), wherenis the number of\nitems in the list, the worst-case peformance could be O(n). Draw a picture of an\n8-element skiplist with worst-case performance, and explain how it got that way.\nExercise 164. Y ou are given a skiplist with probability pandMAX_LEVELM. If the\nlist contains Nnodes, what is the expected number of nodes at each level from 0\ntoM\u00001?\nExercise 165. Modify the LazySkipList class so find () starts at the level of\nthe highest node currently in the structure, instead of the highest level possible\n(MAX_LEVEL ).\nExercise 166. Modify the LazySkipList to support multiple items with the same\nkey.\n350 Chapter 14 Skiplists and Balanced Search\nExercise 167. Suppose we modify the LockFreeSkipList class so that at Line 101\nofFig. 14.12 ,remove () restarts the main loop instead of returning false .\nIs the algorithm still correct? Address both safety and liveness issues. That is,\nwhat is an unsuccessful remove () call\u2019s new linearization point, and is the class\nstill lock-free?\nExercise 168. Explain how, in the LockFreeSkipList class, a node might end up\nin the list at levels 0 and 2, but not at level 1. Draw pictures.\nExercise 169. Modify the LockFreeSkipList so that the find () method", "doc_id": "71e89d55-b7ba-4159-89fd-ce13d910c59b", "embedding": null, "doc_hash": "2714ab6c3827eb2dd5f94252f2b13a50b3a6b14dba13f74c83d19fc6d118991c", "extra_info": null, "node_info": {"start": 898170, "end": 901263}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f1de361f-3c94-41f0-970f-80376e5ab2cb", "3": "fb07b749-5058-41b4-9783-8efb372e3875"}}, "__type__": "1"}, "fb07b749-5058-41b4-9783-8efb372e3875": {"__data__": {"text": "Search\nExercise 167. Suppose we modify the LockFreeSkipList class so that at Line 101\nofFig. 14.12 ,remove () restarts the main loop instead of returning false .\nIs the algorithm still correct? Address both safety and liveness issues. That is,\nwhat is an unsuccessful remove () call\u2019s new linearization point, and is the class\nstill lock-free?\nExercise 168. Explain how, in the LockFreeSkipList class, a node might end up\nin the list at levels 0 and 2, but not at level 1. Draw pictures.\nExercise 169. Modify the LockFreeSkipList so that the find () method snips\nout a sequence of marked nodes with a single compareAndSet() . Explain why\nyour implementation cannot remove a concurrently inserted unmarked node.\nExercise 170. Will the add() method of the LockFreeSkipList work even if the\nbottom level is linked and then all other levels are linked in some arbitrary order?\nIs the same true for the marking of the next references in the remove () method:\nthe bottom level next reference is marked last, but references at all other levels\nare marked in an arbitrary order?\nExercise 171. (Hard) Modify the LazySkipList so that the list at each level is\nbidirectional, and allows threads to add and remove items in parallel by traversing\nfrom either the head or the tail .\nExercise 172. Fig. 14.17 shows a buggy contains () method for the\nLockFreeSkipList class. Give a scenario where this method returns a wrong\nanswer. Hint: the reason this method is wrong is that it takes into account keys of\nnodes that have been removed.\n1 boolean contains(T x) {\n2 int bottomLevel = 0;\n3 int key = x.hashCode();\n4 Node<T> pred = head;\n5 Node<T> curr = null ;\n6 for (int level = MAX_LEVEL; level >= bottomLevel; level--) {\n7 curr = pred.next[level].getReference();\n8 while (curr.key < key ) {\n9 pred = curr;\n10 curr = pred.next[level].getReference();\n11 }\n12 }\n13 return curr.key == key;\n14 }\nFigure 14.17 TheLockFreeSkipList class: an incorrect contains ().\n15Priority Queues\n15.1 Introduction\nApriority queue is a multiset of items , where each item has an associated priority ,\na score that indicates its importance (by convention, smaller scores are more\nimportant, indicating a higher priority). A priority queue typically provides an\nadd() method to add an item to the set, and a removeMin () method to remove\nand return the item of minimal score (highest priority). Priority queues appear\neverywhere from high-level applications to low-level operating system kernels.\nAbounded-range priority queue is one where each item\u2019s score is taken from\na discrete set of items, while an unbounded-range priority queue is one where\nscores are taken from a very large set, say 32-bit integers, or \ufb02oating-point values.\nNot surprisingly, bounded-range priority queues are generally more ef\ufb01cient, but\nmany applications require unbounded ranges. Fig. 15.1 shows the priority queue\ninterface.\n15.1.1 Concurrent Priority Queues\nIn a concurrent setting, where add() and removeMin () method calls can overlap,\nwhat does it mean for an item to be in the set?\nHere, we consider two alternative consistency conditions, both introduced in\nChapter 3. The \ufb01rst is linearizability , which requires that each method call appear\nto take effect at some instant between its invocation and its response. The sec-\nond is quiescent consistency , a weaker condition that requires that in any execu-\ntion, at any point, if no additional method calls are introduced, then when all\npublic interface PQueue<T> {\nvoid add(T item, int score);\nT removeMin();\n}\nFigure 15.1", "doc_id": "fb07b749-5058-41b4-9783-8efb372e3875", "embedding": null, "doc_hash": "0d7e08a712cdc0e00bf9eca82485efad75d74b8b67c9d79e5d27850e1a0f4bfe", "extra_info": null, "node_info": {"start": 901085, "end": 904609}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "71e89d55-b7ba-4159-89fd-ce13d910c59b", "3": "6dd4659e-1c48-465c-9777-9e40d8a61e5a"}}, "__type__": "1"}, "6dd4659e-1c48-465c-9777-9e40d8a61e5a": {"__data__": {"text": "does it mean for an item to be in the set?\nHere, we consider two alternative consistency conditions, both introduced in\nChapter 3. The \ufb01rst is linearizability , which requires that each method call appear\nto take effect at some instant between its invocation and its response. The sec-\nond is quiescent consistency , a weaker condition that requires that in any execu-\ntion, at any point, if no additional method calls are introduced, then when all\npublic interface PQueue<T> {\nvoid add(T item, int score);\nT removeMin();\n}\nFigure 15.1 Priority Queue Interface.\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00015-0\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.351\n352 Chapter 15 Priority Queues\npending method calls complete, the values they return are consistent with some\nvalid sequential execution of the object. If an application does not require its pri-\nority queues to be linearizable, then it is usually more ef\ufb01cient to require them\nto be quiescently consistent. Careful thought is usually required to decide which\napproach is correct for a particular application.\n15.2 An Array-Based Bounded Priority Queue\nA bounded-range priority queue has rangemif its priorities are taken from the\nrange 0,:::,m\u00001. For now, we consider bounded priority queue algorithms\nthat use two component data structures: Counter and Bin. ACounter (see\nChapter 12) holds an integer value, and supports getAndIncrement () and\ngetAndDecrement () methods that atomically increment and decrement the\ncounter value and return the counter\u2019s prior value. These methods may option-\nally be bounded , meaning they do not advance the counter value beyond some\nspeci\ufb01ed bound.\nABin is a pool that holds arbitrary items, and supports a put(x) method for\ninserting an item x, and a get() method for removing and returning an arbitrary\nitem, returning null if the bin is empty. Bins can be implemented using locks or\nin a lock-free manner using the stack algorithms of Chapter 11.\nFig. 15.2 shows the SimpleLinear class, which maintains an array of bins.\nT o add an item with score ia thread simply places the item in the i-th bin. The\n1public class SimpleLinear<T> implements PQueue<T> {\n2 int range;\n3 Bin<T>[] pqueue;\n4 public SimpleLinear( int myRange) {\n5 range = myRange;\n6 pqueue = (Bin<T>[]) new Bin[range];\n7 for (int i = 0; i < pqueue.length; i++){\n8 pqueue[i] = new Bin();\n9 }\n10 }\n11 public void add(T item, int key) {\n12 pqueue[key].put(item);\n13 }\n14 public T removeMin() {\n15 for (int i = 0; i < range; i++) {\n16 T item = pqueue[i].get();\n17 if(item != null ) {\n18 return item;\n19 }\n20 }\n21 return null ;\n22 }\n23 }\nFigure 15.2 TheSimpleLinear class: add()andremoveMin ()methods.\n15.3 A Tree-Based Bounded Priority Queue 353\nremoveMin () method scans the bins in decreasing priority and returns the \ufb01rst\nitem it successfully removes. If no item is found it returns null. If the bins are\nquiescently consistent, so is SimpleLinear . The add() and removeMin () meth-\nods are lock-free if the Bin methods are lock-free.\n15.3 A Tree-Based Bounded Priority Queue\nTheSimpleTree (Fig. 15.3) is a lock-free quiescently consistent bounded-range\npriority queue. It is a binary tree (Fig. 15.4) of treeNode objects", "doc_id": "6dd4659e-1c48-465c-9777-9e40d8a61e5a", "embedding": null, "doc_hash": "9b5da6498365302ae5300c63f04b7d1fe2ad582d9088d80e68f5f27d8ecb19be", "extra_info": null, "node_info": {"start": 904625, "end": 907848}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "fb07b749-5058-41b4-9783-8efb372e3875", "3": "918eed3a-9b25-4319-b1c1-c4e6e6c8cc5a"}}, "__type__": "1"}, "918eed3a-9b25-4319-b1c1-c4e6e6c8cc5a": {"__data__": {"text": "Queue 353\nremoveMin () method scans the bins in decreasing priority and returns the \ufb01rst\nitem it successfully removes. If no item is found it returns null. If the bins are\nquiescently consistent, so is SimpleLinear . The add() and removeMin () meth-\nods are lock-free if the Bin methods are lock-free.\n15.3 A Tree-Based Bounded Priority Queue\nTheSimpleTree (Fig. 15.3) is a lock-free quiescently consistent bounded-range\npriority queue. It is a binary tree (Fig. 15.4) of treeNode objects (Fig. 15.5). As\ndepicted in Fig. 15.3, the tree has mleaves where the i-th leaf node has a bin\nholding items of score i. There arem\u00001 shared bounded counters in the tree\u2019s\nA:add(a,2)(a)B:removeMin()\nD:add(d,4) A:add(a,2)(b)\nD:add(d,3)*\nA:add(a,2)(c)B:removeMin()\nA:add(a,2)(d)\nD:add(d,3)* *C:removeMin()711\n1\nad a d\nd ad00\n0 1\n11\n0\n0 000\n000\n000\n000\n00001\n6 5 4 3 2 1 0\n7 6 5 4 3 2 1 0 7 6 5 4 3 2 1 07 6 5 4 3 2 1 0\nFigure 15.3 TheSimpleTree priority queue is a tree of bounded counters. Items reside in bins at the leaves. Internal nodes\nhold the number of items in the subtree rooted at the node\u2019s left child. In Part (a) threads AandDadd items by traversing\nup the tree, incrementing the counters in the nodes when they ascend from the left. Thread Bfollows the counters down\nthe tree, descending left if the counter had a nonzero value (we do not show the effect of B\u2019s decrements). Parts (b), (c),\nand (d) show a sequence in which concurrent threads AandBmeet at the node marked by a star. In Part (b) thread D\nadds d, then Aadds aand ascends to the starred node, incrementing a counter along the way. In Part (c) Btraverses down\nthe tree, decrementing counters to zero and popping a. In Part (d), Acontinues its ascent, incrementing the counter at\nthe root even though Balready removed any trace of afrom the starred node down. Nevertheless, all is well, because the\nnonzero root counter correctly leads Cto item d, the item with the highest priority.\n354 Chapter 15 Priority Queues\n1public class SimpleTree<T> implements PQueue<T> {\n2 int range;\n3 List<TreeNode> leaves;\n4 TreeNode root;\n5 public SimpleTree( int logRange) {\n6 range = (1 << logRange);\n7 leaves = new ArrayList<TreeNode>(range);\n8 root = buildTree(logRange, 0);\n9 }\n10 public void add(T item, int score) {\n11 TreeNode node = leaves.get(score);\n12 node.bin.put(item);\n13 while (node != root) {\n14 TreeNode parent = node.parent;\n15 if(node == parent.left) {\n16 parent.counter.getAndIncrement();\n17 }\n18 node = parent;\n19 }\n20 }\n21 public T removeMin() {\n22 TreeNode node = root;\n23 while (!node.isLeaf()) {\n24 if(node.counter.boundedGetAndDecrement() > 0 ) {\n25 node = node.left;\n26 }else {\n27 node = node.right;\n28 }\n29 }\n30 return node.bin.get();\n31 }\n32 }\nFigure 15.4 TheSimpleTree bounded-range priority queue.\n33 public class TreeNode {\n34 Counter counter;\n35 TreeNode parent, right, left;\n36 Bin<T>", "doc_id": "918eed3a-9b25-4319-b1c1-c4e6e6c8cc5a", "embedding": null, "doc_hash": "5cddd8e69069281eba7394b82719ba1bfa462f9d8965267a815508f33af7e74d", "extra_info": null, "node_info": {"start": 907887, "end": 910752}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6dd4659e-1c48-465c-9777-9e40d8a61e5a", "3": "5eb7ec54-f407-4a8d-8cd5-7d7e61875761"}}, "__type__": "1"}, "5eb7ec54-f407-4a8d-8cd5-7d7e61875761": {"__data__": {"text": "}\n18 node = parent;\n19 }\n20 }\n21 public T removeMin() {\n22 TreeNode node = root;\n23 while (!node.isLeaf()) {\n24 if(node.counter.boundedGetAndDecrement() > 0 ) {\n25 node = node.left;\n26 }else {\n27 node = node.right;\n28 }\n29 }\n30 return node.bin.get();\n31 }\n32 }\nFigure 15.4 TheSimpleTree bounded-range priority queue.\n33 public class TreeNode {\n34 Counter counter;\n35 TreeNode parent, right, left;\n36 Bin<T> bin;\n37 public boolean isLeaf() {\n38 return right == null ;\n39 }\n40 }\nFigure 15.5 TheSimpleTree class: the inner treeNode class.\ninternal nodes that keep track of the number of items in the leaves of the subtree\nrooted in each node\u2019s left (lower score/higher priority) child.\nAnadd(x,k) call addsxto the bin at the kthleaf, and increments node\ncounters in leaf-to-root order. The removeMin () method traverses the tree in\nroot-to-leaf order. Starting from the root, it \ufb01nds the leaf with highest priority\n15.4 An Unbounded Heap-Based Priority Queue 355\nwhose bin is non empty. It examines each node\u2019s counter, going right if the\ncounter is zero and decrementing it and going left otherwise (Line 24).\nAnadd() traversal by a thread Amoving up may meet a removeMin () traversal\nby a threadBmoving down. As in the story of Hansel and Gretel, the descending\nthreadBfollows the trail of non-zero counters left by the ascending add() to\nlocate and remove A\u2019s item from its bin. Part (a) of Fig. 15.3 shows an execution\nof the SimpleTree .\nOne may be concerned about the following \u201cGrimm\u201d scenario. thread A,\nmoving up, meets thread B, moving down, at a tree node marked by a star, as\ndescribed in Fig. 15.3 . ThreadBmoves down from the starred node to collect\nA\u2019s item at the leaf, while Acontinues up the tree, incrementing counters until it\nreaches the root. What if another thread, C, starts to follow A\u2019s path of nonzero\ncounters from the root down to the starred node where Bencountered A? When\nCreaches the starred node, it may be stranded there in the middle of the tree,\nand seeing no marks it would follow the right child branches to an empty Bin,\neven though there might be other items in the queue.\nFortunately, this scenario cannot happen. As depicted in Parts (b) through\n(d) of Fig. 15.3 , the only way the descending thread Bcould meet the ascend-\ning threadAat the starred node is if another add() call by an earlier thread D\nincremented the same set of counters from the starred node to the root, allowing\nthe descending thread Bto reach the starred node in the \ufb01rst place. The ascend-\ning threadA, when incrementing counters from the starred node to the root, is\nsimply completing the increment sequence leading to the item inserted by some\nother thread D. T o summarize, if the item returned by some thread in Line 24is\nnull, then the priority queue is indeed empty.\nThe SimpleTree algorithm is not linearizable, since threads may overtake\neach other, but it is quiescently consistent. The add() and removeMin () methods\nare lock-free if the bins and counters are lock-free (the number of steps needed\nbyadd() is bounded by the tree depth and removeMin () can fail to complete\nonly if items are continually being added and removed from the tree.) A typical\ninsertion or deletion takes a number of steps logarithmic in the lowest priority\n(maximal score) in the range.\n15.4 An Unbounded Heap-Based Priority", "doc_id": "5eb7ec54-f407-4a8d-8cd5-7d7e61875761", "embedding": null, "doc_hash": "85f406e3f25512879b16a5bf98b6587320fea565baaca70832d716570c5378ec", "extra_info": null, "node_info": {"start": 910815, "end": 914140}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "918eed3a-9b25-4319-b1c1-c4e6e6c8cc5a", "3": "6864d9a2-ea1c-4c00-9fda-d0c2c569a10f"}}, "__type__": "1"}, "6864d9a2-ea1c-4c00-9fda-d0c2c569a10f": {"__data__": {"text": "empty.\nThe SimpleTree algorithm is not linearizable, since threads may overtake\neach other, but it is quiescently consistent. The add() and removeMin () methods\nare lock-free if the bins and counters are lock-free (the number of steps needed\nbyadd() is bounded by the tree depth and removeMin () can fail to complete\nonly if items are continually being added and removed from the tree.) A typical\ninsertion or deletion takes a number of steps logarithmic in the lowest priority\n(maximal score) in the range.\n15.4 An Unbounded Heap-Based Priority Queue\nThis section presents a linearizable priority queue that supports priorities from\nan unbounded range. It uses \ufb01ne-grained locking for synchronization.\nAheap is a tree where each tree node contains an item and a score. If bis\na child node of a, thenb\u2019s priority is no greater than a\u2019s priority (i.e., items\nhigher in the tree have lower scores and are more important). The removeMin ()\nmethod removes and returns the root of the tree, and then rebalances the root\u2019s\nsubtrees. Here, we consider binary trees, where there are only two subtrees to\nrebalance.\n356 Chapter 15 Priority Queues\n15.4.1 A Sequential Heap\nFigs. 15.6 and 15.7 show a sequential heap implementation. An ef\ufb01cient way to\nrepresent a binary heap is as an array of nodes, where the tree\u2019s root is array entry\n1, and the right and left children of array entry iare entries 2\u0001iand (2\u0001i) + 1,\nrespectively. The next \ufb01eld is the index of the \ufb01rst unused node.\nEach node has an item and a score \ufb01eld. T o add an item, the add() method\nsetschild to the index of the \ufb01rst empty array slot (Line 13). (For brevity, we\nomit code to resize a full array.) The method then initializes that node to hold\nthe new item and score (Line 14). At this point, the heap property may be vio-\nlated, because the new node, which is a leaf of the tree, may have higher prior-\nity (smaller score) than an ancestor. T o restore the heap property, the new node\n\u201cpercolates up\u201d the tree. We repeatedly compare the new node\u2019s priority with its\nparent\u2019s, swapping them if the parent\u2019s priority is lower (it has a larger score).\nWhen we encounter a parent with a higher priority, or we reach the root, the new\nnode is correctly positioned, and the method returns.\nT o remove and return the highest-priority item, the removeMin () method\nrecords the root\u2019s item, which is the highest-priority item in the tree. (For brevity,\nwe omit the code to deal with an empty heap.) It then moves a leaf entry up\nto replace the root (Lines 27\u201329). If the tree is empty, the method returns the\nrecorded item (Line 30). Otherwise, the heap property may be violated, because\n1public class SequentialHeap<T> implements PQueue<T> {\n2 private static final int ROOT = 1;\n3 int next;\n4 HeapNode<T>[] heap;\n5 public SequentialHeap( int capacity) {\n6 next = ROOT;\n7 heap = (HeapNode<T>[]) new HeapNode[capacity + 1];\n8 for (int i = 0; i < capacity + 1; i++) {\n9 heap[i] = new HeapNode<T>();\n10 }\n11 }\n12 public void add(T item, int score) {\n13 int child = next++;\n14 heap[child].init(item, score);\n15 while (child > ROOT) {\n16 int parent = child / 2;\n17 int oldChild = child;\n18 if(heap[child].score < heap[parent].score)", "doc_id": "6864d9a2-ea1c-4c00-9fda-d0c2c569a10f", "embedding": null, "doc_hash": "0373d842a5f683ba6538cbee9e15eb013dd390272b6b0c8920f5065f140935be", "extra_info": null, "node_info": {"start": 914031, "end": 917215}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5eb7ec54-f407-4a8d-8cd5-7d7e61875761", "3": "ed1674a3-7b76-4ba7-96d3-4943926b0067"}}, "__type__": "1"}, "ed1674a3-7b76-4ba7-96d3-4943926b0067": {"__data__": {"text": "{\n6 next = ROOT;\n7 heap = (HeapNode<T>[]) new HeapNode[capacity + 1];\n8 for (int i = 0; i < capacity + 1; i++) {\n9 heap[i] = new HeapNode<T>();\n10 }\n11 }\n12 public void add(T item, int score) {\n13 int child = next++;\n14 heap[child].init(item, score);\n15 while (child > ROOT) {\n16 int parent = child / 2;\n17 int oldChild = child;\n18 if(heap[child].score < heap[parent].score) {\n19 swap(child, parent);\n20 child = parent;\n21 }else {\n22 return ;\n23 }\n24 }\n25 }\nFigure 15.6 TheSequentialHeap class: inner node class and add()method.\n15.4 An Unbounded Heap-Based Priority Queue 357\n26 public T removeMin() {\n27 int bottom = --next;\n28 T item = heap[ROOT].item;\n29 heap[ROOT] = heap[bottom];\n30 if(bottom == ROOT) {\n31 return item;\n32 }\n33 int child = 0;\n34 int parent = ROOT;\n35 while (parent < heap.length / 2) {\n36 int left = parent *2;int right = (parent *2) + 1;\n37 if(left >= next) {\n38 return item;\n39 }else if (right >= next || heap[left].score < heap[right].score) {\n40 child = left;\n41 }else {\n42 child = right;\n43 }\n44 if(heap[child].score < heap[parent].score) {\n45 swap(parent, child);\n46 parent = child;\n47 }else {\n48 return item;\n49 }\n50 }\n51 return item;\n52 }\n53 ...\n54 }\nFigure 15.7 TheSequentialHeap class: the removeMin ()method.\nthe leaf node recently promoted to the root may have lower priority than some of\nits descendants. T o restore the heap property, the new root \u201cpercolates down\u201d the\ntree. If both children are empty, we are done (Line 37). If the right child is empty,\nor if the right child has lower priority than the left, then we examine the left child\n(Line 39). Otherwise, we examine the right child (Line 41). If the child has higher\npriority than the parent, then we swap the child and parent, and continue moving\ndown the tree (Line 44). When both children have lower priorities, or we reach a\nleaf, the displaced node is correctly positioned, and the method returns.\n15.4.2 A Concurrent Heap\nBird\u2019s-Eye View\nThe FineGrainedHeap class is mostly just a concurrent version of the\nSequentialHeap class. As in the sequential heap, add() creates a new leaf node,\nand percolates it up the tree until the heap property is restored. T o allow con-\ncurrent calls to proceed in parallel, the FineGrainedHeap class percolates items\nup the tree as a sequence of discrete atomic steps that can be interleaved with\n358 Chapter 15 Priority Queues\nother such steps. In the same way, removeMin () deletes the root node, moves a\nleaf node to the root, and percolates that node down the tree until the heap prop-\nerty is restored. The FineGrainedHeap class percolates items down the tree as a\nsequence of discrete atomic steps that can be interleaved with other such steps.\nIn Detail\nWarning: The code presented here does notdeal with heap over\ufb02ow (adding an\nitem when the heap is full) or under\ufb02ow (removing an item when the heap is\nempty). Dealing with these cases makes the code longer, without adding much of\ninterest.\nThe class uses a heapLock \ufb01eld to make short, atomic modi\ufb01cations to two or\nmore \ufb01elds ( Fig. 15.8", "doc_id": "ed1674a3-7b76-4ba7-96d3-4943926b0067", "embedding": null, "doc_hash": "5b12cbfa8b25656b7a1400a7ab577c71edd9f345cc73660023073aa1cb6d6d83", "extra_info": null, "node_info": {"start": 917366, "end": 920400}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6864d9a2-ea1c-4c00-9fda-d0c2c569a10f", "3": "8088188c-e2f4-4462-96be-28001cc6b1e6"}}, "__type__": "1"}, "8088188c-e2f4-4462-96be-28001cc6b1e6": {"__data__": {"text": "The FineGrainedHeap class percolates items down the tree as a\nsequence of discrete atomic steps that can be interleaved with other such steps.\nIn Detail\nWarning: The code presented here does notdeal with heap over\ufb02ow (adding an\nitem when the heap is full) or under\ufb02ow (removing an item when the heap is\nempty). Dealing with these cases makes the code longer, without adding much of\ninterest.\nThe class uses a heapLock \ufb01eld to make short, atomic modi\ufb01cations to two or\nmore \ufb01elds ( Fig. 15.8 ).\nTheHeapNode class ( Fig. 15.9 ) provides the following \ufb01elds. The lock \ufb01eld is a\nlock (Line 21) held for short-lived modi\ufb01cations, and also while the node is being\npercolated down the tree. For brevity, the class exports lock () and unlock ()\nmethods to lock and unlock the node directly. The tag \ufb01eld has one of the fol-\nlowing states: EMPTY means the node is not in use, AVAILABLE means the node\nholds an item and a score, and BUSY means that the node is being percolated up\nthe tree, and is not yet in its proper position. While the node is BUSY , the owner\n\ufb01eld holds the ID of the thread responsible for moving it. For brevity, the class\nprovides an amOwner method that returns true if and only if the node\u2019s tag is BUSY\nand the owner is the current thread.\nThe asymmetry in synchronization between the removeMin () method, which\npercolates down the tree holding the lock, and the add() method ( Fig. 15.10 ),\nwhich percolates up the tree with the tag \ufb01eld set to BUSY , ensures that a\nremoveMin () call is not delayed if it encounters a node that is in the middle of\nbeing shepherded up the tree by an add() call. As a result, an add() call must\nbe prepared to have its node swapped out from underneath it. If the node van-\nishes, the add() call simply moves up the tree. It is sure to encounter that node\nsomewhere between its present position and the root.\n1public class FineGrainedHeap<T> implements PQueue<T> {\n2 private static int ROOT = 1;\n3 private static int NO_ONE = -1;\n4 private Lock heapLock;\n5 int next;\n6 HeapNode<T>[] heap;\n7 public FineGrainedHeap( int capacity) {\n8 heapLock = new ReentrantLock();\n9 next = ROOT;\n10 heap = (HeapNode<T>[]) new HeapNode[capacity + 1];\n11 for (int i = 0; i < capacity + 1; i++) {\n12 heap[i] = new HeapNode<T>();\n13 }\n14 }\nFigure 15.8 TheFineGrainedHeap class: \ufb01elds.\n15.4 An Unbounded Heap-Based Priority Queue 359\n15 private static enum Status {EMPTY, AVAILABLE, BUSY};\n16 private static class HeapNode<S> {\n17 Status tag;\n18 int score;\n19 S item;\n20 int owner;\n21 Lock lock;\n22 public void init(S myItem, int myScore) {\n23 item = myItem;\n24 score = myScore;\n25 tag = Status.BUSY;\n26 owner = ThreadID.get();\n27 }\n28 public HeapNode() {\n29 tag = Status.EMPTY;\n30 lock = new ReentrantLock();\n31 }\n32 public void lock() {lock.lock();}\n33 ... // other methods omitted\n34 }\nFigure 15.9 TheFineGrainedHeap class: inner HeapNode class.\nTheremoveMin () method ( Fig. 15.11 ) acquires the global heapLock , decre-\nments the next \ufb01eld, returning the index of a leaf node, locks the", "doc_id": "8088188c-e2f4-4462-96be-28001cc6b1e6", "embedding": null, "doc_hash": "8a500391e438daf3936ec6723981c86c367060a370b4e753ca4f6b846324fb02", "extra_info": null, "node_info": {"start": 920302, "end": 923322}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ed1674a3-7b76-4ba7-96d3-4943926b0067", "3": "85c1a2f4-789f-4dbb-bf74-0c854e9e8ba7"}}, "__type__": "1"}, "85c1a2f4-789f-4dbb-bf74-0c854e9e8ba7": {"__data__": {"text": "myItem;\n24 score = myScore;\n25 tag = Status.BUSY;\n26 owner = ThreadID.get();\n27 }\n28 public HeapNode() {\n29 tag = Status.EMPTY;\n30 lock = new ReentrantLock();\n31 }\n32 public void lock() {lock.lock();}\n33 ... // other methods omitted\n34 }\nFigure 15.9 TheFineGrainedHeap class: inner HeapNode class.\nTheremoveMin () method ( Fig. 15.11 ) acquires the global heapLock , decre-\nments the next \ufb01eld, returning the index of a leaf node, locks the \ufb01rst unused\nslot in the array, and releases heapLock (Lines 76\u201380). It then stores the root\u2019s\nitem in a local variable to be returned later as the result of the call (Line 81). It\nmarks the node as EMPTY and unowned, swaps it with the leaf node, and unlocks\nthe (now empty) leaf (Lines 82\u201384).\nAt this point, the method has recorded its eventual result in a local variable,\nmoved the leaf to the root, and marked the leaf\u2019s former position as EMPTY . It\nretains the lock on the root. If the heap had only one item, then the leaf and the\nroot are the same, so the method checks whether the root has just been marked\nasEMPTY . If so, it unlocks the root and returns the item (Lines 85\u201389).\nThe new root node is now percolated down the tree until it reaches its proper\nposition, following much the same logic as the sequential implementation. The\nnode being percolated down is locked until it reaches its proper position. When\nwe swap two nodes, we lock them both, and swap their \ufb01elds. At each step, the\nmethod locks the node\u2019s right and left children (Line 96). If the left child is empty,\nwe unlock both children and return (Line 98). If the right child is empty, but the\nleft child has higher priority, then we unlock the right child and examine the left\n(Line 103). Otherwise, we unlock the left child and examine the right (Line 106).\nIf the child has higher priority, then we swap the parent and child, and unlock\nthe parent (Line 111). Otherwise, we unlock the child and the parent and return.\nThe concurrent add() method acquires the heapLock , allocates, locks, initial-\nizes, and unlocks an empty leaf node (Lines 36\u201341). This leaf node has tag BUSY ,\nand the owner is the calling thread. It then unlocks the leaf node.\nIt then proceeds to percolate that node up the tree, using the child variable to\nkeep track of the node. It locks the parent, then the child (all locks are acquired in\n360 Chapter 15 Priority Queues\n35 public void add(T item, int score) {\n36 heapLock.lock();\n37 int child = next++;\n38 heap[child].lock();\n39 heap[child].init(item, score);\n40 heapLock.unlock();\n41 heap[child].unlock();\n42\n43 while (child > ROOT) {\n44 int parent = child / 2;\n45 heap[parent].lock();\n46 heap[child].lock();\n47 int oldChild = child;\n48 try {\n49 if(heap[parent].tag == Status.AVAILABLE && heap[child].amOwner()) {\n50 if(heap[child].score < heap[parent].score) {\n51 swap(child, parent);\n52 child = parent;\n53 }else {\n54 heap[child].tag = Status.AVAILABLE;\n55 heap[child].owner = NO_ONE;\n56 return ;\n57 }\n58 }else if (!heap[child].amOwner()) {\n59 child = parent;\n60 }\n61 }finally {\n62 heap[oldChild].unlock();\n63 heap[parent].unlock();\n64 }\n65 }\n66 if(child == ROOT) {\n67", "doc_id": "85c1a2f4-789f-4dbb-bf74-0c854e9e8ba7", "embedding": null, "doc_hash": "cdfc108fd74bd03fddbf898b588aebd1e3cf9307e96fd71309de44bdd9193a0e", "extra_info": null, "node_info": {"start": 923358, "end": 926476}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8088188c-e2f4-4462-96be-28001cc6b1e6", "3": "767dc615-f128-4822-baa2-148da39bdec7"}}, "__type__": "1"}, "767dc615-f128-4822-baa2-148da39bdec7": {"__data__": {"text": "{\n50 if(heap[child].score < heap[parent].score) {\n51 swap(child, parent);\n52 child = parent;\n53 }else {\n54 heap[child].tag = Status.AVAILABLE;\n55 heap[child].owner = NO_ONE;\n56 return ;\n57 }\n58 }else if (!heap[child].amOwner()) {\n59 child = parent;\n60 }\n61 }finally {\n62 heap[oldChild].unlock();\n63 heap[parent].unlock();\n64 }\n65 }\n66 if(child == ROOT) {\n67 heap[ROOT].lock();\n68 if(heap[ROOT].amOwner()) {\n69 heap[ROOT].tag = Status.AVAILABLE;\n70 heap[child].owner = NO_ONE;\n71 }\n72 heap[ROOT].unlock();\n73 }\n74 }\nFigure 15.10 TheFineGrainedHeap class: the add()method.\nascending order). If the parent is AVAILABLE and the child is owned by the caller,\nthen it compares their priorities. If the child has higher priority, then the method\nswaps their \ufb01elds, and moves up (Line 50). Otherwise the node is where it belongs,\nand it is marked AVAILABLE and unowned (Line 53). If the child is not owned by\nthe caller, then the node must have been moved up by a concurrent removeMin ()\ncall so the method simply moves up the tree to search for its node (Line 58).\nFig. 15.12 shows an execution of the FineGrainedHeap class. In Part (a) the\nheap tree structure is depicted, with the priorities written in the nodes and the\nrespective array entries above the nodes. The next \ufb01eld is set to 10, the next array\n15.4 An Unbounded Heap-Based Priority Queue 361\n75 public T removeMin() {\n76 heapLock.lock();\n77 int bottom = --next;\n78 heap[ROOT].lock();\n79 heap[bottom].lock();\n80 heapLock.unlock();\n81 T item = heap[ROOT].item;\n82 heap[ROOT].tag = Status.EMPTY;\n83 heap[ROOT].owner = NO_ONE;\n84 swap(bottom, ROOT);\n85 heap[bottom].unlock();\n86 if(heap[ROOT].tag == Status.EMPTY) {\n87 heap[ROOT].unlock();\n88 return item;\n89 }\n90 heap[ROOT].tag = Status.AVAILABLE;\n91 int child = 0;\n92 int parent = ROOT;\n93 while (parent < heap.length / 2) {\n94 int left = parent *2;\n95 int right = (parent *2) + 1;\n96 heap[left].lock();\n97 heap[right].lock();\n98 if(heap[left].tag == Status.EMPTY) {\n99 heap[right].unlock();\n100 heap[left].unlock();\n101 break ;\n102 }else if (heap[right].tag == Status.EMPTY || heap[left].score\n103 < heap[right].score) {\n104 heap[right].unlock();\n105 child = left;\n106 }else {\n107 heap[left].unlock();\n108 child = right;\n109 }\n110 if(heap[child].score < heap[parent].score\n111 && heap[child].tag != Status.EMPTY) {\n112 swap(parent, child);\n113 heap[parent].unlock();\n114 parent = child;\n115 }else {\n116 heap[child].unlock();\n117 break ;\n118 }\n119 }\n120 heap[parent].unlock();\n121 return item;\n122 }\n123 ...\n124 }\nFigure 15.11 TheFineGrainedHeap class: the removeMin ()method.\n362 Chapter 15 Priority", "doc_id": "767dc615-f128-4822-baa2-148da39bdec7", "embedding": null, "doc_hash": "5c89222e0580a32979c5f99d5b3aa7937e4d0e67f2b516103ce7c6bfc04bfeb5", "extra_info": null, "node_info": {"start": 926537, "end": 929142}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "85c1a2f4-789f-4dbb-bf74-0c854e9e8ba7", "3": "05eb2266-4f55-409d-925b-852e70b1957d"}}, "__type__": "1"}, "05eb2266-4f55-409d-925b-852e70b1957d": {"__data__": {"text": "child = right;\n109 }\n110 if(heap[child].score < heap[parent].score\n111 && heap[child].tag != Status.EMPTY) {\n112 swap(parent, child);\n113 heap[parent].unlock();\n114 parent = child;\n115 }else {\n116 heap[child].unlock();\n117 break ;\n118 }\n119 }\n120 heap[parent].unlock();\n121 return item;\n122 }\n123 ...\n124 }\nFigure 15.11 TheFineGrainedHeap class: the removeMin ()method.\n362 Chapter 15 Priority Queues\n(a)\navail\n7\n0avail\n8\n0avail\n5\n0avail\n3\n0avail\n4\n0\navail\n12\n0\navail\n14\n0avail\n10\n00 10avail\n1\n01\n3 2\n45 67\n89status\npriority owner\nlock itemheapLock Next 1\n3 2\n45 67\n89status\npriority owner\nlock itemheapLock Next\n1\n3 2\n45 67\n89status\npriority owner\nlock itemheapLock next 1\n3 2\n45 67\n89status\npriority owner\nlock itemheapLock nextA: removeMin\nwill return 1\n9(b)\navail\n7\n1avail\n8\n0avail\n5\n0avail\n3\n1avail\n4\n0\navail\n12\n0\navail\n14\n0busy\n2\n10avail\n10\n110 9\nB: add(2)BA: swap\nB: swap\n(c)\nbusy\n2\n1avail\n8\n0avail\n5\n0avail\n10\n1avail\n4\n0\navail\n12\n0\navail\n14\n0avail\n7\n10avail\n3\n010\nBA: swap(d)\navail\n7\n0avail\n8\n0avail\n5\n0busy\n2\n0avail\n4\n0\navail\n12\n0\navail\n14\n0avail\n10\n00avail\n3\n010\nB\nA: swapB: where \n    Is 2?\nFigure 15.12 TheFineGrainedHeap class: a heap-based priority queue.\n15.5 A Skiplist-Based Unbounded Priority Queue 363\nentry into which a new item can be added. As can be seen, thread Astarts a\nremoveMin () method call, collecting the value 1 from the root as the one to be\nreturned, moving the leaf node with score 10 to the root, and setting next back\nto 9. The removeMin () method checks whether 10 needs to be percolated down\nthe heap. In Part (b) thread Apercolates 10 down the heap, while thread Badds\na new item with score 2 to the heap in the recently emptied array entry 9. The\nowner of the new node is B, andBstarts to percolate 2 up the heap, swapping it\nwith its parent node of score 7. After this swap, it releases the locks on the nodes.\nAt the same time Aswaps the node with scores 10 and 3. In Part (c), A, ignoring\nthe busy state of 2, swaps 10 and 2 and then 10 and 7 using hand-over-hand lock-\ning. It has thus swapped 2, which was not locked, from under thread B. In Part\n(d), whenBmoves to the parent node in array entry 4, it \ufb01nds that the busy node\nwith score 2 it was percolating up has disappeared. However, it continues up the\nheap and locates the node with 2 as it ascends, moving it to its correct position\nin the heap.\n15.5 A Skiplist-Based Unbounded Priority Queue\nOne drawback of the FineGrainedHeap priority queue algorithm is that the\nunderlying heap structure requires complex, coordinated rebalancing. In this\nsection, we examine an alternative that requires no rebalancing.\nRecall from Chapter 14 that a skiplist is a collection of ordered lists. Each list\nis a sequence", "doc_id": "05eb2266-4f55-409d-925b-852e70b1957d", "embedding": null, "doc_hash": "809d9f5651d9a9dd1641b9f7ccd14993cc8bfdfe71854c11544caa58ba4bde5b", "extra_info": null, "node_info": {"start": 929108, "end": 931816}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "767dc615-f128-4822-baa2-148da39bdec7", "3": "80745ad6-f891-4343-942b-c0d3cd084bce"}}, "__type__": "1"}, "80745ad6-f891-4343-942b-c0d3cd084bce": {"__data__": {"text": "that the busy node\nwith score 2 it was percolating up has disappeared. However, it continues up the\nheap and locates the node with 2 as it ascends, moving it to its correct position\nin the heap.\n15.5 A Skiplist-Based Unbounded Priority Queue\nOne drawback of the FineGrainedHeap priority queue algorithm is that the\nunderlying heap structure requires complex, coordinated rebalancing. In this\nsection, we examine an alternative that requires no rebalancing.\nRecall from Chapter 14 that a skiplist is a collection of ordered lists. Each list\nis a sequence of nodes , and each node contains an item . Each node belongs to a\nsubset of the lists, and nodes in each list are sorted by their hash values. Each\nlist has a level, ranging from 0 to a maximum. The bottom-level list contains all\nthe nodes, and each higher-level list is a sublist of the lower-level lists. Each list\ncontains about half the nodes of the next lower-level list. As a result, inserting or\nremoving a node from a skiplist containing kitems takes expected time O(logk).\nIn Chapter 14 we used skiplists to implement sets of items. Here, we adapt\nskiplists to implement a priority queue of items tagged with priorities. We\ndescribe a PrioritySkipList class that provides the basic functionality needed\nto implement an ef\ufb01cient priority queue. We base the PrioritySkipList\n(Figs. 15.13 and 15.14) class on the LockFreeSkipList class of Chapter 14,\nthough we could just as easily have based it on the LazySkipList class. Later,\nwe describe a SkipQueue wrapper to cover some of the PrioritySkipList<T>\nclass\u2019s rough edges.\nHere is a bird\u2019s-eye view of the algorithm. The PrioritySkipList class sorts\nitems by priority instead of by hash value, ensuring that high-priority items (the\nones we want to remove \ufb01rst) appear at the front of the list. Fig. 15.15 shows\nsuch a PrioritySkipList structure. Removing the item with highest prior-\nity is done lazily (See Chapter 9). A node is logically removed by marking it\nas removed, and is later physically removed by unlinking it from the list. The\nremoveMin () method works in two steps: \ufb01rst, it scans through the bottom-level\n364 Chapter 15 Priority Queues\n1public final class PrioritySkipList<T> {\n2 public static final class Node<T> {\n3 final T item;\n4 final int score;\n5 AtomicBoolean marked;\n6 final AtomicMarkableReference<Node<T>>[] next;\n7 // sentinel node constructor\n8 public Node( int myPriority) { ... }\n9 // ordinary node constructor\n10 public Node(T x, int myPriority) { ... }\n11 }\n12 boolean add(Node node) { ... }\n13 boolean remove(Node<T> node) { ... }\n14 public Node<T> findAndMarkMin() {\n15 Node<T> curr = null ;\n16 curr = head.next[0].getReference();\n17 while (curr != tail) {\n18 if(!curr.marked.get()) {\n19 if(curr.marked.compareAndSet( false ,true ))\n20 return curr;\n21 }else {\n22 curr = curr.next[0].getReference();\n23 }\n24 }\n25 }\n26 return null ;// no unmarked nodes\n27 }\nFigure 15.13 ThePrioritySkipList<T> class: inner Node<T> class.\n1public class SkipQueue<T> {\n2 PrioritySkipList<T> skiplist;\n3 public SkipQueue() {\n4 skiplist = new PrioritySkipList<T>();\n5 }\n6 public boolean add(T item, int score) {\n7 Node<T> node = (Node<T>) new Node(item, score);\n8 return", "doc_id": "80745ad6-f891-4343-942b-c0d3cd084bce", "embedding": null, "doc_hash": "b7f66150edd3a6832768f2acc11c018e62a8d3becac263efff7b4e56cf1d0d7b", "extra_info": null, "node_info": {"start": 931698, "end": 934893}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "05eb2266-4f55-409d-925b-852e70b1957d", "3": "074dc83c-7eae-4dab-b791-9d3f62e711f0"}}, "__type__": "1"}, "074dc83c-7eae-4dab-b791-9d3f62e711f0": {"__data__": {"text": "}else {\n22 curr = curr.next[0].getReference();\n23 }\n24 }\n25 }\n26 return null ;// no unmarked nodes\n27 }\nFigure 15.13 ThePrioritySkipList<T> class: inner Node<T> class.\n1public class SkipQueue<T> {\n2 PrioritySkipList<T> skiplist;\n3 public SkipQueue() {\n4 skiplist = new PrioritySkipList<T>();\n5 }\n6 public boolean add(T item, int score) {\n7 Node<T> node = (Node<T>) new Node(item, score);\n8 return skiplist.add(node);\n9 }\n10 public T removeMin() {\n11 Node<T> node = skiplist.findAndMarkMin();\n12 if(node != null ) {\n13 skiplist.remove(node);\n14 return node.item;\n15 }else {\n16 return null ;\n17 }\n18 }\n19 }\nFigure 15.14 TheSkipQueue<T> class.\n15.5 A Skiplist-Based Unbounded Priority Queue 365\n(a)\n0\n0\n0\n00\n0\n0\n00\n0\n0\n91\n1\n50\n20\n110\n250\n150\n8level\n3\n2\n10\n2 11\nA: deleteMin()11 11 11 1 0 0marked(b)\n0\n0\n0\n00\n0\n0\n00\n0\n0\n91\n1\n50\n200\n180\n110\n250\n150\n8level\n3\n2\n10\nA: deleteMin()11 11 0 11 0 0 marked0\n0\n3\n0B: add(3)\nzB: add(18)\n1 2\nFigure 15.15 The SkipQueue priority queue: an execution that is quiescently consistent but not linearizable. In Part (a)\nthread Astarts a removeMin ()method call. It traverses the lowest-level list in the PrioritySkipList to \ufb01nd and logically\nremove the \ufb01rst unmarked node. It traverses over all marked nodes, even ones like the node with score 5 which is in\nthe process of being physically removed from the SkipList . In Part (b) while Ais visiting the node with score 9, thread\nBadds a node with score 3, and then adds a node with score 18. Thread Amarks and returns the node with score 18.\nA linearizable execution could not return an item with score 18 before the item with score 3 is returned.\nlist for the \ufb01rst unmarked node. When it \ufb01nds one, it tries to mark it. If it fails,\nit continues scanning down the list, but if it succeeds, then removeMin () calls\nthePrioritySkipList class\u2019s logarithmic-time remove () method to physically\nremove the marked node.\nWe now turn our attention to the algorithm details. Fig. 15.13 shows\nan outline of the the PrioritySkipList class, a modi\ufb01ed version of the\nLockFreeSkipList class of Chapter 14 . It is convenient to have the add()\nand remove () calls take skiplist nodes instead of items as arguments and\nresults. These methods are straightforward adaptations of the corresponding\nLockFreeSkipList methods, and are left as exercises. This class\u2019s nodes differ\nfrom LockFreeSkipList nodes in two \ufb01elds: an integer score \ufb01eld (Line 4),\nand an AtomicBoolean marked \ufb01eld used for logical deletion from the priority\nqueue (not from the skiplist) (Line 5). The findAndMarkMin () method scans\nthe lowest-level list until it \ufb01nds a node whose marked \ufb01eld is false , and then\natomically tries to set that \ufb01eld to true (Line 19). If it fails, it tries again. When it\nsucceeds, it returns the newly marked node to the caller (Line 20).\nFig. 15.14 shows the SkipQueue<T> class. This class is", "doc_id": "074dc83c-7eae-4dab-b791-9d3f62e711f0", "embedding": null, "doc_hash": "97443f2c903af30d1b53632ddbc6cd2b896d9da67191c2a3e50b87f9bd6923d9", "extra_info": null, "node_info": {"start": 935019, "end": 937872}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "80745ad6-f891-4343-942b-c0d3cd084bce", "3": "78d2f2fd-a2e2-4a68-a050-e2c58546ccad"}}, "__type__": "1"}, "78d2f2fd-a2e2-4a68-a050-e2c58546ccad": {"__data__": {"text": "(Line 4),\nand an AtomicBoolean marked \ufb01eld used for logical deletion from the priority\nqueue (not from the skiplist) (Line 5). The findAndMarkMin () method scans\nthe lowest-level list until it \ufb01nds a node whose marked \ufb01eld is false , and then\natomically tries to set that \ufb01eld to true (Line 19). If it fails, it tries again. When it\nsucceeds, it returns the newly marked node to the caller (Line 20).\nFig. 15.14 shows the SkipQueue<T> class. This class is just a wrapper for a\nPrioritySkipList<T> . The add(x,p) method adds item xwith scorepby creat-\ning a node to hold both values, and passing that node to the PrioritySkipList\nclass\u2019s add() method. The removeMin () method calls the PrioritySkipList\nclass\u2019s findAndMarkMin () method to mark a node as logically deleted, and then\ncalls remove () to physically remove that node.\nTheSkipQueue class is quiescently consistent: if an item xwas present before\nthe start of a removeMin () call, then the item returned will have a score less than\nor equal to that of x. This class is not linearizable: a thread might add a higher\npriority (lower score) item and then a lower priority item, and the traversing\nthread might \ufb01nd and return the later inserted lower priority item, violating\n366 Chapter 15 Priority Queues\nlinearizability. This behavior is quiescently consistent, however, because one can\nreorder add() calls concurrent with any removeMin () to be consistent with a\nsequential priority queue.\nThe SkipQueue class is lock-free. A thread traversing the lowest level of the\nSkipList might always be beaten to the next logically undeleted node by another\ncall, but it can fail repeatedly only if other threads repeatedly succeed.\nIn general, the quiescently consistent SkipQueue tends to outperform the lin-\nearizable heap-based queue. If there are nthreads, then the \ufb01rst logically undeleted\nnode is always among the \ufb01rst nnodes in the bottom-level list. Once a node has\nbeen logically deleted, then it will be physically deleted in worst-case O(logk)\nsteps, where kis the size of the list. In practice, a node will probably be deleted\nmuch more quickly, since that node is likely to be close to the start of the list.\nThere are, however, several sources of contention in the algorithm that affect its\nperformance and require the use of backoff and tuning. Contention could occur if\nseveral threads concurrently try to mark a node, where the losers proceed together\nto try to mark the next node, and so on. Contention can also arise when physi-\ncally removing an item from the skiplist. All nodes to be removed are likely to be\nneighbors at the start of the skiplist, so chances are high that they share predeces-\nsors, which could cause repeated compareAndSet() failures when attempting to\nsnip out references to the nodes.\n15.6 Chapter Notes\nThe FineGrainedHeap priority queue is by Galen Hunt, Maged Michael,\nSrinivasan Parthasarathy, and Michael Scott [74]. The SimpleLinear and\nSimpleTree priority queues are credited to Nir Shavit and Asaph Zemach [143].\nTheSkipQueue is by Itai Lotan and Nir Shavit [107] who also present a lineariz-\nable version of the algorithm.\n15.7 Exercises\nExercise 173. Give an example of a quiescently consistent priority queue execu-\ntion that is not linearizable.\nExercise 174. Implement a quiescently consistent Counter with a lock-free imple-\nmentation of the boundedGetAndIncrement () and boundedGetAndDecrement ()\nmethods using a counting network or", "doc_id": "78d2f2fd-a2e2-4a68-a050-e2c58546ccad", "embedding": null, "doc_hash": "8b595d2227e6cdddb9035a9a9afd6389265c801405491b6ee1cc7c2d2aa47eee", "extra_info": null, "node_info": {"start": 937840, "end": 941280}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "074dc83c-7eae-4dab-b791-9d3f62e711f0", "3": "e6398d84-5f39-4238-840a-789a1a048ad5"}}, "__type__": "1"}, "e6398d84-5f39-4238-840a-789a1a048ad5": {"__data__": {"text": "SimpleLinear and\nSimpleTree priority queues are credited to Nir Shavit and Asaph Zemach [143].\nTheSkipQueue is by Itai Lotan and Nir Shavit [107] who also present a lineariz-\nable version of the algorithm.\n15.7 Exercises\nExercise 173. Give an example of a quiescently consistent priority queue execu-\ntion that is not linearizable.\nExercise 174. Implement a quiescently consistent Counter with a lock-free imple-\nmentation of the boundedGetAndIncrement () and boundedGetAndDecrement ()\nmethods using a counting network or diffracting tree.\nExercise 175. In the SimpleTree algorithm, what would happen if the\nboundedGetAndDecrement () method were replaced with a regular\ngetAndDecrement ()?\nExercise 176. Devise a SimpleTree algorithm with bounded capacity using\nboundedGetAndIncrement () methods in treeNode counters.\n15.7 Exercises 367\nExercise 177. In the SimpleTree class, what would happen if add(), after placing\nan item in the appropriate Bin, incremented counters in the same top-down man-\nner as in the removeMin () method? Give a detailed example.\nExercise 178. Prove that the SimpleTree is a quiescently consistent priority\nqueue implementation.\nExercise 179. Modify FineGrainedHeap to allocate new heap nodes dynamically.\nWhat are the performance limitations of this approach?\nExercise 180. Fig. 15.16 shows a bit-reversed counter. We could use the bit-\nreversed counter to manage the next \ufb01eld of the FineGrainedHeap class. Prove\n1public class BitReversedCounter {\n2 int counter, reverse, highBit;\n3 BitReversedCounter( int initialValue) {\n4 counter = initialValue;\n5 reverse = 0;\n6 highBit = -1;\n7 }\n8 public int reverseIncrement() {\n9 if(counter++ == 0) {\n10 reverse = highBit = 1;\n11 return reverse;\n12 }\n13 int bit = highBit >> 1;\n14 while (bit != 0) {\n15 reverse \u02c6= bit;\n16 if((reverse & bit) != 0) break ;\n17 bit >>= 1;\n18 }\n19 if(bit == 0)\n20 reverse = highBit <<= 1;\n21 return reverse;\n22 }\n23 public int reverseDecrement() {\n24 counter--;\n25 int bit = highBit >> 1;\n26 while (bit != 0) {\n27 reverse \u02c6= bit;\n28 if((reverse & bit) == 0) {\n29 break ;\n30 }\n31 bit >>= 1;\n32 }\n33 if(bit == 0) {\n34 reverse = counter;\n35 highBit >>= 1;\n36 }\n37 return reverse;\n38 }\n39 }\nFigure 15.16 A bit-reversed counter.\n368 Chapter 15 Priority Queues\nthe following: for any two consecutive insertions, the two paths from the leaves to\nthe root have no common nodes other than the root. Why is this a useful property\nfor the FineGrainedHeap ?\nExercise 181. Provide the code for the PrioritySkipList class\u2019s add() and\nremove () methods.\nExercise 182. ThePrioritySkipList class used in this chapter is based on the\nLockFreeSkipList class. Write another PrioritySkipList class based on the\nLazySkipList class.\nExercise 183. Describe a scenario in the SkipQueue implementation in which\ncontention would arise from multiple concurrent removeMin () method calls.\nExercise 184. TheSkipQueue class is quiescently consistent but not linearizable.\nHere is one way to make this class linearizable by adding a simple time-stamping\nmechanism. After a node is completely inserted into the SkipQueue , it acquires a\ntimestamp. A thread performing a removeMin () notes the time at which it starts\nits traversal of the lower level of the SkipQueue , and only considers nodes whose\ntimestamp is earlier than", "doc_id": "e6398d84-5f39-4238-840a-789a1a048ad5", "embedding": null, "doc_hash": "d2ff79b642a39cd625d7c213913ef228fb676fd960a5a7f2ec21b897c6d05181", "extra_info": null, "node_info": {"start": 941208, "end": 944498}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "78d2f2fd-a2e2-4a68-a050-e2c58546ccad", "3": "17507475-1ef5-4c7e-9395-47be57ee1518"}}, "__type__": "1"}, "17507475-1ef5-4c7e-9395-47be57ee1518": {"__data__": {"text": "class.\nExercise 183. Describe a scenario in the SkipQueue implementation in which\ncontention would arise from multiple concurrent removeMin () method calls.\nExercise 184. TheSkipQueue class is quiescently consistent but not linearizable.\nHere is one way to make this class linearizable by adding a simple time-stamping\nmechanism. After a node is completely inserted into the SkipQueue , it acquires a\ntimestamp. A thread performing a removeMin () notes the time at which it starts\nits traversal of the lower level of the SkipQueue , and only considers nodes whose\ntimestamp is earlier than the time at which it started its traversal, effectively\nignoring nodes inserted during its traversal. Implement this class and justify why\nit works.\n16Futures, Scheduling, and\nWork Distribution\n16.1 Introduction\nIn this chapter we show how to decompose certain kinds of problems into\ncomponents that can be executed in parallel. Some applications break down natu-\nrally into parallel threads. For example, when a request arrives at a web server, the\nserver can just create a thread (or assign an existing thread) to handle the request.\nApplications that can be structured as producers and consumers also tend to be\neasily parallelizable. In this chapter, however, we look at applications that have\ninherent parallelism, but where it is not obvious how to take advantage of it.\nLet us start by thinking about how to multiply two matrices in parallel. Recall\nthat ifaijis the value at position ( i,j) of matrixA, then the product Cof two\nn\u0002nmatricesAandBis given by:\ncij=n\u00001X\nk=0aik\u0001bkj\nAs a \ufb01rst step, we could put one thread in charge of computing each cij. Fig. 16.1\nshows a matrix multiplication program that creates an n\u0002narray of Worker\nthreads (Fig. 16.2), where the worker thread in position ( i,j) computescij. The\nprogram starts each task, and waits for them all to \ufb01nish.1\nIn principle, this might seem like an ideal design. The program is highly\nparallel, and the threads do not even have to synchronize. In practice, however,\nwhile this design might perform well for small matrices, it would perform very\npoorly for matrices large enough to be interesting. Here is why: threads require\nmemory for stacks and other bookkeeping information. Creating, scheduling,\nand destroying threads takes a substantial amount of computation. Creating\nlots of short-lived threads is an inef\ufb01cient way to organize a multi-threaded\ncomputation.\n1In real code, you should check that all the dimensions agree. Here we omit most safety checks\nfor brevity.\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00016-2\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.369\n370 Chapter 16 Futures, Scheduling, and Work Distribution\n1class MMThread {\n2 double [][] a, b, c;\n3 int n;\n4 public MMThread( double [][] myA, double [][] myB) {\n5 n = myA.length;\n6 a = myA;\n7 b = myB;\n8 c = new double [n][n];\n9 }\n10 void multiply() {\n11 Worker[][] worker = new Worker[n][n];\n12 for (int row = 0; row < n; row++)\n13 for (int col = 0; col < n; col++)\n14 worker[row][col] = new Worker(row,col);\n15 for (int row = 0; row < n; row++)\n16 for (int col = 0; col < n; col++)\n17 worker[row][col].start();\n18 for (int row = 0; row < n; row++)\n19 for", "doc_id": "17507475-1ef5-4c7e-9395-47be57ee1518", "embedding": null, "doc_hash": "5e74a7c373cb7c1e6b35df42f0f5a152241563ee60488249924db4138c12201b", "extra_info": null, "node_info": {"start": 944448, "end": 947678}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e6398d84-5f39-4238-840a-789a1a048ad5", "3": "1107e400-7e51-4c61-8db0-368e08d9f33f"}}, "__type__": "1"}, "1107e400-7e51-4c61-8db0-368e08d9f33f": {"__data__": {"text": "b = myB;\n8 c = new double [n][n];\n9 }\n10 void multiply() {\n11 Worker[][] worker = new Worker[n][n];\n12 for (int row = 0; row < n; row++)\n13 for (int col = 0; col < n; col++)\n14 worker[row][col] = new Worker(row,col);\n15 for (int row = 0; row < n; row++)\n16 for (int col = 0; col < n; col++)\n17 worker[row][col].start();\n18 for (int row = 0; row < n; row++)\n19 for (int col = 0; col < n; col++)\n20 worker[row][col].join();\n21 }\nFigure 16.1 TheMMThread task: matrix multiplication using threads.\n22 class Worker extends Thread {\n23 int row, col;\n24 Worker( int myRow, int myCol) {\n25 row = myRow; col = myCol;\n26 }\n27 public void run() {\n28 double dotProduct = 0.0;\n29 for (int i = 0; i < n; i++)\n30 dotProduct += a[row][i] *b[i][col];\n31 c[row][col] = dotProduct;\n32 }\n33 }\n34 }\nFigure 16.2 TheMMThread task: inner Worker thread class.\nA more effective way to organize such a program is to create a pool of long-\nlived threads. Each thread in the pool repeatedly waits until it is assigned a task,\na short-lived unit of computation. When a thread is assigned a task, it executes\nthat task, and then rejoins the pool to await its next assignment. Thread pools can\nbe platform-dependent: it makes sense for large-scale multiprocessors to provide\nlarge pools, and vice versa. Thread pools avoid the cost of creating and destroying\nthreads in response to short-lived \ufb02uctuations in demand.\nIn addition to performance bene\ufb01ts, thread pools have another equally\nimportant, but less obvious advantage: they insulate the application programmer\nfrom platform-speci\ufb01c details such as the number of concurrent threads that can\nbe scheduled ef\ufb01ciently. Thread pools make it possible to write a single program\n16.1 Introduction 371\nthat runs equally well on a uniprocessor, a small-scale multiprocessor, and a\nlarge-scale multiprocessor. They provide a simple interface that hides complex,\nplatform-dependent engineering trade-offs.\nIn Java, a thread pool is called an executor service (interface java.util.Executor-\nService ). It provides the ability to submit a task, the ability to wait for a set\nof submitted tasks to complete, and the ability to cancel uncompleted tasks.\nA task that does not return a result is usually represented as a Runnable object,\nwhere the work is performed by a run() method that takes no arguments and\nreturns no results. A task that returns a value of type Tis usually represented as a\nCallable<T> object, where the result is returned by a call() with the Tmethod\nthat takes no arguments.\nWhen a Callable<T> object is submitted to an executor service, the ser-\nvice returns an object implementing the Future<T> interface. A Future<T>\nis a promise to deliver the result of an asynchronous computation, when it is\nready. It provides a get() method that returns the result, blocking if necessary\nuntil the result is ready. (It also provides methods for canceling uncompleted\ncomputations, and for testing whether the computation is complete.) Submit-\nting a Runnable task also returns a future. Unlike the future returned for a\nCallable<T> object, this future does not return a value, but the caller can use\nthat future\u2019s get() method to block until the computation \ufb01nishes. A future that\ndoes not return an interesting value is declared to have class Future<?>", "doc_id": "1107e400-7e51-4c61-8db0-368e08d9f33f", "embedding": null, "doc_hash": "384346c4ba46c4e4a595a8cf7542d2ccb13b301577bd5570aeca7ec58d1214ff", "extra_info": null, "node_info": {"start": 947888, "end": 951164}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "17507475-1ef5-4c7e-9395-47be57ee1518", "3": "3445637f-4df4-4c4f-8348-0afd5ab37c62"}}, "__type__": "1"}, "3445637f-4df4-4c4f-8348-0afd5ab37c62": {"__data__": {"text": "computation, when it is\nready. It provides a get() method that returns the result, blocking if necessary\nuntil the result is ready. (It also provides methods for canceling uncompleted\ncomputations, and for testing whether the computation is complete.) Submit-\nting a Runnable task also returns a future. Unlike the future returned for a\nCallable<T> object, this future does not return a value, but the caller can use\nthat future\u2019s get() method to block until the computation \ufb01nishes. A future that\ndoes not return an interesting value is declared to have class Future<?> .\nIt is important to understand that creating a future does not guarantee that any\ncomputations actually happen in parallel. Instead, these methods are advisory :\nthey tell an underlying executor service that it may execute these methods in\nparallel.\nWe now consider how to implement parallel matrix operations using an\nexecutor service. Fig. 16.3 shows a Matrix class that provides put() and get()\nmethods to access matrix elements, along with a constant-time split () method\nthat splits an n-by-nmatrix into four ( n=2)-by-(n=2) submatrices. In Java ter-\nminology, the four submatrices are backed by the original matrix, meaning that\nchanges to the submatrices are re\ufb02ected in the original, and vice versa.\nOur job is to devise a MatrixTask class that provides parallel methods to add\nand multiply matrices. This class has one static \ufb01eld, an executor service called\nexec , and two static methods to add and multiply matrices.\nFor simplicity, we consider matrices whose dimension nis a power of 2. Any\nsuch matrix can be decomposed into four submatrices:\nA=\u0012A00A01\nA10A11\u0013\nMatrix addition C=A+Bcan be decomposed as follows:\n\u0012C00C01\nC10C11\u0013\n=\u0012A00A01\nA10A11\u0013\n+\u0012B00B01\nB10B11\u0013\n=\u0012A00+B00A01+B01\nA10+B10A11+B11\u0013\n372 Chapter 16 Futures, Scheduling, and Work Distribution\n1public class Matrix {\n2 int dim;\n3 double [][] data;\n4 int rowDisplace, colDisplace;\n5 public Matrix( int d) {\n6 dim = d;\n7 rowDisplace = colDisplace = 0;\n8 data = new double [d][d];\n9 }\n10 private Matrix( double [][] matrix, int x,int y,int d) {\n11 data = matrix;\n12 rowDisplace = x;\n13 colDisplace = y;\n14 dim = d;\n15 }\n16 public double get( int row, int col) {\n17 return data[row+rowDisplace][col+colDisplace];\n18 }\n19 public void set( int row, int col, double value) {\n20 data[row+rowDisplace][col+colDisplace] = value;\n21 }\n22 public int getDim() {\n23 return dim;\n24 }\n25 Matrix[][] split() {\n26 Matrix[][] result = new Matrix[2][2];\n27 int newDim = dim / 2;\n28 result[0][0] =\n29 new Matrix(data, rowDisplace, colDisplace, newDim);\n30 result[0][1] =\n31 new Matrix(data, rowDisplace, colDisplace + newDim, newDim);\n32 result[1][0] =\n33 new Matrix(data, rowDisplace + newDim, colDisplace, newDim);\n34 result[1][1] =\n35 new Matrix(data, rowDisplace + newDim, colDisplace + newDim, newDim);\n36 return result;\n37 }\n38 }\nFigure 16.3 TheMatrix class.\nThese four sums can be done in parallel.\nThe code for multithreaded matrix addition appears in Fig. 16.4 . The AddTask\nclass has", "doc_id": "3445637f-4df4-4c4f-8348-0afd5ab37c62", "embedding": null, "doc_hash": "0e96ed2cd67629efa9042f391d03a16c1952a9ff5932452d8035491af2d981ee", "extra_info": null, "node_info": {"start": 950975, "end": 953991}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "1107e400-7e51-4c61-8db0-368e08d9f33f", "3": "475fae35-c113-43c1-8b60-1fc2deb86b8d"}}, "__type__": "1"}, "475fae35-c113-43c1-8b60-1fc2deb86b8d": {"__data__": {"text": "=\n31 new Matrix(data, rowDisplace, colDisplace + newDim, newDim);\n32 result[1][0] =\n33 new Matrix(data, rowDisplace + newDim, colDisplace, newDim);\n34 result[1][1] =\n35 new Matrix(data, rowDisplace + newDim, colDisplace + newDim, newDim);\n36 return result;\n37 }\n38 }\nFigure 16.3 TheMatrix class.\nThese four sums can be done in parallel.\nThe code for multithreaded matrix addition appears in Fig. 16.4 . The AddTask\nclass has three \ufb01elds, initialized by the constructor: aandbare the matrices to be\nsummed, and cis the result, which is updated in place. Each task does the follow-\ning. At the bottom of the recursion, it simply adds the two scalar values (Line 20\nofFig. 16.4 ).2Otherwise, it splits each of its arguments into four sub-matrices\n2In practice, it is usually more ef\ufb01cient to stop the recursion well before reaching a matrix size of\none. The best size will be platform-dependent.\n16.1 Introduction 373\n1public class MatrixTask {\n2 static ExecutorService exec = Executors.newCachedThreadPool();\n3 ...\n4 static Matrix add(Matrix a, Matrix b) throws ExecutionException {\n5 int n = a.getDim();\n6 Matrix c = new Matrix(n);\n7 Future<?> future = exec.submit( new AddTask(a, b, c));\n8 future.get();\n9 return c;\n10 }\n11 static class AddTask implements Runnable {\n12 Matrix a, b, c;\n13 public AddTask(Matrix myA, Matrix myB, Matrix myC) {\n14 a = myA; b = myB; c = myC;\n15 }\n16 public void run() {\n17 try {\n18 int n = a.getDim();\n19 if(n == 1) {\n20 c.set(0, 0, a.get(0,0) + b.get(0,0));\n21 }else {\n22 Matrix[][] aa = a.split(), bb = b.split(), cc = c.split();\n23 Future<?>[][] future = (Future<?>[][]) new Future[2][2];\n24 for (int i = 0; i < 2; i++)\n25 for (int j = 0; j < 2; j++)\n26 future[i][j] =\n27 exec.submit( new AddTask(aa[i][j], bb[i][j], cc[i][j]));\n28 for (int i = 0; i < 2; i++)\n29 for (int j = 0; j < 2; j++)\n30 future[i][j].get();\n31 }\n32 }catch (Exception ex) {\n33 ex.printStackTrace();\n34 }\n35 }\n36 }\n37 }\nFigure 16.4 TheMatrixTask class: parallel matrix addition.\n(Line 22), and launches a new task for each sub-matrix (Lines 24\u201327). Then,\nit waits until all futures can be evaluated, meaning that the sub-computations\nhave \ufb01nished (Lines 28\u201330). At that point, the task simply returns, the result of\nthe computation having been stored in the result matrix. Matrix multiplication\nC=A\u0001Bcan be decomposed as follows:\n\u0012C00C01\nC10C11\u0013\n=\u0012A00A01\nA10A11\u0013\n\u0001\u0012B00B01\nB10B11\u0013\n=\u0012A00\u0001B00+A01\u0001B10A00\u0001B01+A01\u0001B11\nA10\u0001B00+A11\u0001B10A10\u0001B01+A11\u0001B11\u0013\n374 Chapter 16 Futures, Scheduling, and Work Distribution\nThe eight product terms can be computed in parallel, and when those\ncomputations are done, the four sums can then be computed in parallel.\nFig. 16.5 shows the code for the parallel", "doc_id": "475fae35-c113-43c1-8b60-1fc2deb86b8d", "embedding": null, "doc_hash": "1cd6732cc5cc6ed821cb45a0ff3f64f56c8e61c54b52f6f684a2e0b81b03ae61", "extra_info": null, "node_info": {"start": 954108, "end": 956795}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3445637f-4df4-4c4f-8348-0afd5ab37c62", "3": "c118a6a2-a1ed-486b-8724-2e24838c4183"}}, "__type__": "1"}, "c118a6a2-a1ed-486b-8724-2e24838c4183": {"__data__": {"text": "Chapter 16 Futures, Scheduling, and Work Distribution\nThe eight product terms can be computed in parallel, and when those\ncomputations are done, the four sums can then be computed in parallel.\nFig. 16.5 shows the code for the parallel matrix multiplication task. Matrix\nmultiplication is structured in a similar way to addition. The MulTask class cre-\nates two scratch arrays to hold the matrix product terms (Line 42). It splits all \ufb01ve\nmatrices (Line 50), submits tasks to compute the eight product terms in parallel\n(Line 56), and waits for them to complete (Line 60). Once they are complete, the\nthread submits tasks to compute the four sums in parallel (Line 64), and waits\nfor them to complete (Line 65).\nThe matrix example uses futures only to signal when a task is complete.\nFutures can also be used to pass values from completed tasks. T o illustrate\nthis use of futures, we consider how to decompose the well-known Fibonacci\n38 static class MulTask implements Runnable {\n39 Matrix a, b, c, lhs, rhs;\n40 public MulTask(Matrix myA, Matrix myB, Matrix myC) {\n41 a = myA; b = myB; c = myC;\n42 lhs = new Matrix(a.getDim());\n43 rhs = new Matrix(a.getDim());\n44 }\n45 public void run() {\n46 try {\n47 if(a.getDim() == 1) {\n48 c.set(0, 0, a.get(0,0) *b.get(0,0));\n49 }else {\n50 Matrix[][] aa = a.split(), bb = b.split(), cc = c.split();\n51 Matrix[][] ll = lhs.split(), rr = rhs.split();\n52 Future<?>[][][] future = (Future<?>[][][]) new Future[2][2][2];\n53 for (int i = 0; i < 2; i++)\n54 for (int j = 0; j < 2; j++) {\n55 future[i][j][0] =\n56 exec.submit( new MulTask(aa[i][0], bb[0][i], ll[i][j]));\n57 future[i][j][1] =\n58 exec.submit( new MulTask(aa[1][i], bb[i][1], rr[i][j]));\n59 }\n60 for (int i = 0; i < 2; i++)\n61 for (int j = 0; j < 2; j++)\n62 for (int k = 0; k < 2; k++)\n63 future[i][j][k].get();\n64 Future<?> done = exec.submit( new AddTask(lhs, rhs, c));\n65 done.get();\n66 }\n67 }catch (Exception ex) {\n68 ex.printStackTrace();\n69 }\n70 }\n71 }\n72 ...\n73 }\nFigure 16.5 TheMatrixTask class: parallel matrix multiplication.\n16.2 Analyzing Parallelism 375\n1class FibTask implements Callable<Integer> {\n2 static ExecutorService exec = Executors.newCachedThreadPool();\n3 int arg;\n4 public FibTask( int n) {\n5 arg = n;\n6 }\n7 public Integer call() {\n8 if(arg >= 2) {\n9 Future<Integer> left = exec.submit( new FibTask(arg-1));\n10 Future<Integer> right = exec.submit( new FibTask(arg-2));\n11 return left.get() + right.get();\n12 }else {\n13 return 1;\n14 }\n15 }\n16 }\nFigure 16.6 TheFibTask class: a Fibonacci task with futures.\nfunction into a multithreaded program. Recall that the Fibonacci sequence is\nde\ufb01ned as follows:\nF(n) =8\n<\n:1 if n= 0,\n1 if n= 1,\nF(n\u00001)", "doc_id": "c118a6a2-a1ed-486b-8724-2e24838c4183", "embedding": null, "doc_hash": "ae2b2b56cf7db0f402e052c983f277284f2f7b665d8e7ea18e8d42592687df48", "extra_info": null, "node_info": {"start": 956967, "end": 959624}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "475fae35-c113-43c1-8b60-1fc2deb86b8d", "3": "897cfaf4-5179-468b-b906-ad124f4073c5"}}, "__type__": "1"}, "897cfaf4-5179-468b-b906-ad124f4073c5": {"__data__": {"text": "= exec.submit( new FibTask(arg-1));\n10 Future<Integer> right = exec.submit( new FibTask(arg-2));\n11 return left.get() + right.get();\n12 }else {\n13 return 1;\n14 }\n15 }\n16 }\nFigure 16.6 TheFibTask class: a Fibonacci task with futures.\nfunction into a multithreaded program. Recall that the Fibonacci sequence is\nde\ufb01ned as follows:\nF(n) =8\n<\n:1 if n= 0,\n1 if n= 1,\nF(n\u00001) +F(n\u00002) ifn>1,\nFig. 16.6 shows one way to compute Fibonacci numbers in parallel. This imple-\nmentation is very inef\ufb01cient, but we use it here to illustrate multithreaded depen-\ndencies. The call () method creates two futures, one that computes F(n\u00002) and\nanother that computes F(n\u00001), and then sums them. On a multiprocessor, time\nspent blocking on the future for F(n\u00001) can be used to compute F(n\u00002).\n16.2 Analyzing Parallelism\nThink of a multithreaded computation as a directed acyclic graph (DAG), where\neach node represents a task, and each directed edge links a predecessor task to a\nsuccessor task, where the successor depends on the predecessor\u2019s result. For exam-\nple, a conventional thread is just a chain of nodes where each node depends on\nits predecessor. By contrast, a node that creates a future has two successors: one\nnode is its successor in the same thread, and the other is the \ufb01rst node in the\nfuture\u2019s computation. There is also an edge in the other direction, from child to\nparent, that occurs when a thread that has created a future calls that future\u2019s get()\nmethod, waiting for the child computation to complete. Fig. 16.7 shows the DAG\ncorresponding to a short Fibonacci execution.\nSome computations are inherently more parallel than others. Let us make\nthis notion precise. Assume that all individual computation steps take the\nsame amount of time, which constitutes our basic measuring unit. Let TPbe\n376 Chapter 16 Futures, Scheduling, and Work Distribution\nget submit1\n2\n34\n5678\ufb01b(4)\n\ufb01b(3)\n\ufb01b(2)\ufb01b(1)\ufb01b(2)\n\ufb01b(1) \ufb01b(0)\ufb01b(1) \ufb01b(0)\nFigure 16.7 The DAG created by a multithreaded Fibonacci execution. The caller creates\na FibTask(4) task, which in turn creates FibTask(3) and FibTask(2) tasks. The round nodes\nrepresent computation steps and the arrows between the nodes represent dependencies.\nFor example, there are arrows pointing from the \ufb01rst two nodes in FibTask(4) to the \ufb01rst\nnodes in FibTask(3) and FibTask(2) respectively, representing submit ()calls, and arrows from\nthe last nodes in FibTask(3) and FibTask(2) to the last node in FibTask(4) representing get()\ncalls. The computation\u2019s critical path has length 8 and is marked by numbered nodes.\nthe minimum time (measured in computation steps) needed to execute a mul-\ntithreaded program on a system of Pdedicated processors. TPis thus the pro-\ngram\u2019s latency , the time it would take it to run from start to \ufb01nish, as measured\nby an outside observer. We emphasize that TPis an idealized measure: it may not\nalways be possible for every processor to", "doc_id": "897cfaf4-5179-468b-b906-ad124f4073c5", "embedding": null, "doc_hash": "98c17ec53b93d232808f8854b2ab90cddec83e6c22aa8a286ad1db66df4c6719", "extra_info": null, "node_info": {"start": 959503, "end": 962408}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "c118a6a2-a1ed-486b-8724-2e24838c4183", "3": "4cbdf18f-c70e-43a2-b532-e19081e12f06"}}, "__type__": "1"}, "4cbdf18f-c70e-43a2-b532-e19081e12f06": {"__data__": {"text": "and FibTask(2) to the last node in FibTask(4) representing get()\ncalls. The computation\u2019s critical path has length 8 and is marked by numbered nodes.\nthe minimum time (measured in computation steps) needed to execute a mul-\ntithreaded program on a system of Pdedicated processors. TPis thus the pro-\ngram\u2019s latency , the time it would take it to run from start to \ufb01nish, as measured\nby an outside observer. We emphasize that TPis an idealized measure: it may not\nalways be possible for every processor to \ufb01nd steps to execute, and actual compu-\ntation time may be limited by other concerns, such as memory usage. Neverthe-\nless,TPis clearly a lower bound on how much parallelism one can extract from\na multithreaded computation.\nSome values of Tare important enough that they have special names. T1, the\nnumber of steps needed to execute the program on a single processor, is called\nthe computation\u2019s work . Work is also the total number of steps in the entire com-\nputation. In one time step (of the outside observer), Pprocessors can execute at\nmostPcomputation steps, so\nTP>T1=P:\nThe other extreme is also of special importance: T1, the number of steps to exe-\ncute the program on an unlimited number of processors, is called the critical-path\nlength . Because \ufb01nite resources cannot do better than in\ufb01nite resources,\nTP>T1:\nThe speedup onPprocessors is the ratio:\nT1=TP\n16.2 Analyzing Parallelism 377\nWe say a computation has linear speedup ifT1=TP=\u00ca(P). Finally, a compu-\ntation\u2019s parallelism is the maximum possible speedup: T1=T1. A computation\u2019s\nparallelism is also the average amount of work available at each step along the\ncritical path, and so provides a good estimate of the number of processors one\nshould devote to a computation. In particular, it makes little sense to use sub-\nstantially more than that number of processors.\nT o illustrate these concepts, we now revisit the concurrent matrix add and\nmultiply implementations introduced in Section 16.1.\nLetAP(n) be the number of steps needed to add two n\u0002nmatrices on P\nprocessors. Recall that matrix addition requires four half-size matrix additions,\nplus a constant amount of work to split the matrices. The work A1(n) is given by\nthe recurrence:\nA1(n) = 4A1(n=2) +\u00ca(1)\n=\u00ca(n2):\nThis program has the same work as the conventional doubly-nested loop imple-\nmentation.\nBecause the half-size additions can be done in parallel, the critical path length\nis given by the following formula.\nA1(n) =A1(n=2) +\u00ca(1)\n=\u00ca(logn)\nLetMP(n) be the number of steps needed to multiply two n\u0002nmatrices on P\nprocessors. Recall that matrix multiplication requires eight half-size matrix mul-\ntiplications and four matrix additions. The work M1(n) is given by the recur-\nrence:\nM1(n) = 8M1(n=2) + 4A1(n)\nM1(n) = 8M1(n=2) +\u00ca(n2)\n=\u00ca(n3):\nThis work is also the same as the conventional triply-nested loop implementa-\ntion. The half-size multiplications can be done in parallel, and so can the addi-\ntions, but the additions must wait for the multiplications to complete. The critical\npath length is given by the following formula:\nM1(n) =M1(n=2)", "doc_id": "4cbdf18f-c70e-43a2-b532-e19081e12f06", "embedding": null, "doc_hash": "b56af9d14b035cdeaad60c95565d53ad4e4049428d986206708668a677d13820", "extra_info": null, "node_info": {"start": 962305, "end": 965392}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "897cfaf4-5179-468b-b906-ad124f4073c5", "3": "6d39224e-a14f-4783-9923-acd54e6eecb7"}}, "__type__": "1"}, "6d39224e-a14f-4783-9923-acd54e6eecb7": {"__data__": {"text": "given by the recur-\nrence:\nM1(n) = 8M1(n=2) + 4A1(n)\nM1(n) = 8M1(n=2) +\u00ca(n2)\n=\u00ca(n3):\nThis work is also the same as the conventional triply-nested loop implementa-\ntion. The half-size multiplications can be done in parallel, and so can the addi-\ntions, but the additions must wait for the multiplications to complete. The critical\npath length is given by the following formula:\nM1(n) =M1(n=2) +A1(n)\n=M1(n=2) +\u00ca(logn)\n=\u00ca(log2n)\nThe parallelism for matrix multiplication is given by:\nM1(n)=M1(n) =\u00ca(n3=log2n),\n378 Chapter 16 Futures, Scheduling, and Work Distribution\nwhich is pretty high. For example, suppose we want to multiply two 1000-by-1000\nmatrices. Here, n3= 109, and logn= log 1000\u001910 (logs are base two), so the\nparallelism is approximately 109=102= 107. Roughly speaking, this instance of\nmatrix multiplication could, in principle, keep roughly a million processors busy\nwell beyond the powers of any multiprocessor we are likely to see in the immedi-\nate future.\nWe should understand that the parallelism in the computation given here is a\nhighly idealized upper bound on the performance of any multithreaded matrix\nmultiplication program. For example, when there are idle threads, it may not\nbe easy to assign those threads to idle processors. Moreover, a program that dis-\nplays less parallelism but consumes less memory may perform better because it\nencounters fewer page faults. The actual performance of a multithreaded compu-\ntation remains a complex engineering problem, but the kind of analysis presented\nin this chapter is an indispensable \ufb01rst step in understanding the degree to which\na problem can be solved in parallel.\n16.3 Realistic Multiprocessor Scheduling\nOur analysis so far has been based on the assumption that each multithreaded\nprogram has Pdedicated processors. This assumption, unfortunately, is not\nrealistic. Multiprocessors typically run a mix of jobs, where jobs come and go\ndynamically. One might start, say, a matrix multiplication application on Ppro-\ncessors. At some point, the operating system may decide to download a new soft-\nware upgrade, preempting one processor, and the application then runs on P\u00001\nprocessors. The upgrade program pauses waiting for a disk read or write to com-\nplete, and in the interim the matrix application has Pprocessors again.\nModern operating systems provide user-level threads that encompass a pro-\ngram counter and a stack. (A thread that includes its own address space is often\ncalled a process .) The operating system kernel includes a scheduler that runs\nthreads on physical processors. The application, however, typically has no control\nover the mapping between threads and processors, and so cannot control when\nthreads are scheduled.\nAs we have seen, one way to bridge the gap between user-level threads and\noperating system-level processors is to provide the software developer with a\nthree-level model. At the top level, multithreaded programs (such as matrix mul-\ntiplication) decompose an application into a dynamically-varying number of\nshort-lived tasks . At the middle level, a user-level scheduler maps these tasks to a\n\ufb01xed number of threads . At the bottom level, the kernel maps these threads onto\nhardware processors , whose availability may vary dynamically. This last level of\nmapping is not under the application\u2019s control: applications cannot tell the ker-\nnel how to schedule threads (especially because", "doc_id": "6d39224e-a14f-4783-9923-acd54e6eecb7", "embedding": null, "doc_hash": "f524832a3fbe93a7f7adb295cd7f2ba51dc0ba19b1c82e05fd2878881ed6c227", "extra_info": null, "node_info": {"start": 965481, "end": 968887}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "4cbdf18f-c70e-43a2-b532-e19081e12f06", "3": "e2d6f93b-9d81-411d-8946-2cbb79b1b7e9"}}, "__type__": "1"}, "e2d6f93b-9d81-411d-8946-2cbb79b1b7e9": {"__data__": {"text": "with a\nthree-level model. At the top level, multithreaded programs (such as matrix mul-\ntiplication) decompose an application into a dynamically-varying number of\nshort-lived tasks . At the middle level, a user-level scheduler maps these tasks to a\n\ufb01xed number of threads . At the bottom level, the kernel maps these threads onto\nhardware processors , whose availability may vary dynamically. This last level of\nmapping is not under the application\u2019s control: applications cannot tell the ker-\nnel how to schedule threads (especially because commercially available operating\nsystems kernels are hidden from users).\n16.3 Realistic Multiprocessor Scheduling 379\nAssume for simplicity that the kernel works in discrete steps: at step i, the\nkernel chooses an arbitrary subset of 0 6pi6Puser-level threads to run for one\nstep. The processor average PAoverTsteps is de\ufb01ned to be:\nPA=1\nTT\u00001X\ni=0pi: (16.3.1)\nInstead of designing a user-level schedule to achieve a P-fold speedup, we can\ntry to achieve a PA-fold speedup. A schedule is greedy if the number of program\nsteps executed at each time step is the minimum of pi, the number of available\nprocessors, and the number of ready nodes (ones whose associated step is ready\nto be executed) in the program DAG. In other words, it executes as many of the\nready nodes as possible, given the number of available processors.\nTheorem 16.3.1. Consider a multithreaded program with work T1, critical-path\nlengthT1, andPuser-level threads. We claim that any greedy execution has\nlengthTwhich is at most\nT1\nPA+T1(P\u00001)\nPA:\nProof: Equation 16.3.1 implies that:\nT=1\nPAT\u00001X\ni=0pi:\nWe boundTby bounding the sum of the pi. At each kernel-level step i, let us\nimagine getting a token for each thread that was assigned a processor. We can\nplace these tokens in one of two buckets. For each user-level thread that executes a\nnode at step i, we place a token in a work bucket , and for each thread that remains\nidle at that step (that is, it was assigned to a processor but was not ready to execute\nbecause the node associated with its next step had dependencies that force it to\nwait for some other threads), we place a token in an idle bucket . After the last step,\nthe work bucket contains T1tokens, one for each node of the computation DAG.\nHow many tokens does the idle bucket contain?\nWe de\ufb01ne an idle step as one in which some thread places a token in the idle\nbucket. Because the application is still running, at least one node is ready for\nexecution in each step. Because the scheduler is greedy, at least one node will be\nexecuted, so at least one processor is not idle. Thus, of the pithreads scheduled\nat stepi, at mostpi\u000016P\u00001 can be idle.\nHow many idle steps could there be? Let Gibe a sub-DAG of the computation\nconsisting of the nodes that have not been executed at the end of step i. Every\nnode that does not have incoming edges (apart from its predecessor in program\norder) inGi\u00001(such as the last node of FibTask(2) at the end of step 6) was\nready at the start of step i. There must be fewer than pisuch nodes, because\notherwise the greedy schedule could execute piof them, and the step iwould\n380 Chapter 16 Futures, Scheduling, and Work Distribution\nnot be idle. Thus, the scheduler must have executed this step. It follows that the\nlongest directed path in Giis one shorter than the", "doc_id": "e2d6f93b-9d81-411d-8946-2cbb79b1b7e9", "embedding": null, "doc_hash": "b896af581879f397350143f740851d034af8390de34fc5c8702d89c9f8df3b4c", "extra_info": null, "node_info": {"start": 968760, "end": 972089}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6d39224e-a14f-4783-9923-acd54e6eecb7", "3": "612f6380-13da-42da-8535-5a4dc85d6bef"}}, "__type__": "1"}, "612f6380-13da-42da-8535-5a4dc85d6bef": {"__data__": {"text": "end of step i. Every\nnode that does not have incoming edges (apart from its predecessor in program\norder) inGi\u00001(such as the last node of FibTask(2) at the end of step 6) was\nready at the start of step i. There must be fewer than pisuch nodes, because\notherwise the greedy schedule could execute piof them, and the step iwould\n380 Chapter 16 Futures, Scheduling, and Work Distribution\nnot be idle. Thus, the scheduler must have executed this step. It follows that the\nlongest directed path in Giis one shorter than the longest directed path in Gi\u00001.\nThe longest directed path before step 0 is T1, so the greedy schedule can have\nat mostT1idle steps. Combining these observations we deduce that at most T1\nidle steps are executed with at most ( P\u00001) tokens added in each, so the idle\nbucket contains at most T1(P\u00001) tokens.\nThe total number of tokens in both buckets is therefore\nT\u00001X\ni=0pi6T1+T1(P\u00001),\nyielding the desired bound. 2\nIt turns out that this bound is within a factor of two of optimal. Actu-\nally, achieving an optimal schedule is NP-complete, so greedy schedules are a\nsimple and practical way to achieve performance that is reasonably close to\noptimal.\n16.4 Work Distribution\nWe now understand that the key to achieving a good speedup is to keep\nuser-level threads supplied with tasks, so that the resulting schedule is as greedy\nas possible. Multithreaded computations, however, create and destroy tasks\ndynamically, sometimes in unpredictable ways. A work distribution algorithm is\nneeded to assign ready tasks to idle threads as ef\ufb01ciently as possible.\nOne simple approach to work distribution is work dealing : an overloaded\nthread tries to of\ufb02oad tasks to other, less heavily loaded threads. This approach\nmay seem sensible, but it has a basic \ufb02aw: if most threads are overloaded, then\nthey waste effort in a futile attempt to exchange tasks. Instead, we \ufb01rst consider\nwork stealing , in which a thread that runs out of work tries to \u201csteal\u201d work from\nothers. An advantage of work stealing is that if all threads are already busy, then\nthey do not waste time trying to of\ufb02oad work on one another.\n16.4.1 Work Stealing\nEach thread keeps a pool of tasks waiting to be executed in the form of a\ndouble-ended queue (DEQueue ), providing pushBottom (),popBottom (), and\npopTop () methods (there is no need for a pushTop () method). When a thread\ncreates a new task, it calls pushBottom () to push that task onto its DEQueue .\nWhen a thread needs a task to work on, it calls popBottom () to remove a task\nfrom its own DEQueue . If the thread discovers its queue is empty, then it becomes\nathief : it chooses a victim thread at random, and calls that thread\u2019s DEQueue \u2019s\npopTop () method to \u201csteal\u201d a task for itself.\n16.4 Work Distribution 381\n1public class WorkStealingThread {\n2 DEQueue[] queue;\n3 Random random;\n4 public WorkStealingThread(DEQueue[] myQueue) {\n5 queue = myQueue;\n6 random = new Random();\n7 }\n8 public void run() {\n9 int me = ThreadID.get();\n10 Runnable task = queue[me].popBottom();\n11 while (true ) {\n12 while (task != null ) {\n13 task.run();\n14 task = queue[me].popBottom();\n15 }\n16 while (task == null ) {\n17 Thread.yield();\n18 int victim =", "doc_id": "612f6380-13da-42da-8535-5a4dc85d6bef", "embedding": null, "doc_hash": "38d4a5283110e9df2251219de2479ec6f08bdbf4502515d7ffeb1875cf8469b2", "extra_info": null, "node_info": {"start": 972121, "end": 975298}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e2d6f93b-9d81-411d-8946-2cbb79b1b7e9", "3": "fbea1bda-3be4-4926-9c16-b5a07d36a28c"}}, "__type__": "1"}, "fbea1bda-3be4-4926-9c16-b5a07d36a28c": {"__data__": {"text": "Work Distribution 381\n1public class WorkStealingThread {\n2 DEQueue[] queue;\n3 Random random;\n4 public WorkStealingThread(DEQueue[] myQueue) {\n5 queue = myQueue;\n6 random = new Random();\n7 }\n8 public void run() {\n9 int me = ThreadID.get();\n10 Runnable task = queue[me].popBottom();\n11 while (true ) {\n12 while (task != null ) {\n13 task.run();\n14 task = queue[me].popBottom();\n15 }\n16 while (task == null ) {\n17 Thread.yield();\n18 int victim = random.nextInt(queue.length);\n19 if(!queue[victim].isEmpty()) {\n20 task = queue[victim].popTop();\n21 }\n22 }\n23 }\n24 }\n25 }\nFigure 16.8 TheWorkStealingThread class: a simpli\ufb01ed work stealing executer pool.\nInSection 16.5 we devise an ef\ufb01cient linearizable implementation of a\nDEQueue .Fig. 16.8 shows one possible way to implement a thread used by a work-\nstealing executor service. The threads share an array of DEQueue s (Line 2), one\nfor each thread. Each thread repeatedly removes a task from its own DEQueue\nand runs it (Lines 12\u201315). If it runs out, then it repeatedly chooses a victim\nthread at random and tries to steal a task from the top of the victim\u2019s DEQueue\n(Lines 16\u201322). T o avoid code clutter, we ignore the possibility that stealing may\ntrigger an exception.\nThis simpli\ufb01ed executer pool may keep trying to steal forever, long after all\nwork in all queues has been completed. T o prevent threads from endlessly search-\ning for nonexistent work, we can use a termination-detecting barrier of the kind\ndescribed in Chapter 17 ,Section 17.6 .\n16.4.2 Yielding and Multiprogramming\nAs noted earlier, multiprocessors provide a three-level model of computation:\nshort-lived tasks are executed by system-level threads , which are scheduled\nby the operating system on a \ufb01xed number of processors . Amultiprogrammed\nenvironment is one in which there are more threads than processors, imply-\ning that not all threads can run at the same time, and that any thread can be\n382 Chapter 16 Futures, Scheduling, and Work Distribution\npreemptively suspended at any time. T o guarantee progress, we must ensure that\nthreads that have work to do are not unreasonably delayed by ( thief ) threads\nwhich are idle except for task-stealing. T o prevent this situation, we have each\nthief call Thread.yield() immediately before trying to steal a task (Line 17\ninFig. 16.8 ). This call yields the thief\u2019s processor to another thread, allowing\ndescheduled threads to regain a processor and make progress. (We note that call-\ningyield () has no effect if there are no descheduled threads capable of running.)\n16.5 Work-Stealing Dequeues\nHere is how to implement a work-stealing DEQueue . Ideally, a work-stealing algo-\nrithm should provide a linearizable implementation whose pop methods always\nreturn a task if one is available. In practice, however, we can settle for something\nweaker, allowing a popTop () call to return null if it con\ufb02icts with a concurrent\npopTop () call. Though we could have the unsuccessful thief simply try again, it\nmakes more sense in this context to have a thread retry the popTop () operation\non a different, randomly chosen DEQueue each time. T o support such a retry, a\npopTop () call may return null if it con\ufb02icts with a concurrent popTop () call.\nWe now describe two implementations of the work-stealing DEQueue . The", "doc_id": "fbea1bda-3be4-4926-9c16-b5a07d36a28c", "embedding": null, "doc_hash": "337a052fb72fd6ebcaaed357417d9e4c36ff9524eed3625cf126f0fe66e13be8", "extra_info": null, "node_info": {"start": 975350, "end": 978633}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "612f6380-13da-42da-8535-5a4dc85d6bef", "3": "02ce576e-1e4a-48a8-8a23-107629626f49"}}, "__type__": "1"}, "02ce576e-1e4a-48a8-8a23-107629626f49": {"__data__": {"text": "a task if one is available. In practice, however, we can settle for something\nweaker, allowing a popTop () call to return null if it con\ufb02icts with a concurrent\npopTop () call. Though we could have the unsuccessful thief simply try again, it\nmakes more sense in this context to have a thread retry the popTop () operation\non a different, randomly chosen DEQueue each time. T o support such a retry, a\npopTop () call may return null if it con\ufb02icts with a concurrent popTop () call.\nWe now describe two implementations of the work-stealing DEQueue . The \ufb01rst\nis simpler, because it has bounded capacity. The second is somewhat more com-\nplex, but virtually unbounded in its capacity; that is, it does not suffer from the\npossibility of over\ufb02ow.\n16.5.1 A Bounded Work-Stealing Dequeue\nFor the executer pool DEQueue , the common case is for a thread to push and pop\na task from its own queue, calling pushBottom () and popBottom (). The uncom-\nmon case is to steal a task from another thread\u2019s DEQueue by calling popTop ().\nNaturally, it makes sense to optimize the common case. The idea behind the\nBoundedDEQueue inFigs. 16.9 and 16.10 is thus to have the pushBottom ()\nand popBottom () methods use only reads\u2013writes in the common case. The\nBoundedDEQueue consists of an array of tasks indexed by bottom andtop\ufb01elds\nthat reference the bottom and top of the dequeue, and depicted in Fig. 16.11 . The\npushBottom () and popBottom () methods use reads\u2013writes to manipulate the\nbottom reference. However, once the topandbottom \ufb01elds are close (there might\nbe only a single item in the array), popBottom () switches to compareAndSet()\ncalls to coordinate with potential popTop () calls.\nLet us describe the algorithm in more detail. The BoundedDEQueue algo-\nrithm is ingenious in the way it avoids the use of costly compareAndSet() calls.\nThis elegance comes at a cost: it is delicate and the order among instructions\nis crucial. We suggest the reader take time to understand how method interac-\ntions among methods are determined by the order in which reads-writes and\ncompareAndSet() calls occur.\n16.5 Work-Stealing Dequeues 383\n1public class BDEQueue {\n2 Runnable[] tasks;\n3 volatile int bottom;\n4 AtomicStampedReference<Integer> top;\n5 public BDEQueue( int capacity) {\n6 tasks = new Runnable[capacity];\n7 top = new AtomicStampedReference<Integer>(0, 0);\n8 bottom = 0;\n9 }\n10 public void pushBottom(Runnable r){\n11 tasks[bottom] = r;\n12 bottom++;\n13 }\n14 // called by thieves to determine whether to try to steal\n15 boolean isEmpty() {\n16 int localTop = top.getReference();\n17 int localBottom = bottom;\n18 return localBottom <= localTop;\n19 }\n20 }\n21 }\nFigure 16.9 The BoundedDEQueue class: \ufb01elds, constructor, pushBottom ()andisEmpty ()\nmethods.\nTheBoundedDEQueue class has three \ufb01elds: tasks ,bottom , and top(Fig. 16.9,\nLines 2\u20134). The tasks \ufb01eld is an array of Runnable tasks that holds the tasks\nin the queue, bottom is the index of the \ufb01rst empty slot in tasks , and top\nis an AtomicStampedReference<Integer> .3The top \ufb01eld encompasses two\nlogical \ufb01elds; the reference is the index of the \ufb01rst task in the queue, and\nthestamp is a counter incremented each time the reference is changed. The\nstamp is needed to avoid an", "doc_id": "02ce576e-1e4a-48a8-8a23-107629626f49", "embedding": null, "doc_hash": "22d834a6d6c48ad1ef69bd1b5eafd749fa93a3335c288fb7339baf89eb977271", "extra_info": null, "node_info": {"start": 978559, "end": 981776}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "fbea1bda-3be4-4926-9c16-b5a07d36a28c", "3": "e847a9fc-90cd-4e85-8116-94d9e58f5ed5"}}, "__type__": "1"}, "e847a9fc-90cd-4e85-8116-94d9e58f5ed5": {"__data__": {"text": "tasks ,bottom , and top(Fig. 16.9,\nLines 2\u20134). The tasks \ufb01eld is an array of Runnable tasks that holds the tasks\nin the queue, bottom is the index of the \ufb01rst empty slot in tasks , and top\nis an AtomicStampedReference<Integer> .3The top \ufb01eld encompasses two\nlogical \ufb01elds; the reference is the index of the \ufb01rst task in the queue, and\nthestamp is a counter incremented each time the reference is changed. The\nstamp is needed to avoid an \u201cABA \u201d problem of the type that often arises when\nusing compareAndSet() . Suppose thread Atries to steal a task from index 3.\nAreads a reference to the task at that position, and tries to steal it by calling\ncompareAndSet() to set the index to 4. It is delayed before making the call, and\nin the meantime, thread Bremoves all the tasks and inserts three new tasks.\nWhenAawakens, its compareAndSet() call will succeed in changing the index\nfrom 3 to 4, but it will have stolen a task that has already been completed,\nand removed a task that will never be completed. The stamp ensures that A\u2019s\ncompareAndSet() call will fail because the stamps no longer match.\nThe popTop () method (Fig. 16.10) checks whether the BoundedDEQueue is\nempty, and if not, tries to steal the top element by calling compareAndSet() to\nincrement top. If the compareAndSet() succeeds, the theft is successful, and\notherwise the method simply returns null. This method is nondeterministic:\nreturning null does not necessarily mean that the queue is empty.\n3See Chapter 10, Pragma 10.6.1.\n384 Chapter 16 Futures, Scheduling, and Work Distribution\n1 public Runnable popTop() {\n2 int[] stamp = new int [1];\n3 int oldTop = top.get(stamp), newTop = oldTop + 1;\n4 int oldStamp = stamp[0], newStamp = oldStamp + 1;\n5 if(bottom <= oldTop)\n6 return null ;\n7 Runnable r = tasks[oldTop];\n8 if(top.compareAndSet(oldTop, newTop, oldStamp, newStamp))\n9 return r;\n10 return null ;\n11 }\n12 public Runnable popBottom() {\n13 if(bottom == 0)\n14 return null ;\n15 bottom--;\n16 Runnable r = tasks[bottom];\n17 int[] stamp = new int [1];\n18 int oldTop = top.get(stamp), newTop = 0;\n19 int oldStamp = stamp[0], newStamp = oldStamp + 1;\n20 if(bottom > oldTop)\n21 return r;\n22 if(bottom == oldTop) {\n23 bottom = 0;\n24 if(top.compareAndSet(oldTop, newTop, oldStamp, newStamp))\n25 return r;\n26 }\n27 top.set(newTop,newStamp);\n28 bottom = 0;\n29 return null ;\n30 }\nFigure 16.10 TheBoundedDEQueue class: popTop ()andpopBottom ()methods.\nAs we noted earlier, we optimize for the common case where each thread\npushes and pops from its own local BoundedDEQueue . Most of the time, a thread\ncan push and pop tasks on and off its own BoundedDEQueue object, simply by\nloading and storing the bottom index. If there is only one task in the queue, then\nthe caller might encounter interference from a thief trying to steal that task. So if\nbottom is close to top, the calling thread switches to using compareAndSet() to\npop tasks.\nThepushBottom () method ( Fig. 16.9 , Line 10) simply stores the new task at\nthebottom queue location and increments bottom .\nThepopBottom () method ( Fig.", "doc_id": "e847a9fc-90cd-4e85-8116-94d9e58f5ed5", "embedding": null, "doc_hash": "4c3c88c7626b538e626f9107cdc33f0ec355f8100927ea3e4adc881b408c766d", "extra_info": null, "node_info": {"start": 981871, "end": 984924}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "02ce576e-1e4a-48a8-8a23-107629626f49", "3": "079a11e7-a00a-4cad-b315-fb8378bdfdbd"}}, "__type__": "1"}, "079a11e7-a00a-4cad-b315-fb8378bdfdbd": {"__data__": {"text": "pops from its own local BoundedDEQueue . Most of the time, a thread\ncan push and pop tasks on and off its own BoundedDEQueue object, simply by\nloading and storing the bottom index. If there is only one task in the queue, then\nthe caller might encounter interference from a thief trying to steal that task. So if\nbottom is close to top, the calling thread switches to using compareAndSet() to\npop tasks.\nThepushBottom () method ( Fig. 16.9 , Line 10) simply stores the new task at\nthebottom queue location and increments bottom .\nThepopBottom () method ( Fig. 16.10 ) is more complex. If the queue is empty,\nthe method returns immediately (Line 13), and otherwise, it decrements bottom ,\nclaiming a task (Line 15). Here is a subtle but important point. If the claimed\ntask was the last in the queue, then it is important that thieves notice that the\nBoundedDEQueue is empty (Line 5). But, because popBottom ()\u2019s decrement is\nneither atomic nor synchronized, the Java memory model does not guarantee\nthat the decrement will be observed right away by concurrent thieves. T o ensure\n16.5 Work-Stealing Dequeues 385\ntop\nbottom0\n1\n2\n345\n6stamp(b) (a)\ntop\nbottom0\n1\n2\n3\n4\n5\n6stamp\nFigure 16.11 The BoundedDEQueue implementation. In Part (a) popTop ()andpopBottom ()\nare called concurrently while there is more than one task in the BoundedDEQueue . The\npopTop ()method reads the element in entry 2 and calls compareAndSet() to redirect the\ntop reference to entry 3. The popBottom ()method redirects the bottom reference from 5\nto 4 using a simple store and then, after checking that bottom is greater than topit removes\nthe task in entry 4. In Part (b) there is only a single task. When popBottom ()detects that\nafter redirecting from 4 to 3 topandbottom are equal, it attempts to redirect topwith\nacompareAndSet() . Before doing so it redirects bottom to 0 because this last task will\nbe removed by one of the two popping methods. If popTop ()detects that topandbottom\nare equal it gives up, and otherwise it tries to advance topusing compareAndSet() . If both\nmethods apply compareAndSet() to the top, one wins and removes the task. In any case,\nwin or lose, popBottom ()resets topto 0 since the BoundedDEQueue is empty.\nthat thieves can recognize an empty BoundedDEQueue , the bottom \ufb01eld must be\ndeclared volatile .4\nAfter the decrement, the caller reads the task at the new bottom index\n(Line 16), and tests whether the current top\ufb01eld refers to a higher index. If so, the\ncaller cannot con\ufb02ict with a thief, and the method returns (Line 20). Otherwise,\nif the top andbottom \ufb01elds are equal, then there is only one task left in the\nBoundedDEQueue , but there is a danger that the caller con\ufb02icts with a thief. The\ncaller resets bottom to 0 (Line 23). (Either the caller will succeed in claiming\nthe task, or a thief will steal it \ufb01rst.) The caller resolves the potential con\ufb02ict by\ncalling compareAndSet() to reset top to 0, matching bottom (Line 22). If this\ncompareAndSet() succeeds, the top has been reset to 0, and the task has been\nclaimed by the caller, so the method returns. Otherwise the queue must be empty\nbecause a thief succeeded, but this means that top points to some entry greater\nthan bottom which was set to 0 earlier. So before the caller returns null, it resets\ntop to 0 (Line 27).\nAs noted, an attractive aspect of this design is that an expensive\ncompareAndSet()", "doc_id": "079a11e7-a00a-4cad-b315-fb8378bdfdbd", "embedding": null, "doc_hash": "55b9657bb893f21a5db8a01c4f2e6405e03d61edee53ed3b4adb59a2c209839d", "extra_info": null, "node_info": {"start": 984822, "end": 988203}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e847a9fc-90cd-4e85-8116-94d9e58f5ed5", "3": "723cf673-bbb1-4500-89f8-9f5cc94b0d84"}}, "__type__": "1"}, "723cf673-bbb1-4500-89f8-9f5cc94b0d84": {"__data__": {"text": "potential con\ufb02ict by\ncalling compareAndSet() to reset top to 0, matching bottom (Line 22). If this\ncompareAndSet() succeeds, the top has been reset to 0, and the task has been\nclaimed by the caller, so the method returns. Otherwise the queue must be empty\nbecause a thief succeeded, but this means that top points to some entry greater\nthan bottom which was set to 0 earlier. So before the caller returns null, it resets\ntop to 0 (Line 27).\nAs noted, an attractive aspect of this design is that an expensive\ncompareAndSet() call is needed only rarely when the BoundedDEQueue is almost\nempty.\n4In a C or C++ implementation you would need to introduce a write barrier as described in\nAppendix B .\n386 Chapter 16 Futures, Scheduling, and Work Distribution\nWe linearize each unsuccessful popTop () call at the point where it detects\nthat the BoundedDEQueue is empty, or at a failed compareAndSet() . Successful\npopTop () calls are linearized at the point when a successful compareAndSet()\ntook place. We linearize pushBottom () calls when bottom is incremented, and\npopBottom () calls when bottom is decremented or set to 0, though the outcome\nofpopBottom () in the latter case is determined by the success or failure of the\ncompareAndSet() that follows.\nThe isEmpty () method ( Fig. 16.10 ) \ufb01rst reads top, then bottom , checking\nwhether bottom is less than or equal to top (Line 4). The order is important for\nlinearizability, because top never decreases unless bottom is \ufb01rst reset to 0, and\nso if a thread reads bottom after top and sees it is no greater, the queue is indeed\nempty because a concurrent modi\ufb01cation of top could only have increased top.\nOn the other hand, if bottom is greater than top, then even if top is increased\nafter it was read and before bottom is read (and the queue becomes empty), it\nis still true that the BoundedDEQueue must not have been empty when top was\nread. The only alternative is that bottom is reset to 0 and then top is reset to 0,\nso reading top and then bottom will correctly return empty. It follows that the\nisEmpty () method is linearizable.\n16.5.2 An Unbounded Work-Stealing DEQueue\nA limitation of the BoundedDEQueue class is that the queue has a \ufb01xed size.\nFor some applications, it may be dif\ufb01cult to predict this size, especially if some\nthreads create signi\ufb01cantly more tasks than others. Assigning each thread its own\nBoundedDEQueue of maximal capacity wastes space.\nT o address these limitations, we now consider an unbounded double-ended\nqueue UnboundedDEQueue class that dynamically resizes itself as needed.\nWe implement the UnboundedDEQueue in a cyclic array, with topandbottom\n\ufb01elds as in the BoundedDEQueue (except indexed modulo the array\u2019s capacity). As\nbefore, if bottom is less than or equal to top, the UnboundedDEQueue is empty.\nUsing a cyclic array eliminates the need to reset bottom andtopto 0. Moreover, it\npermits top to be incremented but never decremented, eliminating the need for\ntop to be an AtomicStampedReference . Moreover, in the UnboundedDEQueue\nalgorithm, if pushBottom () discovers that the current circular array is full, it can\nresize (enlarge) it, copying the tasks into a bigger array, and pushing the new task\ninto the new (larger) array. Because the array is indexed modulo its capacity, there\nis no need to update the top orbottom \ufb01elds when moving the elements into a\nbigger array (although the actual array indexes where the elements are stored\nmight change).\nTheCircularArray() class is depicted in Fig. 16.12 . It provides get()", "doc_id": "723cf673-bbb1-4500-89f8-9f5cc94b0d84", "embedding": null, "doc_hash": "4224f6dd41d8102b3553ac54e4ff8db85b715914e6790cd8b6d3eb4f2b908fae", "extra_info": null, "node_info": {"start": 988231, "end": 991751}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "079a11e7-a00a-4cad-b315-fb8378bdfdbd", "3": "7e194999-39ca-4e04-ad0e-49e420d1f639"}}, "__type__": "1"}, "7e194999-39ca-4e04-ad0e-49e420d1f639": {"__data__": {"text": ". Moreover, in the UnboundedDEQueue\nalgorithm, if pushBottom () discovers that the current circular array is full, it can\nresize (enlarge) it, copying the tasks into a bigger array, and pushing the new task\ninto the new (larger) array. Because the array is indexed modulo its capacity, there\nis no need to update the top orbottom \ufb01elds when moving the elements into a\nbigger array (although the actual array indexes where the elements are stored\nmight change).\nTheCircularArray() class is depicted in Fig. 16.12 . It provides get() and\nput() methods that add and remove tasks, and a resize () method that allocates\na new circular array and copies the old array\u2019s contents into the new array. The\nuse of modular arithmetic ensures that even though the array has changed size\n16.5 Work-Stealing Dequeues 387\n1class CircularArray {\n2 private int logCapacity;\n3 private Runnable[] currentTasks;\n4 CircularArray( int myLogCapacity) {\n5 logCapacity = myLogCapacity;\n6 currentTasks = new Runnable[1 << logCapacity];\n7 }\n8 int capacity() {\n9 return 1 << logCapacity;\n10 }\n11 Runnable get( int i) {\n12 return currentTasks[i % capacity()];\n13 }\n14 void put( int i, Runnable task) {\n15 currentTasks[i % capacity()] = task;\n16 }\n17 CircularArray resize( int bottom, int top) {\n18 CircularArray newTasks =\n19 new CircularArray(logCapacity+1);\n20 for (int i = top; i < bottom; i++) {\n21 newTasks.put(i, get(i));\n22 }\n23 return newTasks;\n24 }\n25 }\nFigure 16.12 TheUnboundedDEQueue class: the circular task array.\nand the tasks may have shifted positions, thieves can still use the top\ufb01eld to \ufb01nd\nthe next task to steal.\nThe UnboundedDEQueue class has three \ufb01elds: tasks ,bottom , and top\n(Fig. 16.13 , Lines 3\u20135). The popBottom () (Fig. 16.13 ) and popTop () methods\n(Fig. 16.14 ) are almost the same as those of the BoundedDEQueue , with one key\ndifference: the use of modular arithmetic to compute indexes means the top\nindex need never be decremented. As noted, there is no need for a timestamp to\nprevent ABA problems. Both methods, when competing for the last task, steal it\nby incrementing top. T o reset the UnboundedDEQueue to empty, simply incre-\nment the bottom \ufb01eld to equal top. In the code, popBottom (), immediately after\nthecompareAndSet() in Line 25, sets bottom to equal top+1 whether or not the\ncompareAndSet() succeeds, because, even if it failed, a concurrent thief must\nhave stolen the last task. Storing top+1 into bottom makes top and bottom\nequal, resetting the UnboundedDEQueue object to an empty state.\nThe isEmpty () method ( Fig. 16.13 ) \ufb01rst reads top, then bottom , checking\nwhether bottom is less than or equal to top (Line 4). The order is important\nbecause top never decreases, and so if a thread reads bottom after top and sees\nit is no greater, the queue is indeed empty because a concurrent modi\ufb01cation of\ntop could only have increased the top value. The same principle applies in the\npopTop () method call. An example execution is provided in Fig. 16.15 .\n388 Chapter 16 Futures, Scheduling, and Work Distribution\n1public class UnboundedDEQueue {\n2 private final static int LOG_CAPACITY = 4;\n3 private volatile CircularArray tasks;\n4 volatile int bottom;\n5 AtomicInteger top;\n6 public", "doc_id": "7e194999-39ca-4e04-ad0e-49e420d1f639", "embedding": null, "doc_hash": "64dced136b6dbf00e9276c96380f46bdccd9e7211d28377e875d9b52d85a2056", "extra_info": null, "node_info": {"start": 991741, "end": 994947}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "723cf673-bbb1-4500-89f8-9f5cc94b0d84", "3": "98733934-b954-4b81-8f36-0fb8a13309bf"}}, "__type__": "1"}, "98733934-b954-4b81-8f36-0fb8a13309bf": {"__data__": {"text": "never decreases, and so if a thread reads bottom after top and sees\nit is no greater, the queue is indeed empty because a concurrent modi\ufb01cation of\ntop could only have increased the top value. The same principle applies in the\npopTop () method call. An example execution is provided in Fig. 16.15 .\n388 Chapter 16 Futures, Scheduling, and Work Distribution\n1public class UnboundedDEQueue {\n2 private final static int LOG_CAPACITY = 4;\n3 private volatile CircularArray tasks;\n4 volatile int bottom;\n5 AtomicInteger top;\n6 public UnboundedDEQueue( int LOG_CAPACITY) {\n7 tasks = new CircularArray(LOG_CAPACITY);\n8 top = new AtomicInteger(0);\n9 bottom = 0;\n10 }\n11 boolean isEmpty() {\n12 int localTop = top.get();\n13 int localBottom = bottom;\n14 return (localBottom <= localTop);\n15 }\n16\n17 public void pushBottom(Runnable r) {\n18 int oldBottom = bottom;\n19 int oldTop = top.get();\n20 int size = oldBottom - oldTop;\n21 if(size >= currentTasks.capacity()-1) {\n22 currentTasks = currentTasks.resize(oldBottom, oldTop);\n23 tasks = currentTasks;\n24 }\n25 tasks.put(oldBottom, r);\n26 bottom = oldBottom + 1;\n27 }\nFigure 16.13 The UnboundedDEQueue class: \ufb01elds, constructor, pushBottom (), and\nisEmpty ()methods.\nThe pushBottom () method ( Fig. 16.13 ) is almost the same as that of the\nBoundedDEQueue . One difference is that the method must enlarge the circular\narray if the current push is about to cause it to exceed its capacity. Another is that\npopTop () does not need to manipulate a timestamp. The ability to resize carries\na price: every call must read top (Line 20) to determine if a resize is necessary,\npossibly causing more cache misses because top is modi\ufb01ed by all processes. We\ncan reduce this overhead by having threads save a local value of top and using\nit to compute the size of the UnboundedDEQueue object. A thread reads the top\n\ufb01eld only when this bound is exceeded, indicating that a resize () may be neces-\nsary. Even though the local copy may become outdated because of changes to the\nshared top,top is never decremented, so the real size of the UnboundedDEQueue\nobject can only be smaller than the one calculated using the local variable.\nIn summary, we have seen two ways to design a nonblocking linearizable\nDEQueue class. We can get away with using only loads and stores in the most\ncommon manipulations of the DEQueue , but at the price of having more com-\nplex algorithms. Such algorithms are justi\ufb01able for an application such as an\nexecuter pool whose performance may be critical to a concurrent multithreaded\nsystem.\n16.5 Work-Stealing Dequeues 389\n1 public Runnable popTop() {\n2 int oldTop = top.get();\n3 int newTop = oldTop + 1;\n4 int oldBottom = bottom;\n5 int size = oldBottom - oldTop;\n6 if(size <= 0) return null ;\n7 Runnable r = tasks.get(oldTop);\n8 if(top.compareAndSet(oldTop, newTop))\n9 return r;\n10 return null ;\n11 }\n12\n13 public Runnable popBottom() {\n14 bottom--;\n15 int oldTop = top.get();\n16 int newTop = oldTop + 1;\n17 int size = bottom - oldTop;\n18 if(size < 0) {\n19 bottom = oldTop;\n20 return null ;\n21 }\n22 Runnable r = tasks.get(bottom);\n23 if(size > 0)\n24 return r;\n25 if(!top.compareAndSet(oldTop,", "doc_id": "98733934-b954-4b81-8f36-0fb8a13309bf", "embedding": null, "doc_hash": "b441f951d7a530602b88f59893bccbde04755b2fde7094549a91efad4a1bdabb", "extra_info": null, "node_info": {"start": 994949, "end": 998092}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7e194999-39ca-4e04-ad0e-49e420d1f639", "3": "7a5fbb5f-1eaa-4a68-848a-ed32a0d6f786"}}, "__type__": "1"}, "7a5fbb5f-1eaa-4a68-848a-ed32a0d6f786": {"__data__": {"text": "if(top.compareAndSet(oldTop, newTop))\n9 return r;\n10 return null ;\n11 }\n12\n13 public Runnable popBottom() {\n14 bottom--;\n15 int oldTop = top.get();\n16 int newTop = oldTop + 1;\n17 int size = bottom - oldTop;\n18 if(size < 0) {\n19 bottom = oldTop;\n20 return null ;\n21 }\n22 Runnable r = tasks.get(bottom);\n23 if(size > 0)\n24 return r;\n25 if(!top.compareAndSet(oldTop, newTop))\n26 r = null ;\n27 bottom = oldTop + 1;\n28 return r;\n29 }\nFigure 16.14 TheUnboundedDEQueue class: popTop ()andpopBottom ()methods.\n16.5.3 Work Balancing\nWe have seen that in work-stealing algorithms, idle threads steal tasks from oth-\ners. An alternative approach is to have each thread periodically balance its work-\nloads with a randomly chosen partner. T o ensure that heavily loaded threads do\nnot waste effort trying to rebalance, we make lightly-loaded threads more likely\nto initiate rebalancing. More precisely, each thread periodically \ufb02ips a biased coin\nto decide whether to balance with another. The thread\u2019s probability of balancing\nis inversely proportional to the number of tasks in the thread\u2019s queue. In other\nwords, threads with few tasks are likely to rebalance, and threads with nothing to\ndo are certain to rebalance. A thread rebalances by selecting a victim uniformly\nat random, and, if the difference between its workload and the victim\u2019s exceeds\na prede\ufb01ned threshold, they transfer tasks until their queues contain the same\nnumber of tasks. It can be shown that this algorithm provides strong fairness\nguarantees: the expected length of each thread\u2019s task queue is pretty close to the\naverage. One advantage of this approach is that the balancing operation moves\nmultiple tasks at each exchange. A second advantage occurs if one thread has\n390 Chapter 16 Futures, Scheduling, and Work Distribution\ntop bottom(b)\n10\n2\n37\n6\n45\ntop bottom(a)\n10\n2\n37\n6\n45\nFigure 16.15 TheUnboundedDEQueue class implementation. In Part (a) popTop ()andpopBottom ()are exe-\ncuted concurrently while there is more than one task in the UnboundedDEQueue object. In Part (b) there is\nonly a single task, and initially bottom refers to Entry 3 and topto 2. The popBottom ()method \ufb01rst decre-\nments bottom from 3 to 2 (we denote this change by a dashed line pointing to Entry 2 since it will change\nagain soon). Then, when popBottom ()detects that the gap between the newly-set bottom andtopis 0, it\nattempts to increment topby 1 (rather than reset it to 0 as in the BoundedDEQueue ). The popTop ()method\nattempts to do the same. The top\ufb01eld is incremented by one of them, and the winner takes the last task.\nFinally, the popBottom ()method sets bottom back to Entry 3, which is equal to top.\nmuch more work than the others, especially if tasks require approximately equal\ncomputation. In the work-stealing algorithm presented here, contention could\noccur if many threads try to steal individual tasks from the overloaded\nthread.\nIn such a case, in the work-stealing executer pool, if some thread has a lot of\nwork, chances are that that other threads will have to repeatedly compete on the\nsame local task queue in an attempt to steal at most a single task each time. On\nthe other hand, in the work-sharing executer pool, balancing multiple tasks at a\ntime means that work will quickly be spread out among tasks, and there will not\nbe a synchronization", "doc_id": "7a5fbb5f-1eaa-4a68-848a-ed32a0d6f786", "embedding": null, "doc_hash": "e3658366e07dff9616bd940e443a3a9cf782068cd0f23d1c4db3481e3c8e5487", "extra_info": null, "node_info": {"start": 998229, "end": 1001550}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "98733934-b954-4b81-8f36-0fb8a13309bf", "3": "e3677d79-1637-4d2f-a1f9-5928d23e128b"}}, "__type__": "1"}, "e3677d79-1637-4d2f-a1f9-5928d23e128b": {"__data__": {"text": "In the work-stealing algorithm presented here, contention could\noccur if many threads try to steal individual tasks from the overloaded\nthread.\nIn such a case, in the work-stealing executer pool, if some thread has a lot of\nwork, chances are that that other threads will have to repeatedly compete on the\nsame local task queue in an attempt to steal at most a single task each time. On\nthe other hand, in the work-sharing executer pool, balancing multiple tasks at a\ntime means that work will quickly be spread out among tasks, and there will not\nbe a synchronization overhead per individual task.\nFig. 16.16 illustrates a work-sharing executor. Each thread has its own queue\nof tasks, kept in an array shared by all threads (Line 2). Each thread repeatedly\ndequeues the next task from its queue (Line 12). If the queue was empty, the\ndeq() call returns null, and otherwise, the thread runs the task (Line 13). At this\npoint, the thread decides whether to rebalance. If the thread\u2019s task queue has size\ns, then the thread decides to rebalance with probability 1 =(s+ 1) (Line 15). T o\nrebalance, the thread chooses a victim thread uniformly at random. The thread\nlocks both queues (Lines 17\u201320), in thread ID order (to avoid deadlock). If the\ndifference in queue sizes exceeds a threshold, it evens out the queue sizes.\n(Fig. 16.16 , Lines 27\u201335).\n16.6 Chapter Notes 391\n1public class WorkSharingThread {\n2 Queue[] queue;\n3 Random random;\n4 private static final int THRESHOLD = ...;\n5 public WorkSharingThread(Queue[] myQueue) {\n6 queue = myQueue;\n7 random = new Random();\n8 }\n9 public void run() {\n10 int me = ThreadID.get();\n11 while (true ) {\n12 Runnable task = queue[me].deq();\n13 if(task != null ) task.run();\n14 int size = queue[me].size();\n15 if(random.nextInt(size+1) == size) {\n16 int victim = random.nextInt(queue.length);\n17 int min = (victim <= me) ? victim : me;\n18 int max = (victim <= me) ? me : victim;\n19 synchronized (queue[min]) {\n20 synchronized (queue[max]) {\n21 balance(queue[min], queue[max]);\n22 }\n23 }\n24 }\n25 }\n26 }\n27 private void balance(Queue q0, Queue q1) {\n28 Queue qMin = (q0.size() < q1.size()) ? q0 : q1;\n29 Queue qMax = (q0.size() < q1.size()) ? q1 : q0;\n30 int diff = qMax.size() - qMin.size();\n31 if(diff > THRESHOLD)\n32 while (qMax.size() > qMin.size())\n33 qMin.enq(qMax.deq());\n34 }\n35 }\nFigure 16.16 TheWorkSharingThread class: a simpli\ufb01ed work sharing executer pool.\n16.6 Chapter Notes\nThe DAG-based model for analysis of multithreaded computation was intro-\nduced by Robert Blumofe and Charles Leiserson [20]. They also gave the \ufb01rst\ndeque-based implementation of work stealing. Some of the examples in this\nchapter were adapted from a tutorial by Charles Leiserson and Harald Prokop\n[102]. The bounded lock-free dequeue algorithm is credited to Anish Arora,\nRobert Blumofe, and Greg Plaxton [15]. The unbounded timestamps used in\nthis algorithm can be made bounded using a technique due to Mark Moir [117].\nThe unbounded dequeue algorithm is credited to David Chase and Y ossi Lev\n[28]. Theorem 16.3.1 and its", "doc_id": "e3677d79-1637-4d2f-a1f9-5928d23e128b", "embedding": null, "doc_hash": "c203f2716797c13dfee22047b6ccea0e6cc5141fd4fed7747c49c8218b571618", "extra_info": null, "node_info": {"start": 1001388, "end": 1004440}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7a5fbb5f-1eaa-4a68-848a-ed32a0d6f786", "3": "02b66e0a-a81a-4c91-b359-606a35f8d02b"}}, "__type__": "1"}, "02b66e0a-a81a-4c91-b359-606a35f8d02b": {"__data__": {"text": "the \ufb01rst\ndeque-based implementation of work stealing. Some of the examples in this\nchapter were adapted from a tutorial by Charles Leiserson and Harald Prokop\n[102]. The bounded lock-free dequeue algorithm is credited to Anish Arora,\nRobert Blumofe, and Greg Plaxton [15]. The unbounded timestamps used in\nthis algorithm can be made bounded using a technique due to Mark Moir [117].\nThe unbounded dequeue algorithm is credited to David Chase and Y ossi Lev\n[28]. Theorem 16.3.1 and its proof are by Anish Arora, Robert Blumofe, and\n392 Chapter 16 Futures, Scheduling, and Work Distribution\nGreg Plaxton [15]. The work-sharing algorithm is by Larry Rudolph, Tali Slivkin-\nAllaluf, and Eli Upfal [134]. The algorithm of Anish Arora, Robert Blumofe, and\nGreg Plaxton [15] was later improved by Danny Hendler and Nir Shavit [56] to\ninclude the ability to steal half of the items in a dequeue.\n16.7 Exercises\nExercise 185. Consider the following code for an in-place merge-sort:\nvoid mergeSort( int[] A, int lo, int hi) {\nif(hi > lo) {\nint mid = (hi - lo)/2;\nexecutor.submit( new mergeSort(A, lo, mid));\nexecutor.submit( new mergeSort(A, mid+1, hi));\nawaitTermination();\nmerge(A, lo, mid, hi);\n}\nAssuming that the merge method has no internal parallelism, give the work, crit-\nical path length, and parallelism of this algorithm. Give your answers both as\nrecurrences and as \u00ca(f(n)), for some function f.\nExercise 186. Y ou may assume that the actual running time of a parallel program\non a dedicated P-processor machine is\nTP=T1=P+T1:\nY our research group has produced two chess programs, a simple one and an\noptimized one. The simple one has T1= 2048 seconds and T1= 1 second. When\nyou run it on your 32-processor machine, sure enough, the running time is 65\nsteps. Y our students then produce an \u201coptimized\u201d version with T0\n1= 1024 seconds\nandT0\n1= 8 seconds. Why is it optimized? When you run it on your 32-processor\nmachine, the running time is 40 seconds, as predicted by our formula.\nWhich program will scale better to a 512-processor machine?\nExercise 187. Write a class, ArraySum that provides a method\nstatic public int sum( int[] a)\nthat uses divide-and-conquer to sum the elements of the array argument in\nparallel.\nExercise 188. Professor Jones takes some measurements of his (deterministic)\nmultithreaded program, which is scheduled using a greedy scheduler, and \ufb01nds\nthatT4= 80 seconds and T64= 10 seconds. What is the fastest that the professor\u2019s\ncomputation could possibly run on 10 processors? Use the following inequalities\nand the bounds implied by them to derive your answer. Note that P is the number\n16.7 Exercises 393\nof processors.\nTP>T1\nP\nTP>T1\nTP6(T1\u0000T1)\nP+T1\n(The last inequality holds on a greedy scheduler.)\nExercise 189. Give an implementation of the Matrix class used in this chapter.\nMake sure your split () method takes constant time.\nExercise 190. LetP(x) =Pd\ni=0pixiandQ(x) =Pd\ni=0qixibe polynomials of\ndegreed, wheredis a power of 2. We can write\nP(x) =P0(x) + (P1(x)\u0001xd=2)\nQ(x)", "doc_id": "02b66e0a-a81a-4c91-b359-606a35f8d02b", "embedding": null, "doc_hash": "919f61d29af5306921540705b3de8bdf318be0d345bedd71faf8e9331772ba2a", "extra_info": null, "node_info": {"start": 1004504, "end": 1007514}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e3677d79-1637-4d2f-a1f9-5928d23e128b", "3": "07bad7aa-81d9-4679-adca-01fda8b47c2b"}}, "__type__": "1"}, "07bad7aa-81d9-4679-adca-01fda8b47c2b": {"__data__": {"text": "last inequality holds on a greedy scheduler.)\nExercise 189. Give an implementation of the Matrix class used in this chapter.\nMake sure your split () method takes constant time.\nExercise 190. LetP(x) =Pd\ni=0pixiandQ(x) =Pd\ni=0qixibe polynomials of\ndegreed, wheredis a power of 2. We can write\nP(x) =P0(x) + (P1(x)\u0001xd=2)\nQ(x) =Q0(x) + (Q1(x)\u0001xd=2)\nwhereP0(x),P1(x),Q0(x), andQ1(x) are polynomials of degree d=2.\nThePolynomial class shown in Fig. 16.17 provides put() and get() methods\nto access coef\ufb01cients and it provides a constant-time split () method that splits\nad-degree polynomial P(x) into the two ( d=2)-degree polynomials P0(x) and\nP1(x) de\ufb01ned above, where changes to the split polynomials are re\ufb02ected in the\noriginal, and vice versa.\nY our task is to devise parallel addition and multiplication algorithms for this\npolynomial class.\n1.The sum ofP(x) andQ(x) can be decomposed as follows:\nP(x) +Q(x) = (P0(x) +Q0(x)) + (P1(x) +Q1(x))\u0001xd=2:\na)Use this decomposition to construct a task-based concurrent polynomial\naddition algorithm in the manner of Fig. 16.13.\nb)Compute the work and critical path length of this algorithm.\n2.The product ofP(x) andQ(x) can be decomposed as follows:\nP(x)\u0001Q(x) = (P0(x)\u0001Q0(x)) + (P0(x)\u0001Q1(x) +P1(x)\u0001Q0(x))\u0001xd=2+ (P1(x)\u0001Q1(x))\na)Use this decomposition to construct a task-based concurrent polynomial\nmultiplication algorithm in the manner of Fig. 16.4\nb)Compute the work and critical path length of this algorithm.\n394 Chapter 16 Futures, Scheduling, and Work Distribution\n1public class Polynomial {\n2 int[] coefficients; // possibly shared by several polynomials\n3 int first; // index of my constant coefficient\n4 int degree; // number of coefficients that are mine\n5 public Polynomial( int d) {\n6 coefficients = new int [d];\n7 degree = d;\n8 first = 0;\n9 }\n10 private Polynomial( int[] myCoefficients, int myFirst, int myDegree) {\n11 coefficients = myCoefficients;\n12 first = myFirst;\n13 degree = myDegree;\n14 }\n15 public int get( int index) {\n16 return coefficients[first + index];\n17 }\n18 public void set( int index, int value) {\n19 coefficients[first + index] = value;\n20 }\n21 public int getDegree() {\n22 return degree;\n23 }\n24 public Polynomial[] split() {\n25 Polynomial[] result = new Polynomial[2];\n26 int newDegree = degree / 2;\n27 result[0] = new Polynomial(coefficients, first, newDegree);\n28 result[1] = new Polynomial(coefficients, first + newDegree, newDegree);\n29 return result;\n30 }\n31 }\nFigure 16.17 ThePolynomial class.\n1 Queue qMin = (q0.size() < q1.size()) ? q0 : q1;\n2 Queue qMax = (q0.size() < q1.size()) ? q1 : q0;\n3 synchronized (qMin) {\n4 synchronized", "doc_id": "07bad7aa-81d9-4679-adca-01fda8b47c2b", "embedding": null, "doc_hash": "34f4730bc8d5c4241c6fa2663d9398fd8e5a81382220d7d24fd011c47a677690", "extra_info": null, "node_info": {"start": 1007649, "end": 1010265}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "02b66e0a-a81a-4c91-b359-606a35f8d02b", "3": "384591f0-576f-4134-963e-41fc4e1708f8"}}, "__type__": "1"}, "384591f0-576f-4134-963e-41fc4e1708f8": {"__data__": {"text": "int newDegree = degree / 2;\n27 result[0] = new Polynomial(coefficients, first, newDegree);\n28 result[1] = new Polynomial(coefficients, first + newDegree, newDegree);\n29 return result;\n30 }\n31 }\nFigure 16.17 ThePolynomial class.\n1 Queue qMin = (q0.size() < q1.size()) ? q0 : q1;\n2 Queue qMax = (q0.size() < q1.size()) ? q1 : q0;\n3 synchronized (qMin) {\n4 synchronized (qMax) {\n5 int diff = qMax.size() - qMin.size();\n6 if(diff > THRESHOLD)\n7 while (qMax.size() > qMin.size())\n8 qMin.enq(qMax.deq());\n9 }\n10 }\nFigure 16.18 Alternate rebalancing code.\nExercise 191. Give an ef\ufb01cient and highly parallel multithreaded algorithm for\nmultiplying an n\u0002nmatrixAby a length-nvectorxthat achieves work \u00ca(n2)\nand critical path \u00ca(logn). Analyze the work and critical-path length of your\nimplementation, and give the parallelism.\n16.7 Exercises 395\nExercise 192. Fig. 16.18 shows an alternate way of rebalancing two work queues:\n\ufb01rst, lock the smaller queue, then lock the larger queue, and rebalance if their\ndifference exceeds a threshold. What is wrong with this code?\nExercise 193.\n1.In the popBottom () method of Fig. 16.10 , the bottom \ufb01eld is volatile to assure\nthat in popBottom () the decrement at Line 15is immediately visible. Describe\na scenario that explains what could go wrong if bottom were not declared as\nvolatile.\n2.Why should we attempt to reset the bottom \ufb01eld to zero as early as possible in\nthepopBottom () method? Which line is the earliest in which this reset can be\ndone safely? Can our BoundedDEQueue over\ufb02ow anyway? Describe how.\nExercise 194.\n\u0004InpopTop (), if the compareAndSet() in Line 8succeeds, it returns the\nelement it read right before the successful compareAndSet() operation.\nWhy is it important to read the element from the array before we do the\ncompareAndSet() ?\n\u0004Can we use isEmpty () in Line 6ofpopTop ()?\nExercise 195. What are the linearization points of the UnboundedDEQueue meth-\nods? Justify your answers.\nExercise 196. Modify the popTop () method of the linearizable BoundedDEQueue\nimplementation so it will return null only if there are no tasks in the queue.\nNotice that you may need to make its implementation blocking.\nExercise 197. Do you expect that the isEmpty () method call of a BoundedDEQueue\nin the executer pool code will actually improve its performance?\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n17Barriers\n17.1 Introduction\nImagine you are writing the graphical display for a computer game. Y our\nprogram prepares a sequence of frames to be displayed by a graphics package\n(perhaps a hardware coprocessor). This kind of program is sometimes called a\nsoft real-time application: real-time because it must display at least 35 frames per\nsecond to be effective, and soft because occasional failure is not catastrophic. On\na single-threaded machine, you might write a loop like this:\nwhile (true ) {\nframe.prepare();\nframe.display();\n}\nIf, instead, you have nparallel threads available, then it makes sense to split the\nframe intondisjoint parts, and to have each thread prepare its own part in par-\nallel with the others.\nint me = ThreadID.get();\nwhile (true )", "doc_id": "384591f0-576f-4134-963e-41fc4e1708f8", "embedding": null, "doc_hash": "8e85ed3822214921d0d8dfbdfb5f7d971243720e7e81ddbae7dd40154e1eb459", "extra_info": null, "node_info": {"start": 1010230, "end": 1013398}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "07bad7aa-81d9-4679-adca-01fda8b47c2b", "3": "8ae95869-9773-43c1-bff8-c77f8050c212"}}, "__type__": "1"}, "8ae95869-9773-43c1-bff8-c77f8050c212": {"__data__": {"text": "This kind of program is sometimes called a\nsoft real-time application: real-time because it must display at least 35 frames per\nsecond to be effective, and soft because occasional failure is not catastrophic. On\na single-threaded machine, you might write a loop like this:\nwhile (true ) {\nframe.prepare();\nframe.display();\n}\nIf, instead, you have nparallel threads available, then it makes sense to split the\nframe intondisjoint parts, and to have each thread prepare its own part in par-\nallel with the others.\nint me = ThreadID.get();\nwhile (true ) {\nframe[me].prepare();\nframe[me].display();\n}\nThe problem with this approach is that different threads will require different\namounts of time to prepare and display their portions of the frame. Some threads\nmight start displaying the ithframe before others have \ufb01nished the ( i\u00001)st.\nT o avoid such synchronization problems, we can organize computations such\nas this as a sequence of phases , where no thread should start the ithphase until\nthe others have \ufb01nished the ( i\u00001)st. We have already seen this phased computa-\ntion pattern before. In Chapter 12, the sorting network algorithms required each\ncomparison phase to be separate from the others. Similarly, in the sample sorting\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00017-4\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.397\n398 Chapter 17 Barriers\n1public interface Barrier {\n2 public void await();\n3}\nFigure 17.1 TheBarrier interface.\n1private Barrier b;\n2 ...\n3 while (true ) {\n4 frame[my].prepare();\n5 b.await();\n6 frame[my].display();\n7}\nFigure 17.2 Using a barrier to synchronize concurrent displays.\nalgorithm, each phase had to make sure that prior phases had completed before\nproceeding.\nThe mechanism for enforcing this kind of synchronization is called a barrier\n(Fig. 17.1 ). A barrier is a way of forcing asynchronous threads to act almost as\nif they were synchronous. When a thread \ufb01nishing phase icalls the barrier\u2019s\nawait () method, it is blocked until all nthreads have also \ufb01nished that phase.\nFig. 17.2 shows how one could use a barrier to make the parallel rendering pro-\ngram work correctly. After preparing frame i, all threads synchronize at a barrier\nbefore starting to display that frame. This structure ensures that all threads con-\ncurrently displaying a frame display the same frame.\nBarrier implementations raise many of the same performance issues as spin\nlocks in Chapter 7 , as well as some new issues. Clearly, barriers should be fast, in\nthe sense that we want to minimize the duration between when the last thread\nreaches the barrier and when the last thread leaves the barrier. It is also impor-\ntant that threads leave the barrier at roughly the same time. A thread\u2019s noti\ufb01ca-\ntion time is the interval between when some thread has detected that all threads\nhave reached the barrier, and when that speci\ufb01c thread leaves the barrier. Having\nuniform noti\ufb01cation times is important for many soft real-time applications. For\nexample, picture quality is enhanced if all portions of the frame are updated at\nmore-or-less the same time.\n17.2 Barrier Implementations\nFig. 17.3 shows the SimpleBarrier class, which creates an AtomicInteger\ncounter initialized to n, the barrier size. Each thread applies getAndDecrement ()\nto lower the counter. If the call returns 1 (Line 10), then that thread is the last to\nreach the barrier, so it resets the counter for the next use (Line 11). Otherwise,\n17.3 Sense-Reversing", "doc_id": "8ae95869-9773-43c1-bff8-c77f8050c212", "embedding": null, "doc_hash": "bc7ac5bb94303abd661542dc224037d77f5d49ed1dbd4fc756ac2198ad807f7f", "extra_info": null, "node_info": {"start": 1013242, "end": 1016730}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "384591f0-576f-4134-963e-41fc4e1708f8", "3": "9b144f3a-8a56-441e-867e-726919db8395"}}, "__type__": "1"}, "9b144f3a-8a56-441e-867e-726919db8395": {"__data__": {"text": "times is important for many soft real-time applications. For\nexample, picture quality is enhanced if all portions of the frame are updated at\nmore-or-less the same time.\n17.2 Barrier Implementations\nFig. 17.3 shows the SimpleBarrier class, which creates an AtomicInteger\ncounter initialized to n, the barrier size. Each thread applies getAndDecrement ()\nto lower the counter. If the call returns 1 (Line 10), then that thread is the last to\nreach the barrier, so it resets the counter for the next use (Line 11). Otherwise,\n17.3 Sense-Reversing Barrier 399\n1public class SimpleBarrier implements Barrier {\n2 AtomicInteger count;\n3 int size;\n4 public SimpleBarrier( int n){\n5 count = new AtomicInteger(n);\n6 size = n;\n7 }\n8 public void await() {\n9 int position = count.getAndDecrement();\n10 if(position == 1) {\n11 count.set(size);\n12 }else {\n13 while (count.get() != 0){};\n14 }\n15 }\n16 }\nFigure 17.3 TheSimpleBarrier class.\nthe thread spins on the counter, waiting for the value to fall to zero (Line 13).\nThis barrier class may look like it works, but it does not.\nUnfortunately, the attempt to make the barrier reusable causes it to break\n(seeFig. 17.2 ). Suppose there are only two threads. Thread Aapplies\ngetAndDecrement () to the counter, discovers it is not the last thread to reach the\nbarrier, and spins waiting for the counter value to reach zero. When Barrives,\nit discovers it is the last thread to arrive, so it resets the counter to nin this case\n2. It \ufb01nishes the next phase and calls await (). Meanwhile, Acontinues to spin,\nand the counter never reaches zero. Eventually, Ais waiting for phase 0 to \ufb01nish,\nwhileBis waiting for phase 1 to \ufb01nish, and the two threads starve.\nPerhaps the simplest way to \ufb01x this problem is just to alternate between two\nbarriers, using one for even-numbered phases, and another for odd-numbered\nones. However, such an approach wastes space, and requires too much book-\nkeeping from applications.\n17.3 Sense-Reversing Barrier\nAsense-reversing barrier is a more elegant and practical solution to the problem\nof reusing barriers. As depicted in Fig. 17.4 , a phase\u2019s sense is a Boolean value:\ntrue for even-numbered phases and false otherwise. Each SenseBarrier object\nhas a Boolean sense \ufb01eld indicating the sense of the currently executing phase.\nEach thread keeps its current sense as a thread-local object (see Pragma 17.3.1 ).\nInitially the barrier\u2019s sense is the complement of the local sense of all the\nthreads. When a thread calls await (), it checks whether it is the last thread\nto decrement the counter. If so, it reverses the barrier\u2019s sense and continues.\n400 Chapter 17 Barriers\n1public SenseBarrier( int n) {\n2 count = new AtomicInteger(n);\n3 size = n;\n4 sense = false ;\n5 threadSense = new ThreadLocal<Boolean>() {\n6 protected Boolean initialValue() { return !sense; };\n7 };\n8}\n9public void await() {\n10 boolean mySense = threadSense.get();\n11 int position = count.getAndDecrement();\n12 if(position == 1) {\n13 count.set(size);\n14 sense = mySense;\n15 }else {\n16 while (sense != mySense) {}\n17 }\n18 threadSense.set(!mySense);\n19 }\nFigure 17.4 TheSenseBarrier class: a sense-reversing barrier.\nOtherwise, it spins waiting for the barrier\u2019s sense \ufb01eld to change to match its own\nlocal", "doc_id": "9b144f3a-8a56-441e-867e-726919db8395", "embedding": null, "doc_hash": "82e13a88ad34edd5050f29aded729fccd3de8f4f5b8b23502ce60e6d70a03d38", "extra_info": null, "node_info": {"start": 1016738, "end": 1019977}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8ae95869-9773-43c1-bff8-c77f8050c212", "3": "e130b2e3-3448-46f3-8867-0e14961e5f37"}}, "__type__": "1"}, "e130b2e3-3448-46f3-8867-0e14961e5f37": {"__data__": {"text": "!sense; };\n7 };\n8}\n9public void await() {\n10 boolean mySense = threadSense.get();\n11 int position = count.getAndDecrement();\n12 if(position == 1) {\n13 count.set(size);\n14 sense = mySense;\n15 }else {\n16 while (sense != mySense) {}\n17 }\n18 threadSense.set(!mySense);\n19 }\nFigure 17.4 TheSenseBarrier class: a sense-reversing barrier.\nOtherwise, it spins waiting for the barrier\u2019s sense \ufb01eld to change to match its own\nlocal sense.\nDecrementing the shared counter may cause memory contention, since all the\nthreads are trying to access the counter at about the same time. Once the counter\nhas been decremented, each thread spins on the sense \ufb01eld. This implementa-\ntion is well suited for cache-coherent architectures, since threads spin on locally\ncached copies of the \ufb01eld, and the \ufb01eld is modi\ufb01ed only when threads are ready\nto leave the barrier. The sense \ufb01eld is an excellent way of maintaining a uniform\nnoti\ufb01cation time on symmetric cache-coherent multiprocessors.\nPragma 17.3.1. The constructor code for the sense-reversing barrier, shown\nin Fig. 17.4, is mostly straightforward. The one exception occurs on lines\n5 and 6, where we initialize the thread-local threadSense \ufb01eld. This some-\nwhat complicated syntax de\ufb01nes a thread-local Boolean value whose initial\nvalue is the complement of the sense \ufb01eld\u2019s initial value. See Appendix A.2.4\nfor a more complete explanation of thread-local objects in Java.\n17.4 Combining Tree Barrier\nOne way to reduce memory contention (at the cost of increased latency) is to use\nthe combining paradigm of Chapter 12. Split a large barrier into a tree of smaller\n17.4 Combining Tree Barrier 401\n1public class TreeBarrier implements Barrier {\n2 int radix;\n3 Node[] leaf;\n4 ThreadLocal<Boolean> threadSense;\n5 ...\n6 public void await() {\n7 int me = ThreadID.get();\n8 Node myLeaf = leaf[me / radix];\n9 myLeaf.await();\n10 }\n11 ...\n12 }\nFigure 17.5 TheTreeBarrier class: each thread indexes into an array of leaf nodes and calls\nthat leaf\u2019s await ()method.\nbarriers, and have threads combine requests going up the tree and distribute noti-\n\ufb01cations going down the tree. As shown in Fig. 17.5, a tree barrier is character-\nized by a sizen, the total number of threads, and a radixr, each node\u2019s number\nof children. For convenience, we assume there are exactly n=rd+1threads, where\ndis the depth of the tree.\nSpeci\ufb01cally, the combining tree barrier is a tree of nodes , where each node\nhas a counter and a sense, just as in the sense-reversing barrier. A node\u2019s imple-\nmentation is shown in Fig. 17.6. Thread istarts at leaf node bi=rc. The node\u2019s\nawait () method is similar to the sense-reversing barrier\u2019s await (), the principal\ndifference being that the last thread to arrive, the one that completes the bar-\nrier, visits the parent barrier before waking up the other threads. When rthreads\nhave arrived at the root, the barrier is complete, and the sense is reversed. As\nbefore, thread-local Boolean sense values allow the barrier to be reused without\nreinitialization.\nThe tree-structured barrier reduces memory contention by spreading memory\naccesses across multiple barriers. It may or may not reduce", "doc_id": "e130b2e3-3448-46f3-8867-0e14961e5f37", "embedding": null, "doc_hash": "643c06f72f56d8cd71163d19b68d3af9caa121b4acd8ab28cc5377fba930d371", "extra_info": null, "node_info": {"start": 1020074, "end": 1023215}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9b144f3a-8a56-441e-867e-726919db8395", "3": "7315a732-fa2f-4c94-8bd6-f71016f863b0"}}, "__type__": "1"}, "7315a732-fa2f-4c94-8bd6-f71016f863b0": {"__data__": {"text": "() method is similar to the sense-reversing barrier\u2019s await (), the principal\ndifference being that the last thread to arrive, the one that completes the bar-\nrier, visits the parent barrier before waking up the other threads. When rthreads\nhave arrived at the root, the barrier is complete, and the sense is reversed. As\nbefore, thread-local Boolean sense values allow the barrier to be reused without\nreinitialization.\nThe tree-structured barrier reduces memory contention by spreading memory\naccesses across multiple barriers. It may or may not reduce latency, depending on\nwhether it is faster to decrement a single location or to visit a logarithmic number\nof barriers.\nThe root node, once its barrier is complete, lets noti\ufb01cations percolate down\nthe tree. This approach may be good for a NUMA architecture, but it may cause\nnonuniform noti\ufb01cation times. Because threads visit an unpredictable sequence\nof locations as they move up the tree, this approach may not work well on cache-\nless NUMA architectures.\nPragma 17.4.1. Tree nodes are declared as an inner class of the tree barrier\nclass, so nodes are not accessible outside the class. As shown in Fig. 17.7,\nthe tree is initialized by a recursive build () method. The method takes a\nparent node and a depth. If the depth is nonzero, it creates radix children,\nand recursively creates the children\u2019s children. If the depth is zero, it places\n402 Chapter 17 Barriers\n1 private class Node {\n2 AtomicInteger count;\n3 Node parent;\n4 volatile boolean sense;\n5 public Node() {\n6 sense = false ;\n7 parent = null ;\n8 count = new AtomicInteger(radix);\n9 }\n10 public Node(Node myParent) {\n11 this ();\n12 parent = myParent;\n13 }\n14 public void await() {\n15 boolean mySense = threadSense.get();\n16 int position = count.getAndDecrement();\n17 if(position == 1) { // I\u2019m last\n18 if(parent != null ) { // Am I root?\n19 parent.await();\n20 }\n21 count.set(radix);\n22 sense = mySense;\n23 }else {\n24 while (sense != mySense) {};\n25 }\n26 threadSense.set(!mySense);\n27 }\n28 }\n29 }\nFigure 17.6 TheTreeBarrier class: internal tree node.\neach node in a leaf [] array. When a thread enters the barrier, it uses this\narray to choose a leaf to start from. See Appendix A.2.1 for a more complete\ndiscussion of inner classes in Java.\n17.5 Static Tree Barrier\nThe barriers seen so far either suffer from contention (the simple and sense-\nreversing barriers) or have excessive communication (the combining-tree barrier).\nIn the last barrier, threads traverse an unpredictable sequence of nodes, which\nmakes it dif\ufb01cult to lay out the barriers on cacheless NUMA architectures. Sur-\nprisingly, there is another simple barrier that allows both static layout and low\ncontention.\n17.5 Static Tree Barrier 403\n1public class TreeBarrier implements Barrier {\n2 int radix;\n3 Node[] leaf;\n4 int leaves;\n5 ThreadLocal<Boolean> threadSense;\n6 public TreeBarrier( int n,int r) {\n7 radix = r;\n8 leaves = 0;\n9 leaf = new Node[n / r];\n10 int depth = 0;\n11 threadSense = new ThreadLocal<Boolean>() {\n12 protected Boolean initialValue() { return true ; };\n13 };\n14 // compute tree depth\n15 while (n > 1) {\n16 depth++;\n17 n = n / r;\n18 }\n19 Node root = new Node();\n20 build(root, depth - 1);\n21 }\n22 // recursive", "doc_id": "7315a732-fa2f-4c94-8bd6-f71016f863b0", "embedding": null, "doc_hash": "db312330b1598926166a5f06803e9eacfd376ac7c9b67f842c5871d94264f6fe", "extra_info": null, "node_info": {"start": 1023110, "end": 1026330}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "e130b2e3-3448-46f3-8867-0e14961e5f37", "3": "28718f79-23f3-4ded-920b-9ae7fe26b2c4"}}, "__type__": "1"}, "28718f79-23f3-4ded-920b-9ae7fe26b2c4": {"__data__": {"text": "int leaves;\n5 ThreadLocal<Boolean> threadSense;\n6 public TreeBarrier( int n,int r) {\n7 radix = r;\n8 leaves = 0;\n9 leaf = new Node[n / r];\n10 int depth = 0;\n11 threadSense = new ThreadLocal<Boolean>() {\n12 protected Boolean initialValue() { return true ; };\n13 };\n14 // compute tree depth\n15 while (n > 1) {\n16 depth++;\n17 n = n / r;\n18 }\n19 Node root = new Node();\n20 build(root, depth - 1);\n21 }\n22 // recursive tree constructor\n23 void build(Node parent, int depth) {\n24 if(depth == 0) {\n25 leaf[leaves++] = parent;\n26 }else {\n27 for (int i = 0; i < radix; i++) {\n28 Node child = new Node(parent);\n29 build(child, depth - 1);\n30 }\n31 }\n32 }\n33 ...\n34 }\nFigure 17.7 The TreeBarrier class: initializing a combining tree barrier. The build ()\nmethod creates rchildren for each node, and then recursively creates the children\u2019s chil-\ndren. At the bottom, it places leaves in an array.\nThe static tree barrier of Fig. 17.8 works as follows. Each thread is assigned to\na node in a tree (see Fig. 17.9). The thread at a node waits until all nodes below\nit in the tree have \ufb01nished, and then informs its parent. It then spins waiting for\nthe global sense bit to change. Once the root learns that its children are done, it\ntoggles the global sense bit to notify the waiting threads that all threads are done.\nOn a cache-coherent multiprocessor, completing the barrier requires log( n) steps\nmoving up the tree, while noti\ufb01cation simply requires changing the global sense,\nwhich is propagated by the cache-coherence mechanism. On machines without\ncoherent caches threads propagate noti\ufb01cation down the tree as in the combining\nbarrier we saw earlier.\n404 Chapter 17 Barriers\n1public class StaticTreeBarrier implements Barrier {\n2 int radix;\n3 boolean sense;\n4 Node[] node;\n5 ThreadLocal<Boolean> threadSense;\n6 int nodes;\n7 public StaticTreeBarrier( int size, int myRadix) {\n8 radix = myRadix;\n9 nodes = 0;\n10 node = new Node[size];\n11 int depth = 0;\n12 while (size > 1) {\n13 depth++;\n14 size = size / radix;\n15 }\n16 build( null , depth);\n17 sense = false ;\n18 threadSense = new ThreadLocal<Boolean>() {\n19 protected Boolean initialValue() { return !sense; };\n20 };\n21 }\n22 // recursive tree constructor\n23 void build(Node parent, int depth) {\n24 if(depth == 0) {\n25 node[nodes++] = new Node(parent, 0);\n26 }else {\n27 Node myNode = new Node(parent, radix);\n28 node[nodes++] = myNode;\n29 for (int i = 0; i < radix; i++) {\n30 build(myNode, depth - 1);\n31 }\n32 }\n33 }\n34 public void await() {\n35 node[ThreadID.get()].await();\n36 }\n37 }\nFigure 17.8 The StaticTreeBarrier class: each thread indexes into a statically assigned\ntree node and calls that node\u2019s await ()method.\n17.6 T ermination Detecting Barriers\nAll the barriers considered so far were directed at computations organized in\nphases, where each thread \ufb01nishes the work for a phase, reaches the barrier, and\nthen starts a new phase.\nThere is, however, another interesting class of programs, in which each thread\n\ufb01nishes its own part of the computation, only to be put to work", "doc_id": "28718f79-23f3-4ded-920b-9ae7fe26b2c4", "embedding": null, "doc_hash": "0389817c1e3a9805332e0139bb162df42c53cd24dc9f8ec5a8c75de10e510ebc", "extra_info": null, "node_info": {"start": 1026462, "end": 1029486}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7315a732-fa2f-4c94-8bd6-f71016f863b0", "3": "190b24b0-06f5-4576-b0c8-8271823ee729"}}, "__type__": "1"}, "190b24b0-06f5-4576-b0c8-8271823ee729": {"__data__": {"text": "}\n37 }\nFigure 17.8 The StaticTreeBarrier class: each thread indexes into a statically assigned\ntree node and calls that node\u2019s await ()method.\n17.6 T ermination Detecting Barriers\nAll the barriers considered so far were directed at computations organized in\nphases, where each thread \ufb01nishes the work for a phase, reaches the barrier, and\nthen starts a new phase.\nThere is, however, another interesting class of programs, in which each thread\n\ufb01nishes its own part of the computation, only to be put to work again when\nanother thread generates new work. An example of such a program is the\n17.6 T ermination Detecting Barriers 405\n1 public Node(Node myParent, int count) {\n2 children = count;\n3 childCount = new AtomicInteger(count);\n4 parent = myParent;\n5 }\n6 public void await() {\n7 boolean mySense = threadSense.get();\n8 while (childCount.get() > 0) {};\n9 childCount.set(children);\n10 if(parent != null ) {\n11 parent.childDone();\n12 while (sense != mySense) {};\n13 }else {\n14 sense = !sense;\n15 }\n16 threadSense.set(!mySense);\n17 }\n18 public void childDone() {\n19 childCount.getAndDecrement();\n20 }\nFigure 17.9 TheStaticTreeBarrier class: internal Node class.\nsimpli\ufb01ed work stealing executer pool from Chapter 16 (see Fig. 17.10). Here,\nonce a thread exhausts the tasks in its local queue, it tries to steal work from other\nthreads\u2019 queues. The execute () method itself may push new tasks onto the call-\ning thread\u2019s local queue. Once all threads have exhausted all tasks in their queues,\nthe threads will run forever while repeatedly attempting to steal items. Instead,\nwe would like to devise a termination detection barrier so that these threads can\nall terminate once they have \ufb01nished all their tasks.\nEach thread is either active (it has a task to execute) or inactive (it has\nnone). Note that any inactive thread may become active as long as some other\nthread is active, since an inactive thread may steal a task from an active one.\nOnce all threads have become inactive, then no thread will ever become active\nagain. Detecting that the computation as a whole has terminated is the prob-\nlem of determining that at some instant in time there are no longer any active\nthreads.\nNone of the barrier algorithms studied so far can solve this problem. T ermi-\nnation cannot be detected by having each thread announce that it has become\ninactive, and simply count how many have done so, because threads may repeat-\nedly change from inactive to active and back. For example, consider threads A,B,\nandCrunning as shown in Fig. 17.10, and assume that each has a Boolean value\nindicating whether it is active or inactive. When Abecomes inactive, it may then\nobserve that Bis also inactive, and then observe that Cis inactive. Nevertheless,\nAcannot conclude that the overall computation has completed; as Bmight have\nstolen work from CafterAcheckedB, but before it checked C.\n406 Chapter 17 Barriers\n1public class WorkStealingThread {\n2 DEQueue[] queue;\n3 int size;\n4 Random random;\n5 public WorkStealingThread( int n) {\n6 queue = new DEQueue[n];\n7 size = n;\n8 random = new Random();\n9 for (int i = 0; i < n; i++) {\n10 queue[i] = new DEQueue();\n11 }\n12 }\n13 public void run() {\n14 int me = ThreadID.get();\n15 Runnable task = queue[me].popBottom();\n16 while (true ) {\n17 while (task != null ) {\n18", "doc_id": "190b24b0-06f5-4576-b0c8-8271823ee729", "embedding": null, "doc_hash": "99c56ec91649fec7edb34a0e021a03eca323a15ea507588d2b55ee44c1053059", "extra_info": null, "node_info": {"start": 1029400, "end": 1032694}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "28718f79-23f3-4ded-920b-9ae7fe26b2c4", "3": "8260f2e2-9c56-45ea-97a4-13ff33644195"}}, "__type__": "1"}, "8260f2e2-9c56-45ea-97a4-13ff33644195": {"__data__": {"text": "Chapter 17 Barriers\n1public class WorkStealingThread {\n2 DEQueue[] queue;\n3 int size;\n4 Random random;\n5 public WorkStealingThread( int n) {\n6 queue = new DEQueue[n];\n7 size = n;\n8 random = new Random();\n9 for (int i = 0; i < n; i++) {\n10 queue[i] = new DEQueue();\n11 }\n12 }\n13 public void run() {\n14 int me = ThreadID.get();\n15 Runnable task = queue[me].popBottom();\n16 while (true ) {\n17 while (task != null ) {\n18 task.run();\n19 task = queue[me].popBottom();\n20 }\n21 while (task == null ) {\n22 int victim = random.nextInt() % size;\n23 if(!queue[victim].isEmpty()) {\n24 task = queue[victim].popTop();\n25 }\n26 }\n27 }\n28 }\n29 }\nFigure 17.10 Work stealing executer pool revisited.\n1public interface TDBarrier {\n2 void setActive( boolean state);\n3 boolean isTerminated();\n4}\nFigure 17.11 T ermination detection barrier interface.\nAtermination-detection barrier ( Fig. 17.11 ) provides methods setActive (v)\nandisTerminated (). Each thread calls setActive (true) to notify the barrier\nwhen it becomes active, and setActive (false) to notify the barrier when it\nbecomes inactive. The isTerminated () method returns true if and only if all\nthreads had become inactive at some earlier instant. Fig. 17.12 shows a simple\nimplementation of a termination-detection barrier.\nThe barrier encompasses an AtomicInteger initialized to 0. Each thread that\nbecomes active increments the counter (Line 8) and each thread that becomes\ninactive decrements it (Line 10). The computation is deemed to have terminated\nwhen the counter reaches zero (Line 14).\n17.6 T ermination Detecting Barriers 407\n1public class SimpleTDBarrier implements TDBarrier {\n2 AtomicInteger count;\n3 public SimpleTDBarrier( int n){\n4 count = new AtomicInteger(n);\n5 }\n6 public void setActive( boolean active) {\n7 if(active) {\n8 count.getAndIncrement();\n9 }else {\n10 count.getAndDecrement();\n11 }\n12 }\n13 public boolean isTerminated() {\n14 return count.get() == 0;\n15 }\n16 }\nFigure 17.12 A simple termination detecting barrier.\n1 public void run() {\n2 int me = ThreadID.get();\n3 tdBarrier.setActive( true );\n4 Runnable task = queue[me].popBottom();\n5 while (true ) {\n6 while (task != null ) {\n7 task.run();\n8 task = queue[me].popBottom();\n9 }\n10 tdBarrier.setActive( false );\n11 while (task == null ) {\n12 int victim = random.nextInt() % queue.length;\n13 if(!queue[victim].isEmpty()) {\n14 tdBarrier.setActive( true );\n15 task = queue[victim].popTop();\n16 if(task == null ) {\n17 tdBarrier.setActive( false );\n18 }\n19 }\n20 if(tdBarrier.isTerminated()) {\n21 return ;\n22 }\n23 }\n24 }\n25 }\n26 }\nFigure 17.13 Work-stealing executer pool: the run()method with termination.\nThe termination-detection barrier works only if used correctly. Fig. 17.13\nshows how to modify the work-stealing thread\u2019s run() method to return when\nthe computation has terminated. Initially, every thread registers as active (Line 3).\nOnce a thread has exhausted its local queue, it registers as inactive (Line 10).\n408 Chapter 17 Barriers\nBefore it tries to steal a new task, however, it must register as active (Line 14). If\nthe", "doc_id": "8260f2e2-9c56-45ea-97a4-13ff33644195", "embedding": null, "doc_hash": "c83cec59c82ef98f940c10a50d6b64590be521009b37f92eeeb6068889c0cde8", "extra_info": null, "node_info": {"start": 1032776, "end": 1035827}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "190b24b0-06f5-4576-b0c8-8271823ee729", "3": "6158f7f3-48d1-4ed7-84a9-41a036bf900d"}}, "__type__": "1"}, "6158f7f3-48d1-4ed7-84a9-41a036bf900d": {"__data__": {"text": "}\n24 }\n25 }\n26 }\nFigure 17.13 Work-stealing executer pool: the run()method with termination.\nThe termination-detection barrier works only if used correctly. Fig. 17.13\nshows how to modify the work-stealing thread\u2019s run() method to return when\nthe computation has terminated. Initially, every thread registers as active (Line 3).\nOnce a thread has exhausted its local queue, it registers as inactive (Line 10).\n408 Chapter 17 Barriers\nBefore it tries to steal a new task, however, it must register as active (Line 14). If\nthe theft fails, it registers as inactive again (Line 17).\nNotice that a thread sets its state to active before stealing a task. Otherwise, if\na thread were to steal a task while inactive, then the thread whose task was stolen\nmight also declare itself inactive, resulting in a computation where all threads\ndeclare themselves inactive while the computation continues.\nHere is a subtle point. A thread tests whether the queue is empty (Line 13)\nbefore it attempts to steal a task. This way, it avoids declaring itself active if there\nis no chance the theft will succeed. Without this precaution, it is possible that the\nthreads will not detect termination because each one repeatedly switches to an\nactive state before a steal attempt that is doomed to fail.\nCorrect use of the termination-detection barrier must satisfy both a safety\nand a liveness property. The safety property is that if isTerminated () returns\ntrue, then the computation really has terminated. Safety requires that no active\nthread ever declare itself inactive, because it could trigger an incorrect termi-\nnation detection. For example, the work-stealing thread of Fig. 17.13 would be\nincorrect if the thread declared itself to be active only after successfully stealing\na task. By contrast, it is safe for an inactive thread to declare itself active, which\nmay occur if the thread is unsuccessful in stealing work at Line 15.\nThe liveness property is that if the computation terminates, then\nisTerminated () eventually returns true. (It is not necessary that termination be\ndetected instantly.) While safety is not jeopardized if an inactive thread declares\nitself active, liveness will be violated if a thread that does not succeed in stealing\nwork fails to declare itself inactive again (Line 15), because termination will not\nbe detected when it occurs.\n17.7 Chapter Notes\nJohn Mellor\u2013Crummey and Michael Scott [ 113] provide a survey of several\nbarrier algorithms, though the performance numbers they provide should be\nviewed from a historical perspective. The combining tree barrier is based on\ncode due to John Mellor\u2013Crummey and Michael Scott [ 113], which is in turn\nbased on the combining tree algorithm of Pen-Chung Y ew, Nian-Feng Tzeng, and\nDuncan Lawrie [ 151]. The dissemination barrier is credited to Debra Hensgen,\nRaphael Finkel, and Udi Manber [ 59]. The tournament tree barrier used in the\nexercises is credited to John Mellor\u2013Crummey and Michael Scott [ 113]. The sim-\nple barriers and the static tree barrier are most likely folklore. We learned of\nthe static tree barrier from Beng-Hong Lim. The termination detection barrier\nand its application to an executer pool are based on a variation suggested by\nPeter Kessler to an algorithm by Dave Detlefs, Christine Flood, Nir Shavit, and\nXiolan Zhang [ 41].\n17.8 Exercises 409\n17.8 Exercises\nExercise 198. Fig. 17.14 shows how to use barriers to make a parallel pre\ufb01x\ncomputation work on an asynchronous architecture.\nAparallel pre\ufb01x computation, given a sequence a0,:::,am\u00001, of", "doc_id": "6158f7f3-48d1-4ed7-84a9-41a036bf900d", "embedding": null, "doc_hash": "898f60db06e877683eccd5d14d856ba6d5f55ad3f42e4baa8cbd53555c687a6a", "extra_info": null, "node_info": {"start": 1035729, "end": 1039270}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8260f2e2-9c56-45ea-97a4-13ff33644195", "3": "4d9bbf3b-3f9c-45fe-8bcc-1f4d1fd48351"}}, "__type__": "1"}, "4d9bbf3b-3f9c-45fe-8bcc-1f4d1fd48351": {"__data__": {"text": "tree barrier from Beng-Hong Lim. The termination detection barrier\nand its application to an executer pool are based on a variation suggested by\nPeter Kessler to an algorithm by Dave Detlefs, Christine Flood, Nir Shavit, and\nXiolan Zhang [ 41].\n17.8 Exercises 409\n17.8 Exercises\nExercise 198. Fig. 17.14 shows how to use barriers to make a parallel pre\ufb01x\ncomputation work on an asynchronous architecture.\nAparallel pre\ufb01x computation, given a sequence a0,:::,am\u00001, of numbers,\ncomputes in parallel the partial sums:\nbi=iX\nj=0aj:\nIn a synchronous system, where all threads take steps at the same time, there\nare simple, well-known algorithms for mthreads to compute the partial sums in\nlogmsteps. The computation proceeds in a sequence of rounds, starting at round\nzero. In round r, ifi>2r, threadireads the value at a[i\u00002r] into a local variable.\nNext, it adds that value to a[i]. Rounds continue until 2r>m. It is not hard to\nsee that after log2(m) rounds, the array acontains the partial sums.\n1.What could go wrong if we executed the parallel pre\ufb01x on n>m threads?\n2.Modify this program, adding one or more barriers, to make it work properly\nin a concurrent setting with nthreads. What is the minimum number of bar-\nriers that are necessary?\nExercise 199. Change the sense-reversing barrier implementation so that waiting\nthreads call wait () instead of spinning.\n1class Prefix extends java.lang.Thread {\n2 private int [] a;\n3 private int i;\n4 public Prefix( int[] myA, int myI) {\n5 a = myA;\n6 i = myI;\n7 }\n8 public void run() {\n9 int d = 1, sum = 0;\n10 while (d < m) {\n11 if(i >= d)\n12 sum = a[i-d];\n13 if(i >= d)\n14 a[i] += sum;\n15 d = d *2;\n16 }\n17 }\n18 }\nFigure 17.14 Parallel pre\ufb01x computation.\n410 Chapter 17 Barriers\n\u0004Give an example of a situation where suspending threads is better than\nspinning.\n\u0004Give an example of a situation where the other choice is better.\nExercise 200. Change the tree barrier implementation so that it takes a Runnable\nobject whose run() method is called once after the last thread arrives at the bar-\nrier, but before any thread leaves the barrier.\nExercise 201. Modify the combining tree barrier so that nodes can use any barrier\nimplementation, not just the sense-reversing barrier.\nExercise 202. Atournament tree barrier (Class TourBarrier in Fig. 17.15) is an\nalternative tree-structured barrier. Assume there are nthreads, where nis a power\nof 2. The tree is a binary tree consisting of 2 n\u00001 nodes. Each leaf is owned\nby a single, statically determined thread. Each node\u2019s two children are linked as\npartners . One partner is statically designated as active , and the other as passive .\nFig. 17.16 illustrates the tree structure.\n1 private class Node {\n2 volatile boolean flag; // signal when done\n3 boolean active; // active or passive?\n4 Node parent; // parent node\n5 Node partner; // partner node\n6 // create passive node\n7 Node() {\n8 flag = false ;\n9 active = false ;\n10 partner = null ;\n11 parent = null ;\n12 }\n13 // create active node\n14 Node(Node myParent) {\n15 this ();\n16 parent = myParent;\n17 active = true ;\n18 }\n19 void await( boolean sense)", "doc_id": "4d9bbf3b-3f9c-45fe-8bcc-1f4d1fd48351", "embedding": null, "doc_hash": "d30408256bb88e2a97b206738ff90575f26691d128c71d5afa31fc9f112a6312", "extra_info": null, "node_info": {"start": 1039319, "end": 1042415}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6158f7f3-48d1-4ed7-84a9-41a036bf900d", "3": "2903ce6d-ebaa-4bf1-87dc-631a4901423f"}}, "__type__": "1"}, "2903ce6d-ebaa-4bf1-87dc-631a4901423f": {"__data__": {"text": ".\nFig. 17.16 illustrates the tree structure.\n1 private class Node {\n2 volatile boolean flag; // signal when done\n3 boolean active; // active or passive?\n4 Node parent; // parent node\n5 Node partner; // partner node\n6 // create passive node\n7 Node() {\n8 flag = false ;\n9 active = false ;\n10 partner = null ;\n11 parent = null ;\n12 }\n13 // create active node\n14 Node(Node myParent) {\n15 this ();\n16 parent = myParent;\n17 active = true ;\n18 }\n19 void await( boolean sense) {\n20 if(active) { // I\u2019m active\n21 while (flag != sense) {}; // wait for partner\n22 if(parent != null ) {parent.await(sense}; // wait for parent\n23 partner.flag = sense; // tell partner\n24 }else { // I\u2019m passive\n25 partner.flag = sense; // tell partner\n26 while (flag != sense) {}; // wait for partner\n27 }\n28 }\n29 }\nFigure 17.15 TheTourBarrier class.\n17.8 Exercises 411\nroot\nloser\nloserwinner\nwinner winner loser\nFigure 17.16 TheTourBarrier class: information \ufb02ow. Nodes are paired statically in active/\npassive pairs. Threads start at the leaves. Each thread in an active node waits for its passive\npartner to show up; then it proceeds up the tree. Each passive thread waits for its active\npartner for noti\ufb01cation of completion. Once an active thread reaches the root, all threads\nhave arrived, and noti\ufb01cations \ufb02ow down the tree in the reverse order.\nEach thread keeps track of the current sense in a thread-local variable. When\na thread arrives at a passive node, it sets its active partner\u2019s sense \ufb01eld to the\ncurrent sense, and spins on its own sense \ufb01eld until its partner changes that\n\ufb01eld\u2019s value to the current sense. When a thread arrives at an active node, it spins\non its sense \ufb01eld until its passive partner sets it to the current sense. When the\n\ufb01eld changes, that particular barrier is complete, and the active thread follows the\nparent reference to its parent node. Note that an active thread at one level may\nbecome passive at the next level. When the root node barrier is complete, noti-\n\ufb01cations percolate down the tree. Each thread moves back down the tree setting\nits partner\u2019s sense \ufb01eld to the current sense.\nThis barrier improves a little on the combining tree barrier of Fig. 17.5.\nExplain how.\nThe tournament barrier code uses parent andpartner references to navigate\nthe tree. We could save space by eliminating these \ufb01elds and keeping all the nodes\nin a single array with the root at index 0, the root\u2019s children at indexes 1 and 2, the\ngrandchildren at indexes 3\u20136, and so on. Re-implement the tournament barrier\nto use indexing arithmetic instead of references to navigate the tree.\nExercise 203. The combining tree barrier uses a single thread-local sense \ufb01eld for\nthe entire barrier. Suppose instead we were to associate a thread-local sense with\neach node as in Fig. 17.8. Either:\n\u0004Explain why this implementation is equivalent to the other one, except that it\nconsumes more memory, or.\n\u0004Give a counterexample showing that this implementation is incorrect.\n412 Chapter 17 Barriers\n1 private class Node {\n2 AtomicInteger count;\n3 Node parent;\n4 volatile boolean sense;\n5 int d;\n6 // construct root node\n7 public Node() {\n8 sense = false ;\n9 parent = null ;\n10 count = new AtomicInteger(radix);\n11 ThreadLocal<Boolean> threadSense;\n12", "doc_id": "2903ce6d-ebaa-4bf1-87dc-631a4901423f", "embedding": null, "doc_hash": "a41f53f7d75c62546d6de8c43fba3e8b8979c873920936580bcd867f59554f9d", "extra_info": null, "node_info": {"start": 1042426, "end": 1045661}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "4d9bbf3b-3f9c-45fe-8bcc-1f4d1fd48351", "3": "3334bdb5-95a7-42e5-a9c8-7e4cd5cf6d26"}}, "__type__": "1"}, "3334bdb5-95a7-42e5-a9c8-7e4cd5cf6d26": {"__data__": {"text": "thread-local sense with\neach node as in Fig. 17.8. Either:\n\u0004Explain why this implementation is equivalent to the other one, except that it\nconsumes more memory, or.\n\u0004Give a counterexample showing that this implementation is incorrect.\n412 Chapter 17 Barriers\n1 private class Node {\n2 AtomicInteger count;\n3 Node parent;\n4 volatile boolean sense;\n5 int d;\n6 // construct root node\n7 public Node() {\n8 sense = false ;\n9 parent = null ;\n10 count = new AtomicInteger(radix);\n11 ThreadLocal<Boolean> threadSense;\n12 threadSense = new ThreadLocal<Boolean>() {\n13 protected Boolean initialValue() { return true ; };\n14 };\n15 }\n16 public Node(Node myParent) {\n17 this ();\n18 parent = myParent;\n19 }\n20 public void await() {\n21 boolean mySense = threadSense.get();\n22 int position = count.getAndDecrement();\n23 if(position == 1) { // I\u2019m last\n24 if(parent != null ) { // root?\n25 parent.await();\n26 }\n27 count.set(radix); // reset counter\n28 sense = mySense;\n29 }else {\n30 while (sense != mySense) {};\n31 }\n32 threadSense.set(!mySense);\n33 }\n34 }\nFigure 17.17 Thread-local tree barrier.\nExercise 204. The tree barrier works \u201cbottom-up,\u201d in the sense that barrier com-\npletion moves from the leaves up to the root, while wake-up information moves\nfrom the root back down to the leaves. Figs. 17.18 and 17.19 show an alternative\ndesign, called a reverse tree barrier , which works just like a tree barrier except for\nthe fact that barrier completion starts at the root and moves down to the leaves.\nEither:\n\u0004Sketch an argument why this is correct, perhaps by reduction to the standard\ntree barrier, or\n\u0004Give a counterexample showing why it is incorrect.\nExercise 205. Implement an n-thread reusable barrier from an n-wire counting\nnetwork and a single Boolean variable. Sketch a proof that the barrier works.\n17.8 Exercises 413\n1public class RevBarrier implements Barrier {\n2 int radix;\n3 ThreadLocal<Boolean> threadSense;\n4 int leaves;\n5 Node[] leaf;\n6 public RevBarrier( int mySize, int myRadix) {\n7 radix = myRadix;\n8 leaves = 0;\n9 leaf = new Node[mySize / myRadix];\n10 int depth = 0;\n11 threadSense = new ThreadLocal<Boolean>() {\n12 protected Boolean initialValue() { return true ; };\n13 };\n14 // compute tree depth\n15 while (mySize > 1) {\n16 depth++;\n17 mySize = mySize / myRadix;\n18 }\n19 Node root = new Node();\n20 root.d = depth;\n21 build(root, depth - 1);\n22 }\n23 // recursive tree constructor\n24 void build(Node parent, int depth) {\n25 // are we at a leaf node?\n26 if(depth == 0) {\n27 leaf[leaves++] = parent;\n28 }else {\n29 for (int i = 0; i < radix; i++) {\n30 Node child = new Node(parent);\n31 child.d = depth;\n32 build(child, depth - 1);\n33 }\n34 }\n35 }\nFigure 17.18 Reverse tree barrier Part 1.\nExercise 206. Can you devise a \u201cdistributed\u201d termination detection algorithm for\nthe executer pool in which threads do not repeatedly update or test a central loca-\ntion for termination, but rather use only local uncontended variables? Variables\nmay be unbounded, but state changes should take constant time, (so you cannot\nparallelize the shared counter).\nHint: adapt the atomic snapshot algorithm from Chapter 4.\nExercise", "doc_id": "3334bdb5-95a7-42e5-a9c8-7e4cd5cf6d26", "embedding": null, "doc_hash": "41c78a0b6da9e15efd13e537626c5a8c894c649c62fbe035783f3ca8b5facca7", "extra_info": null, "node_info": {"start": 1045612, "end": 1048730}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2903ce6d-ebaa-4bf1-87dc-631a4901423f", "3": "3c1e5c8b-c7f8-4091-9e74-1b2cd192ba60"}}, "__type__": "1"}, "3c1e5c8b-c7f8-4091-9e74-1b2cd192ba60": {"__data__": {"text": "child.d = depth;\n32 build(child, depth - 1);\n33 }\n34 }\n35 }\nFigure 17.18 Reverse tree barrier Part 1.\nExercise 206. Can you devise a \u201cdistributed\u201d termination detection algorithm for\nthe executer pool in which threads do not repeatedly update or test a central loca-\ntion for termination, but rather use only local uncontended variables? Variables\nmay be unbounded, but state changes should take constant time, (so you cannot\nparallelize the shared counter).\nHint: adapt the atomic snapshot algorithm from Chapter 4.\nExercise 207. Adissemination barrier is a symmetric barrier implementation in\nwhich threads spin on statically-assigned locally-cached locations using only\nloads and stores. As illustrated in Fig. 17.20, the algorithm runs in a series of\nrounds. At round r, threadinoti\ufb01es thread i+ 2r(modn), (wherenis the num-\nber of threads) and waits for noti\ufb01cation from thread i\u00002r(modn).\n414 Chapter 17 Barriers\n36 public void await() {\n37 int me = ThreadInfo.getIndex();\n38 Node myLeaf = leaf[me / radix];\n39 myLeaf.await(me);\n40 }\n41 private class Node {\n42 AtomicInteger count;\n43 Node parent;\n44 volatile boolean sense;\n45 int d;\n46 // construct root node\n47 public Node() {\n48 sense = false ;\n49 parent = null ;\n50 count = new AtomicInteger(radix);\n51 }\n52 public Node(Node myParent) {\n53 this ();\n54 parent = myParent;\n55 }\n56 public void await( int me) {\n57 boolean mySense = threadSense.get();\n58 // visit parent first\n59 if((me % radix) == 0) {\n60 if(parent != null ) { // root?\n61 parent.await(me / radix);\n62 }\n63 }\n64 int position = count.getAndDecrement();\n65 if(position == 1) { // I\u2019m last\n66 count.set(radix); // reset counter\n67 sense = mySense;\n68 }else {\n69 while (sense != mySense) {};\n70 }\n71 threadSense.set(!mySense);\n72 }\n73 }\n74 }\nFigure 17.19 Reverse tree barrier Part 2: correct or not?\nFor how many rounds must this protocol run to implement a barrier? What if\nnis not a power of 2? Justify your answers.\nExercise 208. Give a reusable implementation of a dissemination barrier in Java.\nHint: you may want to keep track of both the parity and the sense of the current\nphase.\nExercise 209. Create a table that summarizes the total number of operations in\nthe static tree, combining tree, and dissemination barriers.\n17.8 Exercises 415\ni+ 1 mod(6) i+ 2 mod(6) i+ 4 mod(6)\nThread  i=0\n i=1\ni=2\n i=3\n i=4\n i=5\nFigure 17.20 Communication in the dissemination barrier. In each round ra thread i\ncommunicates with thread i+ 2r(mod n).\nExercise 210. In the termination detection barrier, the state is set to active before\nstealing the task; otherwise the stealing thread could be declared inactive; then\nit would steal a task, and before setting its state back to active, the thread it stole\nfrom could become inactive. This would lead to an undesirable situation in which\nall threads are declared inactive yet the computation continues. Can you devise a\nterminating executer pool in which the state is set to active only after successfully\nstealing a task?\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n18Transactional Memory\n18.1", "doc_id": "3c1e5c8b-c7f8-4091-9e74-1b2cd192ba60", "embedding": null, "doc_hash": "c90a6db343be8790939a82af2e84cb68654822e59a818e2ecb79aaef6a6e4f0d", "extra_info": null, "node_info": {"start": 1048720, "end": 1051833}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3334bdb5-95a7-42e5-a9c8-7e4cd5cf6d26", "3": "67363930-4f81-4774-9c81-17a3b7bd24f4"}}, "__type__": "1"}, "67363930-4f81-4774-9c81-17a3b7bd24f4": {"__data__": {"text": "otherwise the stealing thread could be declared inactive; then\nit would steal a task, and before setting its state back to active, the thread it stole\nfrom could become inactive. This would lead to an undesirable situation in which\nall threads are declared inactive yet the computation continues. Can you devise a\nterminating executer pool in which the state is set to active only after successfully\nstealing a task?\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\n18Transactional Memory\n18.1 Introduction\nWe now turn our attention from devising data structures and algorithms to\ncritiquing the tools we use to solve these problems. These tools are the syn-\nchronization primitives provided by today\u2019s architectures, encompassing vari-\nous kinds of locking, both spinning and blocking, and atomic operations such\nascompareAndSet() and its relatives. They have mostly served us well. We, the\ncommunity of multiprocessor programmers, have been able to construct many\nuseful and elegant data structures. Nevertheless, everyone knows that the tools\nare \ufb02awed . In this chapter, we review and analyze the strengths and weaknesses\nof the standard synchronization primitives, and describe some emerging alter-\nnatives that are likely to extend, and perhaps even to displace many of today\u2019s\nstandard primitives.\n18.1.1 What is Wrong with Locking?\nLocking, as a synchronization discipline, has many pitfalls for the inexperienced\nprogrammer. Priority inversion occurs when a lower-priority thread is preempted\nwhile holding a lock needed by higher-priority threads. Convoying occurs when\na thread holding a lock is descheduled, perhaps by exhausting its scheduling\nquantum by a page fault, or by some other kind of interrupt. While the thread\nholding the lock is inactive, other threads that require that lock will queue up,\nunable to progress. Even after the lock is released, it may take some time to\ndrain the queue, in much the same way that an accident can slow traf\ufb01c even\nafter the debris has been cleared away. Deadlock can occur if threads attempt to\nlock the same objects in different orders. Deadlock avoidance can be awkward if\nthreads must lock many objects, particularly if the set of objects is not known in\nadvance. In the past, when highly scalable applications were rare and valuable,\nThe Art of Multiprocessor Programming. DOI: 10.1016/B978-0-12-397337-5.00018-6\nCopyright \u00a9 2012 by Elsevier Inc. All rights reserved.417\n418 Chapter 18 Transactional Memory\n/*\n*When a locked buffer is visible to the I/O layer BH_Launder\n*is set. This means before unlocking we must clear BH_Launder,\n*mb() on alpha and then clear BH_Lock, so no reader can see\n*BH_Launder set on an unlocked buffer and then risk to deadlock.\n*/\nFigure 18.1 Synchronization by convention: a typical comment from the Linux kernel.\nthese hazards were avoided by deploying teams of dedicated expert program-\nmers. T oday, when highly scalable applications are becoming commonplace, the\nconventional approach is just too expensive.\nThe heart of the problem is that no one really knows how to organize and\nmaintain large systems that rely on locking. The association between locks\nand data is established mostly by convention. Ultimately, it exists only in the\nmind of the programmer, and may be documented only in comments. Fig. 18.1\nshows a typical comment from a Linux header \ufb01le1describing the conventions\ngoverning the use of a particular kind of buffer. Over time, interpreting and\nobserving many such conventions spelled out in this way may complicate code\nmaintenance.\n18.1.2 What is Wrong with", "doc_id": "67363930-4f81-4774-9c81-17a3b7bd24f4", "embedding": null, "doc_hash": "1ac2a9e306819fb6424c49103762fec3a93753de830ebf2d553aca54cf33116a", "extra_info": null, "node_info": {"start": 1051815, "end": 1055442}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3c1e5c8b-c7f8-4091-9e74-1b2cd192ba60", "3": "2be66d28-7232-43e1-b066-919831ad00f3"}}, "__type__": "1"}, "2be66d28-7232-43e1-b066-919831ad00f3": {"__data__": {"text": "is just too expensive.\nThe heart of the problem is that no one really knows how to organize and\nmaintain large systems that rely on locking. The association between locks\nand data is established mostly by convention. Ultimately, it exists only in the\nmind of the programmer, and may be documented only in comments. Fig. 18.1\nshows a typical comment from a Linux header \ufb01le1describing the conventions\ngoverning the use of a particular kind of buffer. Over time, interpreting and\nobserving many such conventions spelled out in this way may complicate code\nmaintenance.\n18.1.2 What is Wrong with compareAndSet() ?\nOne way to bypass the problems of locking is to rely on atomic primitives like\ncompareAndSet() . Algorithms that use compareAndSet() and its relatives are\noften hard to devise, and sometimes, though not always, have a high overhead.\nThe principal dif\ufb01culty is that nearly all synchronization primitives, whether\nreading, writing, or applying an atomic compareAndSet() , operate only on a\nsingle word. This restriction often forces a complex and unnatural structure on\nalgorithms.\nLet us review the lock-free queue of Chapter 10 (reproduced in Fig. 18.2 ), this\ntime with an eye toward the underlying synchronization primitives.\nA complication arises between Lines 12and 13. The enq() method calls\ncompareAndSet() to change both the tail node\u2019s next \ufb01eld and the tail\n\ufb01eld itself to the new node. Ideally, we would like to atomically combine both\ncompareAndSet() calls, but because these calls occur one-at-a-time both enq()\nand deq() must be prepared to encounter a half-\ufb01nished enq() (Line 12).\nOne way to address this problem is to introduce a multiCompareAndSet ()\nprimitive, as shown in Fig. 18.3 . This method takes as arguments an array of\nAtomicReference<T> objects, an array of expected Tvalues, and an array of\nT-values used for updates. It performs a simultaneous compareAndSet() on all\n1Kernel v2.4.19 /fs/buffer.c\n18.1 Introduction 419\n1public class LockFreeQueue<T> {\n2 private AtomicReference<Node> head;\n3 private AtomicReference<Node> tail;\n4 ...\n5 public void enq(T item) {\n6 Node node = new Node(item);\n7 while (true ) {\n8 Node last = tail.get();\n9 Node next = last.next.get();\n10 if(last == tail.get()) {\n11 if(next == null ) {\n12 if(last.next.compareAndSet(next, node)) {\n13 tail.compareAndSet(last, node);\n14 return ;\n15 }\n16 }else {\n17 tail.compareAndSet(last, next);\n18 }\n19 }\n20 }\n21 }\n22 }\nFigure 18.2 TheLockFreeQueue class: the enq() method.\n1<T> boolean multiCompareAndSet(\n2 AtomicReference<T>[] target,\n3 T[] expected,\n4 T[] update) {\n5 atomic {\n6 for (int i = 0; i < target.length)\n7 if(!target[i].get().equals(expected[i].get()))\n8 return false ;\n9 for (int i = 0; i < target.length)\n10 target[i].set(update[i].get);\n11 return true ;\n12 }\n13 }\nFigure 18.3 Pseudocode for multiCompareAndSet (). This code is executed atomically.\narray elements, and if any one fails, they all do. In more detail: if, for all i, the\nvalue of target [i] isexpected [i], then set target [i]\u2019s value to update [i] and\nreturn true. Otherwise leave target [i] unchanged, and return false.\nNote that there is no obvious way to implement", "doc_id": "2be66d28-7232-43e1-b066-919831ad00f3", "embedding": null, "doc_hash": "966da7289b3fe86163f6591f5ae747d067cd811e8fc17b109e03454452b4e299", "extra_info": null, "node_info": {"start": 1055411, "end": 1058565}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "67363930-4f81-4774-9c81-17a3b7bd24f4", "3": "6f8de890-a4ca-4fee-b097-e857639b4a9d"}}, "__type__": "1"}, "6f8de890-a4ca-4fee-b097-e857639b4a9d": {"__data__": {"text": "for (int i = 0; i < target.length)\n10 target[i].set(update[i].get);\n11 return true ;\n12 }\n13 }\nFigure 18.3 Pseudocode for multiCompareAndSet (). This code is executed atomically.\narray elements, and if any one fails, they all do. In more detail: if, for all i, the\nvalue of target [i] isexpected [i], then set target [i]\u2019s value to update [i] and\nreturn true. Otherwise leave target [i] unchanged, and return false.\nNote that there is no obvious way to implement multiCompareAndSet () on\nconventional architectures. If there were, comparing the LockFreeQueue imple-\nmentations in Figs. 18.2 and 18.4 illustrates how multiCompareAndSet () sim-\npli\ufb01es concurrent data structures. The complex logic of Lines 11\u201312 is replaced\nby a call to a single multiCompareAndSet () call.\n420 Chapter 18 Transactional Memory\n1 public void enq(T item) {\n2 Node node = new Node(item);\n3 while (true ) {\n4 Node last = tail.get();\n5 Node next = last.next.get();\n6 if(last == tail.get()) {\n7 AtomicReference[] target = {last.next, tail};\n8 T[] expect = {next, last};\n9 T[] update = {node, node};\n10 if(multiCompareAndSet(target, expect, update)) return ;\n11 }\n12 }\n13 }\nFigure 18.4 TheLockFreeQueue class: simpli\ufb01ed enq() method with\nmultiCompareAndSet ().\nWhile multi-word extensions such as multiCompareAndSet () might be use-\nful, they do not help with another serious \ufb02aw, discussed in Section 18.1.3.\n18.1.3 What is Wrong with Compositionality?\nAll the synchronization mechanisms we have considered so far, with or without\nlocks, have a major drawback: they cannot easily be composed . Let us imagine\nthat we want to dequeue an item xfrom queue q0and enqueue it at another,\nq1. The transfer must be atomic : no concurrent thread should observe either that\nxhas vanished, or that it is present in both queues. In Queue implementations\nbased on monitors, each method acquires the lock internally, so it is essentially\nimpossible to combine two method calls in this way.\nFailure to compose is not restricted to mutual exclusion. Let us consider a\nbounded queue class whose deq() method blocks as long as the queue is empty\n(using either wait /notify or explicit condition objects). We imagine that we\nhave two such queues, and we want to dequeue an item from either queue. If both\nqueues are empty, then we want to block until an item shows up in either one.\nInQueue implementations based on monitors, each method provides its own\nconditional waiting, so it is essentially impossible to wait on two conditions in\nthis way.\nNaturally, there are always ad hoc solutions. For the atomic transfer, we could\nintroduce a lock to be acquired by any thread attempting an atomic modi\ufb01ca-\ntion to both q0andq1. But such a lock would be a concurrency bottleneck (no\nconcurrent transfers) and it requires knowing in advance the identities of the\ntwo queues. Or, the queues themselves might export their synchronization state,\n(say, via lock () and unlock () methods), and rely on the caller to manage multi-\nobject synchronization. Exposing synchronization state in this way would have a\ndevastating effect on modularity, complicating interfaces, and relying on callers\n18.2 Transactions and Atomicity 421\nto follow complicated conventions. Also, this approach simply would not work\nfor nonblocking queue implementations.\n18.1.4 What can We Do about It?\nWe can summarize the problems with conventional synchronization primitives\nas", "doc_id": "6f8de890-a4ca-4fee-b097-e857639b4a9d", "embedding": null, "doc_hash": "d8d490bad5eab2a607f7550da42b66c007f787d01b54530149e34a6ebe962e2f", "extra_info": null, "node_info": {"start": 1058679, "end": 1062079}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2be66d28-7232-43e1-b066-919831ad00f3", "3": "7822f6be-a0d7-4be0-9782-6a98a20da6d5"}}, "__type__": "1"}, "7822f6be-a0d7-4be0-9782-6a98a20da6d5": {"__data__": {"text": "Or, the queues themselves might export their synchronization state,\n(say, via lock () and unlock () methods), and rely on the caller to manage multi-\nobject synchronization. Exposing synchronization state in this way would have a\ndevastating effect on modularity, complicating interfaces, and relying on callers\n18.2 Transactions and Atomicity 421\nto follow complicated conventions. Also, this approach simply would not work\nfor nonblocking queue implementations.\n18.1.4 What can We Do about It?\nWe can summarize the problems with conventional synchronization primitives\nas follows.\n\u0004Locks are hard to manage effectively, especially in large systems.\n\u0004Atomic primitives such as compareAndSet() operate on only one word at a\ntime, resulting in complex algorithms.\n\u0004It is dif\ufb01cult to compose multiple calls to multiple objects into atomic units.\nIn Section 18.2, we introduce transactional memory , an emerging programming\nmodel that proposes a solution to each of these problems.\n18.2 Transactions and Atomicity\nAtransaction is a sequence of steps executed by a single thread. Transactions\nmust be serializable , meaning that they appear to execute sequentially, in a one-\nat-a-time order. Serializability is a kind of coarse-grained version of lineariz-\nability. Linearizability de\ufb01ned atomicity of individual objects by requiring that\neach method call of a given object appear to take effect instantaneously between\nits invocation and response, Serializability, on the other hand, de\ufb01nes atomicity\nfor entire transactions, that is, blocks of code that may include calls to multiple\nobjects. It ensures that a transaction appears to take effect between the invoca-\ntion of its \ufb01rst call and the response to its last call.2Properly implemented, trans-\nactions do not deadlock or livelock.\nWe now describe some simple programming language extensions to Java that\nsupport a transactional model of synchronization. These extensions are not cur-\nrently part of Java, but they illustrate the model. The features described here are\na kind of average of features provided by contemporary transactional memory\nsystems. Not all systems provide all these features: some provide weaker guaran-\ntees, some stronger. Nevertheless, understanding these features will go a long way\ntoward understanding modern transactional memory models.\nThe atomic keyword delimits a transaction in much the same way the\nsynchronized keyword delimits a critical section. While synchronized blocks\nacquire a speci\ufb01c lock, and are atomic only with respect to other synchronized\nblocks that acquire the same lock, an atomic block is atomic with respect to all\n2Some de\ufb01nitions of serializability in the literature do not require transactions to be serialized in\nan order compatible with their real-time precedence order.\n422 Chapter 18 Transactional Memory\n1public class TransactionalQueue<T> {\n2 private Node head;\n3 private Node tail;\n4 public TransactionalQueue() {\n5 Node sentinel = new Node( null );\n6 head = sentinel;\n7 tail = sentinel;\n8 }\n9 public void enq(T item) {\n10 atomic {\n11 Node node = new Node(item);\n12 tail.next = node;\n13 tail = node;\n14 }\n15 }\nFigure 18.5 An unbounded transactional queue: the enq() method.\nother atomic blocks. Nested synchronized blocks can deadlock if they acquire\nlocks in opposite orders, while nested atomic blocks cannot.\nBecause transactions allow atomic updates to multiple locations, they elim-\ninate the need for multiCompareAndSet ().Fig. 18.5 shows the enq() method\nfor a transactional queue. Let us compare this code with the lock-free code of\nFig. 18.2 : there is no need for AtomicReference \ufb01elds, compareAndSet() calls,\nor retry loops.", "doc_id": "7822f6be-a0d7-4be0-9782-6a98a20da6d5", "embedding": null, "doc_hash": "eabca36af27cb4d824faf0681565e31b01c42564b9a3b31cadd4180359e6f80a", "extra_info": null, "node_info": {"start": 1061971, "end": 1065626}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6f8de890-a4ca-4fee-b097-e857639b4a9d", "3": "5f6ee1e9-4865-486a-8365-6c6313b35b17"}}, "__type__": "1"}, "5f6ee1e9-4865-486a-8365-6c6313b35b17": {"__data__": {"text": "18.5 An unbounded transactional queue: the enq() method.\nother atomic blocks. Nested synchronized blocks can deadlock if they acquire\nlocks in opposite orders, while nested atomic blocks cannot.\nBecause transactions allow atomic updates to multiple locations, they elim-\ninate the need for multiCompareAndSet ().Fig. 18.5 shows the enq() method\nfor a transactional queue. Let us compare this code with the lock-free code of\nFig. 18.2 : there is no need for AtomicReference \ufb01elds, compareAndSet() calls,\nor retry loops. Here, the code is essentially sequential code bracketed by atomic\nblocks.\nT o explain how transactions are used to write concurrent programs, it is con-\nvenient to say something about how they are implemented. Transactions are\nexecuted speculatively : as a transaction executes, it makes tentative changes to\nobjects. If it completes without encountering a synchronization con\ufb02ict, then\nitcommits (the tentative changes become permanent) or it aborts (the tentative\nchanges are discarded).\nTransactions can be nested. Transactions must be nested for simple modu-\nlarity: one method should be able to start a transaction and then call another\nmethod without caring whether the nested call starts a transaction. Nested trans-\nactions are especially useful if a nested transaction can abort without aborting its\nparent. This property will be important when we discuss conditional synchro-\nnization later on.\nRecall that atomically transferring an item from one queue to another was\nessentially impossible with objects that use internal monitor locks. With transac-\ntions, composing such atomic method calls is almost trivial. Fig. 18.7 shows how\nto compose a deq() call that dequeues an item xfrom a queue q0and an enq(x)\ncall that enqueues that item to another queue q1.\nWhat about conditional synchronization? Fig. 18.6 shows the enq() method\nfor a bounded buffer. The method enters an atomic block (Line 2), and tests\nwhether the buffer is full (Line 3). If so, it calls retry (Line 4), which rolls\n18.2 Transactions and Atomicity 423\n1 public void enq(T x) {\n2 atomic {\n3 if(count == items.length)\n4 retry;\n5 items[tail] = x;\n6 if(++tail == items.length)\n7 tail = 0;\n8 ++count;\n9 }\n10 }\nFigure 18.6 A bounded transactional queue: the enq() method with retry .\n1 atomic {\n2 x = q0.deq();\n3 q1.enq(x);\n4 }\nFigure 18.7 Composing atomic method calls.\n1 atomic {\n2 x = q0.deq();\n3 } orElse {\n4 x = q1.deq();\n5 }\nFigure 18.8 TheorElse statement: waiting on multiple conditions.\nback the enclosing transaction, pauses it, and restarts it when the object state\nhas changed. Conditional synchronization is one reason it may be convenient to\nroll back a nested transaction without rolling back the parent. Unlike the wait ()\nmethod or explicit condition variables, retry does not easily lend itself to lost\nwake-up bugs.\nRecall that waiting for one of several conditions to become true was impos-\nsible using objects with internal monitor condition variables. A novel aspect of\nretry is that such composition becomes easy. Fig. 18.8 shows a code snippet\nillustrating the orElse statement, which joins two or more code blocks. Here,\nthe thread executes the \ufb01rst block (Line 2). If that block calls retry , then that\nsubtransaction is rolled back, and the thread executes the second block (Line 4).\nIf that block also calls retry , then the orElse as a whole pauses, and later reruns\neach of the blocks (when something", "doc_id": "5f6ee1e9-4865-486a-8365-6c6313b35b17", "embedding": null, "doc_hash": "06bd3aad5666c5e44d38380f8592855397bdd2227e04dc8344a5994141715a96", "extra_info": null, "node_info": {"start": 1065679, "end": 1069105}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "7822f6be-a0d7-4be0-9782-6a98a20da6d5", "3": "4a9586ad-f865-4eb9-bcff-b7899383dc53"}}, "__type__": "1"}, "4a9586ad-f865-4eb9-bcff-b7899383dc53": {"__data__": {"text": "to become true was impos-\nsible using objects with internal monitor condition variables. A novel aspect of\nretry is that such composition becomes easy. Fig. 18.8 shows a code snippet\nillustrating the orElse statement, which joins two or more code blocks. Here,\nthe thread executes the \ufb01rst block (Line 2). If that block calls retry , then that\nsubtransaction is rolled back, and the thread executes the second block (Line 4).\nIf that block also calls retry , then the orElse as a whole pauses, and later reruns\neach of the blocks (when something changes) until one completes.\nIn the rest of this chapter, we examine techniques for implementing transac-\ntional memory. Transactional synchronization can be implemented in hardware\n(HTM ), in software ( STM ), or both. In the following sections, we examine STM\nimplementations.\n424 Chapter 18 Transactional Memory\n18.3 Software Transactional Memory\nUnfortunately, the language support sketched in Section 18.2 is not currently\navailable. Instead, this section describes how to support transactional synchro-\nnization using a software library. We present TinyTM , a simple Software Trans-\nactional Memory package that could be the target of the language extensions\ndescribed in Section 18.2. For brevity, we ignore such important issues as nested\ntransactions, retry , and orElse . There are two elements to a software trans-\nactional memory construction: the threads that run the transactions, and the\nobjects that they access.\nWe illustrate these concepts by walking though part of a concurrent SkipList\nimplementation, like those found in Chapter 14. This class uses a skiplist\nto implement a setproviding the usual methods: add(x) addsxto the set,\nremove (x) removesxfrom the set, and contains (x) returns true if, and only\nifxis in the set.\nRecall that a skiplist is a collection of linked lists. Each node in the list contains\nanitem \ufb01eld (an element of the set), a key \ufb01eld (the item\u2019s hash code), and a\nnext \ufb01eld, which is an array of references to successor nodes in the list. Array slot\nzero refers to the very next node in the list, and higher-numbered slots refer to\nsuccessively later successors. T o \ufb01nd a given key, search \ufb01rst though higher levels,\nmoving to a lower level each time a search overshoots. In this way, one can \ufb01nd\nan item in time logarithmic in the length of the list. (Refer to Chapter 14 for a\nmore complete description of skiplists.)\nUsing TinyTM , threads communicate via shared atomic objects , which provide\nsynchronization , ensuring that transactions cannot see one another\u2019s uncommit-\nted effects, and recovery , undoing the effects of aborted transactions. Fields of\natomic objects are not directly accessible. Instead, they are accessed indirectly\nthrough getter and setter methods. For example, the getter method for the key\n\ufb01eld has the form\nint getKey();\nwhile the matching setter method has the form\nvoid setKey( int value);\nAccessing \ufb01elds through getters and setters provides the ability to interpose trans-\nactional synchronization and recovery on each \ufb01eld access. Fig. 18.9 shows the\ncomplete SkipNode interface.\nIn a similar vein, the transactional SkipList implementation cannot use a\nstandard array, because TinyTM cannot intercept access to the array. Instead,\nTinyTM provides an AtomicArray<T> class that serves the same functionality as\na regular array.\nFig. 18.10 shows the \ufb01elds and constructor for the SkipListSet class, and\nFig. 18.11 shows the code for the add() method. Except for the syntactic clutter\n18.3 Software Transactional Memory 425\n1public interface", "doc_id": "4a9586ad-f865-4eb9-bcff-b7899383dc53", "embedding": null, "doc_hash": "f820c3cfe1f58582bca57aaddf32b87d41d60d23164277b678750bd7c045b6b8", "extra_info": null, "node_info": {"start": 1069095, "end": 1072664}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5f6ee1e9-4865-486a-8365-6c6313b35b17", "3": "0376df55-a1ec-4c8d-b5cb-04064c986da9"}}, "__type__": "1"}, "0376df55-a1ec-4c8d-b5cb-04064c986da9": {"__data__": {"text": "on each \ufb01eld access. Fig. 18.9 shows the\ncomplete SkipNode interface.\nIn a similar vein, the transactional SkipList implementation cannot use a\nstandard array, because TinyTM cannot intercept access to the array. Instead,\nTinyTM provides an AtomicArray<T> class that serves the same functionality as\na regular array.\nFig. 18.10 shows the \ufb01elds and constructor for the SkipListSet class, and\nFig. 18.11 shows the code for the add() method. Except for the syntactic clutter\n18.3 Software Transactional Memory 425\n1public interface SkipNode<T> {\n2 public int getKey();\n3 public void setKey( int value);\n4 public T getItem();\n5 public void setItem(T value);\n6 public AtomicArray<SkipNode<T>> getNext();\n7 public void setNext(AtomicArray<SkipNode<T>> value);\n8}\nFigure 18.9 TheSkipNode interface.\n1\n2public final class SkipListSet<T> {\n3 final SkipNode<T> head;\n4 final SkipNode<T> tail;\n5 public SkipListSet() {\n6 head = new TSkipNode<T>(MAX_HEIGHT, Integer.MIN_VALUE, null );\n7 tail = new TSkipNode<T>(0, Integer.MAX_VALUE, null );\n8 AtomicArray<SkipNode<T>> next = head.getNext();\n9 for (int i = 0; i < next.length; i++) {\n10 next.set(i, tail);\n11 }\n12 }\n13 ...\nFigure 18.10 TheSkipListSet class: \ufb01elds and constructor.\n14 public boolean add(T v) {\n15 int topLevel = randomLevel();\n16 SkipNode<T>[] preds = (SkipNode<T>[]) new SkipNode[MAX_HEIGHT];\n17 SkipNode<T>[] succs = (SkipNode<T>[]) new SkipNode[MAX_HEIGHT];\n18 if(find(v, preds, succs) != -1) {\n19 return false ;\n20 }\n21 SkipNode<T> newNode = new TSkipNode<T>(topLevel+1, v);\n22 for (int level = 0; level <= topLevel; level++) {\n23 newNode.getNext().set(level, succs[level]);\n24 preds[level].getNext().set(level, newNode);\n25 }\n26 return true ;\n27 }\nFigure 18.11 TheSkipListSet class: the add()method.\ncaused by the getters and setters, this code is almost identical to that of a sequen-\ntial implementation. (The getter and setter calls could be generated by a compiler\nor preprocessor, but here we will make the calls explicit.) In Line 21we create a\nnewTSkipNode (transactional skip node) implementing the SkipNode interface.\nWe will examine this class later on.\n426 Chapter 18 Transactional Memory\n1public class TThread extends java.lang.Thread {\n2 static Runnable onAbort = ...;\n3 static Runnable onCommit = ...;\n4 static Callable<Boolean> onValidate = ...;\n5 public static <T> T doIt(Callable<T> xaction) throws Exception {\n6 T result = null ;\n7 while (true ) {\n8 Transaction me = new Transaction();\n9 Transaction.setLocal(me);\n10 try {\n11 result = xaction.call();\n12 }catch (AbortedException e) {\n13 }catch (Exception e) {\n14 throw new PanicException(e);\n15 }\n16 if(onValidate.call()) {\n17 if(me.commit()) {\n18 onCommit.run(); return result;\n19 }\n20 }\n21 me.abort();\n22 onAbort.run();\n23 }\n24 }\n25 }\nFigure 18.12 TheTThread class.\n1SkipListSet<Integer> list = new SkipListSet<Integer>();\n2for (int i = 0; i < 100; i++)", "doc_id": "0376df55-a1ec-4c8d-b5cb-04064c986da9", "embedding": null, "doc_hash": "50df139747f37c41e1cd9b602f085f6d710719fc01b0ce5eefe7eb99ddd732f6", "extra_info": null, "node_info": {"start": 1072666, "end": 1075547}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "4a9586ad-f865-4eb9-bcff-b7899383dc53", "3": "feee9d8a-da92-4f4e-8f26-8290be3d6aef"}}, "__type__": "1"}, "feee9d8a-da92-4f4e-8f26-8290be3d6aef": {"__data__": {"text": "try {\n11 result = xaction.call();\n12 }catch (AbortedException e) {\n13 }catch (Exception e) {\n14 throw new PanicException(e);\n15 }\n16 if(onValidate.call()) {\n17 if(me.commit()) {\n18 onCommit.run(); return result;\n19 }\n20 }\n21 me.abort();\n22 onAbort.run();\n23 }\n24 }\n25 }\nFigure 18.12 TheTThread class.\n1SkipListSet<Integer> list = new SkipListSet<Integer>();\n2for (int i = 0; i < 100; i++) {\n3 result = TThread.doIt( new Callable<Boolean>() {\n4 public Boolean call() {\n5 return list.add(i);\n6 }\n7 });\n8}\nFigure 18.13 Adding items to an integer list.\nA transaction that returns a value of type Tis implemented by a Callable<T>\nobject (see Chapter 16) where the code to be executed is encapsulated in the\nobject\u2019s call () method.\nAtransactional thread (class TThread ) is a thread capable of running transac-\ntions. A TThread (Fig. 18.12) runs a transaction by calling the doIt () method\nwith the Callable<T> object as an argument. Fig. 18.13 shows a code snippet\nin which a transactional thread inserts a sequence of values into a skiplist tree.\nThe list variable is a skiplist shared by multiple transactional threads. Here,\nthe argument to doIt () is an anonymous inner class , a Java construct that allows\nshort-lived classes to be declared in-line. The result variable is a Boolean indi-\ncating whether the value was already present in the list.\n18.3 Software Transactional Memory 427\nWe now describe the TinyTM implementation in detail. In Section 18.3.1 , we\ndescribe the transactional thread implementation, and then we describe how to\nimplement transactional atomic objects.\n18.3.1 Transactions and Transactional Threads\nA transaction\u2019s status is encapsulated in a thread-local Transaction object\n(Fig. 18.14 ) which can assume one of three states: ACTIVE ,ABORTED , orCOMMITTED\n(Line 2). When a transaction is created, its default status is ACTIVE (Line 11). It\nis convenient to de\ufb01ne a constant Transaction.COMMITTED transaction object\nfor threads that are not currently executing within a transaction (Line 3). The\nTransaction class also keeps track of each thread\u2019s current transaction through\na thread-local \ufb01eld local (Lines 5\u20138).\nThe commit () method tries to change the transaction state from ACTIVE\ntoCOMMITTED (Line 19), and the abort () method from ACTIVE toABORTED\n1public class Transaction {\n2 public enum Status {ABORTED, ACTIVE, COMMITTED};\n3 public static Transaction COMMITTED = new Transaction(Status.COMMITTED);\n4 private final AtomicReference<Status> status;\n5 static ThreadLocal<Transaction> local = new ThreadLocal<Transaction>() {\n6 protected Transaction initialValue() {\n7 return new Transaction(Status.COMMITTED);\n8 }\n9 };\n10 public Transaction() {\n11 status = new AtomicReference<Status>(Status.ACTIVE);\n12 }\n13 private Transaction(Transaction.Status myStatus) {\n14 status = new AtomicReference<Status>(myStatus);\n15 }\n16 public Status getStatus() {\n17 return status.get();\n18 }\n19 public boolean commit() {\n20 return status.compareAndSet(Status.ACTIVE, Status.COMMITTED);\n21 }\n22 public boolean abort() {\n23 return status.compareAndSet(Status.ACTIVE, Status.ABORTED);\n24 }\n25 public static Transaction getLocal() {\n26 return local.get();\n27 }\n28 public static void setLocal(Transaction transaction) {\n29 local.set(transaction);\n30 }\n31 }\nFigure", "doc_id": "feee9d8a-da92-4f4e-8f26-8290be3d6aef", "embedding": null, "doc_hash": "6417bbb5e3febf45ff70a37cd0f3650f2c5a3a3e8c2c4e3e0cb6dec68006363c", "extra_info": null, "node_info": {"start": 1075663, "end": 1078940}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0376df55-a1ec-4c8d-b5cb-04064c986da9", "3": "f4c8710f-7080-499f-af04-cdc9b423762a"}}, "__type__": "1"}, "f4c8710f-7080-499f-af04-cdc9b423762a": {"__data__": {"text": "status = new AtomicReference<Status>(myStatus);\n15 }\n16 public Status getStatus() {\n17 return status.get();\n18 }\n19 public boolean commit() {\n20 return status.compareAndSet(Status.ACTIVE, Status.COMMITTED);\n21 }\n22 public boolean abort() {\n23 return status.compareAndSet(Status.ACTIVE, Status.ABORTED);\n24 }\n25 public static Transaction getLocal() {\n26 return local.get();\n27 }\n28 public static void setLocal(Transaction transaction) {\n29 local.set(transaction);\n30 }\n31 }\nFigure 18.14 TheTransaction class.\n428 Chapter 18 Transactional Memory\n(Line 22). A thread can test its current transaction state by calling getStatus ()\n(Line 16). If a thread discovers that its current transaction has been aborted, it\nthrows AbortedException . A thread can get and set its current transaction by\ncalling static getLocal () or setLocal () methods.\nThe TThread (transactional thread) class is a subclass of the standard Java\nThread class. Each transactional thread has several associated handlers . The\nonCommit and onAbort handlers are called when a transaction commits or\naborts, and the validation handler is called when a transaction is about to com-\nmit. It returns a Boolean indicating whether the thread\u2019s current transaction\nshould try to commit. These handlers can be de\ufb01ned at run-time. Later, we will\nsee how these handlers can be used to implement different techniques for trans-\naction synchronization and recovery.\nThe doIt () method (Line 5ofFig. 18.12 ) takes a Callable<T> object and\nexecutes its call () method as a transaction. It creates a new ACTIVE transac-\ntion (Line 8), and calls the transaction\u2019s call () method. If that method throws\nAbortedException (Line 12), then the doIt () method simply retries the loop.\nAny other exception means the application has made an error (Line 13), and (for\nsimplicity) the method throws PanicException , which prints an error message\nand shuts down everything. If the transaction returns, then doIt () calls the val-\nidation handler to test whether to commit (Line 16), and if the validation suc-\nceeds, then it tries to commit the transaction (Line 17). If the commit succeeds,\nthen it runs the commit hander and returns (Line 18). Otherwise, if validation\nfails, it explicitly aborts the transaction. If commit fails for any reason, it runs the\nabort handler before retrying (Line 22).\n18.3.2 Zombies and Consistency\nSynchronization con\ufb02icts cause transactions to abort, but it is not always possible\nto halt a transaction\u2019s thread immediately after the con\ufb02ict occurs. Instead, such\nzombie3transactions may continue to run even after it has become impossible\nfor them to commit. This prospect raises another important design issue: how to\nprevent zombie transactions from seeing inconsistent states.\nHere is how an inconsistent state could arise. An object has two \ufb01elds xand\ny, initially 1 and 2. Each transaction preserves the invariant that yis always equal\nto 2x. Transaction Zreadsy, and sees value 2. Transaction Achangesxandy\nto 2 and 4, respectively, and commits. Zis now a zombie, since it keeps running,\nbut will never commit. Zlater readsx, and sees the value 2, which is inconsistent\nwith the value it read for y.\nOne approach is to deny that inconsistent states are a problem. Since zom-\nbie transactions must eventually abort, their updates will be discarded, so why\nshould we care what they observe? Unfortunately, a zombie can cause problems,\n3Azombie is a reanimated human corpse. Stories of zombies originated in the Afro-Caribbean\nspiritual belief system of", "doc_id": "f4c8710f-7080-499f-af04-cdc9b423762a", "embedding": null, "doc_hash": "40c25c938a16d7d829e9c88ad2823d5e5161accc07bc345d46b51c6b41dcbaa7", "extra_info": null, "node_info": {"start": 1078849, "end": 1082379}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "feee9d8a-da92-4f4e-8f26-8290be3d6aef", "3": "a02d6212-2cbf-46f6-97a0-bee556591b4c"}}, "__type__": "1"}, "a02d6212-2cbf-46f6-97a0-bee556591b4c": {"__data__": {"text": "respectively, and commits. Zis now a zombie, since it keeps running,\nbut will never commit. Zlater readsx, and sees the value 2, which is inconsistent\nwith the value it read for y.\nOne approach is to deny that inconsistent states are a problem. Since zom-\nbie transactions must eventually abort, their updates will be discarded, so why\nshould we care what they observe? Unfortunately, a zombie can cause problems,\n3Azombie is a reanimated human corpse. Stories of zombies originated in the Afro-Caribbean\nspiritual belief system of Vodou.\n18.3 Software Transactional Memory 429\neven if its updates never take effect. In the scenario described earlier, where y= 2x\nin every consistent state, but Zhas read the inconsistent value 2 for both xand\ny, ifZevaluates the expression\n1/(x-y)\nit will throw an \u201cimpossible\u201d divide-by-zero exception, halting the thread, and\npossibly crashing the application. For the same reason, if Znow executes the loop\nint i = x + 1; // i is 3\nwhile (i++ != y) { // y is actually 2, should be 4\n...\n}\nit would never terminate.\nThere is no practical way to avoid \u201cimpossible\u201d exceptions and in\ufb01nite loops in\na programming model where invariants cannot be relied on. As a result, TinyTM\nguarantees that all transactions, even zombies, see consistent states.\n18.3.3 Atomic Objects\nAs mentioned earlier, concurrent transactions communicate through shared\natomic objects . As we have seen (Fig. 18.9), access to an atomic object is provided\nby a stylized interface that provides a set of matching getter and setter methods.\nTheAtomicObject interface appears in Fig. 18.15.\nWe will need to construct two classes that implement this interface: a sequen-\ntialimplementation that provides no synchronization or recovery, and a transac-\ntional implementation that does. Here too, these classes could easily be generated\nby a compiler, but we will do these constructions by hand.\nThe sequential implementation is straightforward. For each matching getter\u2013\nsetter pair, for example:\nT getItem();\nvoid setItem(T value);\n1public abstract class AtomicObject <T extends Copyable<T>> {\n2 protected Class<T> internalClass;\n3 protected T internallnit;\n4 public AtomicObject(T init) {\n5 internalInit = init;\n6 internalClass = (Class<T>) init.getClass();\n7 }\n8 public abstract T openRead();\n9 public abstract T openWrite();\n10 public abstract boolean validate();\n11 }\nFigure 18.15 TheAtomicObject<T> abstract class.\n430 Chapter 18 Transactional Memory\nthe sequential implementation de\ufb01nes a private \ufb01eld item of type T. We also\nrequire the sequential implementation to satisfy a simple Copyable<T> inter-\nface that provides a copyTo () method that copies the \ufb01elds of one object to\nanother (Fig. 18.16). As a technical matter, we also require the type to pro-\nvide a no-argument constructor. For brevity, we use the term version to refer\nto an instance of a sequential, Copyable<T> implementation of an atomic object\ninterface.\nFig. 18.17 shows the SSkipNode class, a sequential implementation of the\nSkipNode interface. This class has three parts. The class must provide a\nno-argument constructor for use by atomic object implementations (as described\nlater), and may provide any other constructors convenient to the class. Second,\nit provides the getters and setters de\ufb01ned by the interface, where each getter or\nsetter simply reads or writes its associated \ufb01eld. Finally, the class also implements\ntheCopyable interface, which provides a copyTo ()", "doc_id": "a02d6212-2cbf-46f6-97a0-bee556591b4c", "embedding": null, "doc_hash": "dbeb83354f15d437ed2277214a8f0839533f4d01a6d5fe8d73c16d8cdd6d8a90", "extra_info": null, "node_info": {"start": 1082362, "end": 1085817}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f4c8710f-7080-499f-af04-cdc9b423762a", "3": "97540b97-83ce-434f-a195-7130b7e47e2f"}}, "__type__": "1"}, "97540b97-83ce-434f-a195-7130b7e47e2f": {"__data__": {"text": "implementation of an atomic object\ninterface.\nFig. 18.17 shows the SSkipNode class, a sequential implementation of the\nSkipNode interface. This class has three parts. The class must provide a\nno-argument constructor for use by atomic object implementations (as described\nlater), and may provide any other constructors convenient to the class. Second,\nit provides the getters and setters de\ufb01ned by the interface, where each getter or\nsetter simply reads or writes its associated \ufb01eld. Finally, the class also implements\ntheCopyable interface, which provides a copyTo () method that initializes one\n1public interface Copyable<T> {\n2 void copyTo(T target);\n3}\nFigure 18.16 TheCopyable<T> interface.\n1public class SSkipNode<T>\n2 implements SkipNode<T>, Copyable<SSkipNode<T>> {\n3 AtomicArray<SkipNode<T>> next;\n4 int key;\n5 T item;\n6 public SSkipNode() {}\n7 public SSkipNode( int level) {\n8 next = new AtomicArray<SkipNode<T>>(SkipNode. class , level);\n9 }\n10 public SSkipNode( int level, int myKey, T myItem) {\n11 this (level); key = myKey; item = myItem;\n12 }\n13 public AtomicArray<SkipNode<T>> getNext() { return next;}\n14 public void setNext(AtomicArray<SkipNode<T>> value) {next = value;}\n15 public int getKey() { return key;}\n16 public void setKey( int value) {key = value;}\n17 public T getItem() { return item;}\n18 public void setItem(T value) {item = value;}\n19\n20 public void copyTo(SSkipNode<T> target) {\n21 target.next = next;\n22 target.key = key;\n23 target.item = item;\n24 }\n25 }\nFigure 18.17 TheSSkipNode class: a sequential SkipNode implementation.\n18.3 Software Transactional Memory 431\nobject\u2019s \ufb01elds from another\u2019s. This method is needed to make back-up copies of\nthe sequential object.\n18.3.4 Dependent or Independent Progress?\nOne goal of transactional memory is to free the programmer from worrying\nabout starvation, deadlock, and \u201cthe thousand natural shocks\u201d that locking is\nheir to. Nevertheless, those who implement STMs must decide which progress\ncondition to meet.\nWe recall from Chapter 3 that implementations that meet strong indepen-\ndent progress conditions such as wait-freedom or lock-freedom guarantee that a\nthread always makes progress. While it is possible to design wait-free or lock-free\nSTM systems, no one knows how to make them ef\ufb01cient enough to be practical.\nInstead, research on nonblocking STMs has focused on weaker dependent\nprogress conditions. There are two approaches that promise good performance:\nnonblocking STMs that are obstruction-free, and blocking, lock-based STMs that\nare deadlock-free. Like the other nonblocking conditions, obstruction-freedom\nensures that not all threads can be blocked by delays or failures of other threads.\nThis property is weaker than lock-free synchronization, because it does not guar-\nantee progress when two or more con\ufb02icting threads are executing concurrently.\nThe deadlock-free property does not guarantee progress if threads halt in criti-\ncal sections. Fortunately, as with many of the lock-based data structures we saw\nearlier, scheduling in modern operating systems can minimize the possibility of\nthreads getting swapped out in the middle of a transaction. Like obstruction-\nfreedom, deadlock-freedom does not guarantee progress when two or more\ncon\ufb02icting threads are executing concurrently.\nFor both the nonblocking obstruction-free and blocking deadlock-free STMs,\nprogress for con\ufb02icting transactions is", "doc_id": "97540b97-83ce-434f-a195-7130b7e47e2f", "embedding": null, "doc_hash": "862a23559a486c72d9ad86a95294e4b5fb62a53c54df3ad884b22fb537b76a2e", "extra_info": null, "node_info": {"start": 1085779, "end": 1089178}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a02d6212-2cbf-46f6-97a0-bee556591b4c", "3": "d4718bf2-3848-4913-8e63-d15c9eafb2c5"}}, "__type__": "1"}, "d4718bf2-3848-4913-8e63-d15c9eafb2c5": {"__data__": {"text": "are executing concurrently.\nThe deadlock-free property does not guarantee progress if threads halt in criti-\ncal sections. Fortunately, as with many of the lock-based data structures we saw\nearlier, scheduling in modern operating systems can minimize the possibility of\nthreads getting swapped out in the middle of a transaction. Like obstruction-\nfreedom, deadlock-freedom does not guarantee progress when two or more\ncon\ufb02icting threads are executing concurrently.\nFor both the nonblocking obstruction-free and blocking deadlock-free STMs,\nprogress for con\ufb02icting transactions is guaranteed by a contention manager , a\nmechanism that decides when to delay contending threads, through spinning or\nyielding, so that some thread can always make progress.\n18.3.5 Contention Managers\nInTinyTM , as in many other STMs, a transaction can detect when it is about\nto cause a synchronization con\ufb02ict. The requester transaction then consults a\ncontention manager . The contention manager serves as an oracle,4advising the\ntransaction whether to abort the other transaction immediately, or stall to allow\nthe other a chance to complete. Naturally, no transaction should stall forever\nwaiting for another.\n4Dating back to 1400 BC, Pythia, the Oracle of Delphi, provided advice and predictions about\ncrops and wars.\n432 Chapter 18 Transactional Memory\n1public abstract class ContentionManager {\n2 static ThreadLocal<ContentionManager> local\n3 =new ThreadLocal<ContentionManager>() {\n4 protected ContentionManager initialValue() {\n5 try {\n6 return (ContentionManager) Defaults.MANAGER.newInstance();\n7 }catch (Exception ex) {\n8 throw new PanicException(ex);\n9 }\n10 }\n11 };\n12 public abstract void resolve(Transaction me, Transaction other);\n13 public static ContentionManager getLocal() {\n14 return local.get();\n15 }\n16 public static void setLocal(ContentionManager m) {\n17 local.set(m);\n18 }\n19 }\nFigure 18.18 Contention manager base class.\nFig. 18.18 shows a simpli\ufb01ed base class for contention managers. It provides a\nsingle method, resolve () (Line 12), that takes two transactions, the requester\u2019s\nand the other\u2019s, and either pauses the requester, or aborts the other. It also\nkeeps track of each thread\u2019s local contention manager (Line 2), accessible by\ngetLocal () and setLocal () methods (Lines 16and 13).\nTheContentionManager class is abstract because it does not implement any\ncon\ufb02ict resolution policy. Here are some possible contention manager policies.\nSuppose transaction Ais about to con\ufb02ict with transaction B.\n\u0004Backoff:Arepeatedly backs off for a random duration, doubling the expected\ntime up to some limit. When that limit is reached, it aborts B.\n\u0004Priority: Each transaction takes a timestamp when it starts. If Ahas an older\ntimestamp than B, it abortsB, and otherwise it waits. A transaction that\nrestarts after an abort keeps its old timestamp, ensuring that every transac-\ntion eventually completes.\n\u0004Greedy: Each transaction takes a timestamp when it starts. AabortsBif either\nAhas an older timestamp than B, orBis waiting for another transaction. This\nstrategy eliminates chains of waiting transactions. As in the priority policy,\nevery transaction eventually completes.\n\u0004Karma: Each transaction keeps track of how much work it has accomplished,\nand the transaction that has accomplished more has priority.\nFig. 18.19 shows a contention manager implementation that uses the back-off\npolicy. The manager imposes minimum and maximum delays (Lines 2\u20133). The\nresolve () method checks whether this is the \ufb01rst time it has encountered the\nother thread", "doc_id": "d4718bf2-3848-4913-8e63-d15c9eafb2c5", "embedding": null, "doc_hash": "42b88e26e6b862511e0d464b32e7040ba95414c5d6c4441c232bc1a52424ecc5", "extra_info": null, "node_info": {"start": 1089162, "end": 1092719}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "97540b97-83ce-434f-a195-7130b7e47e2f", "3": "efacd29a-de74-4bd6-abf5-26d7bef9a05c"}}, "__type__": "1"}, "efacd29a-de74-4bd6-abf5-26d7bef9a05c": {"__data__": {"text": "an older timestamp than B, orBis waiting for another transaction. This\nstrategy eliminates chains of waiting transactions. As in the priority policy,\nevery transaction eventually completes.\n\u0004Karma: Each transaction keeps track of how much work it has accomplished,\nand the transaction that has accomplished more has priority.\nFig. 18.19 shows a contention manager implementation that uses the back-off\npolicy. The manager imposes minimum and maximum delays (Lines 2\u20133). The\nresolve () method checks whether this is the \ufb01rst time it has encountered the\nother thread (Line 8). If so, it resets its delay to the minimum, and otherwise it\nuses its current delay. If the current delay is less than the maximum, the thread\n18.3 Software Transactional Memory 433\n1public class BackoffManager extends ContentionManager {\n2 private static final int MIN_DELAY = ...;\n3 private static final int MAX_DELAY = ...;\n4 Random random = new Random();\n5 Transaction previous = null ;\n6 int delay = MIN_DELAY;\n7 public void resolve(Transaction me, Transaction other) {\n8 if(other != previous) {\n9 previous = other;\n10 delay = MIN_DELAY;\n11 }\n12 if(delay < MAX_DELAY) {\n13 Thread.sleep(random.nextInt(delay));\n14 delay = 2 *delay;\n15 }else {\n16 other.abort();\n17 delay = MIN_DELAY;\n18 }\n19 }\n20 }\nFigure 18.19 A simple contention manager implementation.\nsleeps for a random duration bounded by the delay (Line 13), and doubles the\nnext delay. If the current delay exceeds the maximum, the caller aborts the other\ntransaction (Line 16).\n18.3.6 Implementing Atomic Objects\nLinearizability requires that individual method calls appear to take place atomi-\ncally. We now consider how to guarantee serializability: that multiple atomic calls\nhave the same property.\nAtransactional implementation of an atomic object must provide getter\nand setter methods that invoke transactional synchronization and recovery.\nWe review two alternative approaches to synchronization and recovery: the\nFreeObject class is obstruction-free, while the LockObject class uses lock-\ning for synchronization. These alternatives are implementations of the abstract\nAtomicObject class, shown in Fig. 18.15 . The init () method takes the atomic\nobject\u2019s class as argument and records it for later use. The openRead () method\nreturns a version suitable for reading (that is, one can call its getter methods\nonly), while the openWrite () method returns a version that may be written (that\nis, one can call both getters and setters).\nThevalidate () method returns true if and only if the value to be returned is\nguaranteed to be consistent. It is necessary to call validate () before returning\nany information extracted from an atomic object. The openRead (),openWrite (),\nandvalidate () methods are all abstract.\nFig. 18.20 shows the TSkipNode class a transactional SkipNode implementa-\ntion. This class uses the LockObject atomic object implementation for synchro-\nnization and recovery (Line 8).\n434 Chapter 18 Transactional Memory\n1public class TSkipNode<T> implements SkipNode<T> {\n2 AtomicObject<SSkipNode<T>> atomic;\n3 public TSkipNode(int level) {\n4 atomic = new LockObject<SSkipNode<T>>(new SSkipNode<T>(level));\n5 }\n6 public TSkipNode(int level, int key, T item){\n7 atomic =\n8 new LockObject<SSkipNode<T>>(new SSkipNode<T>(level, key, item));\n9 }\n10 public TSkipNode(int level, T item){\n11 atomic = new LockObject<SSkipNode<T>>(new", "doc_id": "efacd29a-de74-4bd6-abf5-26d7bef9a05c", "embedding": null, "doc_hash": "7fa26fee952a6d6651b05c503975caf20212c6fa27fa7c00603ecb01f8da5558", "extra_info": null, "node_info": {"start": 1092741, "end": 1096127}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d4718bf2-3848-4913-8e63-d15c9eafb2c5", "3": "cf6f6a6d-288c-46f3-888f-da68f10551b7"}}, "__type__": "1"}, "cf6f6a6d-288c-46f3-888f-da68f10551b7": {"__data__": {"text": "implements SkipNode<T> {\n2 AtomicObject<SSkipNode<T>> atomic;\n3 public TSkipNode(int level) {\n4 atomic = new LockObject<SSkipNode<T>>(new SSkipNode<T>(level));\n5 }\n6 public TSkipNode(int level, int key, T item){\n7 atomic =\n8 new LockObject<SSkipNode<T>>(new SSkipNode<T>(level, key, item));\n9 }\n10 public TSkipNode(int level, T item){\n11 atomic = new LockObject<SSkipNode<T>>(new SSkipNode<T>(level,\n12 item.hashCode(), item));\n13 }\n14 public AtomicArray<SkipNode<T>> getNext() {\n15 AtomicArray<SkipNode<T>> forward = atomic.openRead().getNext();\n16 if(!atomic.validate())\n17 throw new AbortedException();\n18 return forward;\n19 }\n20 public void setNext(AtomicArray<SkipNode<T>> value) {\n21 atomic.openWrite().setNext(value);\n22 }\n23 // getKey, setKey, getItem, and setItem omitted ...\n24 }\nFigure 18.20 TheTSkipNode class: a transactional SkipNode implementation.\nThis class has a single AtomicObject<SSkipNode> \ufb01eld. The constructor\ntakes as argument values to initialize the AtomicObject<SSkipNode> \ufb01eld. Each\ngetter performs the following sequence of steps.\n1.It calls openRead () to extract a version.\n2.It calls the version\u2019s getter to extract the \ufb01eld value, which it stores in a local\nvariable.\n3.It calls validate () to ensure the value read is consistent.\nThe last step is needed to ensure that the object did not change between the \ufb01rst\nand second step, and that the value recorded in the second step is consistent with\nall the other values observed by that transaction.\nSetters are implemented in a symmetric way, calling the setter in the second\nstep.\nWe now have two alternative atomic object implementations. The implemen-\ntations will be relatively unoptimized to simplify the presentation.\n18.3.7 An Obstruction-Free Atomic Object\nRecall that an algorithm is obstruction-free if any thread that runs by itself for\nlong enough makes progress. In practice, this condition means that a thread\nmakes progress if it runs for long enough without a synchronization con\ufb02ict from\n18.3 Software Transactional Memory 435\na concurrent thread. Here, we describe an obstruction-free implementation of\nAtomicObject .\nBird\u2019s-Eye View\nEach object has three logical \ufb01elds: an owner \ufb01eld, an oldversion, and a new\nversion. (We call them logical \ufb01elds because they may not be implemented as\n\ufb01elds.) The owner is the last transaction to access the object. The old version\nis the object state before that transaction arrived, and the new version re\ufb02ects\nthat transaction\u2019s updates, if any. If owner is COMMITTED , then the new version is\nthe current object state, while if it is ABORTED , then the old version is current. If\nthe owner is ACTIVE , there is no current version, and the future current version\ndepends on whether the owner commits or aborts.\nWhen a transaction starts, it creates a Transaction object to hold the trans-\naction\u2019s status, initially ACTIVE . If that transaction commits, it sets the status to\nCOMMITTED , and if it is aborted by another transaction, the other transaction sets\nthe status to ABORTED .\nEach time transaction Aaccesses an object it \ufb01rst opens that object, possibly\nresetting the owner, old value, and new value \ufb01elds. Let Bbe the object\u2019s", "doc_id": "cf6f6a6d-288c-46f3-888f-da68f10551b7", "embedding": null, "doc_hash": "b349fb6bda41a2b07d2de71fa44540be47de19beeb245971f2db152121f3a0b4", "extra_info": null, "node_info": {"start": 1096275, "end": 1099448}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "efacd29a-de74-4bd6-abf5-26d7bef9a05c", "3": "ffdc80bd-1dd3-451b-8b1b-71945852fb82"}}, "__type__": "1"}, "ffdc80bd-1dd3-451b-8b1b-71945852fb82": {"__data__": {"text": "is ACTIVE , there is no current version, and the future current version\ndepends on whether the owner commits or aborts.\nWhen a transaction starts, it creates a Transaction object to hold the trans-\naction\u2019s status, initially ACTIVE . If that transaction commits, it sets the status to\nCOMMITTED , and if it is aborted by another transaction, the other transaction sets\nthe status to ABORTED .\nEach time transaction Aaccesses an object it \ufb01rst opens that object, possibly\nresetting the owner, old value, and new value \ufb01elds. Let Bbe the object\u2019s prior\nowner.\n1.IfBwasCOMMITTED , then the new version is current. Ainstalls itself as the\nobject\u2019s current owner, sets the old version to the prior new version, and the\nnew version to a copy of the prior new version (if the call is a setter), or to\nthe new version itself (if the call is a getter).\n2.Symmetrically, if BwasABORTED , then the old version is current. Ainstalls\nitself as the object\u2019s current owner, sets the old version to the prior old version,\nand the new version to a copy of the prior old version (if the call is a setter),\nor to the old version itself (if the call is a getter).\n3.IfBis still ACTIVE , thenAandBcon\ufb02ict, soAconsults the contention man-\nager for advice whether to abort B, or to pause, giving Ba chance to \ufb01nish.\nOne transaction aborts another by successfully calling compareAndSet() to\nchange the victim\u2019s status to ABORTED .\nWe leave it to the readers to extend this algorithm to allow concurrent readers.\nAfter opening the object, the getter reads the version\u2019s \ufb01eld into a local vari-\nable. Before returning that value, it calls validate () to check that the calling\ntransaction has not been aborted. If all is well, it returns the \ufb01eld value to the\ncaller. (Setters work symmetrically).\nWhen it is time for Ato commit, it calls compareAndSet() to change its\nstatus to COMMITTED . If it succeeds, the commit is complete. The next transac-\ntion to access an object owned by Awill observe that Ahas committed, and\nwill treat the object\u2019s new version (the one installed by A) as current. If it fails,\nit has been aborted by another transaction. The next transaction to access an\nobject updated by Awill observe that Ahas been aborted, and will treat the\nobject\u2019s old version (the one prior to A) as current. Fig. 18.21 shows an example\nexecution.\n436 Chapter 18 Transactional Memory\nAtomic Memory\n(Objects)\nsequentialobject sequentialobject\nstatuslocal transaction\nof thread A\nlocal transaction\nof thread Bactivelocator\nlocator\nstatuscommittedold new writer\nold new writer\nold new writerlocator\nFigure 18.21 The FreeObject class: an obstruction-free atomic object implementation. Thread Ahas com-\npleted the writing of one object and is in the process of switching a copy of the second object that was last\nwritten by thread B. It prepares a new locator with a fresh new copy of the object and an old object \ufb01eld\nthat refers to the new \ufb01eld of thread B\u2019s locator. It then uses a compareAndSet() to switch the object to\nrefer to the newly created locator.\nWhy It Works\nHere is why every transaction observes a consistent state. When a transaction A\ncalls a getter method to read an object \ufb01eld, it opens the object, installing itself\nas the object\u2019s owner. If the object already had an active owner, B, thenAaborts\nB.Athen reads the \ufb01eld value into a local variable. Before the getter returns that\nvalue to the application,", "doc_id": "ffdc80bd-1dd3-451b-8b1b-71945852fb82", "embedding": null, "doc_hash": "c1247d00e9a6172814b1d58661e115b3d97647bab939257b0271c1f0e8356bd9", "extra_info": null, "node_info": {"start": 1099330, "end": 1102733}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "cf6f6a6d-288c-46f3-888f-da68f10551b7", "3": "6092ed07-ef7c-46c1-a5ff-e8e3e092ef07"}}, "__type__": "1"}, "6092ed07-ef7c-46c1-a5ff-e8e3e092ef07": {"__data__": {"text": "to the new \ufb01eld of thread B\u2019s locator. It then uses a compareAndSet() to switch the object to\nrefer to the newly created locator.\nWhy It Works\nHere is why every transaction observes a consistent state. When a transaction A\ncalls a getter method to read an object \ufb01eld, it opens the object, installing itself\nas the object\u2019s owner. If the object already had an active owner, B, thenAaborts\nB.Athen reads the \ufb01eld value into a local variable. Before the getter returns that\nvalue to the application, however, it calls validate () to check that the value is\nconsistent. If another transaction CdisplacedAas owner of any object, then C\nabortedA, andA\u2019s validation fails. It follows that if a setter returns a value, that\nvalue is consistent.\nHere is why transactions are serializable. If a transaction Asuccessfully\nchanges its status from ACTIVE toCOMMITTED , then it must still be owner of all\nthe objects it accessed, because any transaction that usurps A\u2019s ownership must\nabortA\ufb01rst. It follows that none of the objects it read or wrote have changed\n18.3 Software Transactional Memory 437\nsinceAaccessed them, so Ais effectively updating a snapshot of the objects it\naccessed.\nIn Detail\nOpening an object requires changing multiple \ufb01elds atomically, modifying the\nowner, old version, and new version \ufb01elds. Without locks, the only way to accom-\nplish this atomic multi-\ufb01eld update is to introduce a level of indirection. As\nshown in Fig. 18.22 , the FreeObject class has a single \ufb01eld, start , that is an\nAtomicReference to aLocator object (Line 3), which holds the object\u2019s current\ntransaction, old version, and new version (Lines 5\u20137).\nRecall that an object \ufb01eld is modi\ufb01ed in the following steps: (1) call\nopenWrite () to acquire an object version, (2) tentatively modify that version,\nand (3) call validate () to ensure the version is still good. Fig. 18.23 shows the\nFreeObject class\u2019s openWrite () method. First, the thread tests its own transac-\ntional state (Line 14). If that state is committed, then the thread is not running\nin a transaction, and updates the object directly (Line 15). If that state is aborted,\nthen the thread immediately throws an AbortedException exception (Line 16).\nFinally, if the transaction is active, then the thread reads the current locator and\nchecks whether it has already opened this object for writing, returning imme-\ndiately if so (Line 19). Otherwise, it enters a loop (Line 22) where it repeatedly\ninitializes and tries to install a new locator. T o determine the object\u2019s current\nvalue, the thread checks the status of the last transaction to write the object (Line\n25), using the new version if the owner is committed (Line 27) and the old ver-\nsion if it is ABORTED (Line 30). If the owner is still active (Line 30), there there is a\nsynchronization con\ufb02ict, and the thread calls the contention manager module to\nresolve the con\ufb02ict. In the absence of con\ufb02ict, the thread creates and initializes a\nnew version (Lines 37\u201339). Finally, the thread calls compareAndSet() to replace\nthe old locator with the new, returning if it succeeds, and retrying if it fails.\nTheopenRead () method (not shown) works in the same way, except that it\ndoes not need to make a copy of the old version.\nTheFreeObject class\u2019s validate () method (not shown) simply checks that\ncurrent thread\u2019s transaction status is ACTIVE .\n1public class FreeObject<T extends Copyable<T>>\n2", "doc_id": "6092ed07-ef7c-46c1-a5ff-e8e3e092ef07", "embedding": null, "doc_hash": "1d44b420142e9af0aed993aa7a15af7b134b87d6fe4893894e57cfaa493432a2", "extra_info": null, "node_info": {"start": 1102776, "end": 1106176}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ffdc80bd-1dd3-451b-8b1b-71945852fb82", "3": "8e788139-6a2d-445e-a29d-059f0c55c583"}}, "__type__": "1"}, "8e788139-6a2d-445e-a29d-059f0c55c583": {"__data__": {"text": "the absence of con\ufb02ict, the thread creates and initializes a\nnew version (Lines 37\u201339). Finally, the thread calls compareAndSet() to replace\nthe old locator with the new, returning if it succeeds, and retrying if it fails.\nTheopenRead () method (not shown) works in the same way, except that it\ndoes not need to make a copy of the old version.\nTheFreeObject class\u2019s validate () method (not shown) simply checks that\ncurrent thread\u2019s transaction status is ACTIVE .\n1public class FreeObject<T extends Copyable<T>>\n2 extends TinyTM.AtomicObject<T> {\n3 AtomicReference<Locator> start;\n4 private class Locator {\n5 Transaction owner;\n6 T oldVersion;\n7 T newVersion;\n8 ...\n9 }\n10 ...\n11 }\nFigure 18.22 TheFreeObject class: the inner Locator class.\n438 Chapter 18 Transactional Memory\n12 public T openWrite() {\n13 Transaction me = Transaction.getLocal();\n14 switch (me.getStatus()) {\n15 case COMMITTED: return openSequential();\n16 case ABORTED: throw new AbortedException();\n17 case ACTIVE:\n18 Locator locator = start.get();\n19 if(locator.owner == me)\n20 return locator.newVersion;\n21 Locator newLocator = new Locator();\n22 while (!Thread.currentThread().isInterrupted()) {\n23 Locator oldLocator = start.get();\n24 Transaction owner = oldLocator.owner;\n25 switch (owner.getStatus()) {\n26 case COMMITTED:\n27 newLocator.oldVersion = oldLocator.newVersion;\n28 break ;\n29 case ABORTED:\n30 newLocator.oldVersion = oldLocator.oldVersion;\n31 break ;\n32 case ACTIVE:\n33 ContentionManager.getLocal().resolve(me, owner);\n34 continue ;\n35 }\n36 try {\n37 newLocator.newVersion = (T) _class.newInstance();\n38 }catch (Exception ex) { throw new PanicException(ex);}\n39 newLocator.oldVersion.copyTo(newLocator.newVersion);;\n40 if(start.compareAndSet(oldLocator, newLocator))\n41 return newLocator.newVersion;\n42 }\n43 me.abort();\n44 throw new AbortedException();\n45 default :throw new PanicException(\"Unexpected transaction state\");\n46 }\n47 }\nFigure 18.23 TheFreeObject class: the openWrite ()method.\n18.3.8 A Lock-Based Atomic Object\nThe obstruction-free implementation is somewhat inef\ufb01cient because writes\ncontinually allocate locators and versions and reads must go through two levels\nof indirection (two references) to reach the actual data to be read. In this section,\nwe present a more ef\ufb01cient atomic object implementation that uses short critical\nsections to eliminate the need for a locator and to remove a level of indirection.\nA lock-based STM could lock each object as it is read or written. Many\napplications, however, follow the 80/20 rule: roughly 80% of accesses are reads\nand roughly 20% are writes. Locking an object is expensive, since it requires\nacompareAndSet() call, which seems excessive when read/write con\ufb02icts are\n18.3 Software Transactional Memory 439\nexpected to be infrequent. Is it really necessary to lock objects for reading? The\nanswer is no.\nBird\u2019s-Eye View\nThe lock-based atomic object implementation reads objects optimistically, and\nlater checks for con\ufb02icts. It detects con\ufb02icts using a global version clock , a\ncounter shared by all transactions and incremented each time a transaction com-\nmits. When a transaction starts, it records the current version clock value in a\nthread-local read stamp .\nEach object has the following \ufb01elds: the stamp \ufb01eld is the read stamp of the last\ntransaction to write", "doc_id": "8e788139-6a2d-445e-a29d-059f0c55c583", "embedding": null, "doc_hash": "f22fc620e8c14864a5d2fc3fb90b047d5801deb5e7cfb7a5e793363c66d2e486", "extra_info": null, "node_info": {"start": 1106156, "end": 1109469}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6092ed07-ef7c-46c1-a5ff-e8e3e092ef07", "3": "b8de9fbc-f754-46db-8d6b-36f61a335b39"}}, "__type__": "1"}, "b8de9fbc-f754-46db-8d6b-36f61a335b39": {"__data__": {"text": "be infrequent. Is it really necessary to lock objects for reading? The\nanswer is no.\nBird\u2019s-Eye View\nThe lock-based atomic object implementation reads objects optimistically, and\nlater checks for con\ufb02icts. It detects con\ufb02icts using a global version clock , a\ncounter shared by all transactions and incremented each time a transaction com-\nmits. When a transaction starts, it records the current version clock value in a\nthread-local read stamp .\nEach object has the following \ufb01elds: the stamp \ufb01eld is the read stamp of the last\ntransaction to write to that object, the version \ufb01eld is an instance of the sequential\nobject, and the lock \ufb01eld is a lock. As explained earlier, the sequential type must\nimplement the Copyable interface, and provide a no-argument constructor.\nThe transaction virtually executes a sequence of read and write accesses to\nobjects. By \u201cvirtually\u201d , we mean that no objects are actually modi\ufb01ed. Instead,\nthe transaction uses a thread-local read set to keep track of the objects it has read,\nand a thread-local write set to keep track of the objects it intends to modify, and\ntheir tentative new versions.\nWhen a transaction calls a getter to return a \ufb01eld value, the LockObject \u2019s\nopenRead () method \ufb01rst checks whether the object already appears in the write\nset. If so, it returns the tentative new version. Otherwise, it checks whether the\nobject is locked. If so, there is a synchronization con\ufb02ict, and the transaction\naborts. If not, openRead () adds the object to the read set and returns its version.\nTheopenWrite () method is similar. If the object is not found in the write set,\nthe method creates a new, tentative version, adds the tentative version to the write\nset, and returns that version.\nThevalidate () method checks that the object\u2019s stamp is not greater than the\ntransaction\u2019s read stamp. If so, a con\ufb02ict exists, and the transaction aborts. If not,\nthe getter returns the value read in the previous step.\nIt is important to understand that the LockObject \u2019svalidate () method\nguarantees only that the value is consistent. It does not guarantee that the caller is\nnot a zombie transaction. Instead, a transaction must take the following steps to\ncommit.\n1.It locks each of the objects in its write set, in any convenient order, using time-\nouts to avoid deadlock.\n2.It uses compareAndSet() to increment the global version clock, storing the\nresult in a thread-local write stamp. If the transaction commits, this is the\npoint where it is serialized.\n3.The transaction checks that each object in its read set is not locked by another\nthread, and that each object\u2019s stamp is not greater than the transaction\u2019s read\nstamp. If this validation succeeds, the transaction commits. (In the special case\nwhere the transaction\u2019s write stamp is one more than its read stamp, there is\n440 Chapter 18 Transactional Memory\nno need to validate the read set, because no concurrent modi\ufb01cation could\nhave happened.)\n4.The transaction updates the stamp \ufb01eld of each object in its write set. Once\nthe stamps are updated, the transaction releases its locks.\nIf any of these tests fails, the transaction aborts, discards its read and write sets,\nand releases any locks it is holding.\nFig. 18.24 shows an example execution.\nWhy It Works\nTransactions are serializable in the order they increment the global version clock.\nHere is why every transaction observes a consistent state. If A, with read stamp r,\nobserves that the object is not locked, then this version will have the latest stamp\nnot exceeding r. Any transaction that", "doc_id": "b8de9fbc-f754-46db-8d6b-36f61a335b39", "embedding": null, "doc_hash": "0e054e1b15126065bc79d2f5f87c62b3ec57b07a2ad6274b966202cdcd288b59", "extra_info": null, "node_info": {"start": 1109440, "end": 1112988}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8e788139-6a2d-445e-a29d-059f0c55c583", "3": "6bbe41af-9966-41e5-b6b1-fc7eefb7e8fd"}}, "__type__": "1"}, "6bbe41af-9966-41e5-b6b1-fc7eefb7e8fd": {"__data__": {"text": "of each object in its write set. Once\nthe stamps are updated, the transaction releases its locks.\nIf any of these tests fails, the transaction aborts, discards its read and write sets,\nand releases any locks it is holding.\nFig. 18.24 shows an example execution.\nWhy It Works\nTransactions are serializable in the order they increment the global version clock.\nHere is why every transaction observes a consistent state. If A, with read stamp r,\nobserves that the object is not locked, then this version will have the latest stamp\nnot exceeding r. Any transaction that modi\ufb01es the object at a later time locks the\nobject, increments the global version clock, and sets that object\u2019s stamp to the\nnew version clock value, which exceeds r. IfAobserves the object is not locked,\nthenAcannot miss an update with a stamp less than or equal to r.Aalso checks\nAtomic\nMemory(Objects)Atomic\nMemory(Objects)\nlocks85\n34828744500\n00000global\nversionclockglobal\nversionclock\n97\nB\u2019s local\nstampsB\u2019s local\nstampsA\u2019s local\nstampsA\u2019s local\nstamps\nread\nstampread\nstamp\nwrite\nstampwrite\nstamp(a)\n97 90\n9898\na\nbcdef98\nstamps values locks stamps valuesc\u00b485\n98\n87\n500\n1001098\n(b)\n98 90\n98121\na\nb\nc\u00b4\nd\nef99\n  34\n4499121\nA aborts A commitsb\u00b4\ne\u00b4\nFigure 18.24 TheLockObject class: A lock-based transactional memory implementation. In Part (a) thread\nAstarts its transaction, setting its read stamp rs to 97, the global version clock value. Before Astarts reading\nand writing objects, thread Bcommits: it increments the global version clock to 98, records 98 in its local\nwrite stamp ws \ufb01eld, and after a successful validation writes a new value c0with stamp 98. (B\u2019s acquisition\nand release of the object locks is not shown.) When Areads the object with stamp 98, it detects thread\nB\u2019s modi\ufb01cation because its read stamp is less than 98, so Aaborts. In Part (b) on the other hand, Astarts\nits transaction after Bcompleted, and reads a read stamp value of 98, and does not abort when reading c0.\nAcreates read\u2013write sets, and increments the global version clock. (Notice that other threads have incre-\nmented the clock to 120.). It locks the objects it intends to modify, and successfully validates. It then updates\nthe values and stamps of these objects based on the write stamp value. In the \ufb01gure, we do not show A\u2019s\n\ufb01nal release of the locks on the written objects.\n18.3 Software Transactional Memory 441\nthat the object\u2019s stamp does not exceed rby reading and testing the stamp after it\nreads the \ufb01eld.\nHere is why transactions are serializable. We claim that if AreadsxandA\nlater commits, then xcould not have changed between the time A\ufb01rst readsx\nand the time Aincrements the global version clock. As noted earlier, if Awith\nread stamprobservesxis unlocked at time t, then any subsequent modi\ufb01cations\ntoxwill givexa stamp larger than r. If a transaction Bcommits before A, and\nmodi\ufb01es an object read by A, thenA\u2019s validation handler will either observe that\nxis locked byB, or thatx\u2019s stamp is greater than r, and will abort either way.\nIn Detail\nBefore we describe the algorithm, we describe the basic data structures.\nFig. 18.25 shows the WriteSet class used by the locking implementation. This\nclass is essentially a map from objects to versions, sending each object written by\n1public class WriteSet", "doc_id": "6bbe41af-9966-41e5-b6b1-fc7eefb7e8fd", "embedding": null, "doc_hash": "c877ef7359285b5493eaa9a5a91c4aecd1290749a777dd2cffde52b61d93f1dc", "extra_info": null, "node_info": {"start": 1112979, "end": 1116258}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b8de9fbc-f754-46db-8d6b-36f61a335b39", "3": "8a5464cb-d78a-4982-a225-e2818fa56c5c"}}, "__type__": "1"}, "8a5464cb-d78a-4982-a225-e2818fa56c5c": {"__data__": {"text": "givexa stamp larger than r. If a transaction Bcommits before A, and\nmodi\ufb01es an object read by A, thenA\u2019s validation handler will either observe that\nxis locked byB, or thatx\u2019s stamp is greater than r, and will abort either way.\nIn Detail\nBefore we describe the algorithm, we describe the basic data structures.\nFig. 18.25 shows the WriteSet class used by the locking implementation. This\nclass is essentially a map from objects to versions, sending each object written by\n1public class WriteSet {\n2 static ThreadLocal<Map<LockObject<?>, Object>> map\n3 =new ThreadLocal<Map<LockObject<?>,Object>>() {\n4 protected synchronized Map<LockObject<?>, Object> initialValue() {\n5 return new HashMap();\n6 }\n7 };\n8 public static Object get(LockObject<?> x) {\n9 return map.get().get(x);\n10 }\n11 public static void put(LockObject<?> x, Object y) {\n12 map.get().put(x, y);\n13 }\n14 public static boolean tryLock( long timeout, TimeUnit timeUnit) {\n15 Stack<LockObject<?>> stack = new Stack<LockObject<?>>();\n16 for (LockObject<?> x : map.get().keySet()) {\n17 if(!x.tryLock(timeout, timeUnit)) {\n18 for (LockObject<?> y : stack) {\n19 y.unlock();\n20 }\n21 throw new AbortedException();\n22 }\n23 }\n24 return true ;\n25 }\n26 public static void unlock() {\n27 for (LockObject<?> x : map.get().keySet()) {\n28 x.unlock();\n29 }\n30 }\n31 ...\n32 }\nFigure 18.25 TheLockObject class: the inner WriteSet class.\n442 Chapter 18 Transactional Memory\nthe transaction to its tentative new version. In addition to the get() and set()\nmethods, the class also includes methods to lock and unlock each object in the\ntable. The ReadSet class (not shown) is just a set of objects.\nFig. 18.26 shows the version clock. All \ufb01elds and methods are static. The\nclass manages a singe global version counter, and a set of thread-local read\nstamps. The getWriteStamp () method returns the current global version and\nsetWriteStamp () advances it by one. The getReadStamp () method returns the\ncaller\u2019s thread-local read stamp, and setReadStamp () sets the thread-local read\nstamp to the current global clock value.\nTheLockObject class ( Fig. 18.27 ) has three \ufb01elds: the object\u2019s lock, its read\nstamp, and the object\u2019s actual data. Fig. 18.28 shows how to open an object for\nreading. If a copy of the object is not already in the transaction\u2019s write set (Line\n13), then it places the object in the transaction\u2019s read set. If, however, the object\nis locked, then it is in the middle of an update by a concurrent transaction, and\n1public class VersionClock {\n2 // global clock read and advanced by all\n3 static AtomicLong global = new AtomicLong();\n4 // thread-local cached copy of global clock\n5 static ThreadLocal<Long> local = new ThreadLocal<Long>() {\n6 protected Long initialValue() {\n7 return 0L;\n8 }\n9 };\n10 public static void setReadStamp() {\n11 local.set(global.get());\n12 }\n13 public static long getReadStamp() {\n14 return local.get();\n15 }\n16 public static void setWriteStamp() {\n17 local.set(global.incrementAndGet());\n18 }\n19 public static long getWriteStamp() {\n20 return local.get();\n21 }\n22 }\nFigure 18.26 TheVersionClock class.\n1public class LockObject<T extends Copyable<T>> extends AtomicObject<T> {\n2 ReentrantLock lock;\n3", "doc_id": "8a5464cb-d78a-4982-a225-e2818fa56c5c", "embedding": null, "doc_hash": "c34dd7c78d2d87d3931f417c4208e3b285a282fae3389eccff444afebb8b7d48", "extra_info": null, "node_info": {"start": 1116318, "end": 1119504}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "6bbe41af-9966-41e5-b6b1-fc7eefb7e8fd", "3": "5d8ae302-8b4d-4778-bf93-9bea81aabf55"}}, "__type__": "1"}, "5d8ae302-8b4d-4778-bf93-9bea81aabf55": {"__data__": {"text": "0L;\n8 }\n9 };\n10 public static void setReadStamp() {\n11 local.set(global.get());\n12 }\n13 public static long getReadStamp() {\n14 return local.get();\n15 }\n16 public static void setWriteStamp() {\n17 local.set(global.incrementAndGet());\n18 }\n19 public static long getWriteStamp() {\n20 return local.get();\n21 }\n22 }\nFigure 18.26 TheVersionClock class.\n1public class LockObject<T extends Copyable<T>> extends AtomicObject<T> {\n2 ReentrantLock lock;\n3 volatile long stamp;\n4 T version;\n5 ...\nFigure 18.27 TheLockObject class: \ufb01elds.\n18.3 Software Transactional Memory 443\n6 public T openRead() {\n7 ReadSet readSet = ReadSet.getLocal();\n8 switch (Transaction.getLocal().getStatus()) {\n9 case COMMITTED:\n10 return version;\n11 case ACTIVE:\n12 WriteSet writeSet = WriteSet.getLocal();\n13 if(writeSet.get( this ) == null ) {\n14 if(lock.isLocked()) {\n15 throw new AbortedException();\n16 }\n17 readSet.add( this );\n18 return version;\n19 }else {\n20 T scratch = (T)writeSet.get( this );\n21 return scratch;\n22 }\n23 case ABORTED:\n24 throw new AbortedException();\n25 default :\n26 throw new PanicException(\"unexpected transaction state\");\n27 }\n28 }\nFigure 18.28 TheLockObject class: the openRead ()method.\nthe reader aborts (Line 15). If the object has a tentative version in the write set, it\nreturns that version (Line 19).\nFig. 18.29 shows the LockObject class\u2019s openWrite () method. If the call\noccurs outside a transaction (Line 31), it simply returns the object\u2019s current ver-\nsion. If the transaction is active (Line 33), it tests whether the object is in its\nwrite set (Line 35). If so, it returns that version. If not, the caller aborts if the\nobject is locked (Line 37). Otherwise it creates a new, tentative version using the\ntype\u2019s no-argument constructor (Line 39), initializes it by copying the old version\n(Line 40), puts it in the write set (Line 41), and returns the tentative version.\nThevalidate () method simply checks whether the object\u2019s read stamp is less\nthan or equal to the transaction\u2019s read stamp (Line 56).\nWe now look at how a transaction commits. Recall that TinyTM allows users\nto register handlers to be executed on validation, commit, and abort. Fig. 18.31\nshows how the locking TM validates transactions. It \ufb01rst locks each object in the\nwrite set (Line 66). If the lock acquisition times out, there may be a deadlock,\nso the method returns false, meaning the transaction should not commit. It then\nvalidates the read set. For each object, it checks that it is not locked by another\ntransaction (Line 70) and that the object\u2019s stamp does not exceed the transaction\u2019s\nread stamp (Line 72).\nIf validation succeeds, the transaction may now commit. Fig. 18.32 shows the\nonCommit () handler. It increments the version clock (Line 83), copies the ten-\ntative versions from the write set back to the original objects (Lines 86\u201389) and\n444 Chapter 18 Transactional Memory\n29 public T openWrite() {\n30 switch (Transaction.getLocal().getStatus()) {\n31 case COMMITTED:\n32 return version;\n33 case ACTIVE:\n34 WriteSet writeSet = WriteSet.getLocal();\n35 T scratch = (T) writeSet.get( this );\n36 if(scratch == null ) {\n37 if(lock.isLocked())\n38 throw new AbortedException();\n39 scratch = myClass.newInstance();\n40", "doc_id": "5d8ae302-8b4d-4778-bf93-9bea81aabf55", "embedding": null, "doc_hash": "777a03c574661e044ddd59dcee1a05bf745e005a9b352f6a65fc7d230d48ba6a", "extra_info": null, "node_info": {"start": 1119527, "end": 1122738}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8a5464cb-d78a-4982-a225-e2818fa56c5c", "3": "4876e045-a56f-443d-a39b-b4ca4adbcf06"}}, "__type__": "1"}, "4876e045-a56f-443d-a39b-b4ca4adbcf06": {"__data__": {"text": "clock (Line 83), copies the ten-\ntative versions from the write set back to the original objects (Lines 86\u201389) and\n444 Chapter 18 Transactional Memory\n29 public T openWrite() {\n30 switch (Transaction.getLocal().getStatus()) {\n31 case COMMITTED:\n32 return version;\n33 case ACTIVE:\n34 WriteSet writeSet = WriteSet.getLocal();\n35 T scratch = (T) writeSet.get( this );\n36 if(scratch == null ) {\n37 if(lock.isLocked())\n38 throw new AbortedException();\n39 scratch = myClass.newInstance();\n40 version.copyTo(scratch);\n41 writeSet.put( this , scratch);\n42 }\n43 return scratch;\n44 case ABORTED:\n45 throw new AbortedException();\n46 default :\n47 throw new PanicException(\"unexpected transaction state\");\n48 }\n49 }\nFigure 18.29 TheLockObject class: the openWrite ()method.\n50 public boolean validate() {\n51 Transaction.Status status = Transaction.getLocal().getStatus();\n52 switch (status) {\n53 case COMMITTED:\n54 return true ;\n55 case ACTIVE:\n56 return stamp <= VersionClock.getReadStamp(); ;\n57 case ABORTED:\n58 return false ;\n59 }\n60 }\n61 }\nFigure 18.30 TheLockObject class: the validate ()method.\nsets each object\u2019s stamp to the newly incremented version clock value (Line 90).\nFinally, it releases the locks, and clears the thread-local read\u2013write sets for the\nnext transaction.\nWhat have we learned so far? We have seen how a single transactional mem-\nory framework can support two substantially different kinds of synchronization\nmechanisms: one obstruction-free, and one employing short-lived locking. Each\nof these implementations, by itself, provides weak progress guarantees, so we rely\non a separate contention manager to ensure progress.\n18.4 Hardware Transactional Memory 445\n62 public class OnValidate implements Callable<Boolean>{\n63 public Boolean call() throws Exception {\n64 WriteSet writeSet = WriteSet.getLocal();\n65 ReadSet readSet = ReadSet.getLocal();\n66 if(!writeSet.tryLock(TIMEOUT, TimeUnit.MILLISECONDS)) {\n67 return false ;\n68 }\n69 for (LockObject x : readSet) {\n70 if(x.lock.isLocked() && !x.lock.isHeldByCurrentThread())\n71 return false ;\n72 if(stamp > VersionClock.getReadStamp()) {\n73 return false ;\n74 }\n75 }\n76 return true ;\n77 }\n78 }\nFigure 18.31 TheLockObject class: the onValidate ()hander.\n79 public class OnCommit implements Runnable {\n80 public void run() {\n81 WriteSet writeSet = WriteSet.getLocal();\n82 ReadSet readSet = ReadSet.getLocal();\n83 VersionClock.setWriteStamp();\n84 long writeVersion = VersionClock.getWriteStamp();\n85 for (Map.Entry<LockObject<?>, Object> entry : writeSet) {\n86 LockObject<?> key = (LockObject<?>) entry.getKey();\n87 Copyable destin = (Copyable) key.openRead();\n88 Copyable source = (Copyable) entry.getValue();\n89 source.copyTo(destin);\n90 key.stamp = writeVersion;\n91 }\n92 writeSet.unlock();\n93 writeSet.clear();\n94 readSet.clear();\n95 }\n96 }\nFigure 18.32 TheLockObject class: the onCommit ()handler.\n18.4 Hardware Transactional Memory\nWe now describe how a standard hardware architecture can be augmented\nto support short, small transactions directly in hardware. The HTM design\npresented here is high-level and simpli\ufb01ed, but it covers the principal aspects\n446 Chapter 18 Transactional Memory\nof HTM design. Readers unfamiliar with cache", "doc_id": "4876e045-a56f-443d-a39b-b4ca4adbcf06", "embedding": null, "doc_hash": "6ef386b46c7a3dfe68ea9f48f337d4f04bd7a760fdaf2b25b274c7bfecede414", "extra_info": null, "node_info": {"start": 1122710, "end": 1125912}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "5d8ae302-8b4d-4778-bf93-9bea81aabf55", "3": "665a6a5a-bf7a-440f-90de-53809621086a"}}, "__type__": "1"}, "665a6a5a-bf7a-440f-90de-53809621086a": {"__data__": {"text": "key.stamp = writeVersion;\n91 }\n92 writeSet.unlock();\n93 writeSet.clear();\n94 readSet.clear();\n95 }\n96 }\nFigure 18.32 TheLockObject class: the onCommit ()handler.\n18.4 Hardware Transactional Memory\nWe now describe how a standard hardware architecture can be augmented\nto support short, small transactions directly in hardware. The HTM design\npresented here is high-level and simpli\ufb01ed, but it covers the principal aspects\n446 Chapter 18 Transactional Memory\nof HTM design. Readers unfamiliar with cache coherence protocols may consult\nAppendix B.\nThe basic idea behind HTM is that modern cache-coherence protocols already\ndo most of what we need to do to implement transactions. They already detect\nand resolve synchronization con\ufb02icts between writers, and between readers and\nwriters, and they already buffer tentative changes instead of updating memory\ndirectly. We need change only a few details.\n18.4.1 Cache Coherence\nIn most modern multiprocessors, each processor has an attached cache , a small,\nhigh-speed memory used to avoid communicating with large and slow main\nmemory. Each cache entry holds a group of neighboring words called a line, and\nhas some way of mapping addresses to lines. Consider a simple architecture in\nwhich processors and memory communicate over a shared broadcast medium\ncalled a bus. Each cache line has a tag, which encodes state information. We start\nwith the standard MESI protocol, in which each cache line is marked with one of\nthe following states:\n\u0004Modi\ufb01ed : the line in the cache has been modi\ufb01ed, and must eventually be\nwritten back to memory. No other processor has this line cached.\n\u0004Exclusive : the line has not been modi\ufb01ed, but no other processor has this line\ncached. (A line is typically loaded in exclusive mode before being modi\ufb01ed.)\n\u0004Shared : the line has not been modi\ufb01ed, and other processors may have this\nline cached.\n\u0004Invalid : the line does not contain meaningful data.\nThe cache coherence protocol detects synchronization con\ufb02icts among indivi-\ndual loads and stores, and ensures that different processors agree on the state\nof the shared memory. When a processor loads or stores a memory address a, it\nbroadcasts the request on the bus, and the other processors and memory listen in\n(sometimes called snooping ).\nA full description of a cache coherence protocol can be complex, but here are\nthe principal transitions of interest to us.\n\u0004When a processor requests to load a line in exclusive mode, the other proces-\nsors invalidate any copies of that line. Any processor with a modi\ufb01ed copy\nof that line must write the line back to memory before the load can be\nful\ufb01lled.\n\u0004When a processor requests to load a line into its cache in shared mode, any\nprocessor with an exclusive copy must change its state to shared, and any pro-\ncessor with a modi\ufb01ed copy must write that line back to memory before the\nload can be ful\ufb01lled.\n18.4 Hardware Transactional Memory 447\n\u0004If the cache becomes full, it may be necessary to evict a line. If the line is shared\nor exclusive, it can simply be discarded, but if it is modi\ufb01ed, it must be written\nback to memory.\nWe now show how to adapt this protocol to support transactions.\n18.4.2 Transactional Cache Coherence\nWe keep the same MESI protocol as before, except that we add a transactional\nbit to each cache line\u2019s tag. Normally, this bit is unset. When a value is placed\nin the cache line on behalf of a transaction, this bit is set, and we say the entry\nistransactional . We only need to ensure that modi\ufb01ed transactional lines cannot\nbe written back to memory, and that", "doc_id": "665a6a5a-bf7a-440f-90de-53809621086a", "embedding": null, "doc_hash": "afa85d705e42502cb42bf104bf4702a4933a577e44c77b022e9a8791f81d1f19", "extra_info": null, "node_info": {"start": 1125897, "end": 1129463}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "4876e045-a56f-443d-a39b-b4ca4adbcf06", "3": "b828656c-52d1-40ce-8cb5-848988876d1e"}}, "__type__": "1"}, "b828656c-52d1-40ce-8cb5-848988876d1e": {"__data__": {"text": "but if it is modi\ufb01ed, it must be written\nback to memory.\nWe now show how to adapt this protocol to support transactions.\n18.4.2 Transactional Cache Coherence\nWe keep the same MESI protocol as before, except that we add a transactional\nbit to each cache line\u2019s tag. Normally, this bit is unset. When a value is placed\nin the cache line on behalf of a transaction, this bit is set, and we say the entry\nistransactional . We only need to ensure that modi\ufb01ed transactional lines cannot\nbe written back to memory, and that invalidating a transactional line aborts the\ntransaction.\nHere are the rules in more detail.\n\u0004If the MESI protocol invalidates a transactional entry, then that transaction\nis aborted. Such an invalidation represents a synchronization con\ufb02ict, either\nbetween two stores, or a load and a store.\n\u0004If a modi\ufb01ed transactional line is invalidated or evicted, its value is discarded\ninstead of being written to memory. Because any transactionally written value\nis tentative, we cannot let it \u201cescape\u201d while the transaction is active. Instead,\nwe must abort the transaction.\n\u0004If the cache evicts a transactional line, then that transaction must be aborted,\nbecause once the line is no longer in the cache, then the cache-coherence pro-\ntocol cannot detect synchronization con\ufb02icts.\nIf, when a transaction \ufb01nishes, none of its transactional lines have been invali-\ndated or evicted, then it can commit, clearing the transactional bits in its cache\nlines. If an invalidation or eviction causes the transaction to abort, its transac-\ntional cache lines are invalidated. These rules ensure that commit and abort are\nprocessor-local steps.\n18.4.3 Enhancements\nAlthough the scheme correctly implements a transactional memory in hardware,\nit has a number of \ufb02aws and limitations. One limitation, common to nearly all\nHTM proposals, is that the size of the transaction is limited by the size of the\ncache. Most operating systems clean out the cache when a thread is descheduled,\nso the duration of the transaction may be limited by the length of the platform\u2019s\nscheduling quantum. It follows that HTM is best suited for short, small transac-\ntions. Applications that need longer transactions should use STM, or a combina-\ntion of HTM and STM. When a transaction aborts, however, it is important that\nthe hardware return a condition code indicating whether the abort was due to a\n448 Chapter 18 Transactional Memory\nsynchronization con\ufb02ict (so the transaction should be retried), or whether it was\ndue to resource exhaustion (so there is no point in retrying the transaction).\nThis particular design, however, has some additional drawbacks. Many caches\naredirect-mapped , meaning that an address amaps to exactly one cache line. Any\ntransaction that accesses two addresses that map to the same cache line is doomed\nto fail, because the second access will evict the \ufb01rst, aborting the transaction. Some\ncaches are set-associative , mapping each address to a set of klines. Any transaction\nthat accesses k+ 1 addresses that map to the same set is doomed to fail. Few caches\narefully-associative , mapping each address to any line in the cache.\nThere are several ways to alleviate this problem by splitting the cache. One is to\nsplit the cache into a large, direct-mapped main cache and a small fully associated\nvictim cache used to hold entries that over\ufb02ow from the main cache. Another\nis to split the cache into a large set-associated non-transactional cache, and a\nsmall fully-associative transactional cache for transactional lines. Either way, the\ncache coherence protocol", "doc_id": "b828656c-52d1-40ce-8cb5-848988876d1e", "embedding": null, "doc_hash": "ed948133480bd2704f3d1900848ebbdc699a539dccaf803db8e33c93991b12bb", "extra_info": null, "node_info": {"start": 1129475, "end": 1133051}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "665a6a5a-bf7a-440f-90de-53809621086a", "3": "9d42f92d-ac94-41e0-a4a7-3c801a389ab2"}}, "__type__": "1"}, "9d42f92d-ac94-41e0-a4a7-3c801a389ab2": {"__data__": {"text": "k+ 1 addresses that map to the same set is doomed to fail. Few caches\narefully-associative , mapping each address to any line in the cache.\nThere are several ways to alleviate this problem by splitting the cache. One is to\nsplit the cache into a large, direct-mapped main cache and a small fully associated\nvictim cache used to hold entries that over\ufb02ow from the main cache. Another\nis to split the cache into a large set-associated non-transactional cache, and a\nsmall fully-associative transactional cache for transactional lines. Either way, the\ncache coherence protocol must be adapted to handle coherence between the split\ncaches.\nAnother \ufb02aw is the absence of contention management, which means that\ntransactions can starve one another. Transaction Aloads address ain exclusive\nmode, then transaction Bloadsain exclusive mode, aborting A.Aimmediately\nrestarts, aborting B, and so on. This problem could be addressed at the level of\nthe coherence protocol, allowing a processor to refuse or delay an invalidation\nrequest, or it could be addressed at the software level, perhaps by having aborted\ntransactions execute exponential backoff in software.\nReaders interested in addressing these issues in depth may consult the chapter\nnotes.\n18.5 Chapter Notes\nMaurice Herlihy and Eliot Moss [67] were the \ufb01rst to propose hardware trans-\nactional memory as a general-purpose programming model for multiproces-\nsors. Nir Shavit and Dan T ouitou [142] proposed the \ufb01rst software transactional\nmemory. The retry andorElse constructs are credited to Tim Harris, Simon\nMarlowe, Simon Peyton-Jones, and Maurice Herlihy [54]. Many papers, both\nearlier and later, have contributed to this area. Larus and Rajwar [97] provide the\nauthoritative survey of both the technical issues and the literature.\nThe Karma contention manager is taken from William Scherer and Michael\nScott [137], and the Greedy contention manager from Rachid Guerraoui,\nMaurice Herlihy, and Bastian Pochon [49]. The obstruction-free STM is based\non the Dynamic Software Transactional Memory algorithm of Maurice Herlihy,\nVictor Luchangco, Mark Moir, and Bill Scherer [66]. The lock-based STM is\nbased on the Transactional Locking 2 algorithm of Dave Dice, Ori Shalev, and\nNir Shavit [32].\n18.6 Exercises 449\n18.6 Exercises\nExercise 211. Implement the Priority ,Greedy , and Karma contention managers.\nExercise 212. Describe the meaning of orElse without mentioning transaction\nroll-back.\nExercise 213. InTinyTM , implement the openRead () method of the FreeObject\nclass. Notice that the order in which the Locator \ufb01elds are read is important.\nArgue why your implementation provides a serializable read of an object.\nExercise 214. Invent a way to reduce the contention on the global version clock in\nTinyTM .\nExercise 215. Extend the LockObject class to support concurrent readers.\nExercise 216. InTinyTM , the LockObject class\u2019s onCommit () handler \ufb01rst checks\nwhether the object is locked by another transaction, and then whether its stamp\nis less than or equal to the transaction\u2019s read stamp.\n\u0004Give an example showing why it is necessary to check whether the object is\nlocked.\n\u0004Is it possible that the object could be locked by the committing transaction?\n\u0004Give an example showing why it is necessary to check whether the object is\nlocked before checking the version number.\nExercise 217. Design an AtomicArray<T> implementation optimized for small\narrays such as used in a skiplist.\nExercise 218. Design an AtomicArray<T> implementation optimized for large\narrays in which transactions access", "doc_id": "9d42f92d-ac94-41e0-a4a7-3c801a389ab2", "embedding": null, "doc_hash": "c7320411352faa8650166f07a6114d3ea5ef79f2a919eeaf64c1595d8006ee6f", "extra_info": null, "node_info": {"start": 1132997, "end": 1136554}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b828656c-52d1-40ce-8cb5-848988876d1e", "3": "63d885cc-8e6a-4c49-92f4-f1e2579cd19f"}}, "__type__": "1"}, "63d885cc-8e6a-4c49-92f4-f1e2579cd19f": {"__data__": {"text": "another transaction, and then whether its stamp\nis less than or equal to the transaction\u2019s read stamp.\n\u0004Give an example showing why it is necessary to check whether the object is\nlocked.\n\u0004Is it possible that the object could be locked by the committing transaction?\n\u0004Give an example showing why it is necessary to check whether the object is\nlocked before checking the version number.\nExercise 217. Design an AtomicArray<T> implementation optimized for small\narrays such as used in a skiplist.\nExercise 218. Design an AtomicArray<T> implementation optimized for large\narrays in which transactions access disjoint regions within the array.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nIIIAppendix\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nASoftware Basics\nA.1 Introduction\nThis appendix describes the basic programming language constructs needed to\nunderstand our examples and to write your own concurrent programs. Mostly,\nwe use Java, but the same ideas could be equally well expressed in other high-level\nlanguages and libraries. Here, we review the basic software concepts needed to\nunderstand this text, \ufb01rst in Java, and then in other important models such as C#\nor the Pthreads library for C and C++. Unfortunately, our discussion here must\nbe incomplete: if in doubt, consult the current documentation for the language\nor library of interest.\nA.2 Java\nThe Java programming language uses a concurrency model in which threads and\nobjects are separate entities.1Threads manipulate objects by calling the objects\u2019\nmethods, coordinating these possibly concurrent calls using various language\nand library constructs. We begin by explaining the basic Java constructs used\nin this text.\nA.2.1 Threads\nAthread executes a single, sequential program. In Java, a thread is usually a\nsubclass of java.lang.Thread, which provides methods for creating threads,\nstarting them, suspending them, and waiting for them to \ufb01nish.\n1T echnically, threads are objects.\n453\n454 Appendix A Software Basics\nFirst, create a class that implements the Runnable interface. The class\u2019s run()\nmethod does all the work. For example, here is a simple thread that prints a\nstring.\npublic class HelloWorld implements Runnable {\nString message;\npublic HelloWorld(String m) {\nmessage = m;\n}\npublic void run() {\nSystem.out.println(message);\n}\n}\nARunnable object can be turned into a thread by calling the Thread class\nconstructor that takes a Runnable object as its argument, like this:\nString m = \"Hello World from Thread\" + i;\nThread thread = new Thread( new HelloWorld(m));\nJava provides a syntactic shortcut, called an anonymous inner class , that allows\nyou to avoid de\ufb01ning an explicit HelloWorld class:\nfinal String m = \"Hello world from thread\" + i;\nthread = new Thread( new Runnable() {\npublic void run() {\nSystem.out.println(m);\n}\n});\nThis snippet creates an anonymous class implementing the Runnable interface,\nwhose run() method behaves as shown.\nAfter a thread has been created, it must be started :\nthread.start();\nThis method causes the thread to run. The thread that calls this method returns\nimmediately. If the caller wants to wait for the thread to \ufb01nish, it must join the\nthread:\nthread.join();\nThe caller is blocked until the thread\u2019s run() method returns.\nFig. A.1 shows a method that initializes multiple threads, starts them, waits\nfor them to \ufb01nish, and then prints out a message. The method creates an array\nof threads, and initializes", "doc_id": "63d885cc-8e6a-4c49-92f4-f1e2579cd19f", "embedding": null, "doc_hash": "0807f5fc60c72e60f31865ac47565b3b9284b4a977fc3a45004c7a3c7b37c314", "extra_info": null, "node_info": {"start": 1136523, "end": 1140068}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9d42f92d-ac94-41e0-a4a7-3c801a389ab2", "3": "d04a3e50-4c30-407d-9f0a-cf8448f9e909"}}, "__type__": "1"}, "d04a3e50-4c30-407d-9f0a-cf8448f9e909": {"__data__": {"text": "method behaves as shown.\nAfter a thread has been created, it must be started :\nthread.start();\nThis method causes the thread to run. The thread that calls this method returns\nimmediately. If the caller wants to wait for the thread to \ufb01nish, it must join the\nthread:\nthread.join();\nThe caller is blocked until the thread\u2019s run() method returns.\nFig. A.1 shows a method that initializes multiple threads, starts them, waits\nfor them to \ufb01nish, and then prints out a message. The method creates an array\nof threads, and initializes them in Lines 2\u201310, using the anonymous inner class\nsyntax. At the end of this loop, it has created an array of dormant threads. In\nLines 11\u201313, it starts the threads, and each thread executes its run() method,\ndisplaying its message. Finally, in Lines 14\u201316, it waits for each thread to \ufb01nish,\nand displays a message when they are done.\nA.2 Java 455\n1 public static void main(String[] args) {\n2 Thread[] thread = new Thread[8];\n3 for (int i = 0; i < thread.length; i++) {\n4 final String message = \"Hello world from thread\" + i;\n5 thread[i] = new Thread(new Runnable() {\n6 public void run() {\n7 System.out.println(message);\n8 }\n9 });\n10 }\n11 for (int i = 0; i < thread.length; i++) {\n12 thread[i].start();\n13 }\n14 for (int i = 0; i < thread.length; i++) {\n15 thread[i].join();\n16 }\n17 }\nFigure A.1 This method initializes a number of Java threads, starts them, and then waits for\nthem to \ufb01nish.\nA.2.2 Monitors\nJava provides a number of ways to synchronize access to shared data, both built-in\nand through packages. Here we describe the built-in model, called the monitor\nmodel, which is the simplest and most commonly used approach. We discuss\nmonitors in Chapter 8.\nImagine you are in charge of software for a call center. During peak hours, calls\narrive faster than they can be answered. When a call arrives, your switchboard\nsoftware places that call in a queue, and it plays a recorded announcement assur-\ning the caller that you consider this call to be very important, and that calls will be\nanswered in the order received. An employee in charge of answering a call is called\nanoperator. Each operator dispatches an operator thread to dequeue and answer\nthe next call. When an operator has \ufb01nished with one call, he or she dequeues the\nnext call from the queue and answers it.\nFig. A.2 is a simple (but incorrect) queue class. The calls are kept in an array\ncalls, where head is the index of the next call to remove, and tail is the index\nof the next free slot in the array.\nIt is easy to see that this class does not work correctly if two operators try to\ndequeue a call at the same time. The expression\nreturn calls[(head++) % QSIZE]\ndoes not happen as an indivisible, atomic step. Instead, the compiler produces\ncode that looks something like this:\nint temp0 = head;\nhead = temp0 + 1;\nint temp1 = (temp0 % QSIZE);\nreturn calls[temp1];\n456 Appendix A Software Basics\n1 class CallQueue {\n2 final static int QSIZE = 100; // arbitrary size\n3 int head = 0; // next item to dequeue\n4 int tail = 0; // next empty slot\n5 Call[] calls = new Call[QSIZE];\n6 public enq(Call x) { // called by switchboard\n7 calls[(tail++) % QSIZE] = x;\n8 }\n9 public Call deq() { // called by operators\n10 return calls[(head++) % QSIZE]\n11", "doc_id": "d04a3e50-4c30-407d-9f0a-cf8448f9e909", "embedding": null, "doc_hash": "e8fee7c581e4f4fc7f102a6a1b422fdb20b18d12e4474b655f9268490456dd00", "extra_info": null, "node_info": {"start": 1140139, "end": 1143385}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "63d885cc-8e6a-4c49-92f4-f1e2579cd19f", "3": "65e0aea6-37c8-40fc-b6bd-5ddab6a1a7ee"}}, "__type__": "1"}, "65e0aea6-37c8-40fc-b6bd-5ddab6a1a7ee": {"__data__": {"text": "1;\nint temp1 = (temp0 % QSIZE);\nreturn calls[temp1];\n456 Appendix A Software Basics\n1 class CallQueue {\n2 final static int QSIZE = 100; // arbitrary size\n3 int head = 0; // next item to dequeue\n4 int tail = 0; // next empty slot\n5 Call[] calls = new Call[QSIZE];\n6 public enq(Call x) { // called by switchboard\n7 calls[(tail++) % QSIZE] = x;\n8 }\n9 public Call deq() { // called by operators\n10 return calls[(head++) % QSIZE]\n11 }\n12 }\nFigure A.2 Anincorrect queue class.\nTwo operators might execute these statements together: they execute Line 1at\nthe same time, then Line 2, and so on. In the end, both operators dequeue and\nanswer the same call, possibly annoying the customer.\nT o make this queue work correctly, we must ensure that only one operator at\na time can dequeue the next call, a property called mutual exclusion . Java pro-\nvides a useful built-in mechanism to support mutual exclusion. Each object has\nan (implicit) lock. If a thread Aacquires the object\u2019s lock (or, equivalently, locks\nthat object), then no other thread can acquire that lock until Areleases the lock\n(or, equivalently, until it unlocks that object). If a class declares a method to be\nsynchronized , then that method implicitly acquires the lock when it is called,\nand releases it when it returns.\nHere is one way to ensure mutual exclusion for the enq() and deq() methods:\npublic synchronized T deq() {\nreturn call[(head++) % QSIZE]\n}\npublic synchronized enq(T x) {\ncall[(tail++) % QSIZE] = x;\n}\nOnce a call to a synchronized method has acquired the object\u2019s lock, any call to\nanother synchronized method for that object is blocked until the lock is released.\n(Calls to other objects, subject to other locks, are not blocked.) The body of a\nsynchronized method is often called a critical section .\nThere is more to synchronization than mutual exclusion. What should an\noperator do if he or she tries to dequeue a call, but there are no calls waiting in\nthe queue? The call might throw an exception or return null, but what could the\noperator do then, other than try again? Instead, it makes sense for the operator\ntowait for a call to appear. Here is a \ufb01rst attempt at a solution:\npublic synchronized T deq() {\nwhile (head == tail) {}; // spin while empty\ncall[(head++) % QSIZE];\n}\nA.2 Java 457\nThis solution is not just wrong, it is disastrously wrong. The dequeuing thread\nwaits inside a synchronized method, locking out every other thread, including\nthe switchboard thread that could be trying to enqueue a call. This is a deadlock :\nthe dequeuing thread holds the lock waiting for an enqueuing thread, while the\nenqueuing thread waits for the dequeuing thread to release the lock. Nothing will\never happen.\nFrom this we learn that if a thread executing a synchronized method needs\nto wait for something to happen, then it must unlock the object while it waits.\nThe waiting thread should periodically reacquire the lock to test whether it can\nproceed. If so, it proceeds, and if not, it releases the lock and goes back to\nwaiting.\nIn Java, each object provides a wait () method that unlocks the object and\nsuspends the caller. While that thread is waiting, another thread can lock and\nchange the object. Later, when the suspended thread resumes, it locks the object\nagain before it returns from the wait () call. Here is a revised (but still not correct)\ndequeue method2\npublic synchronized T deq() {\nwhile (head == tail)", "doc_id": "65e0aea6-37c8-40fc-b6bd-5ddab6a1a7ee", "embedding": null, "doc_hash": "57238fbd8192f4f68bee6904f51ca5c209b3994785f187872cc11ae48eb82a08", "extra_info": null, "node_info": {"start": 1143478, "end": 1146887}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d04a3e50-4c30-407d-9f0a-cf8448f9e909", "3": "97e0c188-4028-44d0-93cb-5c154f2da1aa"}}, "__type__": "1"}, "97e0c188-4028-44d0-93cb-5c154f2da1aa": {"__data__": {"text": "waiting thread should periodically reacquire the lock to test whether it can\nproceed. If so, it proceeds, and if not, it releases the lock and goes back to\nwaiting.\nIn Java, each object provides a wait () method that unlocks the object and\nsuspends the caller. While that thread is waiting, another thread can lock and\nchange the object. Later, when the suspended thread resumes, it locks the object\nagain before it returns from the wait () call. Here is a revised (but still not correct)\ndequeue method2\npublic synchronized T deq() {\nwhile (head == tail) {wait();}\nreturn call[(head++) % QSIZE];\n}\nHere, each operator thread, seeking a call to answer, repeatedly tests whether the\nqueue is empty. If so, it releases the lock and waits, and if not, it removes and\nreturns the item. In a similar way, an enqueuing thread checks whether the buffer\nis full.\nWhen does a waiting thread wake up? It is the programmer\u2019s responsibi-\nlity to notify waiting threads when something signi\ufb01cant happens. The notify ()\nmethod wakes up one waiting thread, eventually, chosen arbitrarily from the set\nof waiting threads. When that thread awakens, it competes for the lock like any\nother thread. When that thread reacquires the lock, it returns from its wait () call.\nY ou cannot control which waiting thread is chosen. By contrast, the notifyAll ()\nmethod wakes up all waiting threads, eventually. Each time the object is\nunlocked, one of these newly awakened threads will reacquire the lock and return\nfrom its wait () call. Y ou cannot control the order in which the threads reacquire\nthe lock.\nIn the call center example, there are multiple operators and one switchboard.\nSuppose the switchboard software decides to optimize its use of notify () as fol-\nlows. If it adds a call to an empty queue, then it should notify only one blocked\ndequeuer, since there is only one call to consume. While this optimization may\nseem reasonable, it is \ufb02awed. Suppose the operator threads AandBdiscover the\nqueue is empty, and block waiting for calls to answer. The switchboard thread\n2 This program will not compile because the wait () call can throw InterruptedException,\nwhich must be caught or rethrown. As discussed in Pragma 8.3 in Chapter 8, we often ignore\nsuch exceptions to make the examples easier to read.\n458 Appendix A Software Basics\nSputs a call in the queue, and calls notify () to wake up one operator thread.\nBecause the noti\ufb01cation is asynchronous, however, there is a delay. Sthen returns\nand places another call in the queue, and because the queue already had a wait-\ning call, it does not notify other threads. The switchboard\u2019s notify () \ufb01nally takes\neffect, waking up A, but notB, even though there is a call for Bto answer.\nThis pitfall is called the lost wakeup problem: one or more waiting threads fail\nto be noti\ufb01ed that the condition for which they are waiting has become true. See\nSection 8.2.2 (Chapter 8 ) for a more detailed discussion.\nA.2.3 Yielding and Sleeping\nIn addition to the wait () method, which allows a thread holding a lock to release\nthe lock and pause, Java provides other ways for a thread that does not hold a lock\nto pause. A yield () call pauses the thread, asking the schedule to run something\nelse. The scheduler decides whether to pause the thread, and when to restart it.\nIf there are no other threads to run, the schedule may ignore the yield () call.\nSection 16.4.1 inChapter 16 describes how yielding can be an effective way to\nprevent livelock. A call to sleep (t), wheretis a time value, instructs the", "doc_id": "97e0c188-4028-44d0-93cb-5c154f2da1aa", "embedding": null, "doc_hash": "6bc1f9a692b86bc78982466b4e4f297986455e1e368de00a53b5f653b41ba3b6", "extra_info": null, "node_info": {"start": 1146777, "end": 1150313}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "65e0aea6-37c8-40fc-b6bd-5ddab6a1a7ee", "3": "223bad54-02b3-4c7c-b72a-8c111e419a17"}}, "__type__": "1"}, "223bad54-02b3-4c7c-b72a-8c111e419a17": {"__data__": {"text": "addition to the wait () method, which allows a thread holding a lock to release\nthe lock and pause, Java provides other ways for a thread that does not hold a lock\nto pause. A yield () call pauses the thread, asking the schedule to run something\nelse. The scheduler decides whether to pause the thread, and when to restart it.\nIf there are no other threads to run, the schedule may ignore the yield () call.\nSection 16.4.1 inChapter 16 describes how yielding can be an effective way to\nprevent livelock. A call to sleep (t), wheretis a time value, instructs the scheduler\nnot to run that thread for that duration. The scheduler is free to restart the thread\nat any later time.\nA.2.4 Thread-Local Objects\nOften it is useful for each thread to have its own private instance of a variable. Java\nsupports such thread-local objects through the ThreadLocal <T>class, which\nmanages a collection of objects of type T, one for each thread. Because thread-\nlocal variables were not built into Java, they have a somewhat complicated and\nawkward interface. Nevertheless, they are extremely useful, and we use them\noften, so we review how to use them here.\nTheThreadLocal <T>class provides get() and set() methods that read and\nupdate the thread\u2019s local value. The initialValue () method is called the \ufb01rst\ntime a thread tries to get the value of a thread-local object. We cannot use the\nThreadLocal <T>class directly. Instead, we must de\ufb01ne a thread-local variable\nas a subclass ofThreadLocal <T>that overrides the parent\u2019s initialValue ()\nmethod to initialize each thread\u2019s object appropriately.\nThis mechanism is best illustrated by an example. In many of our algorithms,\nwe assume that each of nconcurrent threads has a unique thread-local iden-\nti\ufb01er between 0 and n\u00001. T o provide such an identi\ufb01er, we show how to\nde\ufb01ne a ThreadID class with a single static method: get() returns the calling\nthread\u2019s identi\ufb01er. When a thread calls get() for the \ufb01rst time, it is assigned the\nnext unused identi\ufb01er. Each subsequent call by that thread returns that thread\u2019s\nidenti\ufb01er.\nFig. A.3 shows the simplest way to use a thread-local object to implement this\nuseful class. Line 2declares an integer nextID \ufb01eld that holds the next identi\ufb01er to\nA.2 Java 459\n1public class ThreadID {\n2 private static volatile int nextID = 0;\n3 private static class ThreadLocalID extends ThreadLocal<Integer> {\n4 protected synchronized Integer initialValue() {\n5 return nextID++;\n6 }\n7 }\n8 private static ThreadLocalID threadID = new ThreadLocalID();\n9 public static int get() {\n10 return threadID.get();\n11 }\n12 public static void set( int index) {\n13 threadID.set(index);\n14 }\nFigure A.3 TheThreadID class: give each thread a unique identi\ufb01er.\nbe issued. Lines 3through 7de\ufb01ne an inner class accessible only within the body\nof the enclosing ThreadID class. This inner class manages the thread\u2019s identi\ufb01er.\nIt is a subclass of ThreadLocal <Integer>that overrides the initialValue ()\nmethod to assign the next unused identi\ufb01er to the current thread.\nBecause the inner ThreadLocalID class is used exactly once, it makes little\nsense to give it a name (for the same reason that it makes little sense to name\nyour Thanks-giving turkey). Instead, it is more common to use an anonymous\nclass as described earlier.\nHere is an example how the ThreadID class might be used:\nthread = new Thread( new Runnable() {\npublic void run() {\nSystem.out.println(\"Hello world from thread\" +", "doc_id": "223bad54-02b3-4c7c-b72a-8c111e419a17", "embedding": null, "doc_hash": "db463361dcff0164bd16f6e0ce07c89949b967d37b8c24e9e10c00e3b198304a", "extra_info": null, "node_info": {"start": 1150317, "end": 1153750}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "97e0c188-4028-44d0-93cb-5c154f2da1aa", "3": "f59a8808-7afd-446a-b2c7-054837a8a10f"}}, "__type__": "1"}, "f59a8808-7afd-446a-b2c7-054837a8a10f": {"__data__": {"text": "is a subclass of ThreadLocal <Integer>that overrides the initialValue ()\nmethod to assign the next unused identi\ufb01er to the current thread.\nBecause the inner ThreadLocalID class is used exactly once, it makes little\nsense to give it a name (for the same reason that it makes little sense to name\nyour Thanks-giving turkey). Instead, it is more common to use an anonymous\nclass as described earlier.\nHere is an example how the ThreadID class might be used:\nthread = new Thread( new Runnable() {\npublic void run() {\nSystem.out.println(\"Hello world from thread\" + ThreadID.get());\n}\n});\nPragma A.2.1. In the type expression ThreadLocal <Integer>, you must\nuseInteger instead of intbecause intis a primitive type, while Integer is\na reference type, and only reference types are allowed in angle brackets. Since\nJava 1.5, a feature called auto-boxing allows you to use int and Integer\nvalues more-or-less interchangeably, for example:\nInteger x = 5;\nint y = 6;\nInteger z = x + y;\nConsult your Java reference manual for complete details.\n460 Appendix A Software Basics\nA.3 C#\nC# is a Java-like language that runs on Microsoft\u2019s .Net platform.\nA.3.1 Threads\nC# provides a threading model similar to Java\u2019s. C# threads are implemented by\ntheSystem.Threading.Thread class. When you create a thread, you tell it what\nto do by passing it a ThreadStart delegate, a kind of pointer to the method you\nwant to call. For example, here is a method that prints a simple message:\nvoid HelloWorld()\n{\nConsole.WriteLine(\"Hello World\");\n}\nWe then turn this method into a ThreadStart delegate, and pass that delegate to\nthe thread constructor.\nThreadStart hello = new ThreadStart(HelloWorld);\nThread thread = new Thread(hello);\nC# provides a syntactic shortcut, called an anonymous method, that allows you\nto de\ufb01ne a delegate directly, for example, by combining the previous steps into a\nsingle expression:\nThread thread = new Thread(delegate()\n{\nConsole.WriteLine(\"Hello World\");\n});\nAs in Java, after a thread has been created, it must be started :\nthread.Start();\nThis call causes the thread to run, while the caller returns immediately. If the\ncaller wants to wait for the thread to \ufb01nish, it must join the thread:\nthread.Join();\nThe caller is blocked until the thread\u2019s method returns.\nFig. A.4 shows a method that initializes a number of threads, starts them, waits\nfor them to \ufb01nish, and then prints out a message. The method creates an array\nof threads, initializing each thread with its own ThreadStart delegate. We then\nstart the threads, and each thread executes its delegate, displaying its message.\nFinally, we wait for each thread to \ufb01nish, and display a message when they are\nA.3 C# 461\n1 static void Main(string[] args)\n2 {\n3 Thread[] thread = new Thread[8];\n4 // create threads\n5 for (int i = 0; i < thread.Length; i++)\n6 {\n7 String message = \"Hello world from thread\" + i;\n8 ThreadStart hello = delegate()\n9 {\n10 Console.WriteLine(message);\n11 };\n12 thread[i] = new Thread(hello);\n13 }\n14 // start threads\n15 for (int i = 0; i < thread.Length; i++)\n16 {\n17 thread[i].Start();\n18 }\n19 // wait for them to finish\n20 for (int i = 0; i < thread.Length; i++)\n21 {\n22 thread[i].Join();\n23 }\n24 Console.WriteLine(\"done!\");\n25 }\nFigure A.4 This method initializes a number of C# threads, starts them, waits for them to\n\ufb01nish, and then prints out a", "doc_id": "f59a8808-7afd-446a-b2c7-054837a8a10f", "embedding": null, "doc_hash": "022917ec98d98a99db0cbbd179981d9ccb76c825a00f429f353fed3b3de21563", "extra_info": null, "node_info": {"start": 1153740, "end": 1157073}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "223bad54-02b3-4c7c-b72a-8c111e419a17", "3": "90fb0068-e5f0-4b09-b605-39a3b2b3b4f8"}}, "__type__": "1"}, "90fb0068-e5f0-4b09-b605-39a3b2b3b4f8": {"__data__": {"text": "};\n12 thread[i] = new Thread(hello);\n13 }\n14 // start threads\n15 for (int i = 0; i < thread.Length; i++)\n16 {\n17 thread[i].Start();\n18 }\n19 // wait for them to finish\n20 for (int i = 0; i < thread.Length; i++)\n21 {\n22 thread[i].Join();\n23 }\n24 Console.WriteLine(\"done!\");\n25 }\nFigure A.4 This method initializes a number of C# threads, starts them, waits for them to\n\ufb01nish, and then prints out a message.\nall done. Except for minor syntactic differences, this code is similar to what you\nwould write in Java.\nA.3.2 Monitors\nFor simple mutual exclusion, C# provides the ability to lock an object much like\nthesynchronized modi\ufb01er in Java:\nint GetAndIncrement()\n{\nlock (this)\n{\nreturn value++;\n}\n}\nUnlike Java, C# does not allow you to use a lock statement to modify a method\ndirectly. Instead, the lock statement is used to enclose the method body.\nConcurrent data structures require more than mutual exclusion: they also\nrequire the ability to wait and signal conditions. Unlike in Java, where every\n462 Appendix A Software Basics\nobject is an implicit monitor, in C# you must explicitly create the monitor associ-\nated with an object. T o acquire a monitor lock, call Monitor.Enter(this), and\nto release the lock, call Monitor.Exit(this). Each monitor has a single implicit\ncondition, which is waited upon by Monitor.Wait(this), and signaled by\nMonitor.Pulse(this) orMonitor.PulseAll(this), which respectively wake\nup one or all sleeping threads. Figs. A.5 andA.6show how to implement a simple\nbounded queue using C# monitor calls.\nA.3.3 Thread-Local Objects\nC# provides a very simple way to make a static \ufb01eld thread-local: simply pre\ufb01x\nthe \ufb01eld declaration with the attribute [ThreadStatic].\n[ThreadStatic]\nstatic int value;\nDo not provide an initial value for a [ThreadStatic] \ufb01eld, because the initial-\nization happens once, not once per thread. Instead, each thread will \ufb01nd the \ufb01eld\n1 class Queue<T>\n2 {\n3 int head, tail;\n4 T[] call;\n5 public Queue(int capacity)\n6 {\n7 call = new T[capacity];\n8 head = tail = 0;\n9 }\n10 public void Enq(T x)\n11 {\n12 Monitor.Enter(this);\n13 try\n14 {\n15 while (tail - head == call.Length)\n16 {\n17 Monitor.Wait(this); // queue is full\n18 }\n19 calls[(tail++) % call.Length] = x;\n20 Monitor.Pulse(this); // notify waiting dequeuers\n21 }\n22 finally\n23 {\n24 Monitor.Exit(this);\n25 }\n26 }\n27 }\n28 }\nFigure A.5 A bounded Queue class: \ufb01elds and enq() method.\nA.3 C# 463\n29 public T Deq()\n30 {\n31 Monitor.Enter(this);\n32 try\n33 {\n34 while (tail == head)\n35 {\n36 Monitor.Wait(this); // queue is empty\n37 }\n38 T y = calls[(head++) % call.Length];\n39 Monitor.Pulse(this); // notify waiting enqueuers\n40 return y;\n41 }\n42 finally\n43 {\n44 Monitor.Exit(this);\n45 }\n46 }\n47 }\nFigure A.6 A bounded Queue class: the deq() method.\n1 class ThreadID\n2 {\n3 [ThreadStatic] static int myID;\n4 static int counter;\n5 public static int get()\n6 {\n7 if(myID == 0)\n8 {\n9 myID = Interlocked.Increment(ref", "doc_id": "90fb0068-e5f0-4b09-b605-39a3b2b3b4f8", "embedding": null, "doc_hash": "70a43352547679b96d87152b2dec65d32be5c4cfbd6394220802e85822b771ab", "extra_info": null, "node_info": {"start": 1157215, "end": 1160125}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f59a8808-7afd-446a-b2c7-054837a8a10f", "3": "94bb8087-9ea1-491a-9566-b9e696746325"}}, "__type__": "1"}, "94bb8087-9ea1-491a-9566-b9e696746325": {"__data__": {"text": "// queue is empty\n37 }\n38 T y = calls[(head++) % call.Length];\n39 Monitor.Pulse(this); // notify waiting enqueuers\n40 return y;\n41 }\n42 finally\n43 {\n44 Monitor.Exit(this);\n45 }\n46 }\n47 }\nFigure A.6 A bounded Queue class: the deq() method.\n1 class ThreadID\n2 {\n3 [ThreadStatic] static int myID;\n4 static int counter;\n5 public static int get()\n6 {\n7 if(myID == 0)\n8 {\n9 myID = Interlocked.Increment(ref counter);\n10 }\n11 return myID - 1;\n12 }\n13 }\nFigure A.7 The ThreadID class provides each thread a unique identi\ufb01er implemented using\n[ThreadStatic] .\ninitially has that type\u2019s default value: zero for integers, null for references, and\nso on.\nFig. A.7 shows how to implement the ThreadID class (Java version in Fig. A.3).\nThere is one point about this program that may require comment. The \ufb01rst time\na thread inspects its [ThreadStatic] identi\ufb01er, that \ufb01eld will be zero, the default\nvalue for integers. T o distinguish between an uninitialized zero and a thread ID\nzero, this \ufb01eld holds the thread ID displaced by one: thread 0 has \ufb01eld value 1,\nand so on.\n464 Appendix A Software Basics\nA.4 Pthreads\nPthreads provides much of the same functionality for C or C++. Programs that\nuse Pthreads must import the include \ufb01le:\n#include <pthread.h>\nThe following function creates and starts a thread:\nint pthread_create (\npthread_t *thread_id,\nconst pthread_attr_t *attributes,\nvoid *(*thread_function)(void *),\nvoid *argument);\nThe \ufb01rst argument is a pointer to the thread itself. The second allows you to\nspecify various aspects of the thread, the third is a pointer to the code the thread\nis to run (in C# this would be a delegate, and in Java a Runnable object), and\nthe fourth is the argument to the thread function. Unlike Java or C#, a single call\nboth creates and starts a thread.\nA thread terminates when the function returns or calls pthread_exit().\nThreads can also join by the call:\nint pthread_join (pthread_t thread, void **status_ptr);\nThe exit status is stored in the last argument. For example, the following program\nprints out a simple per-thread message.\n#include <pthread.h>\n#define NUM_THREADS 8\nvoid *hello(void *arg) {\nprintf(\"Hello from thread %i\\n\", (int)arg);\n}\nint main() {\npthread_t thread[NUM_THREADS];\nint status;\nint i;\nfor (i = 0; i < NUM_THREADS; i++) {\nif( pthread_create(&thread[i], NULL, hello, (void *)i) != 0 ) {\nprintf(\"pthread_create() error\");\nexit();\n}\n}\nfor (i = 0; i < NUM_THREADS; i++) {\npthread_join(thread[i], NULL);\n}\n}\nThe Pthreads library calls locks mutexes. A mutex is created by calling\nint pthread_mutex_init (pthread_mutex_t *mutex,\nconst pthread_mutexattr_t *attr);\nA.4 Pthreads 465\nA mutex can be locked:\nint pthread_mutex_lock (pthread_mutex_t *mutex);\nand unlocked:\nint pthread_mutex_unlock (pthread_mutex_t *mutex);\nLike a Java lock, it is possible to return immediately if a mutex is busy:\nint pthread_mutex_trylock (pthread_mutex_t", "doc_id": "94bb8087-9ea1-491a-9566-b9e696746325", "embedding": null, "doc_hash": "b69d9ebca7ba603a7b0dedcfb1910ebc1cc840e1216cbc6f62b238c880a2c6f8", "extra_info": null, "node_info": {"start": 1160115, "end": 1163001}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "90fb0068-e5f0-4b09-b605-39a3b2b3b4f8", "3": "b27a658b-17ed-42c9-8328-dd95fd1a554d"}}, "__type__": "1"}, "b27a658b-17ed-42c9-8328-dd95fd1a554d": {"__data__": {"text": "mutexes. A mutex is created by calling\nint pthread_mutex_init (pthread_mutex_t *mutex,\nconst pthread_mutexattr_t *attr);\nA.4 Pthreads 465\nA mutex can be locked:\nint pthread_mutex_lock (pthread_mutex_t *mutex);\nand unlocked:\nint pthread_mutex_unlock (pthread_mutex_t *mutex);\nLike a Java lock, it is possible to return immediately if a mutex is busy:\nint pthread_mutex_trylock (pthread_mutex_t *mutex);\nThe Pthreads library provides condition variables, which can be created by\ncalling:\nint pthread_cond_init (pthread_cond_t *cond, pthread_condattr_t *attr);\nAs usual, the second argument sets attributes to nondefault values. Unlike in Java\nor C#, the association between a lock and a condition variable is explicit, not\nimplicit. The following call releases a lock and waits on a condition variable:\nint pthread_cond_wait (pthread_cond_t *cond, pthread_mutex_t *mutex);\n(Just as in the other languages, when a thread awakens, there is no guarantee\nthat the condition it is awaiting holds, so it must be checked explicitly.) It is also\npossible to wait with a timeout.\nThe following call is similar to Java\u2019s notify (), awakening at least one\nsuspended thread:\nint pthread_cond_signal (pthread_cond_t *cond);\nThe following is like Java\u2019s notifyAll (), awakening all suspended threads:\nint pthread_cond_broadcast (pthread_cond_t *cond);\nBecause C is not garbage collected, threads, locks, and condition variables all\nprovide destroy() functions that allow their resources to be reclaimed.\nFigs. A.8 and A.9 illustrate a simple concurrent FIFO queue. Call are kept\nin an array, and head andtail \ufb01elds count the number of call enqueued and\ndequeued. Unlike the Java implementation, it uses two different condition vari-\nables to wait for the buffer to become either not full or not empty.\nA.4.1 Thread-Local Storage\nFig. A.10 illustrates how Pthreads manages thread-local storage. The Pthreads\nlibrary associates a thread-speci\ufb01c value with a key, which is declared at Line 1\nand initialized at Line 6. The value is a pointer, initially null. A thread acquires\nan ID by calling threadID_get() . This method looks up the thread-local value\nbound to the key (Line 10). On the \ufb01rst call, that value is null (Line 11), so the\nthread must take a new unique ID by incrementing the counter variable. Here,\nwe use a mutex to synchronize access to a counter (Lines 12\u201316).\n466 Appendix A Software Basics\n1#include <pthread.h>\n2#define QSIZE 16\n3typedef struct {\n4 int buf[QSIZE];\n5 long head, tail;\n6 pthread_mutex_t *mutex;\n7 pthread_cond_t *notFull, *notEmpty;\n8} queue;\n9void queue_enq(queue *q,int item) {\n10 // lock object\n11 pthread_mutex_lock (q->mutex);\n12 // wait while full\n13 while (q->tail - q->head == QSIZE) {\n14 pthread_cond_wait (q->notFull, q->mutex);\n15 }\n16 q->buf[q->tail % QSIZE] = item;\n17 q->tail++;\n18 // release lock\n19 pthread_mutex_unlock (q->mutex);\n20 // inform waiting dequeuers\n21 pthread_cond_signal (q->notEmpty);\n22 }\n23 queue *queue_init (void) {\n24 queue *q;\n25 q =", "doc_id": "b27a658b-17ed-42c9-8328-dd95fd1a554d", "embedding": null, "doc_hash": "777adaa5f2abe547e2ee798236861e7b6bfec260790092cbe33bb1a7b59fbe29", "extra_info": null, "node_info": {"start": 1162996, "end": 1165985}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "94bb8087-9ea1-491a-9566-b9e696746325", "3": "9420320a-5485-47af-b3b3-9d2394e0fb28"}}, "__type__": "1"}, "9420320a-5485-47af-b3b3-9d2394e0fb28": {"__data__": {"text": "pthread_mutex_lock (q->mutex);\n12 // wait while full\n13 while (q->tail - q->head == QSIZE) {\n14 pthread_cond_wait (q->notFull, q->mutex);\n15 }\n16 q->buf[q->tail % QSIZE] = item;\n17 q->tail++;\n18 // release lock\n19 pthread_mutex_unlock (q->mutex);\n20 // inform waiting dequeuers\n21 pthread_cond_signal (q->notEmpty);\n22 }\n23 queue *queue_init (void) {\n24 queue *q;\n25 q = (queue *)malloc (sizeof (queue));\n26 if(q == NULL) return (NULL);\n27 q->head = 0;\n28 q->tail = 0;\n29 q->mutex = (pthread_mutex_t *) malloc (sizeof (pthread_mutex_t));\n30 pthread_mutex_init (q->mutex, NULL);\n31 q->notFull = (pthread_cond_t *) malloc (sizeof (pthread_cond_t));\n32 pthread_cond_init (q->notFull, NULL);\n33 q->notEmpty = (pthread_cond_t *) malloc (sizeof (pthread_cond_t));\n34 pthread_cond_init (q->notEmpty, NULL);\n35 return (q);\n36 }\nFigure A.8 Initialization and Enqueue methods of a concurrent FIFO Queue using Pthreads.\nA.5 Chapter Notes\nThe Java programming language was created by James Gosling [46]. Dennis\nRitchie is credited with creating C. Pthreads was invented as part of the IEEE\nPosix package. The basic monitor model is credited to T ony Hoare [71] and Per\nBrinch Hansen [52], although they used different mechanisms for waiting and\nnoti\ufb01cation. The mechanisms used by Java (and later by C#) were originally pro-\nposed by Butler Lampson and David Redell [96].\nA.5 Chapter Notes 467\n37 int queue_deq(queue *q) {\n38 int result;\n39 // lock object\n40 pthread_mutex_lock (q->mutex);\n41 // wait while full\n42 while (q->tail == q->head) {\n43 pthread_cond_wait (q->notEmpty, q->mutex);\n44 }\n45 result = q->buf[q->head % QSIZE];\n46 q->head++;\n47 // release lock\n48 pthread_mutex_unlock (q->mutex);\n49 // inform waiting dequeuers\n50 pthread_cond_signal (q->notFull);\n51 return result;\n52 }\n53 void queue_delete (queue *q) {\n54 pthread_mutex_destroy (q->mutex);\n55 free (q->mutex);\n56 pthread_cond_destroy (q->notFull);\n57 free (q->notFull);\n58 pthread_cond_destroy (q->notEmpty);\n59 free (q->notEmpty);\n60 free (q);\n61 }\nFigure A.9 Pthreads: a concurrent FIFO queue\u2019s dequeue and delete methods.\n1pthread_key_t key; /*key */\n2int counter; /*generates unique value */\n3pthread_mutex_t mutex; /*synchronizes counter */\n4threadID_init() {\n5 pthread_mutex_init(&mutex, NULL);\n6 pthread_key_create(&key, NULL);\n7 counter = 0;\n8}\n9int threadID_get() {\n10 int*id = (int *)pthread_getspecific(key);\n11 if(id == NULL) { /*first time? */\n12 id = (int *)malloc(sizeof(int));\n13 pthread_mutex_lock(&mutex);\n14 *id = counter++;\n15 pthread_setspecific(key, id);\n16 pthread_mutex_unlock(&mutex);\n17 }\n18 return *id;\n19 }\nFigure A.10", "doc_id": "9420320a-5485-47af-b3b3-9d2394e0fb28", "embedding": null, "doc_hash": "0c6f3fe8bfdb3d46e93027df1d94ddf0611944beb62bc118b80321214578fe44", "extra_info": null, "node_info": {"start": 1166010, "end": 1168617}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b27a658b-17ed-42c9-8328-dd95fd1a554d", "3": "10c270f7-c6a4-426c-99cb-62497edfa3ec"}}, "__type__": "1"}, "10c270f7-c6a4-426c-99cb-62497edfa3ec": {"__data__": {"text": "NULL);\n6 pthread_key_create(&key, NULL);\n7 counter = 0;\n8}\n9int threadID_get() {\n10 int*id = (int *)pthread_getspecific(key);\n11 if(id == NULL) { /*first time? */\n12 id = (int *)malloc(sizeof(int));\n13 pthread_mutex_lock(&mutex);\n14 *id = counter++;\n15 pthread_setspecific(key, id);\n16 pthread_mutex_unlock(&mutex);\n17 }\n18 return *id;\n19 }\nFigure A.10 This program provides each thread a unique identi\ufb01er using Pthreads\nthread-local storage management calls.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nBHardware Basics\nA novice was trying to \ufb01x a broken Lisp machine by turning the power off and\non. Knight, seeing what the student was doing spoke sternly: \u201cY ou cannot \ufb01x a\nmachine just by power-cycling it with no understanding of what is going wrong.\u201d\nKnight turned the machine off and on. The machine worked.\n(From \u201cAI Koans\u201d , a collection of jokes popular at MIT in the 1980s).\nB.1 Introduction (and a Puzzle)\nY ou cannot program a multiprocessor effectively unless you know what a multi-\nprocessor is. Y ou can do a pretty good job of programming a uniprocessor with-\nout understanding much about computer architecture, but the same is not true\nof multiprocessors. We will illustrate this point by a puzzle. We will consider two\nprograms that are logically equivalent, except that one is much less ef\ufb01cient than\nthe other. Ominously, the simpler program is the inef\ufb01cient one. This discrep-\nancy cannot be explained, nor the danger avoided, without a basic understanding\nof modern multiprocessor architectures.\nHere is the background to the puzzle. Suppose two threads share a resource\nthat can be used by only one thread at a time. T o prevent concurrent use, each\nthread must lock the resource before using it, and unlock it afterward. We will\nstudymanywaystoimplementlocksinChapter7.Forthepuzzle,weconsider\ntwo simple implementations in which the lock is a single Boolean \ufb01eld. If the\n\ufb01eld is false , the lock is free, and otherwise it is in use. We manipulate the lock\nwith the getAndSet (v) method, which atomically swaps its argument vwith the\n\ufb01eld value. T o acquire the lock, a thread calls getAndSet (true). If the call returns\nfalse , then the lock was free, and the caller succeeded in locking the object. Other-\nwise, the object was already locked, and the thread must try again later. A thread\nreleases a lock simply by storing false into the Boolean \ufb01eld.\nInFig. B.1 , the test-and-set (TASLock ) lock repeatedly calls getAndSet (true)\n(Line 4) until it returns false . By contrast, in Fig. B.2 , the test-and-test-and-\nsetlock ( TTASLock ) repeatedly reads the lock \ufb01eld (by calling state.get() at\n469\n470 Appendix B Hardware Basics\n1public class TASLock implements Lock {\n2 ...\n3 public void lock() {\n4 while (state.getAndSet( true )) {} // spin\n5 }\n6 ...\n7}\nFigure B.1 TheTASLock class.\n1public class TTASLock implements Lock {\n2 ...\n3 public void lock() {\n4 while (true ) {\n5 while (state.get()) {}; // spin\n6 if(!state.getAndSet( true ))\n7 return ;\n8 }\n9 }\n10 ...\n11", "doc_id": "10c270f7-c6a4-426c-99cb-62497edfa3ec", "embedding": null, "doc_hash": "201adf089abcbc48eaaa4c0c2538580c42d5b38bf0e7bf13429d72c4ba594edc", "extra_info": null, "node_info": {"start": 1168628, "end": 1171680}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9420320a-5485-47af-b3b3-9d2394e0fb28", "3": "58a28981-4c63-4f88-8fc9-dedb222a2be4"}}, "__type__": "1"}, "58a28981-4c63-4f88-8fc9-dedb222a2be4": {"__data__": {"text": "(by calling state.get() at\n469\n470 Appendix B Hardware Basics\n1public class TASLock implements Lock {\n2 ...\n3 public void lock() {\n4 while (state.getAndSet( true )) {} // spin\n5 }\n6 ...\n7}\nFigure B.1 TheTASLock class.\n1public class TTASLock implements Lock {\n2 ...\n3 public void lock() {\n4 while (true ) {\n5 while (state.get()) {}; // spin\n6 if(!state.getAndSet( true ))\n7 return ;\n8 }\n9 }\n10 ...\n11 }\nFigure B.2 TheTTASLock class.\nLine 5) until it returns false , and only then calls getAndSet () (Line 6). It is\nimportant to understand that reading the lock value is atomic, and applying\ngetAndSet () to the lock value is atomic, but the combination is not atomic:\nbetween the time a thread reads the lock value and the time it calls getAndSet (),\nthe lock value may have changed.\nBefore you proceed, you should convince yourself that the TASLock and\nTTASLock algorithms are logically the same. The reason is simple: in the\nTTASLock algorithm, reading that the lock is free does not guarantee that the next\ncall to getAndSet () will succeed, because some other thread may have acquired\nthe lock in the interval between reading the lock and trying to acquire it. So why\nbother reading the lock before trying to acquire it?\nHere is the puzzle. While the two lock implementations may be logically equiv-\nalent, they perform very differently. In a classic 1989 experiment, Anderson mea-\nsured the time needed to execute a simple test program on several contemporary\nmultiprocessors. He measured the elapsed time for nthreads to execute a short\ncritical section one million times. Fig. B.3 shows how long each lock takes, plotted\nas a function of the number of threads. In a perfect world, both the TASLock and\nTTASLock curves would be as \ufb02at as the ideal curve on the bottom, since each run\ndoes the same number of increments. Instead, we see that both curves slope up,\nindicating that lock-induced delay increases with the number of threads. Curi-\nously, however, the TASLock is much slower than the TTASLock lock, especially\nas the number of threads increases. Why?\nB.1 Introduction (and a Puzzle) 471\nTASLock\nTTASLock\nIdealLocktime\nnumber of threads\nFigure B.3 Schematic performance of a TASLock , aTTASLock , and an ideal lock.\nThis chapter covers much of what you need to know about multiprocessor\narchitecture to write ef\ufb01cient concurrent algorithms and data structures. (Along\nthe way, we will explain the divergent curves in Fig. B.3.)\nWe will be concerned with the following components:\n\u0004The processors are hardware devices that execute software threads. There are\ntypically more threads than processors, and each processor runs a thread for a\nwhile, sets it aside, and turns its attention to another thread.\n\u0004The interconnect is a communication medium that links processors to proces-\nsors and processors to memory.\n\u0004The memory is actually a hierarchy of components that store data, ranging\nfrom one or more levels of small, fast caches to a large and relatively slow main\nmemory. Understanding how these levels interact is essential to understan-\nding the actual performance of many concurrent algorithms.\nFrom our point of view, one architectural principle drives everything else: pro-\ncessors and main memory are far apart. It takes a long time for a processor to read\na value from memory. It also takes a long time for a processor to write a value to\nmemory, and longer still for the processor to be sure that value has actually been\ninstalled in memory. Accessing memory is more like mailing a letter than making\na phone call. Almost everything we examine in this chapter", "doc_id": "58a28981-4c63-4f88-8fc9-dedb222a2be4", "embedding": null, "doc_hash": "360460bc80dc80af7cdebc0541efd0c00364886dba7db13e9c680a2e941dd044", "extra_info": null, "node_info": {"start": 1171656, "end": 1175240}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "10c270f7-c6a4-426c-99cb-62497edfa3ec", "3": "95fc0a79-4dea-4ec5-aed3-a15d7baa996b"}}, "__type__": "1"}, "95fc0a79-4dea-4ec5-aed3-a15d7baa996b": {"__data__": {"text": "to a large and relatively slow main\nmemory. Understanding how these levels interact is essential to understan-\nding the actual performance of many concurrent algorithms.\nFrom our point of view, one architectural principle drives everything else: pro-\ncessors and main memory are far apart. It takes a long time for a processor to read\na value from memory. It also takes a long time for a processor to write a value to\nmemory, and longer still for the processor to be sure that value has actually been\ninstalled in memory. Accessing memory is more like mailing a letter than making\na phone call. Almost everything we examine in this chapter is the result of trying\nto alleviate the long time it takes (\u201chigh latency\u201d) to access memory.\nBoth processor and memory speed change over time, but their relative perfor-\nmance changes slowly. Let us consider the following analogy. We imagine that it\nis 1980, and you are in charge of a messenger service in mid-town Manhattan.\nWhile cars outperform bicycles on the open road, bicycles outperform cars in\nheavy traf\ufb01c, so you choose to use bicycles. Even though the technology behind\nboth bicycles and cars has advanced, the architectural comparison remains the\n472 Appendix B Hardware Basics\nsame. Then as now, if you are designing an urban messenger service, you should\nuse bicycles, not cars.\nB.2 Processors and Threads\nA multiprocessor consists of multiple hardware processors, each of which executes\na sequential program. When discussing multiprocessor architectures, the basic\nunit of time is the cycle : the time it takes a processor to fetch and execute a single\ninstruction. In absolute terms, cycle times change as technology advances (from\nabout 10 million cycles per second in 1980 to about 3000 million in 2005), and\nthey vary from one platform to another (processors that control toasters have\nlonger cycles than processors that control web servers). Nevertheless, the relative\ncost of instructions such as memory access changes slowly when expressed in\nterms of cycles.\nAthread is a sequential program. While a processor is a hardware device, a\nthread is a software construct. A processor can run a thread for a while and then\nset it aside and run another thread, an event known as a context switch. A proces-\nsor may set aside a thread, or deschedule it, for a variety of reasons. Perhaps the\nthread has issued a memory request that will take some time to satisfy, or perhaps\nthat thread has simply run long enough, and it is time for another thread to make\nprogress. When a thread is descheduled, it may resume execution on another\nprocessor.\nB.3 Interconnect\nThe interconnect is the medium by which processors communicate with the\nmemory and with other processors. There are essentially two kinds of inter-\nconnect architectures in use: SMP (symmetric multiprocessing) and NUMA\n(nonuniform memory access).\nIn an SMP architecture, processors and memory are linked by a busintercon-\nnect, a broadcast medium that acts like a tiny Ethernet. Both processors and the\nmain memory have bus controller units in charge of sending and listening for\nmessages broadcast on the bus. (Listening is sometimes called snooping ). T oday,\nSMP architectures are the most common, because they are the easiest to build,\nbut they are not scalable to large numbers of processors because eventually the\nbus becomes overloaded.\nIn a NUMA architecture, a collection of nodes are linked by a point-to-point\nnetwork, like a tiny local area network. Each node contains one or more pro-\ncessors and a local memory. One node\u2019s local memory is accessible to the other\nnodes, and together, the nodes\u2019 memories form a global memory shared by all\nprocessors. The NUMA name re\ufb02ects the fact that a processor can access memory\nB.5 Caches", "doc_id": "95fc0a79-4dea-4ec5-aed3-a15d7baa996b", "embedding": null, "doc_hash": "36688b3f882baba7333c1010f4e6fd632028b0953ff75d9c8c379243ae5bde01", "extra_info": null, "node_info": {"start": 1175046, "end": 1178805}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "58a28981-4c63-4f88-8fc9-dedb222a2be4", "3": "28be53d0-40c1-49d3-82c4-015ffd58e553"}}, "__type__": "1"}, "28be53d0-40c1-49d3-82c4-015ffd58e553": {"__data__": {"text": "because they are the easiest to build,\nbut they are not scalable to large numbers of processors because eventually the\nbus becomes overloaded.\nIn a NUMA architecture, a collection of nodes are linked by a point-to-point\nnetwork, like a tiny local area network. Each node contains one or more pro-\ncessors and a local memory. One node\u2019s local memory is accessible to the other\nnodes, and together, the nodes\u2019 memories form a global memory shared by all\nprocessors. The NUMA name re\ufb02ects the fact that a processor can access memory\nB.5 Caches 473\nprocessors\ncachesbus\nmemory\nmemoryprocessors\nFigure B.4 An SMP architecture with caches on the right and a cacheless NUMA architecture\non the left.\nresiding on its own node faster than it can access memory residing on other\nnodes. Networks are more complex than buses, and require more elaborate pro-\ntocols, but they scale better than buses to large numbers of processors.\nThe division between SMP and NUMA architectures is a bit of a simpli\ufb01ca-\ntion: one could design hybrid architectures, where processors within a cluster\ncommunicate over a bus, but processors in different clusters communicate over\na network.\nFrom the programmer\u2019s point of view, it may not seem important whether the\nunderlying platform is based on a bus, a network, or a hybrid interconnect. It\nis important, however, to realize that the interconnect is a \ufb01nite resource shared\namong the processors. If one processor uses too much of the interconnect\u2019s band-\nwidth, then the others may be delayed.\nB.4 Memory\nProcessors share a main memory, which is a large array of words, indexed by\naddress. Depending on the platform, a word is typically either 32 or 64 bits, and\nso is an address. Simplifying somewhat, a processor reads a value from memory\nby sending a message containing the desired address to memory. The response\nmessage contains the associated data, that is, the contents of memory at that\naddress. A processor writes a value by sending the address and the new data to\nmemory, and the memory sends back an acknowledgment when the new data\nhas been installed.\nB.5 Caches\nUnfortunately, on modern architectures a main memory access may take hun-\ndreds of cycles, so there is a real danger that a processor may spend much of\nits time just waiting for the memory to respond to requests. We can alleviate\nthis problem by introducing one or more caches : small memories that are situ-\nated closer to the processors and are therefore much faster than main memory.\n474 Appendix B Hardware Basics\nThese caches are logically situated \u201cbetween\u201d the processor and the memory:\nwhen a processor attempts to read a value from a given memory address, it \ufb01rst\nlooks to see if the value is already in the cache, and if so, it does not need to per-\nform the slower access to memory. If the desired address\u2019s value was found, we\nsay the processor hitsin the cache, and otherwise it misses. In a similar way, if a\nprocessor attempts to write an address that is in the cache, it does not need to\nperform the slower access to memory. The proportion of requests satis\ufb01ed in the\ncache is called the cache hit ratio (orhit rate ).\nCaches are effective because most programs display a high degree of locality :\nif a processor reads or writes a memory address (also called a memory location),\nthen it is likely to read or write the same location again soon. Moreover, if a pro-\ncessor reads or writes a memory location, then it is also likely to read or write\nnearby locations soon. T o exploit this second observation, caches typically oper-\nate at a granularity larger than a single word: a cache holds a group of neighboring\nwords called cache lines (sometimes called cache blocks ).\nIn", "doc_id": "28be53d0-40c1-49d3-82c4-015ffd58e553", "embedding": null, "doc_hash": "54fbdc687ff034af39ee945de0ae03f3a96938dc78294c9aeac2d70ddb9cb365", "extra_info": null, "node_info": {"start": 1178887, "end": 1182576}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "95fc0a79-4dea-4ec5-aed3-a15d7baa996b", "3": "a7269508-7340-4022-ac78-7a0d8a4c3d6f"}}, "__type__": "1"}, "a7269508-7340-4022-ac78-7a0d8a4c3d6f": {"__data__": {"text": "is called the cache hit ratio (orhit rate ).\nCaches are effective because most programs display a high degree of locality :\nif a processor reads or writes a memory address (also called a memory location),\nthen it is likely to read or write the same location again soon. Moreover, if a pro-\ncessor reads or writes a memory location, then it is also likely to read or write\nnearby locations soon. T o exploit this second observation, caches typically oper-\nate at a granularity larger than a single word: a cache holds a group of neighboring\nwords called cache lines (sometimes called cache blocks ).\nIn practice, most processors have two levels of caches, called the L1 andL2\ncaches. The L1 cache typically resides on the same chip as the processor, and takes\none or two cycles to access. The L2 cache may reside either on or off-chip, and\nmay take tens of cycles to access. Both are signi\ufb01cantly faster than the hundreds of\ncycles required to access the memory. Of course, these times vary from platform to\nplatform, and many multiprocessors have even more elaborate cache structures.\nThe original proposals for NUMA architectures did not include caches\nbecause it was felt that local memory was enough. Later, however, commercial\nNUMA architectures did include caches. Sometimes the term cc-NUMA (for\ncache-coherent NUMA ) is used to mean NUMA architectures with caches. Here,\nto avoid ambiguity, we use NUMA to include cache-coherence unless we explic-\nitly state otherwise.\nCaches are expensive to build and therefore signi\ufb01cantly smaller than the\nmemory: only a fraction of the memory locations will \ufb01t in a cache at the same\ntime. We would therefore like the cache to maintain values of the most highly used\nlocations. This implies that when a location needs to be cached and the cache is full,\nit is necessary to evict a line, discarding it if it has not been modi\ufb01ed, and writing it\nback to main memory if it has. A replacement policy determines which cache line to\nreplace to make room for a given new location. If the replacement policy is free to\nreplace any line then we say the cache is fully associative. If, on the other hand,\nthere is only one line that can be replaced then we say the cache is direct mapped.\nIf we split the difference, allowing any line from a setof sizekto be replaced to\nmake room for a given line, then we say the cache is k-way set associative.\nB.5.1 Coherence\nSharing (or, less politely, memory contention ), occurs when one processor reads or\nwrites a memory address that is cached by another. If both processors are reading\nthe data without modifying it, then the data can be cached at both processors.\nIf, however, one processor tries to update the shared cache line, then the other\u2019s\ncopy must be invalidated to ensure that it does not read an out-of-date value.\nB.5 Caches 475\nIn its most general form, this problem is called cache coherence. The literature\ncontains a variety of very complex and clever cache coherence protocols. Here we\nreview one of the most commonly used, called the MESI protocol (pronounced\n\u201cmessy\u201d) after the names of possible cache line states. This protocol has been used\nin the Pentium and PowerPC processors. Here are the cache line states.\n\u0004Modi\ufb01ed : the line has been modi\ufb01ed in the cache, and it must eventually be\nwritten back to main memory. No other processor has this line cached.\n\u0004Exclusive : the line has not been modi\ufb01ed, and no other processor has this line\ncached.\n\u0004Shared : the line has not been modi\ufb01ed, and other processors may have this\nline cached.\n\u0004Invalid : the line does not contain meaningful data.\nWe illustrate this", "doc_id": "a7269508-7340-4022-ac78-7a0d8a4c3d6f", "embedding": null, "doc_hash": "2c7f0362e3402a8fdb6dc0f21fd8f3139d58bb42c0c60b27b19dbd6152434d5c", "extra_info": null, "node_info": {"start": 1182531, "end": 1186141}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "28be53d0-40c1-49d3-82c4-015ffd58e553", "3": "24d2310b-1630-49c0-bca7-8a6073a9977d"}}, "__type__": "1"}, "24d2310b-1630-49c0-bca7-8a6073a9977d": {"__data__": {"text": "states. This protocol has been used\nin the Pentium and PowerPC processors. Here are the cache line states.\n\u0004Modi\ufb01ed : the line has been modi\ufb01ed in the cache, and it must eventually be\nwritten back to main memory. No other processor has this line cached.\n\u0004Exclusive : the line has not been modi\ufb01ed, and no other processor has this line\ncached.\n\u0004Shared : the line has not been modi\ufb01ed, and other processors may have this\nline cached.\n\u0004Invalid : the line does not contain meaningful data.\nWe illustrate this protocol by a short example depicted in Fig. B.5. For simplicity,\nwe assume processors and memory are linked by a bus.\nProcessorAreads data from address a, and stores the data in its cache in\ntheexclusive state. When processor Battempts to read from the same address,\nAdetects the address con\ufb02ict, and responds with the associated data. Now ais\nAB\nEa\naC(a)\nAB\nSa a\naSC(b)\nAB\nIa a\naC(c)\nAB\nSa a\naSC(d)\nM\nFigure B.5 Example of the MESI cache coherence protocol\u2019s state transitions. (a) Processor\nAreads data from address a, and stores the data in its cache in the exclusive state. (b) When\nprocessor Battempts to read from the same address, Adetects the address con\ufb02ict, and\nresponds with the associated data. Now ais cached at both AandBin the shared state.\n(c) If Bwrites to the shared address a, it changes its state to modi\ufb01ed, and broadcasts a\nmessage warning A(and any other processor that might have that data cached) to set its\ncache line state to invalid. (d) If Athen reads from a, it broadcasts a request, and Bresponds\nby sending the modi\ufb01ed data both to Aand to the main memory, leaving both copies in the\nshared state.\n476 Appendix B Hardware Basics\ncached at both AandBin the shared state. IfBwrites to the shared address a, it\nchanges its state to modi\ufb01ed, and broadcasts a message warning A(and any other\nprocessor that might have that data cached) to set its cache line state to invalid.\nIfAthen reads from a, it broadcasts a request, and Bresponds by sending the\nmodi\ufb01ed data both to Aand to the main memory, leaving both copies in the\nshared state.\nFalse sharing occurs when processors that are accessing logically distinct data\nnevertheless con\ufb02ict because the locations they are accessing lie on the same\ncache line. This observation illustrates a dif\ufb01cult tradeoff: large cache lines are\ngood for locality, but they increase the likelihood of false sharing. The likeli-\nhood of false sharing can be reduced by ensuring that data objects that might\nbe accessed concurrently by independent threads lie far enough apart in mem-\nory. For example, having multiple threads share a byte array invites false sharing,\nbut having them share an array of double-precision integers is less dangerous.\nB.5.2 Spinning\nA processor is spinning if it is repeatedly testing some word in memory, waiting\nfor another processor to change it. Depending on the architecture, spinning can\nhave a dramatic effect on overall system performance.\nOn an SMP architecture without caches, spinning is a very bad idea. Each time\nthe processor reads the memory, it consumes bus bandwidth without accom-\nplishing any useful work. Because the bus is a broadcast medium, these requests\ndirected to memory may prevent other processors from making progress.\nOn a NUMA architecture without caches, spinning may be acceptable if the\naddress in question resides in the processor\u2019s local memory. Even though multi-\nprocessor architectures without caches are rare, we will still ask when we consider\na", "doc_id": "24d2310b-1630-49c0-bca7-8a6073a9977d", "embedding": null, "doc_hash": "04f953bddb30012ab6979b21f227c249ef5488c4d1cf863c1862cdd4c1f0e570", "extra_info": null, "node_info": {"start": 1186220, "end": 1189703}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a7269508-7340-4022-ac78-7a0d8a4c3d6f", "3": "651fab2c-916f-43b6-b194-4e1352c1f7d7"}}, "__type__": "1"}, "651fab2c-916f-43b6-b194-4e1352c1f7d7": {"__data__": {"text": "can\nhave a dramatic effect on overall system performance.\nOn an SMP architecture without caches, spinning is a very bad idea. Each time\nthe processor reads the memory, it consumes bus bandwidth without accom-\nplishing any useful work. Because the bus is a broadcast medium, these requests\ndirected to memory may prevent other processors from making progress.\nOn a NUMA architecture without caches, spinning may be acceptable if the\naddress in question resides in the processor\u2019s local memory. Even though multi-\nprocessor architectures without caches are rare, we will still ask when we consider\na synchronization protocol that involves spinning, whether it permits each pro-\ncessor to spin on its own local memory.\nOn an SMP or NUMA architecture with caches, spinning consumes signi\ufb01-\ncantly fewer resources. The \ufb01rst time the processor reads the address, it takes a\ncache miss, and loads the contents of that address into a cache line. Thereafter, as\nlong as that data remains unchanged, the processor simply rereads from its own\ncache, consuming no interconnect bandwidth, a process known as local spinning.\nWhen the cache state changes, the processor takes a single cache miss, observes\nthat the data has changed, and stops spinning.\nB.6 Cache-Conscious Programming, or the Puzzle\nSolved\nWe now know enough to explain why the TTASLock examined in Section B.1\noutperforms the TASLock. Each time the TASLock applies getAndSet (true) to\nthe lock, it sends a message on the interconnect causing a substantial amount of\ntraf\ufb01c. In an SMP architecture, the resulting traf\ufb01c may be enough to saturate the\nB.7 Multi-Core and Multi-Threaded Architectures 477\ninterconnect, delaying all threads, including a thread trying to release the lock, or\neven threads not contending for the lock. By contrast, while the lock is busy, the\nTTASLock spins, reading a locally cached copy of the lock, and producing no\ninterconnect traf\ufb01c, explaining its improved performance.\nTheTTASLock is itself however far from ideal. When the lock is released, all\nits cached copies are invalidated, and all waiting threads call getAndSet (true),\nresulting in a burst of traf\ufb01c, smaller than that of the TASLock, but nevertheless\nsigni\ufb01cant.\nWe will further discuss the interactions of caches with locking in Chapter 7. In\nthe meantime, here are some simple ways to structure data to avoid false sharing.\nSome of these techniques are easier to carry out in languages like C or C++ that\nprovide \ufb01ner-grained control over memory use than Java.\n\u0004Objects or \ufb01elds that are accessed independently should be aligned and\npadded so that they end up on different cache lines.\n\u0004Keep read-only data separate from data that is modi\ufb01ed frequently. For\nexample, consider a list whose structure is constant, but whose elements\u2019 value\n\ufb01elds change frequently. T o ensure that modi\ufb01cations do not slow down list\ntraversals, one could align and pad the value \ufb01elds so that each one \ufb01lls up a\ncache line.\n\u0004When possible, split an object into thread-local pieces. For example, a counter\nused for statistics could be split into an array of counters, one per thread,\neach one residing on a different cache line. While a shared counter would\ncause invalidation traf\ufb01c, the split counter allows each thread to update its\nown replica without causing coherence traf\ufb01c.\n\u0004If a lock protects data that is frequently modi\ufb01ed, then keep the lock and the\ndata on distinct cache lines, so that threads trying to acquire the lock do not\ninterfere with", "doc_id": "651fab2c-916f-43b6-b194-4e1352c1f7d7", "embedding": null, "doc_hash": "3c5965b4225a7797c15bf2e19d7813e43ad8f49bbaf3cc88c74d7c1981aa3565", "extra_info": null, "node_info": {"start": 1189617, "end": 1193103}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "24d2310b-1630-49c0-bca7-8a6073a9977d", "3": "88463729-bd29-40ff-9152-f053362d28bb"}}, "__type__": "1"}, "88463729-bd29-40ff-9152-f053362d28bb": {"__data__": {"text": "up a\ncache line.\n\u0004When possible, split an object into thread-local pieces. For example, a counter\nused for statistics could be split into an array of counters, one per thread,\neach one residing on a different cache line. While a shared counter would\ncause invalidation traf\ufb01c, the split counter allows each thread to update its\nown replica without causing coherence traf\ufb01c.\n\u0004If a lock protects data that is frequently modi\ufb01ed, then keep the lock and the\ndata on distinct cache lines, so that threads trying to acquire the lock do not\ninterfere with the lock-holder\u2019s access to the data.\n\u0004If a lock protects data that is frequently uncontended, then try to keep the lock\nand the data on the same cache lines, so that acquiring the lock will also load\nsome of the data into the cache.\nB.7 Multi-Core and Multi-Threaded Architectures\nIn a multi-core architecture, as in Fig. B.6, multiple processors are placed on the\nsame chip. Each processor on that chip typically has its own L1 cache, but they\nshare a common L2 cache. Processors can communicate ef\ufb01ciently through the\nshared L2 cache, avoiding the need to go through memory, and to invoke the\ncumbersome cache coherence protocol.\nIn a multi-threaded architecture, a single processor may execute two or more\nthreads at once. Many modern processors have substantial internal parallelism.\nThey can execute instructions out of order, or in parallel (e.g., keeping both\n\ufb01xed and \ufb02oating-point units busy), or even execute instructions speculatively\n478 Appendix B Hardware Basics\nprocessing cores\nL1 caches\nL2 cache\noff-chip memorymulticore Chip\nFigure B.6 A multi-core SMP architecture. The L2 cache is on chip and shared by all proces-\nsors while the memory is off-chip.\nbefore branches or data have been computed. T o keep hardware units busy, multi-\nthreaded processors can mix instructions from multiple streams.\nModern processor architectures combine multi-core with multi-threading,\nwhere multiple individually multi-threaded cores may reside on the same chip.\nThe context switches on some multi-core chips are inexpensive and are performed\nat a very \ufb01ne granularity, essentially context switching on every instruction. Thus,\nmulti-threading serves to hide the high latency of accessing memory: whenever a\nthread accesses memory, the processor allows another thread to execute.\nB.7.1 Relaxed Memory Consistency\nWhen a processor writes a value to memory, that value is kept in the cache and\nmarked as dirty, meaning that it must eventually be written back to main mem-\nory. On most modern processors, write requests are not applied to memory when\nthey are issued. Rather, they are collected in a hardware queue, called a write\nbuffer (orstore buffer ), and applied to memory together at a later time. A write\nbuffer provides two bene\ufb01ts. First, it is often more ef\ufb01cient to issue a number of\nrequests all at once, a phenomenon called batching. Second, if a thread writes to\nan address more than once, the earlier request can be discarded, saving a trip to\nmemory, a phenomenon called write absorption.\nThe use of write buffers has a very important consequence: the order in which\nreads\u2013writes are issued to memory is not necessarily the order in which they\noccur in the program. For example, recall the \ufb02ag principle of Chapter 1 which\nwas crucial to the correctness of mutual exclusion: if two processors each \ufb01rst\nwrite their own \ufb02ag and then read the other\u2019s \ufb02ag location, then one of them will\nsee the other\u2019s newly written \ufb02ag value. Using write buffers this is no longer true,\nboth may write, each in its respective write buffer, but the buffers", "doc_id": "88463729-bd29-40ff-9152-f053362d28bb", "embedding": null, "doc_hash": "936b627d98998e40946440a3f12fa1efe4d5a346ff677b66861729b50872b16c", "extra_info": null, "node_info": {"start": 1193151, "end": 1196755}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "651fab2c-916f-43b6-b194-4e1352c1f7d7", "3": "8a2f2222-114b-422b-b98c-bc8573c6bea8"}}, "__type__": "1"}, "8a2f2222-114b-422b-b98c-bc8573c6bea8": {"__data__": {"text": "buffers has a very important consequence: the order in which\nreads\u2013writes are issued to memory is not necessarily the order in which they\noccur in the program. For example, recall the \ufb02ag principle of Chapter 1 which\nwas crucial to the correctness of mutual exclusion: if two processors each \ufb01rst\nwrite their own \ufb02ag and then read the other\u2019s \ufb02ag location, then one of them will\nsee the other\u2019s newly written \ufb02ag value. Using write buffers this is no longer true,\nboth may write, each in its respective write buffer, but the buffers may both be\nwritten only after both processors each read the other\u2019s \ufb02ag location in memory.\nThus, neither reads the other\u2019s \ufb02ag.\nCompilers make matters even worse. They are very good at optimizing\nperformance on single-processor architectures. Often, this optimization requires\nB.8 Hardware Synchronization Instructions 479\nreordering an individual thread\u2019s reads\u2013writes to memory. Such reordering is\ninvisible for single-threaded programs, but it can have unexpected consequences\nfor multi-threaded programs in which threads may observe the order in which\nwrites occur. For example, if one thread \ufb01lls a buffer with data and then sets an\nindicator to mark the buffer as full, then concurrent threads may see the indicator\nset before they see the new data, causing them to read stale values. The erroneous\ndouble-checked locking pattern described in Chapter 3 is an example of a pitfall\nproduced by unintuitive aspects of the Java memory model.\nDifferent architectures provide different guarantees about the extent to which\nmemory reads\u2013writes can be reordered. As a rule, it is better not to rely on such\nguarantees, and to use more expensive techniques, described in the following\nparagraph, to prevent such reordering.\nAll architectures allow you to force your writes to take place in the order they\nare issued, but at a price. A memory barrier instruction (sometimes called a fence )\n\ufb02ushes write buffers, ensuring that all writes issued before the barrier become\nvisible to the processor that issued the barrier. Memory barriers are often inserted\ntransparently by atomic read-modify-write operations such as getAndSet (),\nor by standard concurrency libraries. Thus, explicit use of memory barriers\nis needed only when processors perform read\u2013write instructions on shared\nvariables outside of critical sections.\nOn the one hand, memory barriers are expensive (100s of cycles, maybe\nmore), and should be used only when necessary. On the other, synchronization\nbugs can be very dif\ufb01cult to track down, so memory barriers should be used lib-\nerally, rather than relying on complex platform-speci\ufb01c guarantees about limits\nto memory instruction reordering.\nThe Java language itself allows reads\u2013writes to object \ufb01elds to be reordered if\nthey occur outside synchronized methods or blocks. Java provides a volatile\nkeyword that ensures that reads\u2013writes to a volatile object \ufb01eld that occur out-\nsidesynchronized blocks or methods are not reordered. Using this keyword can\nbe expensive, so it should be used only when necessary. We notice that in princi-\nple, one could use volatile \ufb01elds to make double-checked locking work correctly,\nbut there would not be much point, since accessing volatile variables requires\nsynchronization anyway.\nHere ends our primer on multiprocessor hardware. We will continue to dis-\ncuss these architectural concepts in the context of speci\ufb01c data structures and\nalgorithms. A pattern will emerge: the performance of multiprocessor programs\nis highly dependent on synergy with the underlying hardware.\nB.8 Hardware Synchronization Instructions\nAs discussed in Chapter 5, any modern multiprocessor architecture must support\npowerful synchronization primitives to be universal, that", "doc_id": "8a2f2222-114b-422b-b98c-bc8573c6bea8", "embedding": null, "doc_hash": "114191b3591357a274873fba18f34805b97e0088820040fcda5ca8195d0a3bfa", "extra_info": null, "node_info": {"start": 1196775, "end": 1200518}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "88463729-bd29-40ff-9152-f053362d28bb", "3": "52f119fb-66f2-4af5-bcc7-db9266c7ba07"}}, "__type__": "1"}, "52f119fb-66f2-4af5-bcc7-db9266c7ba07": {"__data__": {"text": "work correctly,\nbut there would not be much point, since accessing volatile variables requires\nsynchronization anyway.\nHere ends our primer on multiprocessor hardware. We will continue to dis-\ncuss these architectural concepts in the context of speci\ufb01c data structures and\nalgorithms. A pattern will emerge: the performance of multiprocessor programs\nis highly dependent on synergy with the underlying hardware.\nB.8 Hardware Synchronization Instructions\nAs discussed in Chapter 5, any modern multiprocessor architecture must support\npowerful synchronization primitives to be universal, that is, provide concur-\nrent computation\u2019s equivalent of a Universal Turing machine. It is therefore not\n480 Appendix B Hardware Basics\nsurprising that the implementation of the Java language relies on such special-\nized hardware instructions (also called hardware primitives) in implementing\nsynchronization, from spin-locks and monitors to the most complex lock-free\nstructures.\nModern architectures typically provide one of two kinds of universal synchro-\nnization primitives. The compare-and-swap (CAS) instruction is supported in\narchitectures by AMD, Intel, and Sun. It takes three arguments: an address ain\nmemory, an expected valuee, and an update valuev. It returns a Boolean. It\natomically executes the following steps:\n\u0004If the memory at address acontains the expected value e,\n\u0004write the update value vto that address and return true,\n\u0004otherwise leave the memory unchanged and return false .\nOn Intel and AMD architectures, CAS is called CMPXCHG, while on SPARCTM\nit is called CAS.1Java\u2019s java.util.concurrent.atomic library provides atomic\nBoolean, integer, and reference classes that implement CAS by a compareAndSet()\nmethod. (Because our examples are mostly in Java, we refer to compareAndSet()\ninstead of CAS everywhere else.) C# provides the same functionality with the\nInterlocked.CompareExchange method.\nThe CAS instruction has one pitfall. Perhaps the most common use of CAS\nis the following. An application reads value afrom a given memory address,\nand computes a new value cfor that location. It intends to store c, but only if\nthe valueain the address has not changed since it was read. One might think\nthat applying a CAS with expected value aand update value cwould accomplish\nthis goal. There is a problem: a thread could have overwritten the value awith\nanother value b, and later written aagain to the address. The compare-and-swap\nwill replaceawithc, but the application may not have done what it was intended\nto do (for example, if the address stores a pointer, the new value amay be the\naddress of a recycled object). The CAS call will replace ewithv, but the applica-\ntion may not have done what it was intended to do. This problem is known as the\nABA problem, discussed in detail in Chapter 10.\nAnother hardware synchronization primitive is a pair of instructions:\nload-linked and store-conditional (LL/SC). The LL instruction reads from an\naddressa. A later SC instruction to aattempts to store a new value at that address.\nThe instruction succeeds if the contents of address aare unchanged since that\nthread issued the earlier LL instruction to a. It fails if the contents of that address\nhas changed in the interval.\nThe LL and SC instructions are supported by a number of architectures: Alpha\nAXP (ldl l/stl c), IBM PowerPC (lwarx/stwcx) MIPS ll/sc, and ARM\n(ldrex/strex). LL/SC does not suffer from the ABA problem, but in practice\nthere are often severe restrictions on what a thread can do between a LL and the\n1Instead of a Boolean, CAS on Sparc returns the location\u2019s prior value, which can be used to", "doc_id": "52f119fb-66f2-4af5-bcc7-db9266c7ba07", "embedding": null, "doc_hash": "fac7bfd74af8f5f39f3817f329c4740278fadab957a7897ccf7f4b79a50de203", "extra_info": null, "node_info": {"start": 1200446, "end": 1204076}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "8a2f2222-114b-422b-b98c-bc8573c6bea8", "3": "a3cb16e7-7c58-4084-867f-1970821bfb25"}}, "__type__": "1"}, "a3cb16e7-7c58-4084-867f-1970821bfb25": {"__data__": {"text": "instruction to a. It fails if the contents of that address\nhas changed in the interval.\nThe LL and SC instructions are supported by a number of architectures: Alpha\nAXP (ldl l/stl c), IBM PowerPC (lwarx/stwcx) MIPS ll/sc, and ARM\n(ldrex/strex). LL/SC does not suffer from the ABA problem, but in practice\nthere are often severe restrictions on what a thread can do between a LL and the\n1Instead of a Boolean, CAS on Sparc returns the location\u2019s prior value, which can be used to retry\nan unsuccessful CAS. CMPXCHG on Intel\u2019s Pentium effectively returns both a Boolean and the\nprior value.\nB.10 Exercises 481\nmatching SC. A context switch, another LL, or another load or store instruction\nmay cause the SC to fail.\nIt is good idea to use atomic \ufb01elds and their associated methods sparingly\nbecause they are often based on CAS or LL/SC. A CAS or LL/SC instruction takes\nsigni\ufb01cantly more cycles to complete than a load or store: it includes a memory\nbarrier and prevents out-of-order execution and various compiler optimizations.\nThe precise cost depends on many factors, and varies not only from one archi-\ntecture to the next, but also from one application of the instruction to the next\nwithin the same architecture. It suf\ufb01ces to say that CAS or LL/SC can be an order\nof magnitude slower than a simple load or store.\nB.9 Chapter Notes\nJohn Hennessy and David Patterson [58] give a comprehensive treatment of com-\nputer architecture. The MESI protocol is used by Intel\u2019s Pentium processor [75].\nThe tips on cache-conscious programming are adapted from Benjamin Gamsa,\nOrran Krieger, Eric Parsons, and Michael Stumm [43]. Sarita Adve and Karosh\nGharachorloo [1] give an excellent survey of memory consistency models.\nB.10 Exercises\nExercise 219. ThreadAmust wait for a thread on another processor to change a\n\ufb02ag bit in memory. The scheduler can either allow Ato spin, repeatedly retesting\nthe \ufb02ag, or it can deschedule A, allowing some other thread to run. Suppose it\ntakes a total of 10 milliseconds for the operationg system to switch a processor\nfrom one thread to another. If the operating system deschedules thread Aand\nimmediately reschedules it, then it wastes 20 milliseconds. If, instead, Astarts\nspinning at time t0, and the \ufb02ag changes at t1, then the operating system will\nhave wasted t1\u0000t0time doing unproductive work.\nAprescient scheduler is one that can predict the future. If it foresees that the\n\ufb02ag will change in less than 20 milliseconds, it makes sense to have Aspin, wast-\ning less than 20 milliseconds, because descheduling and rescheduling Awastes\n20 milliseconds. If, on the other hand, it takes more than 20 milliseconds for the\n\ufb02ag to change, it makes sense to replace Awith another thread, wasting no more\nthan 20 milliseconds.\nY our assignment is to implement a scheduler that never wastes more than\ntwice the time a prescient scheduler would have wasted under the same circum-\nstances.\nExercise 220. Imagine you are a lawyer, paid to make the best case you can for\na particular point of view. How would you argue the following claim: if context\nswitches took negligible time, then processors would not need caches, at least for\napplications that encompass large numbers of threads.\nExtra credit: critique your argument.\n482 Appendix B Hardware Basics\nExercise 221. Consider a direct-mapped cache with 16 cache", "doc_id": "a3cb16e7-7c58-4084-867f-1970821bfb25", "embedding": null, "doc_hash": "5a5a94cc1e6a33008bea7b7363850cd4ea551e729bd2de407ad1f0db03d517aa", "extra_info": null, "node_info": {"start": 1204196, "end": 1207536}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "52f119fb-66f2-4af5-bcc7-db9266c7ba07", "3": "69c35237-3d27-4b3b-bf9e-33473e1d29f9"}}, "__type__": "1"}, "69c35237-3d27-4b3b-bf9e-33473e1d29f9": {"__data__": {"text": "implement a scheduler that never wastes more than\ntwice the time a prescient scheduler would have wasted under the same circum-\nstances.\nExercise 220. Imagine you are a lawyer, paid to make the best case you can for\na particular point of view. How would you argue the following claim: if context\nswitches took negligible time, then processors would not need caches, at least for\napplications that encompass large numbers of threads.\nExtra credit: critique your argument.\n482 Appendix B Hardware Basics\nExercise 221. Consider a direct-mapped cache with 16 cache lines, indexed 0 to\n15, where each cache line encompasses 32 words.\n\u0004Explain how to map an address ato a cache line in terms of bit shifting and\nmasking operations. Assume for this question that addresses refer to words,\nnot bytes: address 7 refers to the 7th word in memory.\n\u0004Compute the best and worst possible hit ratios for a program that loops 4\ntimes through an array of 64 words.\n\u0004Compute the best and worst possible hit ratios for a program that loops 4\ntimes through an array of 512 words.\nExercise 222. Consider a direct-mapped cache with 16 cache lines, indexed 0 to\n15, where each cache line encompasses 32 words.\nConsider a two-dimensional, 32 \u000232 array of words a. This array is laid out in\nmemory so that a[0, 0] is next to a[0, 1], and so on. Assume the cache is initially\nempty, but that a[0, 0] maps to the \ufb01rst word of cache line 0.\nConsider the following column-\ufb01rst traversal:\nint sum = 0;\nfor (int i = 0; i < 32; i++) {\nfor (int j = 0; j < 32; j++) {\nsum += a[i,j]; // 2nd dim changes fastest\n}\n}\nand the following row-\ufb01rst traversal:\nint sum = 0;\nfor (int i = 0; i < 32; i++) {\nfor (int j = 0; j < 32; j++) {\nsum += a[j,i]; // 1st dim changes fastest\n}\n}\nCompare the number of cache misses produced by the two traversals, assuming\nthe oldest cache line is evicted \ufb01rst.\nExercise 223. In the MESI cache-coherence protocol, what is the advantage of\ndistinguishing between exclusive and modi\ufb01ed modes?\nWhat is the advantage of distinguishing between exclusive and shared modes?\nExercise 224. Implement the test-and-set and test-and-test-and-set locks shown\ninFigs. B.1 and B.2, test their relative performance on a multiprocessor, and\nanalyze the results.\nBibliography\n[1] S. V . Adve and K. Gharachorloo. Shared memory consistency models:\nA tutorial. Computer, 29(12):66\u201376, 1996.\n[2] Y. Afek, H. Attiya, D. Dolev, E. Gafni, M. Merritt, and N. Shavit. Atomic\nsnapshots of shared memory. Journal of the ACM (JACM), 40(4):873\u2013890,\n1993.\n[3] Y. Afek, D. Dauber, and D. T ouitou. Wait-free made fast. In STOC \u201995: Proc.\nof the Twenty-seventh Annual ACM Symposium on Theory of Computing,\npp. 538\u2013547, NY, USA, 1995, ACM Press.\n[4] Y. Afek, G. Stupp, and D. T ouitou. Long-lived and adaptive atomic snap-\nshot and immediate snapshot (extended abstract). In PODC \u201900: Proc.\nof the Nineteenth Annual ACM Symposium on Principles of Distributed\nComputing, Portland, Oregon, USA, pp. 71\u201380, NY, USA, 2000,", "doc_id": "69c35237-3d27-4b3b-bf9e-33473e1d29f9", "embedding": null, "doc_hash": "36c5c0de938ab703a99f4c0ae3d0b0af287f1a3e8e1d83faa006156470b8546c", "extra_info": null, "node_info": {"start": 1207458, "end": 1210434}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a3cb16e7-7c58-4084-867f-1970821bfb25", "3": "a4dc758c-ff84-4542-898b-853228f99617"}}, "__type__": "1"}, "a4dc758c-ff84-4542-898b-853228f99617": {"__data__": {"text": "Wait-free made fast. In STOC \u201995: Proc.\nof the Twenty-seventh Annual ACM Symposium on Theory of Computing,\npp. 538\u2013547, NY, USA, 1995, ACM Press.\n[4] Y. Afek, G. Stupp, and D. T ouitou. Long-lived and adaptive atomic snap-\nshot and immediate snapshot (extended abstract). In PODC \u201900: Proc.\nof the Nineteenth Annual ACM Symposium on Principles of Distributed\nComputing, Portland, Oregon, USA, pp. 71\u201380, NY, USA, 2000, ACM\nPress.\n[5] Y. Afek, E. Weisberger, and H. Weisman. A completeness theorem for a\nclass of synchronization objects. In PODC \u201993: Proc. of the Twelfth Annual\nACM Symposium on Principles of Distributed Computing, pp. 159\u2013170, NY,\nUSA, 1993, ACM Press.\n[6] A. Agarwal and M. Cherian. Adaptive backoff synchronization techniques.\nInProc. of the Sixteenth International Symposium on Computer Architec-\nture, pp. 396\u2013406, May 1989.\n[7] O. Agesen, D. Detlefs, A. Garthwaite, R. Knippel, Y. S. Ramakrishna, and\nD. White. An ef\ufb01cient meta-lock for implementing ubiquitous synchro-\nnization. ACM SIGPLAN Notices, 34(10):207\u2013222, 1999.\n[8] M. Ajtai, J. Koml \u00b4os, and E. Szemer \u00b4edi. AnO(nlogn) sorting network.\nCombinatorica, 3(1):1\u201319, 1983.\n[9] G. M. Amdahl. Validity of the single-processor approach to achieving large\nscale computing capabilities. In AFIPS Conference Proceedings, pp. 483\u2013\n485, Atlantic City, NJ, April 1967, Reston, V A, USA, AFIPS Press.\n483\n484 Bibliography\n[10] J. H. Anderson. Composite registers. Distributed Computing, 6(3):141\u2013154,\n1993.\n[11] J. H. Anderson and M. Moir. Universal constructions for multi-object\noperations. In PODC \u201995: Proc. of the Fourteenth Annual ACM Symposium\non Principles of Distributed Computing, pp. 184\u2013193, NY, USA, 1995, ACM\nPress.\n[12] J. H. Anderson, M. G. Gouda, and A. K. Singh. The elusive atomic register.\nT echnical Report TR 86.29, University of T exas at Austin, 1986.\n[13] J. H. Anderson, M. G. Gouda, and A. K. Singh. The elusive atomic register.\nJournal of the ACM, 41(2):311\u2013339, 1994.\n[14] T. E. Anderson. The performance of spin lock alternatives for\nshared-memory multiprocessors. IEEE Transactions on Parallel and Dis-\ntributed Systems, 1(1):6\u201316, 1990.\n[15] N. S. Arora, R. D. Blumofe, and C. G. Plaxton. Thread scheduling for\nmultiprogrammed multiprocessors. In Proc. of the Tenth Annual ACM\nSymposium on Parallel Algorithms and Architectures, pp. 119\u2013129, NY,\nUSA, 1998, ACM Press.\n[16] J. Aspnes, M. Herlihy, and N. Shavit. Counting networks. Journal of the\nACM, 41(5):1020\u20131048, 1994.\n[17] D. F. Bacon, R. B. Konuru, C. Murthy, and M. J. Serrano. Thin locks:\nFeatherweight synchronization for Java. In PLDI \u201998: Proc. of the ACM\nSIGPLAN", "doc_id": "a4dc758c-ff84-4542-898b-853228f99617", "embedding": null, "doc_hash": "1aca51367087dc03ef35b600076270f6ccedab4daea608fa270c84fa8e67c6b9", "extra_info": null, "node_info": {"start": 1210556, "end": 1213182}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "69c35237-3d27-4b3b-bf9e-33473e1d29f9", "3": "10a0ac3d-9c0d-4a07-b474-42ad22f888e4"}}, "__type__": "1"}, "10a0ac3d-9c0d-4a07-b474-42ad22f888e4": {"__data__": {"text": "Annual ACM\nSymposium on Parallel Algorithms and Architectures, pp. 119\u2013129, NY,\nUSA, 1998, ACM Press.\n[16] J. Aspnes, M. Herlihy, and N. Shavit. Counting networks. Journal of the\nACM, 41(5):1020\u20131048, 1994.\n[17] D. F. Bacon, R. B. Konuru, C. Murthy, and M. J. Serrano. Thin locks:\nFeatherweight synchronization for Java. In PLDI \u201998: Proc. of the ACM\nSIGPLAN 1998 conference on Programming Language Design and Imple-\nmentation, Montreal, Quebec, Canada, pp. 258\u2013268, NY, USA, 1998, ACM\nPress.\n[18] K. Batcher. Sorting Networks and Their Applications. In Proc. of the AFIPS\nSpring Joint Computer Conference, 32:307\u2013314, Reston, V A, USA, 1968.\n[19] R. Bayer and M. Schkolnick. Concurrency of operations on B-trees. Acta\nInformatica, 9:1\u201321, 1977.\n[20] R. D. Blumofe and C. E. Leiserson. Scheduling multithreaded compu-\ntations by work stealing. Journal of the ACM (JACM), 46(5):720\u2013748,\n1999.\n[21] H. J. Boehm. Threads cannot be implemented as a library. In PLDI \u201905:\nProc. of the 2005 ACM SIGPLAN Conference on Programming Language\nDesign and Implementation, pp. 261\u2013268, NY, USA, 2005, ACM Press.\n[22] E. Borowsky and E. Gafni. Immediate atomic snapshots and fast renaming.\nInPODC \u201993: Proc. of the Twelfth Annual ACM Symposium on Principles of\nDistributed Computing, pp. 41\u201351, NY, USA, 1993, ACM Press.\n[23] J. E. Burns and N. A. Lynch. Bounds on shared memory for mutual exclu-\nsion. Information and Computation, 107(2):171\u2013184, December 1993.\n[24] J. E. Burns and G. L. Peterson. Constructing multi-reader atomic values\nfrom non-atomic values. In PODC \u201987: Proc. of the Sixth Annual ACM\nSymposium on Principles of Distributed Computing , pp. 222\u2013231, NY, USA,\n1987, ACM Press.\n[25] C. Busch and M. Mavronicolas. A combinatorial treatment of balancing\nnetworks. Journal of the ACM, 43(5):794\u2013839, 1996.\nBibliography 485\n[26] T. D. Chandra, P . Jayanti, and K. Tan. A polylog time wait-free construc-\ntion for closed objects. In PODC \u201998: Proc. of the Seventeenth Annual ACM\nSymposium on Principles of Distributed Computing , pp. 287\u2013296, NY, USA,\n1998, ACM Press.\n[27] G. Chapman, J. Cleese, T. Gilliam, E. Idle, T. Jones, and M. Palin. Monty\nphyton and the holy grail, Motion Picture, Michael White Productions,\nReleased 10 May 1975, USA.\n[28] D. Chase and Y. Lev. Dynamic circular work-stealing deque. In SPAA \u201905:\nProc. of the Seventeenth Annual ACM Symposium on Parallelism in Algo-\nrithms and Architectures, pp. 21\u201328, NY, USA, 2005, ACM Press.\n[29] A. Church. A note on the entscheidungs problem. Journal of Symbolic\nLogic, 1936.\n[30] T. Craig. Building FIFO and priority-queueing spin locks from atomic\nswap. T echnical Report TR 93-02-02, University of Washington, Depart-\nment of Computer", "doc_id": "10a0ac3d-9c0d-4a07-b474-42ad22f888e4", "embedding": null, "doc_hash": "b7a14a54ce0ee5fbcdd72675d19d21c630088250b223cca2f1be8645409a0957", "extra_info": null, "node_info": {"start": 1213233, "end": 1215934}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a4dc758c-ff84-4542-898b-853228f99617", "3": "3cd5081b-a795-4657-812e-3c96aec80c97"}}, "__type__": "1"}, "3cd5081b-a795-4657-812e-3c96aec80c97": {"__data__": {"text": "and Y. Lev. Dynamic circular work-stealing deque. In SPAA \u201905:\nProc. of the Seventeenth Annual ACM Symposium on Parallelism in Algo-\nrithms and Architectures, pp. 21\u201328, NY, USA, 2005, ACM Press.\n[29] A. Church. A note on the entscheidungs problem. Journal of Symbolic\nLogic, 1936.\n[30] T. Craig. Building FIFO and priority-queueing spin locks from atomic\nswap. T echnical Report TR 93-02-02, University of Washington, Depart-\nment of Computer Science, February 1993.\n[31] D. Dice. Implementing fast Java monitors with relaxed-locks. Proc. of\nthe JavaTM Virtual Machine Research and Technology Symposium on\nJavaTM Virtual Machine Research and Technology Symposium, Monterey,\nCalifornia, p. 13, April 23\u201324, 2001.\n[32] D. Dice, O. Shalev, and N. Shavit. Transactional locking II. Proc. of\nthe Twentieth International Symposium on Distributed Computing (DISC\n2006), Stockholm, Sweden, pp. 194\u2013208, 2006.\n[33] E. W. Dijkstra. The structure of the THE multiprogramming system.\nCommunications of the ACM, 11(5):341\u2013346, NY, USA, 1968, ACM Press.\n[34] D. Dolev and N. Shavit. Bounded concurrent time-stamping. SIAM Jour-\nnal of Computing, 26(2):418\u2013455, 1997.\n[35] M. Dowd, Y. Perl, L. Rudolph, and M. Saks. The periodic balanced sorting\nnetwork. Journal of the ACM, 36(4):738\u2013757, 1989.\n[36] A. C. Doyle. A Study in Scarlet and the Sign of Four. Berkley Publishing\nGroup, NY, 1994. ISBN: 0425102408.\n[37] C. Dwork and O. Waarts. Simple and ef\ufb01cient bounded concurrent times-\ntamping and the traceable use abstraction. Journal of the ACM (JACM),\n46(5):633\u2013666, 1999.\n[38] C. Ellis. Concurrency in linear hashing. ACM Transactions on Database\nSystems (TODS), 12(2):195\u2013217, 1987.\n[39] F. E. Fich, D. Hendler, and N. Shavit. On the inherent weakness of condi-\ntional primitives. Distributed Computing, 18(4):267\u2013277, 2006.\n[40] M. J. Fischer, N. A. Lynch, and M. S. Paterson. Impossibility of distri-\nbuted consensus with one faulty process. Journal of the ACM (JACM),\n32(2):374\u2013382, 1985.\n[41] C. H. Flood, D. Detlefs, N. Shavit, and X. Zhang. Parallel garbage collec-\ntion for shared memory multiprocessors. In JVM \u201901 Proc. of the JavaTM\nVirtual Machine Research and Technology Symposium on JavaTM Virtual\nMachine Research and Technology Symposium, Monterey, California, 2001.\nBerkelay, CA, USA, USENIX Association.\n486 Bibliography\n[42] K. Fraser. Practical Lock-Freedom. Ph.D. dissertation, Kings College,\nUniversity of Cambridge, Cambridge, England, September 2003.\n[43] B. Gamsa, O. Kreiger, E. W. Parsons, and M. Stumm. Performance issues\nfor multiprocessor operating systems. T echnical report, Computer Sys-\ntems Research Institute, University of T oronto, 1995.\n[44] H. Gao, J. F. Groote, and W. H. Hesselink. Lock-free dynamic hash tables\nwith open addressing. Distributed Computing, 18(1):21\u201342, 2005.\n[45]", "doc_id": "3cd5081b-a795-4657-812e-3c96aec80c97", "embedding": null, "doc_hash": "0688a6caba14e18df750aec7cc811d606fd8c82fc5bc99c563409bc68f19b51c", "extra_info": null, "node_info": {"start": 1215861, "end": 1218672}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "10a0ac3d-9c0d-4a07-b474-42ad22f888e4", "3": "3ec3efef-9b4d-4bc9-8294-3b8e66604083"}}, "__type__": "1"}, "3ec3efef-9b4d-4bc9-8294-3b8e66604083": {"__data__": {"text": "dissertation, Kings College,\nUniversity of Cambridge, Cambridge, England, September 2003.\n[43] B. Gamsa, O. Kreiger, E. W. Parsons, and M. Stumm. Performance issues\nfor multiprocessor operating systems. T echnical report, Computer Sys-\ntems Research Institute, University of T oronto, 1995.\n[44] H. Gao, J. F. Groote, and W. H. Hesselink. Lock-free dynamic hash tables\nwith open addressing. Distributed Computing, 18(1):21\u201342, 2005.\n[45] J. R. Goodman, M. K. Vernon, and P . J. Woest. Ef\ufb01cient synchronization\nprimitives for large-scale cache-coherent multiprocessors. In Proc. of the\nThird International Conference on Architectural support for Programming\nLanguages and Operating Systems, pp. 64\u201375, 1989, ACM Press.\n[46] J. Gosling, B. Joy, G. L. Steele Jr., and G. Bracha. The Java Language Speci-\n\ufb01cation, Prentice Hall PTR, third edition, Upper Saddle River, New Jersey,\nUSA, 2005. ISBN: 0321246780.\n[47] A. Gottlieb, R. Grishman, C. P . Kruskal, K. P . McAuliffe, L. Rudolph, and\nM. Snir. The NYU ultracomputer-designing an MIMD parallel computer.\nIEEE Transactions on Computers, C-32(2):175\u2013189, February 1984.\n[48] M. Greenwald. Two-handed emulation: How to build non-blocking\nimplementations of complex data-structures using DCAS. In PODC \u201902:\nProc. of the Twenty-\ufb01rst Annual Symposium on Principles of Distributed\nComputing, Monterey, California, pp. 260\u2013269, NY, USA, July 2002, ACM\nPress.\n[49] R. Guerraoui, M. Herlihy, and B. Pochon. T oward a theory of transactional\ncontention managers. In PODC \u201905: Proc. of the Twenty-fourth Annual\nACM Symposium on Principles of Distributed Computing, pp. 258\u2013264,\nLas Vegas, NY, USA, 2005, ACM Press.\n[50] S. Haldar and K. Vidyasankar. Constructing 1-writer multireader mul-\ntivalued atomic variables from regular variables. Journal of the ACM,\n42(1):186\u2013203, 1995.\n[51] S. Haldar and P . Vit \u00b4anyi. Bounded concurrent timestamp systems using\nvector clocks. Journal of the ACM (JACM), 49(1):101\u2013126, 2002.\n[52] P . B. Hansen. Structured multi-programming. Communications of the\nACM, 15(7):574\u2013578, 1972.\n[53] T. Harris. A pragmatic implementation of non-blocking linked-lists.\nInProc. of Fifteenth International Symposium on Distributed Computing\n(DISC 2001), Lisbon, Portugal, volume 2180 of Lecture Notes in Computer\nScience, pp. 300\u2013314, October 2001, Springer-Verlag.\n[54] T. Harris, S. Marlowe, S. Peyton-Jones, and M. Herlihy. Composable mem-\nory transactions. In PPoPP \u201905: Proc. of the Tenth ACM SIGPLAN Sympo-\nsium on Principles and Practice of Parallel Programming, Chicago, IL, USA,\npp. 48\u201360, NY, USA, 2005, ACM Press.\n[55] S. Heller, M. Herlihy, V . Luchangco, M. Moir, W. N. Scherer III, and\nN. Shavit. A Lazy Concurrent List-Based Set Algorithm. Proc. of the", "doc_id": "3ec3efef-9b4d-4bc9-8294-3b8e66604083", "embedding": null, "doc_hash": "b20e908e748c013ba7ecd1202be50b7e32ffc6d5356e3e789224a3c5ff146827", "extra_info": null, "node_info": {"start": 1218672, "end": 1221397}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3cd5081b-a795-4657-812e-3c96aec80c97", "3": "56c50e83-778c-4a8d-a6c9-82b1dddf9c61"}}, "__type__": "1"}, "56c50e83-778c-4a8d-a6c9-82b1dddf9c61": {"__data__": {"text": "Peyton-Jones, and M. Herlihy. Composable mem-\nory transactions. In PPoPP \u201905: Proc. of the Tenth ACM SIGPLAN Sympo-\nsium on Principles and Practice of Parallel Programming, Chicago, IL, USA,\npp. 48\u201360, NY, USA, 2005, ACM Press.\n[55] S. Heller, M. Herlihy, V . Luchangco, M. Moir, W. N. Scherer III, and\nN. Shavit. A Lazy Concurrent List-Based Set Algorithm. Proc. of the Ninth\nInternational Conference on Principles of Distributed Systems (OPODIS\n2005), Pisa, Italy, pp. 3\u201316, 2005.\nBibliography 487\n[56] D. Hendler and N. Shavit. Non-blocking Steal-half Work Queues. In Proc.\nof the Twenty-\ufb01rst Annual ACM Symposium on Principles of Distributed\nComputing (PODC), Monterey, California, pp. 280\u2013289, 2002, ACM Press.\n[57] D. Hendler, N. Shavit, and L. Y erushalmi. A scalable lock-free stack\nalgorithm. In SPAA \u201904: Proc. of the Sixteenth Annual ACM Symposium on\nParallelism in Algorithms and Architectures, pp. 206\u2013215, NY, USA, 2004,\nACM Press.\n[58] J. L. Hennessy and D. A. Patterson. Computer Architecture: A Quantitative\nApproach. Morgan Kaufmann Publishers, 1995.\n[59] D. Hensgen, R. Finkel, and U. Manber. Two algorithms for barrier\nsynchronization. International Journal of Parallel Programming, 17(1):\n1\u201317, 0885-7458 1988.\n[60] M. Herlihy. A methodology for implementing highly concurrent data\nobjects. ACM Transactions on Programming Languages and Systems,\n15(5):745\u2013770, November 1993.\n[61] M. Herlihy, V . Luchangco, and M. Moir. Obstruction-Free Synchroniza-\ntion: Double-Ended Queues as an Example. In ICDCS \u201903: Proc. of the\nTwenty-third International Conference on Distributed Computing Systems,\np. 522, Washington, DC, USA, 2003. IEEE Computer Society.\n[62] M. Herlihy. Wait-free synchronization. ACM Transactions on Programming\nLanguages and Systems (TOPLAS), 13(1):124\u2013149, 1991.\n[63] M. Herlihy and N. Shavit. On the nature of progress. In Principles of Dis-\ntributed Systems\u201415th International Conference, OPODIS 2011, T oulouse,\nFrance, pp. 313\u2013328, December 13\u201316, 2011.\n[64] M. Herlihy, Y. Lev, and N. Shavit. A lock-free concurrent skiplist with\nwait-free search. Unpublished Manuscript, Sun Microsystems Laborato-\nries, Burlington, Massachusetts, 2007.\n[65] M. Herlihy, B.-H. Lim, and N. Shavit. Scalable concurrent counting. ACM\nTransactions on Computer Systems, 13(4):343\u2013364, 1995.\n[66] M. Herlihy, V . Luchangco, M. Moir, and W. N. Scherer III. Software trans-\nactional memory for dynamic-sized data structures. In PODC \u201903, Proc. of\nthe Twenty-second Annual Symposium on Principles of Distributed Comput-\ning, Boston, Massachusetts, pp. 92\u2013101, NY, USA, 2003, ACM Press.\n[67] M. Herlihy and J. E. B. Moss. Transactional memory: architectural sup-\nport for lock-free data structures. In Proc. of the Twentieth Annual Inter-\nnational", "doc_id": "56c50e83-778c-4a8d-a6c9-82b1dddf9c61", "embedding": null, "doc_hash": "60c3c982dbf461df850b563df241f8854aada6fb74bc2cb99b919ecd743cef67", "extra_info": null, "node_info": {"start": 1221467, "end": 1224229}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "3ec3efef-9b4d-4bc9-8294-3b8e66604083", "3": "dc69d50e-6515-4bbe-a837-98585e516100"}}, "__type__": "1"}, "dc69d50e-6515-4bbe-a837-98585e516100": {"__data__": {"text": "V . Luchangco, M. Moir, and W. N. Scherer III. Software trans-\nactional memory for dynamic-sized data structures. In PODC \u201903, Proc. of\nthe Twenty-second Annual Symposium on Principles of Distributed Comput-\ning, Boston, Massachusetts, pp. 92\u2013101, NY, USA, 2003, ACM Press.\n[67] M. Herlihy and J. E. B. Moss. Transactional memory: architectural sup-\nport for lock-free data structures. In Proc. of the Twentieth Annual Inter-\nnational Symposium on Computer Architecture, pp. 289\u2013300, San Diego,\nCalifornia, 1993, ACM Press.\n[68] M. Herlihy, N. Shavit, and M. Tzafrir. Concurrent cuckoo hashing. T ech-\nnical report, Providence RI, Brown University, 2007.\n[69] M. Herlihy and J. M. Wing. Linearizability: a correctness condition for\nconcurrent objects. ACM Transactions on Programming Languages and\nSystems (TOPLAS), 12(3):463\u2013492, 1990.\n[70] C. A. R. Hoare. \u201cpartition: Algorithm 63,\u201d \u201cquicksort: Algorithm 64,\u201d\nand \u201c\ufb01nd: Algorithm 65.\u201d Communications of the ACM, 4(7):321\u2013322,\n1961.\n488 Bibliography\n[71] C. A. R. Hoare. Monitors: an operating system structuring concept.\nCommunications of the ACM, 17(10):549\u2013557, 1974.\n[72] M. Hsu and W. P . Y ang. Concurrent operations in extendible hashing. In\nSymposium on Very Large Data Bases, pp. 241\u2013247, San Francisco, CA,\nUSA, 1986. Morgan Kaufmann Publishers Inc.\n[73] J. S. Huang and Y. C. Chow. Parallel sorting and data partitioning by sam-\npling. In Proc. of the IEEE Computer Society\u2019s Seventh International Com-\nputer Software and Applications Conference, pp. 627\u2013631, 1983.\n[74] G. C. Hunt, M. M. Michael, S. Parthasarathy, and M. L. Scott. An ef\ufb01-\ncient algorithm for concurrent priority queue heaps. Inf. Process. Lett.,\n60(3):151\u2013157, 1996.\n[75] Intel Corporation. Pentium Processor User\u2019s Manual. Intel Books, 1993.\nISBN: 1555121934.\n[76] A. Israeli and L. Rappaport. Disjoint-access-parallel implementations of\nstrong shared memory primitives. In PODC \u201994: Proc. of the Thirteenth\nAnnual ACM Symposium on Principles of Distributed Computing, Los\nAngeles, California, United States, pp. 151\u2013160, NY, USA, August 14\u201317\n1994, ACM Press.\n[77] A. Israeli and M. Li. Bounded time stamps. Distributed Computing, 6(5):\n205\u2013209, 1993.\n[78] A. Israeli and A. Shaham. Optimal multi-writer multi-reader atomic\nregister. In PODC \u201992: Proc. of the Eleventh Annual ACM Symposium\non Principles of Distributed Computing, Vancouver, British Columbia,\nCanada, pp. 71\u201382, NY, USA, 1992, ACM Press.\n[79] P . Jayanti. Robust wait-free hierarchies. Journal of the ACM, 44(4):592\u2013\n614, 1997.\n[80] P . Jayanti. A lower bound on the local time complexity of universal\nconstructions. In PODC \u201998: Proc. of the Seventeenth Annual ACM Sympo-\nsium on Principles of Distributed Computing, pp. 183\u2013192, NY, USA, 1998,\nACM Press.\n[81] P . Jayanti and S. T", "doc_id": "dc69d50e-6515-4bbe-a837-98585e516100", "embedding": null, "doc_hash": "883a747f435853df011242c56717937d49525a16a52c9d939982faf5f005ac41", "extra_info": null, "node_info": {"start": 1224170, "end": 1226951}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "56c50e83-778c-4a8d-a6c9-82b1dddf9c61", "3": "558109a6-7026-4c8f-8778-2cdf16c09cfa"}}, "__type__": "1"}, "558109a6-7026-4c8f-8778-2cdf16c09cfa": {"__data__": {"text": "British Columbia,\nCanada, pp. 71\u201382, NY, USA, 1992, ACM Press.\n[79] P . Jayanti. Robust wait-free hierarchies. Journal of the ACM, 44(4):592\u2013\n614, 1997.\n[80] P . Jayanti. A lower bound on the local time complexity of universal\nconstructions. In PODC \u201998: Proc. of the Seventeenth Annual ACM Sympo-\nsium on Principles of Distributed Computing, pp. 183\u2013192, NY, USA, 1998,\nACM Press.\n[81] P . Jayanti and S. T oueg. Some results on the impossibility, universality, and\ndecidability of consensus. In WDAG \u201992: Proc. of the Sixth International\nWorkshop on Distributed Algorithms, pp. 69\u201384, London, UK, 1992.\nSpringer-Verlag.\n[82] D. Jim \u00b4enez-Gonz \u00b4alez, J. Larriba-Pey, and J. Navarro. CC-Radix: A cache\nconscious sorting based on Radix sort. In Proc. Eleventh Euromicro Confer-\nence on Parallel, Distributed, and Network-Based Processing, pp. 101\u2013108,\n2003. ISBN: 0769518753.\n[83] L. M. Kirousis, P . G. Spirakis, and P . Tsigas. Reading many variables\nin one atomic operation: Solutions with linear or sublinear complexity.\nInIEEE Trans. Parallel Distributed System, 5(7): 688\u2013696, Piscataway, NJ,\nUSA, 1994, IEEE Press.\n[84] M. R. Klugerman. Small-depth counting networks and related top-\nics. T echnical Report MIT/LCS/TR-643, MIT Laboratory for Computer\nScience, 1994.\nBibliography 489\n[85] M. Klugerman and C. Greg Plaxton. Small-depth counting networks. In\nSTOC \u201992: Proc. of the Twenty-fourth Annual ACM Symposium on Theory\nof Computing, pp. 417\u2013428, NY, USA, 1992, ACM Press.\n[86] D. E. Knuth. The Art of Computer Programming: Second Ed. (Addison-\nWesley Series in Computer Science and Information). Boston, MA, USA,\n1978 Addison-Wesley Longman Publishing Co., Inc.\n[87] V . Kumar. Concurrent operations on extendible hashing and its perfor-\nmance. Communications of the ACM, 33(6):681\u2013694, 1990.\n[88] L. Lamport. A new solution of Dijkstra\u2019s concurrent programming\nproblem. Communications of the ACM, 17(5):543\u2013545, 1974.\n[89] L. Lamport. Time, clocks, and the ordering of events. Communications of\nthe ACM, 21(7):558\u2013565, July 1978.\n[90] L. Lamport. How to make a multiprocessor computer that cor-\nrectly executes multiprocess programs. IEEE Transactions on Computers,\nC-28(9):690, September 1979.\n[91] L. Lamport. Specifying concurrent program modules. ACM Transactions\non Programming Languages and Systems, 5(2):190\u2013222, 1983.\n[92] L. Lamport. Invited address: Solved problems, unsolved problems and\nnon-problems in concurrency. In Proc. of the Third Annual ACM Sym-\nposium on Principles of Distributed Computing, pp. 1\u201311, 1984, ACM\nPress.\n[93] L. Lamport. The mutual exclusion problem\u2014Part I: A theory of interpro-\ncess communication. Journal of the ACM (JACM), 33(2):313\u2013326, 1986,\nACM Press.\n[94] L. Lamport. The mutual exclusion problem\u2014Part II: Statement and solu-\ntions. Journal of the ACM (JACM),", "doc_id": "558109a6-7026-4c8f-8778-2cdf16c09cfa", "embedding": null, "doc_hash": "ba3404a89c5e0ea40f711dbf3ed519f23fb8b5febbda676cd1b68a3832d3a2a1", "extra_info": null, "node_info": {"start": 1226979, "end": 1229794}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "dc69d50e-6515-4bbe-a837-98585e516100", "3": "c9fc9f92-cb2d-4421-b846-55e0057af71c"}}, "__type__": "1"}, "c9fc9f92-cb2d-4421-b846-55e0057af71c": {"__data__": {"text": "problems, unsolved problems and\nnon-problems in concurrency. In Proc. of the Third Annual ACM Sym-\nposium on Principles of Distributed Computing, pp. 1\u201311, 1984, ACM\nPress.\n[93] L. Lamport. The mutual exclusion problem\u2014Part I: A theory of interpro-\ncess communication. Journal of the ACM (JACM), 33(2):313\u2013326, 1986,\nACM Press.\n[94] L. Lamport. The mutual exclusion problem\u2014Part II: Statement and solu-\ntions. Journal of the ACM (JACM), 33(2):327\u2013348, 1986.\n[95] L. Lamport. A fast mutual exclusion algorithm. ACM Trans. Comput. Syst.,\n5(1):1\u201311, 1987.\n[96] B. Lampson and D. Redell. Experience with processes and monitors in\nmesa. Communications of the ACM, 2(23):105\u2013117, 1980.\n[97] J. R. Larus and R. Rajwar. Transactional Memory. Morgan and Claypool,\nSan Francisco, 2006.\n[98] D. Lea. Java community process, JSR 166, concurrency utilities. http://\ngee.cs.oswego.edu/dl/concurrency-interest/index.html, 2003.\n[99] D. Lea. Concurrent hash map in JSR 166 concurrency utilities. http://\ngee.cs.oswego.edu/dl/concurrency-interest/index.html. Dec 2007.\n[100] D. Lea, Personal Communication, 2007.\n[101] S.-J. Lee, M. Jeon, D. Kim, and A. Sohn. Partitioned parallel radix sort.\nJ. Parallel Distributed Computing, 62(4):656\u2013668, 2002.\n[102] C. E. Leiserson and H. Prokop. A minicourse on multithreaded program-\nming, Massachusetts Institute of T echnology, Available on the Internet\nfrom http://theory.lcs.mit.edu/ \u0018click, 1998. citeseer.ist\n.psu.edu/leiserson98minicourse.html.\n[103] Y. Lev, M. Herlihy, V . Luchangco, and N. Shavit. A Simple Optimistic\nSkiplist Algorithm. Fourteenth Colloquium on structural information and\n490 Bibliography\ncommunication complexity (SIROCCO) 2007 pp. 124\u2013138, June 5\u20138,\n2007, Castiglioncello (LI), Italy.\n[104] M. Li, J. Tromp, and P . M. B. Vit \u00b4anyi. How to share concurrent wait-free\nvariables. Journal of the ACM, 43(4):723\u2013746, 1996.\n[105] B.-H. Lim. Personal Communication, Cambridge, Massachusetts. 1995.\n[106] W.-K. Lo and V . Hadzilacos. All of us are smarter than any of us: wait-free\nhierarchies are not robust. In STOC \u201997: Proc. of the Twenty-ninth Annual\nACM Symposium on Theory of Computing, pp. 579\u2013588, NY, USA, 1997,\nACM Press.\n[107] I. Lotan and N. Shavit. Skiplist-based concurrent priority queues. In Proc.\nof the Fourteenth International Parallel and Distributed Processing Sympo-\nsium (IPDPS), pp. 263\u2013268, Cancun, Mexico, 2000.\n[108] M. Loui and H. Abu-Amara. Memory requirements for agreement among\nunreliable asynchronous processes. In F. P . Preparata, editor, Advances in\nComputing Research, volume 4, pages 163\u2013183. JAI Press, Greenwich, CT,\n1987.\n[109] V . Luchangco, D. Nussbaum, and N. Shavit. A Hierarchical CLH Queue\nLock. In Proc. of the European", "doc_id": "c9fc9f92-cb2d-4421-b846-55e0057af71c", "embedding": null, "doc_hash": "e393aaba7725e56be62316cee64d6085d4221d0f591aa23970930f805b74ffca", "extra_info": null, "node_info": {"start": 1229763, "end": 1232478}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "558109a6-7026-4c8f-8778-2cdf16c09cfa", "3": "2c0837de-2cb3-47e7-b0f6-d0e2038dad0c"}}, "__type__": "1"}, "2c0837de-2cb3-47e7-b0f6-d0e2038dad0c": {"__data__": {"text": "International Parallel and Distributed Processing Sympo-\nsium (IPDPS), pp. 263\u2013268, Cancun, Mexico, 2000.\n[108] M. Loui and H. Abu-Amara. Memory requirements for agreement among\nunreliable asynchronous processes. In F. P . Preparata, editor, Advances in\nComputing Research, volume 4, pages 163\u2013183. JAI Press, Greenwich, CT,\n1987.\n[109] V . Luchangco, D. Nussbaum, and N. Shavit. A Hierarchical CLH Queue\nLock. In Proc. of the European Conference on Parallel Computing (EuroPar\n2006), pp. 801\u2013810, Dresdan, Germany, 2006.\n[110] P . Magnussen, A. Landin, and E. Hagersten. Queue locks on cache coher-\nent multiprocessors. In Proc. of the Eighth International Symposium on Par-\nallel Processing (IPPS), pp. 165\u2013171, April 1994. IEEE Computer Society,\nApril 1994. Vancouver, British Columbia, Canada, NY, USA, 1987, ACM\nPress.\n[111] J. Manson, W. Pugh, and S. V . Adve. The Java memory model. In POPL \u201905:\nProc. of the Thirty-second ACM SIGPLAN-SIGACT Symposium on Princi-\nples of Programming Languages, pp. 378\u2013391, NY, USA, 2005, ACM Press.\n[112] P . E. McKenney. Selecting locking primitives for parallel programming.\nCommunications of the ACM, 39(10):75\u201382, 1996.\n[113] J. Mellor-Crummey and M. L. Scott. Algorithms for scalable synchroniza-\ntion on shared-memory multiprocessors. ACM Transactions on Computer\nSystems, 9(1):21\u201365, 1991.\n[114] M. M. Michael. High performance dynamic lock-free hash tables and\nlist-based sets. In SPAA \u201902: Proc. of the Fourteenth Annual ACM Sympo-\nsium on Parallel Algorithms and Architectures, pp. 73\u201382. Winnipeg, Man-\nitoba, Canada, NY, USA, 2002, ACM Press.\n[115] M. M. Michael and M. L. Scott. Simple, fast, and practical non-blocking\nand blocking concurrent queue algorithms. In Proc. of the Fifteenth Annual\nACM Symposium on Principles of Distributed Computing, pp. 267\u2013275,\n1996, ACM Press.\n[116] J. Misra. Axioms for memory access in asynchronous hardware systems.\nACM Transactions on Programming Languages and Systems (TOPLAS),\n8(1):142\u2013153, 1986.\n[117] M. Moir. Practical implementations of non-blocking synchronization\nprimitives. In PODC \u201997: Proc. of the Sixteenth Annual ACM Symposium\nBibliography 491\non Principles of Distributed Computing, pp. 219\u2013228, NY, USA, 1997, ACM\nPress.\n[118] M. Moir. Laziness pays! Using lazy synchronization mechanisms to\nimprove non-blocking constructions. In PODC \u201900: Proc. of the Nine-\nteenth Annual ACM Symposium on Principles of Distributed Computing,\npp. 61\u201370, NY, USA, 2000, ACM Press.\n[119] M. Moir, D. Nussbaum, O. Shalev, and N. Shavit. Using elimination to\nimplement scalable and lock-free \ufb01fo queues. In SPAA \u201905: Proc. of the\nSeventeenth Annual ACM Symposium on Parallelism in Algorithms and\nArchitectures, pp. 253\u2013262, NY, USA, 2005, ACM Press.\n[120] M. Moir V . Marathe and N. Shavit. Composite abortable locks. In Proc.\nof the", "doc_id": "2c0837de-2cb3-47e7-b0f6-d0e2038dad0c", "embedding": null, "doc_hash": "4ef82305b1716459328fbac3bc344612767a63dd462ac7cfde9caaf4badeacbd", "extra_info": null, "node_info": {"start": 1232480, "end": 1235303}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "c9fc9f92-cb2d-4421-b846-55e0057af71c", "3": "60437352-f29f-457f-ac7e-aaee006ab40e"}}, "__type__": "1"}, "60437352-f29f-457f-ac7e-aaee006ab40e": {"__data__": {"text": "NY, USA, 2000, ACM Press.\n[119] M. Moir, D. Nussbaum, O. Shalev, and N. Shavit. Using elimination to\nimplement scalable and lock-free \ufb01fo queues. In SPAA \u201905: Proc. of the\nSeventeenth Annual ACM Symposium on Parallelism in Algorithms and\nArchitectures, pp. 253\u2013262, NY, USA, 2005, ACM Press.\n[120] M. Moir V . Marathe and N. Shavit. Composite abortable locks. In Proc.\nof the 20th IEEE International Parallel & Distributed Processing Symposium\n(IPDPS), pages 1\u201310, 2006.\n[121] I. Newton, I. B. Cohen (Translator), and A. Whitman (Translator). The\nPrincipia: Mathematical Principles of Natural Philosophy . University of\nCalifornia Press, CA, USA, 1999.\n[122] R. Pagh and F. F. Rodler. Cuckoo hashing. J. Algorithms, 51(2):122\u2013144,\n2004.\n[123] C. H. Papadimitriou. The serializability of concurrent database updates.\nJournal of the ACM (JACM), 26(4):631\u2013653, 1979.\n[124] G. Peterson. Myths about the mutual exclusion problem. Information\nProcessing Letters, 12(3):115\u2013116, June 1981.\n[125] G. L. Peterson. Concurrent reading while writing. ACM Trans. Program.\nLang. Syst., 5(1):46\u201355, 1983.\n[126] S. A. Plotkin. Sticky bits and universality of consensus. In PODC \u201989: Proc.\nof the Eighth Annual ACM Symposium on Principles of Distributed Comput-\ning, pp. 159\u2013175, NY, USA, 1989, ACM Press.\n[127] W. Pugh. Concurrent maintenance of skip lists. T echnical Report CS-\nTR-2222.1, Institute for Advanced Computer Studies, Department of\nComputer Science, University of Maryland, 1989.\n[128] W. Pugh. Skip lists: a probabilistic alternative to balanced trees. ACM\nTransactions on Database Systems, 33(6):668\u2013676, 1990.\n[129] C. Purcell and T. Harris. Non-blocking hashtables with open addressing.\nLecture Notes in Computer Science. Distributed Computing. In DISC,\nSpringer Berlin/Heidelberg, pp. 108\u2013121, 2005.\n[130] Z. Radovi \u00b4c and E. Hagersten. Hierarchical Backoff Locks for Nonuni-\nform Communication Architectures. In Ninth International Symposium\non High Performance Computer Architecture, pp. 241\u2013252, Anaheim,\nCalifornia, USA, February 2003.\n[131] M. Raynal. Algorithms for Mutual Exclusion . The MIT Press, Cambridge,\nMA, 1986.\n[132] J. H. Reif and L. G. Valiant. A logarithmic time sort for linear size net-\nworks. Journal of the ACM, 34(1):60\u201376, 1987.\n[133] L. Rudolph and Z. Segall. Dynamic decentralized cache schemes for\nmimd parallel processors. In Proceedings of the 11th Annual International\n492 Bibliography\nSymposium on Computer Architecture, ISCA \u201984, pp. 340\u2013347. ACM, New\nY ork, NY, USA, 1984.\n[134] L. Rudolph, M. Slivkin-Allalouf, and E. Upfal. A simple load balanc-\ning scheme for task allocation in parallel machines. In Proc. of the Third\nAnnual ACM Symposium on Parallel Algorithms and Architectures, pp. 237\u2013\n245, July 1991, ACM Press.\n[135] M. Saks, N. Shavit, and H. Woll. Optimal time randomized consensus\u2014\nmaking", "doc_id": "60437352-f29f-457f-ac7e-aaee006ab40e", "embedding": null, "doc_hash": "dea2f904000e59d49708f976e89e2cd6cce70ebd1a50c95f2d6561c5e223553b", "extra_info": null, "node_info": {"start": 1235363, "end": 1238202}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2c0837de-2cb3-47e7-b0f6-d0e2038dad0c", "3": "f8acfccb-67d4-49bf-9c56-e98f49ad0663"}}, "__type__": "1"}, "f8acfccb-67d4-49bf-9c56-e98f49ad0663": {"__data__": {"text": "on Computer Architecture, ISCA \u201984, pp. 340\u2013347. ACM, New\nY ork, NY, USA, 1984.\n[134] L. Rudolph, M. Slivkin-Allalouf, and E. Upfal. A simple load balanc-\ning scheme for task allocation in parallel machines. In Proc. of the Third\nAnnual ACM Symposium on Parallel Algorithms and Architectures, pp. 237\u2013\n245, July 1991, ACM Press.\n[135] M. Saks, N. Shavit, and H. Woll. Optimal time randomized consensus\u2014\nmaking resilient algorithms fast in practice. In SODA \u201991: Proc. of the\nSecond Annual ACM-SIAM Symposium on Discrete Algorithms, pp. 351\u2013\n362, Philadelphia, PA, USA, 1991. Society for Industrial and Applied\nMathematics.\n[136] W. N. Scherer III, D. Lea, and M. L. Scott. Scalable synchronous queues. In\nPPoPP \u201906: Proc. of the Eleventh ACM SIGPLAN Symposium on Principles\nand Practice of Parallel Programming, pp. 147\u2013156, NY, USA, 2006, ACM\nPress.\n[137] W. N. Scherer III and M. L. Scott. Advanced contention management for\ndynamic software transactional memory. In PODC \u201905: Proc. of the Twenty-\nfourth Annual ACM Symposium on Principles of Distributed Computing,\npp. 240\u2013248, NY, USA, 2005, ACM Press.\n[138] M. L. Scott. Non-blocking timeout in scalable queue-based spin locks. In\nPODC \u201902: Proc. of the Twenty-\ufb01rst Annual Symposium on Principles of Dis-\ntributed Computing, pp. 31\u201340, NY, USA, 2002, ACM Press.\n[139] M. L. Scott and W. N. Scherer III. Scalable queue-based spin locks with\ntimeout. ACM SIGPLAN Notices, 36(7):44\u201352, 2001.\n[140] M. Sendak. Where the Wild Things Are. Publisher: HarperCollins, NY,\nUSA, 1988. ISBN: 0060254920.\n[141] O. Shalev and N. Shavit. Split-ordered lists: lock-free extensible hash\ntables. In Journal of the ACM, 53(3):379\u2013405, NY, USA, 2006, ACM Press.\n[142] N. Shavit and D. T ouitou. Software transactional memory. In Distributed\nComputing, Special Issue 10(2):99\u2013116, 1997.\n[143] N. Shavit and A. Zemach. Diffracting trees. ACM Trans. Comput. Syst.,\n14(4):385\u2013428, 1996.\n[144] E. Shenk. The consensus hierarchy is not robust. In PODC \u201997: Proc. of the\nSixteenth Annual ACM Symposium on Principles of Distributed Computing,\np. 279, NY, USA, 1997, ACM Press.\n[145] R. K. Treiber. Systems programming: Coping with parallelism. T echnical\nReport RJ 5118, IBM Almaden Research Center, April 1986. San Jose, CA.\n[146] A. Turing. On computable numbers, with an application to the\nentscheidungs problem. Proc. Lond. Math. Soc, Historical document, 1937.\n[147] J. D. Valois. Lock-free linked lists using compare-and-swap. In PODC \u201995:\nProc. of the Fourteenth Annual ACM Symposium on Principles of Distributed\nComputing, pp. 214\u2013222. Ottowa, Ontario, Canada, NY, USA, 1995, ACM\nPress.\n[148] P . Vit \u00b4anyi and B. Awerbuch. Atomic shared register access by asyn-\nchronous hardware. In", "doc_id": "f8acfccb-67d4-49bf-9c56-e98f49ad0663", "embedding": null, "doc_hash": "e25c32d73fbc83d54b207c6466c74eb1e072205a13666ccb6c0f0a84e08074a6", "extra_info": null, "node_info": {"start": 1238172, "end": 1240891}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "60437352-f29f-457f-ac7e-aaee006ab40e", "3": "392950b5-8248-4e1d-88f1-fe62343fa59c"}}, "__type__": "1"}, "392950b5-8248-4e1d-88f1-fe62343fa59c": {"__data__": {"text": "with an application to the\nentscheidungs problem. Proc. Lond. Math. Soc, Historical document, 1937.\n[147] J. D. Valois. Lock-free linked lists using compare-and-swap. In PODC \u201995:\nProc. of the Fourteenth Annual ACM Symposium on Principles of Distributed\nComputing, pp. 214\u2013222. Ottowa, Ontario, Canada, NY, USA, 1995, ACM\nPress.\n[148] P . Vit \u00b4anyi and B. Awerbuch. Atomic shared register access by asyn-\nchronous hardware. In Twenty-seventh Annual Symposium on Foundations\nBibliography 493\nof Computer Science, pp. 233\u2013243, Los Angeles, CA, USA, October 1986,\nIEEE Computer Society Press.\n[149] W. E. Weihl. Local atomicity properties: modular concurrency control for\nabstract data types. ACM Transactions on Programming Languages and Sys-\ntems (TOPLAS), 11(2):249\u2013282, 1989.\n[150] R. N. Wolfe. A protocol for wait-free, atomic, multi-reader shared\nvariables. In PODC \u201987: Proc. of the Sixth Annual ACM Symposium\non Principles of Distributed Computing, pp. 232\u2013248, NY, USA, 1987,\nACM Press.\n[151] P . Y ew, N. Tzeng, and D. Lawrie. Distributing hot-spot addressing in large-\nscale multiprocessors. IEEE Transactions on Computers, C-36(4):388\u2013395,\nApril 1987.\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nIndex\nA\nABA problem\nbasic scenario, 235\nload-linked\u2013store-conditional, 237\nand memory reclamation, 233\u2013237\nAbort, memory transactions, 422\nAbstract\nBaseHashSet class, 301\nconcurrent Cuckoo hashing, 318\ncontention manager, 432\nAbstraction map\nconcurrent reasoning, 199\nLockFreeList class, 216\nAbstract value, concurrent reasoning,\n198\nAcquires\nCLHLock class, 154\nCompositeLock class, 161\nde\ufb01nition, 23\nFineList class, 203\nHCLHLock lock, 173\nJava concepts, 456\nlocks, 178\nMCSLock class, 156\nActive thread\nin software combining, 260\ntermination detection barrier, 405\nAddressing\nclosed-address hash sets, 300\u2013302\nconcurrent closed-addressing, 325\nde\ufb01nitions, 300hardware concepts, 473\nopen-addressed hash set, 316\u2013318\nAlgorithms\nBakery lock, 31\u201333\nbitonic sorting, 288\u2013289\nconcurrent, 2, 15\nDynamic Software Transactional\nMemory, 448\nfast path, 43\nLock algorithm, 24, 37\u201340, 38\nlock-based concurrent skiplist,\n333\u2013339\nlock-free concurrent skiplist, 341\u2013348\nlock-free universal, 127\nlock-free universal construction, 128\nquicksort, 290\nSkipList class, 349\nTransactional Locking 2, 448\nTTASLock, 147\u2013149\nwait-free universal construction, 131\nAmdahl\u2019s Law\nde\ufb01nition, 13\nin parallelization, 13\u201314\nAnnounce event, wait-free universal\nconstruction, 130\u2013132\nAnonymous inner class\nJava thread concepts, 454\nsoftware transactional memory, 426\nAnonymous method, C# concepts, 460\nAntisymmetric, timestamps, 34Array-based bounded priority queues,\nimplementation, 352\u2013353\nArray-based locks\nimplementation, 150\u2013151\nwithout false sharing, 152\nAsynchronous\nde\ufb01nition, 1\nthreads, 71\nAtomic hardware step\ncompare-and-swap, 480\nshared counter implementation, 5\u20136\nAtomicity and transactions, 421\u2013423\nAtomicMarkableReference class\nfunction, 234\nfunction and", "doc_id": "392950b5-8248-4e1d-88f1-fe62343fa59c", "embedding": null, "doc_hash": "44cd4ea2719ee57a27908b49acdf78375640f8f24ed98ad0df8338ac5de78483", "extra_info": null, "node_info": {"start": 1240873, "end": 1243869}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "f8acfccb-67d4-49bf-9c56-e98f49ad0663", "3": "57c9f308-cb02-44b2-92e6-caaadec73cf6"}}, "__type__": "1"}, "57c9f308-cb02-44b2-92e6-caaadec73cf6": {"__data__": {"text": "universal\nconstruction, 130\u2013132\nAnonymous inner class\nJava thread concepts, 454\nsoftware transactional memory, 426\nAnonymous method, C# concepts, 460\nAntisymmetric, timestamps, 34Array-based bounded priority queues,\nimplementation, 352\u2013353\nArray-based locks\nimplementation, 150\u2013151\nwithout false sharing, 152\nAsynchronous\nde\ufb01nition, 1\nthreads, 71\nAtomic hardware step\ncompare-and-swap, 480\nshared counter implementation, 5\u20136\nAtomicity and transactions, 421\u2013423\nAtomicMarkableReference class\nfunction, 234\nfunction and methods, 213\u2013215\nAtomicMRSWRegister class,\nimplementation, 83\nAtomic objects\nimplementation, 433\u2013434\nlock-based, 438\u2013445\nobstruction-free, 434\u2013438\nsoftware transactional memory,\n429\u2013431\nAtomic primitives, problems, 418\u2013420\nAtomicReference class\nfunction, 236\nunbounded lock-free queue, 231\nAtomic register\nfor consensus problem, 103\u2013105\nde\ufb01nition, 73\n\ufb01rst de\ufb01nition, 93\n495\n496 Index\nAtomic register (continued)\nhistory, 65\nimplementation considerations,\n75\u201376\nMRMW, 85\u201387\nMRSW, 82\u201385\nSRSW, 77\u201378, 81\u201382, 83\nAtomic snapshots\nconstruction, 93\ncorrectness arguments, 90\u201393\nde\ufb01nition, 87\nand multiple assignment, 110\nobstruction-free, 87\u201388\nwait-free snapshot, 88\u201390\nAtomicSRSWRegister class,\nimplementation, 83\nAtomicStampedReference class\nbounded work-stealing dequeue, 383\nfunction, 234\nAtomic step, 456\nAuxiliary variables, wait-free universal\nconstruction, 133\u2013134\nB\nBackoff lock\ncontention manager, 432\nhierarchical, 167\u2013168\nBackoffLock class\nimplementation, 148\nproblems, 149\nBakery lock\nbounds, 39\nin mutual exclusion, 31\u201333\nas spin locks, 141\nBalancer\ncounting networks, 270\noperation, 271\nsorting networks, 286\u2013287\nBalancer class, software bitonic\ncounting network, 275\nBalancing, work, 389\u2013391\nBalancing network, construction,\n271\u2013272\nBarriers\ncombining tree, 400\u2013402\nde\ufb01nition, 398\nimplementation, 398\u2013399sense-serving, 399\u2013400\nstatic tree, 402\u2013404\nsupporting concepts, 397\u2013398\nsurvey, 408\nBaseHashSet class\nimplementation, 300\u2013301\nmethods, 302\nthresholds, 301\nBatching, de\ufb01nition, 478\nBenevolent side effect, lock-free lists, 217\nBitonic class, implementation, 276\nBitonic counting network\nbasic concepts, 273\u2013274\nproof of correctness, 276\u2013278\nrecursive structure, 274\nsequential execution, 272\nsoftware implementation, 274\u2013276\nBitonic sorting algorithm, design and\nfunction, 288\u2013289\nBivalence, protocol state, 102\nBlocking\nconcurrent objects, 59\nde\ufb01nition, 45, 141\nin Java memory model, 62\u201363\nlocks, 178\nBlock network, implementation, 281\nBoolean register\natomic SRSW, 81\u201382\nde\ufb01nition, 72\nMRSW, 77\u201378\nsafe MRSW, 78\u201379\nsafe SRSW, 86\nBottom wires\ncounting networks, 270\nsorting networks, 286\nBouncer class\narray layout, 44\nimplementation, 43\nBounded, pool, 223\nBoundedDEQueue class, implementation,\n382\u2013384\nBounded partial queue\nconsiderations and drawbacks,\n228\u2013229\nimplementation, 225\u2013227\nBoundedQueue class\nC# constructs, 462\u2013463\ufb01elds and constructor, 225\nlist node, 227\nmethods, 226\u2013227\nBounded-range priority queue\narray-based, 352\u2013353\nde\ufb01nition,", "doc_id": "57c9f308-cb02-44b2-92e6-caaadec73cf6", "embedding": null, "doc_hash": "f492d059395c2576398a4288d7995ec2c50ba5499afc0f720f8b9374428eec05", "extra_info": null, "node_info": {"start": 1243760, "end": 1246738}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "392950b5-8248-4e1d-88f1-fe62343fa59c", "3": "753d1ba4-8a8c-45c3-8de2-fa459870eed6"}}, "__type__": "1"}, "753d1ba4-8a8c-45c3-8de2-fa459870eed6": {"__data__": {"text": "78\u201379\nsafe SRSW, 86\nBottom wires\ncounting networks, 270\nsorting networks, 286\nBouncer class\narray layout, 44\nimplementation, 43\nBounded, pool, 223\nBoundedDEQueue class, implementation,\n382\u2013384\nBounded partial queue\nconsiderations and drawbacks,\n228\u2013229\nimplementation, 225\u2013227\nBoundedQueue class\nC# constructs, 462\u2013463\ufb01elds and constructor, 225\nlist node, 227\nmethods, 226\u2013227\nBounded-range priority queue\narray-based, 352\u2013353\nde\ufb01nition, 351\ntree-based, 353\u2013355\nBounded transactional queue,\nimplementation, 423\nBounded wait-free method, concurrent\nobjects, 59\nBounded work-stealing double-ended\nqueues\ncreator, 391\nimplementation, 382\u2013386\nBucket\nBaseHashSet class, 301\nde\ufb01nition, 300\nsplit-ordered hash set, 311\nBucketList class, implementation,\n312\u2013313\nBucket threshold, BaseHashSet class,\n301\nBuffer, between producers and\nconsumers, 223\nBursty, producers, 223\nBus\ncache coherence, 446\nde\ufb01nition, 145\nBus controller, 472\nBusy\u2013waiting, de\ufb01nition, 141\nC\nCache\nde\ufb01nition, 146, 446\nhardware concepts, 471, 473\u2013474\nCache blocks, de\ufb01nition, 474\nCache coherence\nasBackoffLock problem, 149\nhardware concepts, 474\u2013476\nhardware transactional memory,\n446\u2013447\nCache-coherent NUMA, 474\nCache-conscious programming,\n476\u2013477, 481\nCache hit, de\ufb01nition, 146\nCache lines, de\ufb01nition, 474\nIndex 497\nCache miss, de\ufb01nition, 146\nCallable object, de\ufb01nition, 371\nCalls\natomic methods, 423\nsuccessful or unsuccessful, 196\nCapacity\nBaseHashSet class, 301\npool, 223\nsemaphore, 189\nCAS, seeCompare-and-swap (CAS)\ninstruction\nChurch-Turing Thesis, 71\nCircular array, modulo,\nunboundedDEQueue class, 46,\n150, 387\nClean double collect, obstruction-free\nsnapshot, 88\nCLHLock lock, creators, 174\nCLH queue lock\n\ufb01elds and constructor, 151, 153\nhierarchical, 168\u2013172\nlock acquisition and release, 154\nmethods, 153\nClosed-address hash sets, basic concepts,\n300\u2013302\nClosed addressing, de\ufb01nition, 300\nCluster id, hierarchical locks, 167\nCoarse-grained hash set,\nimplementation, 302\u2013303\nCoarse-grained synchronization\nde\ufb01nition, 195\nimplementations, 200\u2013201\nCoarseHashSet class, implementation,\n302\u2013303\nCoarseList class, methods, 200\u2013201\nCollects\nobstruction-free snapshot, 87\u201388\nsingle-writer atomic snapshot class,\n91\nCollisions, hash sets, 300\nCombining, software, 260\u2013261\nCombining phase, CombiningTree, 265\nCombining status, de\ufb01nition, 261\nCombining tree barrier\ncreators, 408\nimplementation, 400\u2013402CombiningTree class\ncombining status, 261\u2013262\nconstructor, 262\ndisadvantages, 261\ndistribution phase, 267\nexecution phases, 263, 267\nmethods, 264\nNode class, 265\u2013267\nperformance, 269\nprecombining phase, 262\nrobustness, 269\nstatus, 268\nCombining trees, original idea, 292\nCommon2 register\nde\ufb01nition, 117\nRMW, 114\u2013116\nCommunication, and mutual exclusion,\n9\u201310\nComparator, comparison network, 286\ncompareAndSet() operation\nhardware synchronization, 480\nsynchronization primitives, 116\u2013117\nCompare-and-swap (CAS)", "doc_id": "753d1ba4-8a8c-45c3-8de2-fa459870eed6", "embedding": null, "doc_hash": "8d8bf65b0fc2fd4cbb6bdb461b2c956b49e05a47fda0eeeb160fb0703a351bc5", "extra_info": null, "node_info": {"start": 1246813, "end": 1249688}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "57c9f308-cb02-44b2-92e6-caaadec73cf6", "3": "9ba97870-6d0a-4de2-96c0-ea5eaddfcf49"}}, "__type__": "1"}, "9ba97870-6d0a-4de2-96c0-ea5eaddfcf49": {"__data__": {"text": "262\ndisadvantages, 261\ndistribution phase, 267\nexecution phases, 263, 267\nmethods, 264\nNode class, 265\u2013267\nperformance, 269\nprecombining phase, 262\nrobustness, 269\nstatus, 268\nCombining trees, original idea, 292\nCommon2 register\nde\ufb01nition, 117\nRMW, 114\u2013116\nCommunication, and mutual exclusion,\n9\u201310\nComparator, comparison network, 286\ncompareAndSet() operation\nhardware synchronization, 480\nsynchronization primitives, 116\u2013117\nCompare-and-swap (CAS) instruction,\nhardware considerations, 480\nComparison network, de\ufb01nition, 286\nCompositeFastPathLock class,\nmethods, 166\nComposite lock\nadvantages, 159\u2013160\nfast-path, 165\u2013167\nCompositeLock class\nexecution, 164\n\ufb01elds, constructor, and methods, 160\nmethods, 161\u2013163\nproperties, 165\nQNode class, 161\nCompositional, correctness property, 51\nCompositionality, problems, 420\u2013421\nCompositional linearizability,\nconcurrent objects, 57\u201358\nConcrete representation, concurrent\nreasoning, 198\nConcurrency\nconcurrent objects, 45\u201348\nreasoning, 198\u2013200Concurrent algorithm\nchallenges, 15\nde\ufb01nition, 2\nConcurrent closed-addressing schemes,\ncreators, 325\nConcurrent Cuckoo hashing, de\ufb01nition\nand implementation, 318\u2013322\nConcurrent heap\noverview, 357\u2013358\nstructure and implementation,\n358\u2013363\nConcurrent objects\ncompositional linearizability, 57\u201358\nconcurrency, 45\u201348\ncorrectness, 45\u201348\ndependent progress conditions,\n60\u201361\nformal de\ufb01nitions, 55\u201357\nlinearizability, 54\u201355, 57\nnonblocking property, 58\u201359\nprogress conditions, 59\u201360\nquiescent consistency, 49\u201351\nsequential consistency, 51\u201354\nsequential objects, 48\u201349\nConcurrent priority queues,\nimplementation, 351\u2013352\nConcurrent program\nde\ufb01nition, 2\nsynchronization universality\nhierarchy, 126\nConcurrent shared memory computing,\nde\ufb01nition, 71\nConcurrent skiplists, overview, 348\nConcurrent timestamping, Lock class,\n34\nCondition \ufb01eld, bounded partial\nqueues, 225\nConditions objects\ninterface, 180\ninterrupts, 179\nLockedQueue class, 182\nlost wakeups, 181\u2013183\nusage example, 179\u2013180\nConsensus numbers\nde\ufb01nition, 100\ninterface and de\ufb01nitions, 100\u2013101\nstates and valence, 101\u2013103\n498 Index\nConsensus object\ninterface, 100\nlock-free universal construction,\n126\u2013130\nuniversality de\ufb01nition, 126\nwait-free universal construction,\n130\u2013136\nConsensus synchronization problem\natomic register solution, 103\u2013105\nCommon2 registers, 114\u2013116\nconsensus protocols, 106\nde\ufb01nition, 100\nFIFO queues, 106\u2013109\nConsenus protocols, generic, 106\nConsistency, software transactional\nmemory, 428\u2013429\nConsumers\nnaive synchronous queue, 237\npools, 223\nContention\nde\ufb01nition, 147\nhigh and low, 147\nLockFreeStack, 248\nContention manager\ngreedy and karma, 448\nimplementation, 433\nsoftware transactional memory,\n431\u2013433\nContext switch, de\ufb01nition, 472\nConvoying, in locking, 417\nCoordination protocol (OR Protocol),\nde\ufb01nition, 6\nCopyable class, interface, 430\nCorrectness\nbitonic counting network, 276\u2013278\ncompositional, 51\nconcurrent objects, 45\u201348\nMerger class, 277\nCorrectness arguments, atomic\nsnapshots, 90\u201393\nCounters\nde\ufb01nition,", "doc_id": "9ba97870-6d0a-4de2-96c0-ea5eaddfcf49", "embedding": null, "doc_hash": "2d11a7552ca6024bf0c8950df8cc539136aac02d898e235c5385641354a3c567", "extra_info": null, "node_info": {"start": 1249678, "end": 1252656}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "753d1ba4-8a8c-45c3-8de2-fa459870eed6", "3": "9a4bd243-7f99-4ccd-8d9d-9d92c425910f"}}, "__type__": "1"}, "9a4bd243-7f99-4ccd-8d9d-9d92c425910f": {"__data__": {"text": "147\nhigh and low, 147\nLockFreeStack, 248\nContention manager\ngreedy and karma, 448\nimplementation, 433\nsoftware transactional memory,\n431\u2013433\nContext switch, de\ufb01nition, 472\nConvoying, in locking, 417\nCoordination protocol (OR Protocol),\nde\ufb01nition, 6\nCopyable class, interface, 430\nCorrectness\nbitonic counting network, 276\u2013278\ncompositional, 51\nconcurrent objects, 45\u201348\nMerger class, 277\nCorrectness arguments, atomic\nsnapshots, 90\u201393\nCounters\nde\ufb01nition, 22\nquiescient consistency, 269\u2013270\nas shared counter implementation,\n4\u20135\nCounting, shared, 259\u2013260Counting networks\nbasic concepts, 270\nbitonic, 272\u2013278\ncomponents, 270\u2013273\ninvention, 292\nperformance, 280\u2013281\nperiodic, 278\u2013280\npipelining, 280\u2013281\nCovering state, Lock algorithm, 38, 40\nCritical-path length, parallelism,\n376\u2013377\nCritical sections\nasBackoffLock problem, 149\nJava concepts, 456\nin mutual exclusion, 22\u201324\nC# construct concepts\ncreators, 466\nmonitors, 461\u2013462\nthread-local objects, 462\u2013463\nthreads, 460\u2013461\nCuckoo hashing\ncreators, 325\nde\ufb01nition and implementation,\n316\u2013318\nD\nDAG, seeDirected acyclic graph (DAG)\nData, and memory, 473\nData structure design\ndual data structures, 239\u2013241\nSkipList class, 330\nDeadlock\navoidance, 417\u2013418\nFineList class, 204\nfreedom from deadlock, 24\nsoftware transactional memory, 431\nunbounded total queue, 230\nDeadlock-freedom property\nde\ufb01nition, 8\nas dependent progress condition, 60\nFilter lock, 31\nDecision value, consensus numbers, 101\nDelegate, C# concepts, 460\nDEQueue, seeDouble-ended queue\n(DEQueue)\nDeschedule, threads, 472\nDiffractingBalancer class,\nimplementation, 284Diffracting trees\nbasic concept, 282\nDiffractingBalancer, 284\nimplementation, 285\nperformance, 285\nPrism, 283\u2013284\nDirected acyclic graph (DAG)\ncreators, 391\nde\ufb01nition, 375\nFibonacci sequence, 375\u2013376\nsteps, 380\nDirect mapped cache, de\ufb01nition, 474\nDirect-mapped caches, de\ufb01nition, 448\nDirty, de\ufb01nition, 478\nDisjoint-access-parallelism\ncoining of term, 325\nde\ufb01nition, 299\nDissemination barrier, creators, 408\nDistributed coordination, overview,\n291\u2013292\nDistribution phase, CombiningTree,\n267\nDoorway section, in Lock class, 31\nDouble-checked locking\nin Java memory model, 61\u201362\nand memory consistency, 479\nDouble-ended queue (DEQueue)\nbounded work-stealing, 382\u2013386, 391\nunbounded work-stealing, 386\u2013388,\n391\nwork stealing, 380\u2013389\nDown state, balancers, 271\nDual data structures\nde\ufb01nition, 239\nimplementation, 239\u2013241\nreservation, 238\u2013239\nDynamic Software Transactional Memory\nalgorithm, creators, 448\nE\nElimination, LockFreeStack, 248\u2013249\nElimination array, implementation,\n251\u2013252, 255\nEliminationBackoffStack class\nbasic concept, 249\nef\ufb01ciency, 255\nelimination array, 251\u2013254\nIndex 499\nlock-free exchanger, 249\u2013251\nmethods, 253\nstructure, 248\nEventListener class, incorrect\nexample, 64\nEvents, in state machine, 21\nExclusive state, cache coherence, 446,\n475\u2013476\nExecutor service, de\ufb01nition, 371\nExpected value, compareAndSet()\noperation, 116, 480\nExponential backoff\nimplementation,", "doc_id": "9a4bd243-7f99-4ccd-8d9d-9d92c425910f", "embedding": null, "doc_hash": "fbfb82a432d5539cc1c1ba10e2d9480ac8747499b19bf565b6850844ad0bbcbb", "extra_info": null, "node_info": {"start": 1252654, "end": 1255619}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9ba97870-6d0a-4de2-96c0-ea5eaddfcf49", "3": "2f21ded7-7be9-4082-a2c6-361aaa664f8d"}}, "__type__": "1"}, "2f21ded7-7be9-4082-a2c6-361aaa664f8d": {"__data__": {"text": "LockFreeStack, 248\u2013249\nElimination array, implementation,\n251\u2013252, 255\nEliminationBackoffStack class\nbasic concept, 249\nef\ufb01ciency, 255\nelimination array, 251\u2013254\nIndex 499\nlock-free exchanger, 249\u2013251\nmethods, 253\nstructure, 248\nEventListener class, incorrect\nexample, 64\nEvents, in state machine, 21\nExclusive state, cache coherence, 446,\n475\u2013476\nExecutor service, de\ufb01nition, 371\nExpected value, compareAndSet()\noperation, 116, 480\nExponential backoff\nimplementation, 149\nTTASLock algorithm, 147\u2013149\nExtensible hashing, de\ufb01nition, 300\nF\nFactory, Lock interface, 178\nFairness\nin mutual exclusion, 31\npools, 224\nFair readers\u2013writers lock\n\ufb01elds, 185\n\ufb01elds and public methods, 186\ninner read lock, 186\ninner write lock, 187\nFalse sharing\nabscence in ALock, 152\narray-based locks, 151\noccurrence, 476\nFault-tolerance, in mutual exclusioin, 9\nFence, de\ufb01nition, 479\nFibonacci sequence\nde\ufb01nition, 375\ndirected acyclic graph, 375\u2013376\nFibTask class\nimplementation, 375\nFIFO queue, lock-based, 45\u201348\nFilter lock\nin mutual exclusion, 28\u201331\nas spin locks, 141\nFinal \ufb01elds, in Java memory model,\n63\u201364\nFinal state, consensus numbers, 101\nFineGrainedHeap class\ncreators, 366\nimplementation, 358\u2013361overview, 357\u2013358\nstructure, 362\nFine-grained synchronization\nbasic concepts, 201\u2013202\nde\ufb01nition, 195\nFineList class\ndeadlock, 204\nhand-over-hand locking, 204\nlock acquisitions, 203\nmethods, 202\u2013203\nFIRST, CombiningTree, 264\u2013265\nFirst-come-\ufb01rst-served\nBakery lock algorithm, 31, 33\nde\ufb01nition, 31\nFirst-in-\ufb01rst-out (FIFO)\nfor consensus problem, 106\u2013108\npool fairness, 224\nvia Pthreads, 466\nquiescent consistency, 51\nsequential objects, 48\u201349\nFIRST status\nCombiningTree, 268\nde\ufb01nition, 261\nFrames, de\ufb01nition, 397\nFreedom from deadlock property, in\nLock algorithm, 24\nFreedom from starvation, in Lock\nalgorithm, 24\nFree list, for node recycling, 233\u2013234\nFreeObject class, structure, 436\u2013438\nFully-associative caches, de\ufb01nition, 448,\n474\nFuture interface, de\ufb01nition, 371\nG\nGhost variables, seeAuxiliary variables\nGlobal queue, in HCLHLock queue, 168\nGlobal state, de\ufb01nition, 37\nGlobal threshold, BaseHashSet class,\n301\nGranularity, and cache, 474\nGreedy\ncontention manager, 432\nschedule, 379Greedy contention manager, creators,\n448\nH\nHand-and-over-hand locking,\nFineList class, 204\nHandlers, software transactional\nmemory, 428\nHardware concepts\narchitectures, 477\u2013479\nbasic considerations, 469\u2013472\ncache-conscious programming,\n476\u2013477\ncaches, 473\u2013474\ncoherence, 474\u2013476\ninterconnect, 472\u2013473\nmemory, 473\nprocessors and threads, 472\nspinning, 476\nsynchronization instructions,\n480\u2013481\nHardware transactional memory (HTM)\ncache coherence, 446\u2013447\nenhancements, 447\u2013448\n\ufb01rst proposal, 448\noverview, 445\u2013446\ntransactional cache coherence, 447\nHash function, de\ufb01nition, 300\nHashing, de\ufb01nition, 299\nHash sets\nclosed-address, 300\u2013302\nde\ufb01nition,", "doc_id": "2f21ded7-7be9-4082-a2c6-361aaa664f8d", "embedding": null, "doc_hash": "e52ed5b18009a643f1859a22852070f89c89f1adb6311d9af8b1368d1cb1427b", "extra_info": null, "node_info": {"start": 1255605, "end": 1258421}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "9a4bd243-7f99-4ccd-8d9d-9d92c425910f", "3": "287dca0e-1408-46b6-8928-04f9df06a57a"}}, "__type__": "1"}, "287dca0e-1408-46b6-8928-04f9df06a57a": {"__data__": {"text": "473\u2013474\ncoherence, 474\u2013476\ninterconnect, 472\u2013473\nmemory, 473\nprocessors and threads, 472\nspinning, 476\nsynchronization instructions,\n480\u2013481\nHardware transactional memory (HTM)\ncache coherence, 446\u2013447\nenhancements, 447\u2013448\n\ufb01rst proposal, 448\noverview, 445\u2013446\ntransactional cache coherence, 447\nHash function, de\ufb01nition, 300\nHashing, de\ufb01nition, 299\nHash sets\nclosed-address, 300\u2013302\nde\ufb01nition, 299\u2013300\nresizing, 305\nHBOLock class, implementation, 168\nHCLHLock queue\nacquisition and release, 173\ncomponents, 168\n\ufb01elds and constructor, 169\nglobal queue, 171\ninner QNode class, 169\nmethods, 170, 172\nHead, de\ufb01nition, 224\nHierarchical backoff lock,\nimplementation, 167\u2013168\nHierarchical CLH queue lock, design,\n168\u2013172\nHierarchical locks, de\ufb01nition, 167\n500 Index\nHigh contention, de\ufb01nition, 147\nHit rate, and cache, 474\nHit ratio, and cache, 474\nHits, in cache, 474\nHold, locks, 178\nHot spot, de\ufb01nition, 259\u2013260\nHTM, seeHardware transactional\nmemory (HTM)\nI\nIDLE, CombiningTree, 264\nIdle step, scheduler, 379\nInactive thread, termination detection\nbarrier, 405\nInitial state, consensus numbers, 101\nInner classes\nanonymous, 426\nde\ufb01nition, 183\ntree nodes, 402\nInner read lock\nfair readers\u2013writers lock, 187\nsimple readers\u2013writers lock, 184\nInner write lock\nfair readers\u2013writers lock, 187\nsimple readers\u2013writers lock, 185\nIn-place array sorting, function, 288\nInstantaneous events, in mutual\nexclusion, 22\nInterconnect, hardware concepts, 471,\n472\u2013473\nInterference\nconcurrent reasoning, 198\noptimistic synchronization, 207\nInterrupts\nConditions objects, 179\nde\ufb01nition, 9\nInvalid state, cache coherence, 446,\n475\u2013476\nInvariants, concurrent reasoning, 198\nInvocation events\nconcurrent objects, 56\nlock-free universal algorithm, 127\nand register space, 76\nIrre\ufb02exive, timestamps, 34\nItems\nhash sets, 300\npriority queue, 351PrioritySkipList class, 363\nset de\ufb01nition, 196\nsoftware transactional memory, 426\nJ\nJava construct concepts\ncreator, 466\nmonitors, 454\u2013458\nthread-local objects, 458\u2013459\nthreads, 453\u2013454\nyielding and sleeping, 458\nJava memory model\nbasic principles, 61\u201362\n\ufb01nal \ufb01elds, 63\u201364\nlocks and synchronized blocks, 62\u201363\nvolatile \ufb01elds, 63\nJoin\nC# constructs, 460\nJoin, Java threads, 454\nK\nKarma contention manager\ncharacteristics, 432\ncreators, 448\nKernel maps, function, 378\nk-way set associative, caches, 474\nL\nLabels\nBakery lock algorithm, 32\ntimestamps as, 34\nLast-in-\ufb01rst-out (LIFO), StackT class,\n245\nLatency, de\ufb01nition, 259\nLayer network, implementation,\n279\u2013280\nLayout, distributed coordination, 292\nLazy\nPrioritySkipList class, 363\nunbounded lock-free queue, 231\u2013232\nLazyList class\n\ufb01elds, 214\nlinearizability, 212\nmethods, 209\u2013210\nvalidation, 209, 211\nLazySkipList class\ncreators, 348implementation, 333\u2013337, 339\noverview, 331\u2013333\nLazy synchronization\nadvantages, 213\nbasic concepts, 208\u2013209\ncreators, 219\nde\ufb01nition,", "doc_id": "287dca0e-1408-46b6-8928-04f9df06a57a", "embedding": null, "doc_hash": "830144fc110fa124210f5f4670c603085666b5186f77682ed14c7e52c218a8ab", "extra_info": null, "node_info": {"start": 1258489, "end": 1261319}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "2f21ded7-7be9-4082-a2c6-361aaa664f8d", "3": "981affea-7639-4462-973d-498b13d93eb5"}}, "__type__": "1"}, "981affea-7639-4462-973d-498b13d93eb5": {"__data__": {"text": "de\ufb01nition, 259\nLayer network, implementation,\n279\u2013280\nLayout, distributed coordination, 292\nLazy\nPrioritySkipList class, 363\nunbounded lock-free queue, 231\u2013232\nLazyList class\n\ufb01elds, 214\nlinearizability, 212\nmethods, 209\u2013210\nvalidation, 209, 211\nLazySkipList class\ncreators, 348implementation, 333\u2013337, 339\noverview, 331\u2013333\nLazy synchronization\nadvantages, 213\nbasic concepts, 208\u2013209\ncreators, 219\nde\ufb01nition, 196\nlinearization, 211\u2013212\nmethods, 209\u2013210\nLevels, in Filter lock, 28\nLine, cache coherence, 446\nLinearizability\napplications, 45\nconcurrent objects, 54\u201355, 57\nconcurrent priority queues, 351\nconcurrent reasoning, 199\n\ufb01rst de\ufb01nition, 93\nLazyList class, 212\nwait-free universal construction, 133\nLinearization points\nconcurrent objects, 55\n\ufb01ne-grained synchronization, 204\nLazyList class, 211\nLinear speedup, parallelism, 377\nLinked lists\ncoarse-grained synchronization,\n200\u2013201\nconcurrent reasoning, 198\u2013200\nearly work, 219\n\ufb01ne-grained synchronization,\n201\u2013205\nlazy synchronization, 208\u2013213\nlist-based sets, 196\u2013198\nlock-free list, 213\u2013218\noptimistic synchronization, 205\u2013208\noverview, 195\u2013196\nList-based sets, basic concepts,\n196\u2013198\nList nodes\nbounded partial queue, 227\nlock-free stack, 247\nunbounded lock-free queue, 230\nLiveness property, de\ufb01nition, 2\nLoad-linked\u2013store-conditional (LL/SC)\nand ABA problem, 237\nhardware synchronization, 480\norigins, 136\nIndex 501\nLoads\nde\ufb01nition, 37\nLock algorithm bounds, 37\nin registers, 72\nLocality, and cache, 474\nLocal spinning, de\ufb01nition, 147\nLocal state, de\ufb01nition, 37\nLock-based atomic object\nconsistency, 440\u2013441\noverview, 439\u2013440\nstructure details, 441\u2013445\nLock-based concurrent skiplist\nalgorithm, 333\u2013339\noverview, 331\u2013333\nLock-based hash table, resizing, 305\nLock class\ninterface, 178\u2013179\nlower bounds, 37\u201340\nfor mutual exclusion, 141\u2013144\ntimeout, 157\ntimestamping, 34\nLock coupling\nde\ufb01nition, 202\ninvention, 219\nLockedQueue class, with locks and\nconditions, 182\nLock-free concurrent skiplist\nalgorithm, 341\u2013348\noverview, 339\u2013341\nLock-free exchanger\nbasic function, 249\nimplementation, 250\nLock-free hash set\nbasic concept, 309\nBucketList class, 312\u2013313\nimplementation, 313\u2013315\nrecursive initialization, 316\nrecursive split-ordering, 309\u2013312\nLockFreeHashSet class,\nimplementation, 313\u2013315\nLockFreeList class, methods, 217\u2013219\nLock-free lists\nabstraction maps, 216\nAtomicMarkableReference,\n213\u2013215\nbasic concepts, 213\u2013214methods, 217\u2013219\nWindow class, 216\nLock-free method\nconcurrent objects, 60\nconcurrent reasoning, 199\nde\ufb01nition, 99\nLockFreeQueue class\ncreators, 241\nlist node, 230\nmethods, 230\u2013231, 419\u2013420\nLockFreeQueueRecycle class,\nimplementation, 236\nLockFreeSkipList class\ncall structure, 340\ncreators, 348\u2013349\nimplementation, 342\u2013344, 346\u2013347,\n349\noverview, 339\nLockFreeStack class\ncreators, 255\nelimination, 248\u2013249\nimplementation, 246\u2013247\nstructure, 246\nLock-free universal construction\nalgorithm, 128\nexecution, 129\ngeneric sequential object,", "doc_id": "981affea-7639-4462-973d-498b13d93eb5", "embedding": null, "doc_hash": "9a0a9ace7270ada23c99ba89cf7d7505671e3d835efb6ab328a16928d0bd2d8b", "extra_info": null, "node_info": {"start": 1261305, "end": 1264222}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "287dca0e-1408-46b6-8928-04f9df06a57a", "3": "260e993e-e5f4-4110-8e2e-18b4003fc8d6"}}, "__type__": "1"}, "260e993e-e5f4-4110-8e2e-18b4003fc8d6": {"__data__": {"text": "objects, 60\nconcurrent reasoning, 199\nde\ufb01nition, 99\nLockFreeQueue class\ncreators, 241\nlist node, 230\nmethods, 230\u2013231, 419\u2013420\nLockFreeQueueRecycle class,\nimplementation, 236\nLockFreeSkipList class\ncall structure, 340\ncreators, 348\u2013349\nimplementation, 342\u2013344, 346\u2013347,\n349\noverview, 339\nLockFreeStack class\ncreators, 255\nelimination, 248\u2013249\nimplementation, 246\u2013247\nstructure, 246\nLock-free universal construction\nalgorithm, 128\nexecution, 129\ngeneric sequential object, 126\u2013127\nLocking\ndouble-checked, in Java memory\nmodel, 61\u201362\nexecution, 47\nhand-and-over-hand locking,\nFineList class, 204\nproblems, 417\u2013418\nLockObject class, implementation, 23,\n440\u2013445\nLockOne class, for mutual exclusion,\n25\u201326\nLockout-freedom property\nde\ufb01nition, 8\u20139\nas dependent progress condition, 60\nin Lock algorithm, 24\nin producer\u2013consumer problem,\n10\u201311\nLocks\nacquires, 178\narray-based locks, 150\u2013152backoff lock, 167\u2013168\nBakery lock, 31\u201333, 39, 141\nblock, 178\nC# constructs, 461\nCLH queue lock, 151, 153\u2013154,\n168\u2013172\ncomposite lock, 159\u2013160, 165\u2013167\nde\ufb01nition, 22\nfair readers\u2013writers lock, 186\u2013187\nFilter lock, 28\u201331, 141\nhardware concepts, 469\nhierarchical backoff lock, 167\u2013168\nhierarchical CLH queue lock,\n168\u2013172\nhierarchical locks, 167\nhold, 178\ninner read lock, 184, 186\ninner write lock, 185, 187\ninterface, 23\nJava concepts, 456\nin Java memory model, 62\u201363\nlock-based atomic object, 439\nMCS queue lock, 154\u2013156\nmonitor locks, 178\u2013179, 181, 189\nPeterson lock, 27\u201328, 39, 142\u2013143\nqueue locks, 149\u2013159\nreaders\u2013writers lock, 183\u2013187\nread lock, 183\nreentrant lock, 187\u2013189\nreleases, 178\nsimple readers\u2013writers lock, 183\u2013185\nspin, 178\ntest-and-set locks, 144\u2013146\nwrite lock, 184\nLock striping, hash sets, 304\nLockTwo class, for mutual exclusion,\n26\u201327\nLogical buckets, lock-free hash set, 309\nLogical \ufb01elds, obstruction-free atomic\nobject, 435\nLogical removal\nlazy synchronization, 196, 208\nPrioritySkipList class, 363\nLoser thread, Common2 register, 114\nLost-wakeup problem\nConditions objects, 182\u2013185\nexample, 183\n502 Index\nLost-wakeup problem (continued)\nJava concepts, 458\nLow contention, de\ufb01nition, 147\nM\nMain cache, hardware transactional\nmemory, 448\nMain memory, hardware concepts, 471,\n473\nMatrix class, implementation, 372\nMatrix multiplication, parallel, see\nParallel matrix multiplication\nMatrixTask class, implementation,\n373\u2013374\nMCSLock class, creators, 174\nMCS queue lock\n\ufb01elds and constructor, 154\u2013155\nlock acquisition and release, 156\nmethods, 155\nQNode, 155\u2013156\nMemory, hardware concepts,\n471, 473\nMemory barriers, de\ufb01nition, 53, 144,\n479\nMemory consistency models\nrelaxed, 478\u2013479\nsurvey, 481\nMemory contention\nde\ufb01nition, 474\nand shared counting, 260\nMemory fence, seeMemory barriers\nMemory locations, lower bounds,\n37\u201340\nMemory reclamation, and ABA\nproblem, 233\u2013237\nMerger class\ncorrectness, 277\nsoftware bitonic counting network,\n275\nMERGER network, logical structure,\n273\nMESI protocol\nIntel processors, 481\nstate transition examples,", "doc_id": "260e993e-e5f4-4110-8e2e-18b4003fc8d6", "embedding": null, "doc_hash": "ae5a5b2a11cbc51aed9519d424c87f0c7dac26c798a431b33cddfbe552d1f0ed", "extra_info": null, "node_info": {"start": 1264164, "end": 1267104}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "981affea-7639-4462-973d-498b13d93eb5", "3": "596e2186-01bd-468c-bf08-e946c1da24ee"}}, "__type__": "1"}, "596e2186-01bd-468c-bf08-e946c1da24ee": {"__data__": {"text": "473\nMemory barriers, de\ufb01nition, 53, 144,\n479\nMemory consistency models\nrelaxed, 478\u2013479\nsurvey, 481\nMemory contention\nde\ufb01nition, 474\nand shared counting, 260\nMemory fence, seeMemory barriers\nMemory locations, lower bounds,\n37\u201340\nMemory reclamation, and ABA\nproblem, 233\u2013237\nMerger class\ncorrectness, 277\nsoftware bitonic counting network,\n275\nMERGER network, logical structure,\n273\nMESI protocol\nIntel processors, 481\nstate transition examples, 475\nMetalock, 174\nMethod calls\nconcurrent objects, 56de\ufb01nition, 48\nquiescent consistency, 49\u201350\nand register space, 76\nRMW registers, 112\u2013113\nMisses, in cache, 474\nMMThread class, implementation,\n370\nModi\ufb01ed state, cache coherence, 446,\n475\u2013476\nMonitor locks\nde\ufb01nitions, 178\u2013179\nexecution, 180\u2013181\ninvention, 190\nMonitors\ncreators, 466\nas C# constructs, 461\u2013462\nde\ufb01nition, 177, 182\nas Java constructs, 454\u2013458\nMRMW, seeMulti-reader\nmultiple-writer (MRMW)\nMRSW, seeMulti-reader single-writer\n(MRSW)\nmultiCompareAndSet, pseudocode, 419\nMulti-core architecture, de\ufb01nition,\n477\u2013479\nMulticores, programming challenges, 1\nMultiple assignment objects, basic\nconcept, 110\nMultiprogrammed environment, work\ndistribution, 381\u2013382\nMulti-reader multiple-writer (MRMW)\natomic register, 85\u201387\nconstruction, 93\nMulti-reader single-writer (MRSW)\natomic register, 82\u201385\nBoolean register, 77\u201378\nconstruction, 93\nand register space, 73\nregular Boolean register, 78\u201379\nregular M-valued register, 79\u201381\nsafe registers, 78\nwrite order, 76\nMulti-threaded architecture, de\ufb01nition,\n477\u2013479\nMutexes, in Pthreads, 464\u2013465\nMutual exclusion\nBakery lock algorithm, 31\u201333bounded timestamps, 33\u201336\nand communication, 9\u201310\nin concurrent programming, 15\ncritical sections, 22\u201324\nde\ufb01nition, 6\nfairness, 31\nfast path algorithm, 43\nFilter lock, 28\u201331\nJava concepts, 456\nLockOne class, 25\u201326\nLockTwo class, 26\u201327\nnumber of locations, 37\u201340\nPeterson Lock, 27\u201328\nin producer\u2013consumer problem,\n10\u201311\nproperties, 8\u20139\nreal-world approach, 141\u2013144\nand register space, 73\ntime, 21\u201322\nM-valued register\nde\ufb01nition, 72\nregular MRSW, 79\u201381\nN\nNaive synchronous queue\nbasic concepts, 237\nimplementation, 238\nNew version \ufb01eld, obstruction-free\natomic object, 435\nNode class\nCombiningTree, 262, 265\u2013267\nimplementation, 127\nLazySkipList class, 333, 338\nStaticTreeBarrier class, 405\ntree barriers, 400\u2013402\nNodes\nand free lists, 233\u2013234\nlist-based, 197\u2013198, 227, 230, 247\nNUMA architecture, 472\npredecessor nodes, 171\nregular nodes, 197\nsentinel nodes, 197, 225, 311\nNonblocking methods, de\ufb01nition, 45\nNonblocking progress, concurrent\nobjects, 59\nIndex 503\nNonblocking property, concurrent\nobjects, 58\u201359\nNonblocking synchronization,\nde\ufb01nition, 196\nNon-transactional cache, hardware\ntransactional memory, 448\nNonuniform memory access (NUMA)\narchitecture\nbasic concepts, 472\u2013473\nspinning, 476\nNorth wires\ncounting networks, 270\nperiodic network, 279\nNotify, Java concepts, 457\nNUMA,", "doc_id": "596e2186-01bd-468c-bf08-e946c1da24ee", "embedding": null, "doc_hash": "9c53853840a0bd41a400d1245fe7b8d2155f57b404459790eeb796bf2f152468", "extra_info": null, "node_info": {"start": 1267138, "end": 1270008}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "260e993e-e5f4-4110-8e2e-18b4003fc8d6", "3": "0728fa30-975f-4a4e-b383-1aa7d7ee4924"}}, "__type__": "1"}, "0728fa30-975f-4a4e-b383-1aa7d7ee4924": {"__data__": {"text": "nodes, 197\nsentinel nodes, 197, 225, 311\nNonblocking methods, de\ufb01nition, 45\nNonblocking progress, concurrent\nobjects, 59\nIndex 503\nNonblocking property, concurrent\nobjects, 58\u201359\nNonblocking synchronization,\nde\ufb01nition, 196\nNon-transactional cache, hardware\ntransactional memory, 448\nNonuniform memory access (NUMA)\narchitecture\nbasic concepts, 472\u2013473\nspinning, 476\nNorth wires\ncounting networks, 270\nperiodic network, 279\nNotify, Java concepts, 457\nNUMA, seeNonuniform memory access\n(NUMA) architecture\nO\nObject, de\ufb01nition, 48\nObstruction-free atomic object\nbasis, 448\nconsistency, 436\u2013437\noperation details, 437\noverview, 435\nObstruction-free property, as dependent\nprogress condition, 60\nObstruction-free snapshot, collects,\n87\u201388\nOddEven sorting network, design, 288\nOld version \ufb01eld, obstruction-free\natomic object, 435\nOpen-addressed hash set, Cuckoo\nhashing, 316\u2013318\nOpen addressing, de\ufb01nition, 300\nOperator, de\ufb01nition, 455\nOperator thread, de\ufb01nition, 455\nOptimisticList class\nimplementation, 205\nmethods, 206\u2013207\nvalidation, 208\nOptimistic synchronization\nbasic concepts, 205\nclass implementations, 205\u2013207\nde\ufb01nition, 195\nvalidation, 207\nOwner \ufb01eld, obstruction-free atomic\nobject, 435P\nParallelism\nanalysis, 375\u2013378\nde\ufb01nition, 1\nand shared counting, 260\nParallelization, realities, 13\u201314\nParallel matrix multiplication\nMatrixTask class, 373\u2013374\noverview, 369\u2013370\nParallel programming, challenges, 15\nParallel sorting, basic concepts, 286\nPartial method\ncreators, 241\npool, 224\nPartial order, concurrent objects, 56\nPassive thread, in software combining,\n260\nPending\ninvocation, concurrent objects, 56\nmethod calls, 50\nPerformance\nCombiningTree, 269\ncounting networks, 280\u2013281\ndiffracting trees, 285\nhardware concepts, 471\nPeriodic counting networks\nimplementation, 281\nsoftware implementation, 279\u2013280\nstructure, 278\u2013279\nPersistent communication, and mutual\nexclusion, 9\nPeterson lock\nbounds, 39\nimplementation, 142\u2013143\nin mutual exclusion, 27\u201328\nPhasedCuckooHashSet class,\nimplementation, 318\u2013321\nPhases\ncomputation organization, 397\nconcurrent Cuckoo hashing, 318\nPhysical removal\nlazy synchronization, 196, 208\nPrioritySkipList class, 363\nPipelining, counting networks, 280\u2013281\nPools\nde\ufb01nition, 223\nparallel matrix multiplication, 370queues, 224\nquiescently-consistent, 269\u2013270\nand shared counting, 259\ntermination detection barrier, 408\nvarieties, 223\u2013224\nwork stealing, 406\u2013407\nPopulation-oblivious method,\nconcurrent objects, 59\nPostcondition, sequential objects, 48\nPrecedence graph, timestamps, 34\nPrecedence relation, in mutual\nexclusion, 22\nPrecombining phase, CombiningTree,\n262\nPrecondition, sequential objects, 48\nPredecessor nodes, HCLHLock queue, 172\nPredecessor task, directed acyclic graph,\n375\nPriority\ncontention manager, 432\nin priority queue, 351\nPriority inversion, in locking, 417\nPriority queues, de\ufb01nition, 351\nPrioritySkipList class\nimplementation, 364\noverview, 363\nstructure, 365\nPrism\ndiffracting trees, 283\ndistributed coordination, 291\nimplementation, 284\nProbabilistic data structure, SkipList\nclass, 330\nProbe sets, concurrent Cuckoo", "doc_id": "0728fa30-975f-4a4e-b383-1aa7d7ee4924", "embedding": null, "doc_hash": "4d110dba88d609c077968ce8107739897f7445bbdf78bbc51c34cbac38e6a4ed", "extra_info": null, "node_info": {"start": 1269997, "end": 1273072}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "596e2186-01bd-468c-bf08-e946c1da24ee", "3": "ca1a8d00-af59-48dc-8ff8-a29bd8a2b2f7"}}, "__type__": "1"}, "ca1a8d00-af59-48dc-8ff8-a29bd8a2b2f7": {"__data__": {"text": "sequential objects, 48\nPredecessor nodes, HCLHLock queue, 172\nPredecessor task, directed acyclic graph,\n375\nPriority\ncontention manager, 432\nin priority queue, 351\nPriority inversion, in locking, 417\nPriority queues, de\ufb01nition, 351\nPrioritySkipList class\nimplementation, 364\noverview, 363\nstructure, 365\nPrism\ndiffracting trees, 283\ndistributed coordination, 291\nimplementation, 284\nProbabilistic data structure, SkipList\nclass, 330\nProbe sets, concurrent Cuckoo hashing,\n318\nProcessors, hardware concepts, 471, 472\nProducer\u2013consumer problem, example,\n10\u201311\nProducer\u2013consumer property, in\nproducer\u2013consumer problem,\n10\u201311\nProducers\nnaive synchronous queue, 237\npools, 223\nProgram correctness (or Correctness),\nde\ufb01nition, 2\n504 Index\nProgram order, de\ufb01nition, 52\nProgram performance, de\ufb01nition, 2\nProgress conditions\nconcurrent objects, 59\u201360\ndependent, concurrent objects, 60\u201361\nProgress property, concurrent objects,\n45\nProtocol state\nbivalence and univalence, 102\nconsensus numbers, 101\nPthreads\nbasic functionality, 464\nimplementation, 467\ninvention, 466\nthread-local storage, 465\u2013466\nQ\nQNode class\nCLH queue locks, 153\nCompositeLock class, 161\nHCLHLock queue, 169\nMCS queue lock, 155\u2013156\nSynchronousDualQueue class, 239\ntimeout lock, 157\u2013158\nQueue locks\narray-based, 150\u2013151\nCLH, 151\u2013154\nMCS, 154\u2013157\noverview, 149\u2013150\nwith timeouts, 157\u2013159\nQueues\narray-based bounded priority\nqueues, 352\u2013353\nBoundedDEQueue class, 382\u2013384\nbounded partial queue, 225\u2013229\nBoundedQueue class, 225\u2013227\nbounded-range priority queue,\n351\u2013353\nbounded transactional queue, 423\nCLH queue lock, 151, 153\u2013154,\n168\u2013172\nconcurrent priority queues, 351\u2013352\nFIFO queue, 45\u201348\nglobal queue, 168\nHCLHLock queue, 168\u2013173\nhierarchical CLH queue lock,\n168\u2013172LockedQueue class, 182\nlock-free queue, 241\nLockFreeQueue class, 230\u2013231,\n419\u2013420\nLockFreeQueueRecycle class, 236\nlocking queue, 47\nMCS queue lock, 154\u2013156\nnaive synchronous queue, 237\u2013238\npool fairness, 224\npriority queues, 351\nSimpleTree priority queues,\n353\u2013354\nskiplist-based unbounded priority\nqueues, 363\u2013366\nSkipQueue class, 365\u2013366\nSynchronousDualQueue class,\n239\u2013240\nsynchronous queue, 237\u2013238, 241\nSynchronousQueue class, 238\ntree-based bounded priority queues,\n353\u2013355\nunboundedDEQueue class, 386\u2013388\nunbounded heap-based priority\nqueues, 357\u2013363\nunbounded lock-free queue, 230\u2013233\nUnboundedQueue class, 229\nunbounded-range priority queue,\n351\nunbounded total queue, 229\u2013230\nunbounded transactional queue, 422\nQuicksort algorithm, for sample sorting,\n290\nQuiescent consistency\napplications, 45\nconcurrent objects, 49\u201351\nconcurrent priority queues, 351\u2013352\npools and counters, 269\u2013270\nvs.sequential consistency, 52\u201353\nshared counters, 270\nR\nRadix children, tree nodes, 402\nReachable, concurrent reasoning, 199\nReaders\u2013writers lock\nbasic concepts, 183\nfair lock, 185\u2013187\nsimple lock, 184\u2013185Readers\u2013writers problem, example,\n11\u201312\nRead lock, de\ufb01nition, 183\nRead\u2013modify\u2013write (RMW) registers\nCommon2 registers, 114\u2013116\nmethods, 112\u2013113\nshared counter implementation, 5\u20136\nsource,", "doc_id": "ca1a8d00-af59-48dc-8ff8-a29bd8a2b2f7", "embedding": null, "doc_hash": "5839e2b0edfe92347c0fa348cc9cac1b2d2193e01b1ec015d3430fbf5c994494", "extra_info": null, "node_info": {"start": 1273069, "end": 1276083}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "0728fa30-975f-4a4e-b383-1aa7d7ee4924", "3": "32fd8de8-d400-4e85-b96a-6897b0dff592"}}, "__type__": "1"}, "32fd8de8-d400-4e85-b96a-6897b0dff592": {"__data__": {"text": "priority queues, 351\u2013352\npools and counters, 269\u2013270\nvs.sequential consistency, 52\u201353\nshared counters, 270\nR\nRadix children, tree nodes, 402\nReachable, concurrent reasoning, 199\nReaders\u2013writers lock\nbasic concepts, 183\nfair lock, 185\u2013187\nsimple lock, 184\u2013185Readers\u2013writers problem, example,\n11\u201312\nRead lock, de\ufb01nition, 183\nRead\u2013modify\u2013write (RMW) registers\nCommon2 registers, 114\u2013116\nmethods, 112\u2013113\nshared counter implementation, 5\u20136\nsource, 117\nRead set, lock-based atomic object, 439\nRealistic multiprocessor scheduling,\nde\ufb01nitions and operation,\n378\u2013380\nReal-time order, vs.sequential\nconsistency, 53\nRebalancing, de\ufb01nition, 329\nRecursive initialization, lock-free hash\nset, 316\nRecursive split-ordering, lock-free hash\nset, 309\u2013312\nReentrant\nBaseHashSet class, 301\nConditions objects, 180\nReentrant lock\nde\ufb01nition, 187\nmethods, 188\nReference, BoundedDEQueue class, 383\nRe\ufb01nable concurrent Cuckoo hash set,\n324\u2013325\nRefinableCuckooHashSet class,\n324\u2013325\nRefinableHashSet class\nimplementation, 306\u2013308\nresizing, 305\nRegBoolMRSWRegister class,\nimplementation, 79\nRegisters\nconstruction overview, 77\u201378\nde\ufb01nition, 50, 71, 72\nand mutual exclusion, 73\nsafe, 74\u201375\n3D implementation space, 76\nwrite order, 76\nRegMRSWRegister class,\nimplementation, 80\nRegular nodes, list-based sets, 197\nRegular registers\nBoolean MRSW, 78\u201379\nIndex 505\nconditions, 77\nde\ufb01nition, 75\n\ufb01rst de\ufb01nition, 93\nimplementation considerations,\n75\u201376\nM-valued MRSW register,\n79\u201381\nsafe, 75\nRelaxedLock, 174\nReleases\nCLHLock class, 154\nde\ufb01nition, 23\nHCLHLock lock, 173\nJava concepts, 456\nlocks, 178\nMCSLock class, 156\nReordering, and memory consistency,\n479\nReplacement policy, and cache, 474\nRepresentation invariant, concurrent\nreasoning, 198\u2013199\nReservation object, dual data structures,\n239\nResponse events\nconcurrent objects, 56\nlock-free universal algorithm, 127\nand register space, 76\nRESULT status\nCombiningTree, 267\nde\ufb01nition, 262\nReverse tree barrier, implementation,\n412\u2013414\nRMW, seeRead\u2013modify\u2013write (RMW)\nregisters\nRobustness, CombiningTree, 269\nROOT status\nCombiningTree, 265\u2013266\nde\ufb01nition, 262\nRunnable object, Java thread concepts,\n453\u2013454\nS\nSafe\nregisters, 74\u201375\nregular register, 75\nSafeBoolMRSWRegister class,\nimplementation, 78Safe registers\n\ufb01rst de\ufb01nition, 93\nMRSW, 78\nSRSW Boolean, 86\nSafety property, de\ufb01nition, 2\nSample sorting\noriginal ideas, 293\nphases, 290\u2013291\nSaturation, counting networks, 280\nScan-and-label operations, timestamps,\n34\nScheduler\nfunction, 378\ngreedy, 379\nidle step, 379\nSECOND status\nCombiningTree, 268\nde\ufb01nition, 261\nSemaphores, de\ufb01nition and\nimplementation, 189\nSenseBarrier class, constructor, 400\nSense-serving barrier, implementation,\n399\u2013400\nSentinel nodes\nbounded partial queues, 225\nlist-based sets, 197\nsplit-ordered hash set, 311\nSequential bottleneck\nLockFreeStack, 248\nand shared counting,", "doc_id": "32fd8de8-d400-4e85-b96a-6897b0dff592", "embedding": null, "doc_hash": "4d972b85570068be3325b1737bb272780c4e5aa5f0ab32724541fa1a0a719004", "extra_info": null, "node_info": {"start": 1276100, "end": 1278914}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "ca1a8d00-af59-48dc-8ff8-a29bd8a2b2f7", "3": "b3e3209e-36f4-4604-ad05-89bf87475424"}}, "__type__": "1"}, "b3e3209e-36f4-4604-ad05-89bf87475424": {"__data__": {"text": "counting networks, 280\nScan-and-label operations, timestamps,\n34\nScheduler\nfunction, 378\ngreedy, 379\nidle step, 379\nSECOND status\nCombiningTree, 268\nde\ufb01nition, 261\nSemaphores, de\ufb01nition and\nimplementation, 189\nSenseBarrier class, constructor, 400\nSense-serving barrier, implementation,\n399\u2013400\nSentinel nodes\nbounded partial queues, 225\nlist-based sets, 197\nsplit-ordered hash set, 311\nSequential bottleneck\nLockFreeStack, 248\nand shared counting, 259\nSequential consistency\napplications, 45\nconcurrent objects, 51\u201354\nvs.quiescent consistency, 52\u201353\nvs.real-time order, 53\nSequentialHeap class, implementation,\n356\u2013357\nSequential objects\ngeneric de\ufb01nition, 127\nspeci\ufb01cations, 48\u201349\nSequentialRegister class,\nimplementation, 73\nSequential skiplists, de\ufb01nition and\ndesign, 329\u2013331\nSequential speci\ufb01cation\nconcurrent objects, 56\nde\ufb01nition, 49\nSequential timestamping, Lock class, 34Serializable, transactional memory, 421\nSets\nde\ufb01nition, 196\nlist-based, 196\u2013198\nShared concurrent objects, and register\nspace, 72\nShared counter\napproach, 259\u2013260\nimplementation, 4\u20135\nquiescently consistent, 270\nShared-memory multiprocessors,\nprogramming challenges, 1\nShared objects, and synchronization,\n3\u20136\nShared state, cache coherence, 446,\n475\u2013476\nSharing, de\ufb01nition, 474\nSimpleBarrier class, implementation,\n399\nSimpleLinear class\ncreators, 366\nimplementation, 352\nSimple readers\u2013writers lock\nbasic concepts, 184\u2013185\n\ufb01elds and public methods, 184\ninner read lock, 184\ninner write lock, 185\nSimpleTree priority queues\nimplementation, 354\nstructure, 353\nSingle-reader, single-writer (SRSW)\natomic register, 77\u201378, 81\u201383\nregister space, 73\nsafe, 74\nwrite order, 76\nSingle-writer atomic snapshot class\ncollect method, 91\nupdate and scan methods, 91\nSkipList class\nalgorithm, 349\nlevels, 330\nSkiplists\ninvention, 348\nLazySkipList class, 331\u2013332\nsoftware transactional memory,\n424\u2013425\nunbounded priority queues, 363\u2013366\n506 Index\nSkipListSet class, implementation,\n425\nSkipNode class, interface, 425\nSkipQueue class\ncharacteristics, 365\u2013366\ncreators, 366\nSleeping, as Java construct, 458\nSlot, array-based locks, 150\nSMP , seeSymmetric multiprocessing\n(SMP) architecture\nSnapshots\nconstruction, 93\ncorrectness arguments, 90\u201393\nde\ufb01nition, 87\nand multiple assignment, 110\nobstruction-free, 87\u201388\nwait-free snapshot, 88\u201390\nSnooping\ncache coherence, 446\ninterconnects, 472\nSoft real-time application, de\ufb01nition,\n397\nSoftware combining, basic concepts,\n260\u2013261\nSoftware implementation\nbitonic network classes, 274\u2013276\nbitonic network proof of correctness,\n276\u2013278\nperiodic counting network, 279\u2013280\nSoftware transactional memory (STM)\natomic object implementation,\n433\u2013434\natomic objects, 429\u2013431\ncontention manager, 431\u2013433\ndependent or independent, 431\n\ufb01rst proposal, 448\nlock-based atomic objects, 438\u2013445\nobstruction-free atomic object,\n434\u2013438\noverview, 424\u2013427\nskip lists, 424\u2013425\ntransactions and transactional\nthreads, 427\u2013428\nTThread class, 426\nzombies and consistency, 428\u2013429\nSorting\nbitonic algorithm, 288\u2013289parallel, 286\nSorting networks\ndesigning,", "doc_id": "b3e3209e-36f4-4604-ad05-89bf87475424", "embedding": null, "doc_hash": "e77d5810bfbe10f2ed8d4c7614e486cd33b5de981ac7a670e9d4e20c89b3196b", "extra_info": null, "node_info": {"start": 1278908, "end": 1281944}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "32fd8de8-d400-4e85-b96a-6897b0dff592", "3": "bfbc89a4-4b63-41bf-aa13-f46cd01554af"}}, "__type__": "1"}, "bfbc89a4-4b63-41bf-aa13-f46cd01554af": {"__data__": {"text": "network, 279\u2013280\nSoftware transactional memory (STM)\natomic object implementation,\n433\u2013434\natomic objects, 429\u2013431\ncontention manager, 431\u2013433\ndependent or independent, 431\n\ufb01rst proposal, 448\nlock-based atomic objects, 438\u2013445\nobstruction-free atomic object,\n434\u2013438\noverview, 424\u2013427\nskip lists, 424\u2013425\ntransactions and transactional\nthreads, 427\u2013428\nTThread class, 426\nzombies and consistency, 428\u2013429\nSorting\nbitonic algorithm, 288\u2013289parallel, 286\nSorting networks\ndesigning, 287\u2013288\nOddEven design, 288\nstructure, 286\u2013287\nSouth wires\ncounting networks, 270\nperiodic network, 279\nSpectulativeness, memory transactions,\n422\nSpin-locks\nde\ufb01nition, 141, 178\nTAS-based, 146\u2013147\nSpinning\nde\ufb01nition, 141\nhardware concepts, 476\nSplitters, sample sorting, 290\nSRSW, seeSingle-reader, single-writer\n(SRSW)\nSSkipNode class, implementation,\n430\nStack class, de\ufb01nition, 245\nStamp\nABA problem, 234\nBoundedDEQueue class, 383\nlock-based atomic object, 439\nStamped snapshot class,\nimplementation, 90\nStart\nC# constructs, 460\nJava constructs, 454\nStarvation, in Lock object, 24\nStarvation-freedom property\nde\ufb01nition, 8\u20139\nas dependent progress condition, 60\nin Lock algorithm, 24\nin producer\u2013consumer problem,\n10\u201311\nState\nconsensus numbers, 101\u2013103\nobjects, 48\nState machine, de\ufb01nition, 21\nStatic tree barrier, implementation,\n402\u2013404\nStaticTreeBarrier class, Node class,\n405Step property\nbalancing networks, 272\nbitonic counting network, 276\nscheduler, 379\nTree class, 282\nSteps, directed acyclic graph, 380\nSTM, seeSoftware transactional\nmemory (STM)\nStore buffer, seeWrite buffer\nde\ufb01nition, 478\nStores\nde\ufb01nition, 37\nLock algorithm bounds, 37\nin registers, 72\nStriped concurrent Cuckoo hashing,\n322\u2013324\nStripedCuckooHashSet class, 322\u2013323\nStriped hash set\nbasic concepts, 303\u2013304\nimplementation, 304\nlock striping, 304\nStripedHashSet class\nimplementation, 304\nlock-based hash table, 305\nresizing, 306\nSubhistory, concurrent objects, 56\nSuccessful call, de\ufb01nition, 196\nSuccessor state, consensus numbers, 101\nSymmetric multiprocessing (SMP)\nspinning, 476\nSymmetric multiprocessing (SMP)\narchitecture\nbasic concepts, 472\u2013473\nSynchronization\ncoarse-grained, 195\ncoarse-grained hash sets, 302\n\ufb01ne-grained, 195\nhardware instructions, 480\u2013481\ninstructions, memory barriers, 144\nin Java memory model, 62\nlazy, 196\nnonblocking, 196\noptimisitc, 195\nand shared objects, 3\u20136\nSynchronization primitives\natomic registers, 103\u2013105\nCommon2 RMW registers, 114\u2013116\nIndex 507\ncompareAndSet() operation,\n116\u2013117\nconsensus numbers, 100\u2013103\nconsensus protocols, 106\nde\ufb01nition, 99\u2013100\nFIFO queues, 106\u2013109\nmultiple assignment objects, 110\u2013112\nread\u2013modify\u2013write operations,\n112\u2013114\nSynchronized blocks, in Java memory\nmodel, 62\u201363\nSynchronousDualQueue class\nmethod and constructor, 240\nqueue node, 239\nSynchronous method, pool, 224\nSynchronous queue\nbasic concepts, 237\ncreators, 241\nimplementation, 238\nSynchronousQueue class,\nimplementation, 238\nT\nTable\nCuckoo hashing,", "doc_id": "bfbc89a4-4b63-41bf-aa13-f46cd01554af", "embedding": null, "doc_hash": "8f06b8c0eb6a4a79f4fd6c8d2a32c2bc62d44e6300ab311ce84f1fddf714302c", "extra_info": null, "node_info": {"start": 1281915, "end": 1284851}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "b3e3209e-36f4-4604-ad05-89bf87475424", "3": "a375a15d-8da4-4e5a-a22e-e5adfc613f71"}}, "__type__": "1"}, "a375a15d-8da4-4e5a-a22e-e5adfc613f71": {"__data__": {"text": "numbers, 100\u2013103\nconsensus protocols, 106\nde\ufb01nition, 99\u2013100\nFIFO queues, 106\u2013109\nmultiple assignment objects, 110\u2013112\nread\u2013modify\u2013write operations,\n112\u2013114\nSynchronized blocks, in Java memory\nmodel, 62\u201363\nSynchronousDualQueue class\nmethod and constructor, 240\nqueue node, 239\nSynchronous method, pool, 224\nSynchronous queue\nbasic concepts, 237\ncreators, 241\nimplementation, 238\nSynchronousQueue class,\nimplementation, 238\nT\nTable\nCuckoo hashing, 316\nde\ufb01nition, 299\u2013300\nTail, de\ufb01nition, 224\nTasks, work distribution, 380\u2013382\nTASLock class\nimplementation, 144\u2013145\nperformance, 146\nT entativeness, memory transactions, 422\nT ermination detection barrier\ncreators, 408\nimplementation, 404\u2013408\nT est-and-set locks\nbasic principles, 144\nhardware concepts, 469\nreal-world processing, 145\u2013146\nT est-and-test-and-set\nde\ufb01nition, 144\u2013145\nhardware concepts, 469\nThief, work stealing, 380\u2013382\nThinlock, 174\nThread-local objects\nas C# constructs, 462\u2013463\nas Java constructs, 458\u2013459Thread-local storage, Pthreads, 465\u2013466\nThread-local variables, array-based\nlocks, 150\nThreads\nas C# constructs, 460\u2013461\nde\ufb01nition, 71\nhardware concepts, 472\nas Java constructs, 453\u2013454\nThroughput, de\ufb01nition, 259\nTime, in mutual exclusion, 21\u201322\nTimeouts, in queue lock, 157\u2013159\nTimestamps\nBakery lock labels, 34\nde\ufb01nition, 72\noverview, 33\nTOLock class\n\ufb01elds and constructor, 158\nmethods, 158\nand timed-out nodes, 159\nT op wires\ncounting networks, 270\nsorting networks, 286\nT otal method, pool, 223\nT otal order, concurrent objects, 56\nTourBarrier class\nimplementation, 410\ninformation \ufb02ow, 411\nT ournament tree barrier, creators, 408\nTransactional cache, hardware\ntransactional memory, 448\nTransactional cache coherence,\nhardware transactional\nmemory, 447\nTransactional Locking 2 algorithm,\ncreators, 448\nTransactional memory\natomic primitive problems,\n418\u2013420\ncompositionality problems,\n420\u2013421\nde\ufb01nition, 3\nlocking problems, 417\u2013418\noverview, 417\ntransactions and atomicity, 421\u2013423\nTransactional threads, and transactions,\n427\u2013428Transactions\nand atomicity, 421\u2013423\nde\ufb01nition, 421\nand transactional threads, 427\u2013428\nTransient communication, and mutual\nexclusion, 9\nTreeBarrier class\ncombining tree, 403\nimplementation, 400\u2013402\nTree-based bounded priority queues,\nstructure and implementation,\n353\u2013355\nTree class, structure, 282\nTSkipNode class, implementation, 434\nTTASLock class\ncreators, 172\nexponential backoff, 147\u2013149\nimplementation, 145, 470\non shared-bus, 146\u2013147\nperformance, 146\nTThread class, software transactional\nmemory, 426\nU\nunboundedDEQueue class,\nimplementation, 386\u2013388\nUnbounded heap-based priority queues\nconcurrent heap, 357\u2013363\nde\ufb01nitions, 355\nsequential heap, 356\u2013357\nUnbounded lock-free queue\nimplementation, 230\u2013231\nlazy, 231\u2013232\nstep-wise operation, 232\u2013233\nUnbounded lock-free stack\nde\ufb01nition, 245\nimplementation, 246\u2013247\nstructure, 246\nUnbounded pool, characteristics, 223\nUnboundedQueue class, methods, 229\nUnbounded-range priority", "doc_id": "a375a15d-8da4-4e5a-a22e-e5adfc613f71", "embedding": null, "doc_hash": "d51fa4d1360b141c0c2a8bca62621efa83444d9013462b7916826763e419d8c2", "extra_info": null, "node_info": {"start": 1284884, "end": 1287816}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "bfbc89a4-4b63-41bf-aa13-f46cd01554af", "3": "d49e7643-39c7-4754-9ea2-315fdc4efcfe"}}, "__type__": "1"}, "d49e7643-39c7-4754-9ea2-315fdc4efcfe": {"__data__": {"text": "426\nU\nunboundedDEQueue class,\nimplementation, 386\u2013388\nUnbounded heap-based priority queues\nconcurrent heap, 357\u2013363\nde\ufb01nitions, 355\nsequential heap, 356\u2013357\nUnbounded lock-free queue\nimplementation, 230\u2013231\nlazy, 231\u2013232\nstep-wise operation, 232\u2013233\nUnbounded lock-free stack\nde\ufb01nition, 245\nimplementation, 246\u2013247\nstructure, 246\nUnbounded pool, characteristics, 223\nUnboundedQueue class, methods, 229\nUnbounded-range priority queue,\nde\ufb01nition, 351\nUnbounded total queue\ndeadlock, 230\nimplementation, 229\u2013230\nUnbounded transactional queue,\nimplementation, 422\n508 Index\nUnbounded work-stealing\ndouble-ended queues\ncreator, 392\nimplementation, 386\u2013388\nUnivalence, protocol state, 102\nUniversality, de\ufb01nition, 126\nUnlocks\nCLHLock class, 154\nde\ufb01nition, 22, 23\nhardware concepts, 469\nHCLHLock lock, 173\ninterface, 23\nJava concepts, 456\u2013457\nlocks, 178\nMCSLock class, 156\nUnsuccessful call, de\ufb01nition, 196\nUpdate value, compareAndSet()\noperation, 116, 480\nUp state, balancers, 271\nV\nValence, consensus numbers, 101\u2013103\nValidation\nLazyList class, 209, 211\noptimistic synchronization, 207\ntransaction handlers, 428\nVersion, lock-based atomic object, 439\nVersion clock, lock-based atomic object,\n439, 441\nVictim\nunbounded work-stealing DEQueue,\n390\nwork stealing, 380\nVictim cache, hardware transactional\nmemory, 448\nVolatile \ufb01elds, in Java memory model, 63W\nWait-free method\nconcurrent objects, 59\nconcurrent reasoning, 199\nde\ufb01nition, 99\nhistory, 65\nLazySkipList class, 335\nLockFreeList class, 219\nMRMW atomic register, 86\nand register space, 73\nWait-free snapshot, construction, 88\u201390\nWait-free universal construction\nalgorithm, 131\nauxiliary variables, 133\u2013134\nexecution, 132\nhelping pattern, 130\u2013131\nlinearizability, 133\nmodi\ufb01cation considerations, 133\nnode announcement, 130\u2013132\nproof, 135\u2013136\nWaiting\nJava concepts, 456\ninLock class, 31\nin mutual exclusion, 15\nin readers\u2013writers problem, 12\nWell-formed thread, requirements, 23\nWindow class, lock-free lists, 216\nWinner thread, Common2 register, 114\nWork, parallelism, 376\nWork distribution\nbalancing, 389\u2013391\nbounded work-stealing dequeue,\n382\u2013386\noverview, 380\nunbounded work-stealing DEQueue,\n386\u2013388work stealing, 380\nyielding and multiprogramming,\n380\u2013382\nWork sharing, creators, 391\nWorkSharingThread class,\nimplementation, 391\nWork stealing\ncharacteristics, 381\ncreators, 391\nexecutor pool, 406\u2013407\nWork-stealing double-ended queues\nbounded, 382\u2013386\noverview, 382\nunbounded, 386\u2013388\nWorkStealingThread class,\nimplementation, 382\nWrite absorption, de\ufb01nition,\n478\nWrite buffer\nde\ufb01nition, 478\nshared memory writes, 143\nWrite lock, de\ufb01nition, 183\nWrite order\nand register space, 76\nSRSW and MRSW, 76\nWrite set, lock-based atomic object, 439\nY\nYielding\nas Java construct concepts, 458\nwork distribution, 380\u2013382\nZ\nZombies, software transactional\nmemory, 428\u2013429\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nEDELKAMP", "doc_id": "d49e7643-39c7-4754-9ea2-315fdc4efcfe", "embedding": null, "doc_hash": "49811da734a778a962cb6f9a28c7fcdd1e8039829a3448cae4adc3844c371807", "extra_info": null, "node_info": {"start": 1287832, "end": 1290754}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "a375a15d-8da4-4e5a-a22e-e5adfc613f71", "3": "86a40205-2f59-4ab9-a6d5-4b434ea7ba79"}}, "__type__": "1"}, "86a40205-2f59-4ab9-a6d5-4b434ea7ba79": {"__data__": {"text": "buffer\nde\ufb01nition, 478\nshared memory writes, 143\nWrite lock, de\ufb01nition, 183\nWrite order\nand register space, 76\nSRSW and MRSW, 76\nWrite set, lock-based atomic object, 439\nY\nYielding\nas Java construct concepts, 458\nwork distribution, 380\u2013382\nZ\nZombies, software transactional\nmemory, 428\u2013429\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank\nEDELKAMP 19-ch15-671-700-9780123725127 2011/5/28 14:50 Page 672 #2\nThis page intentionally left blank", "doc_id": "86a40205-2f59-4ab9-a6d5-4b434ea7ba79", "embedding": null, "doc_hash": "15d0f3a941e5c53a004035d65d7e486b0fdf0622228cbbbab270506a84d0157c", "extra_info": null, "node_info": {"start": 1290753, "end": 1291449}, "relationships": {"1": "871d3b50-3fad-4381-abb5-2153027503ae", "2": "d49e7643-39c7-4754-9ea2-315fdc4efcfe"}}, "__type__": "1"}, "a9a40071-6002-475a-a7c5-c224caa05099": {"__data__": {"text": "[ Team LiB ]\n\u00a0 \u00a0\n\u2022\u00a0 Table of Contents\nIntroduction to Parallel Computing, Second Edition\nBy Ananth\u00a0Grama , Anshul\u00a0Gupta , George\u00a0Karypis , Vipin\u00a0Kumar\n\u00a0\nPublisher : Addison Wesley\nPub Date : January 16, 2003\nISBN: 0-201-64865-2\nPages : 856\nIncreasingly, parallel processing is being seen as the only cost-effective method for the fast\nsolution of computationally large and data-intensive problems. The emergence of inexpensive\nparallel computers such as commodity desktop multiprocessors and clusters of workstations or\nPCs has made such parallel methods generally applicable, as have software standards for\nportable parallel programming. This sets the stage for substantial growth in parallel software.\nData-intensive applications such as transaction processing and information retrieval, data\nmining and analysis and multimedia services have provided a new challenge for the modern\ngeneration of parallel platforms. Emerging areas such as computational biology and\nnanotechnology have implications for algorithms and systems development, while changes in\narchitectures, programming models and applications have implications for how parallel\nplatforms are made available to users in the form of grid-based services.\nThis book takes into account these new developments as well as covering the more traditional\nproblems addressed by parallel computers.Where possible it employs an architecture-\nindependent view of the underlying platforms and designs algorithms for an abstract model.\nMessage Passing Interface (MPI), POSIX threads and OpenMP have been selected as\nprogramming models and the evolving application mix of parallel computing is reflected in\nvarious examples throughout the book.[ Team LiB ]\n[ Team LiB ]\n\u00a0 \u00a0\n\u2022\u00a0 Table of Contents\nIntroduction to Parallel Computing, Second Edition\nBy Ananth\u00a0Grama , Anshul\u00a0Gupta , George\u00a0Karypis , Vipin\u00a0Kumar\n\u00a0\nPublisher : Addison Wesley\nPub Date : January 16, 2003\nISBN: 0-201-64865-2\nPages : 856\nIncreasingly, parallel processing is being seen as the only cost-effective method for the fast\nsolution of computationally large and data-intensive problems. The emergence of inexpensive\nparallel computers such as commodity desktop multiprocessors and clusters of workstations or\nPCs has made such parallel methods generally applicable, as have software standards for\nportable parallel programming. This sets the stage for substantial growth in parallel software.\nData-intensive applications such as transaction processing and information retrieval, data\nmining and analysis and multimedia services have provided a new challenge for the modern\ngeneration of parallel platforms. Emerging areas such as computational biology and\nnanotechnology have implications for algorithms and systems development, while changes in\narchitectures, programming models and applications have implications for how parallel\nplatforms are made available to users in the form of grid-based services.\nThis book takes into account these new developments as well as covering the more traditional\nproblems addressed by parallel computers.Where possible it employs an architecture-\nindependent view of the underlying platforms and designs algorithms for an abstract model.\nMessage Passing Interface (MPI), POSIX threads and OpenMP have been selected as\nprogramming models and the evolving application mix of parallel computing is reflected in\nvarious examples throughout the book.[ Team LiB ]\n\n[ Team LiB ]\n  \n\u00a0 \u00a0\n\u2022\u00a0 Table of Contents\nIntroduction to Parallel Computing, Second Edition\nBy Ananth\u00a0Grama , Anshul\u00a0Gupta , George\u00a0Karypis , Vipin\u00a0Kumar\n\u00a0\nPublisher : Addison Wesley\nPub Date : January 16, 2003\nISBN: 0-201-64865-2\nPages : 856\n\u00a0\u00a0\u00a0 Copyright\n\u00a0\u00a0\u00a0 Pearson Education\n\u00a0\u00a0\u00a0 Preface\n\u00a0\u00a0\u00a0 Acknowledgments\n\u00a0\u00a0\u00a0 Chapter 1.\u00a0 Introduction to Parallel Computing\n\u00a0\u00a0\u00a0\u00a0 Section 1.1.\u00a0 Motivating Parallelism\n\u00a0\u00a0\u00a0\u00a0 Section 1.2.\u00a0 Scope of Parallel Computing\n\u00a0\u00a0\u00a0\u00a0 Section 1.3.\u00a0 Organization and Contents of the Text\n\u00a0\u00a0\u00a0\u00a0 Section 1.4.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 2.\u00a0", "doc_id": "a9a40071-6002-475a-a7c5-c224caa05099", "embedding": null, "doc_hash": "c7a9803367723b778330fbc8b336a0e10000b20ef59a6d0b2bd69ccc4e478e02", "extra_info": null, "node_info": {"start": 0, "end": 3989}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "3": "e1f5b31c-339e-4eb1-9f89-dafe5eec57ea"}}, "__type__": "1"}, "e1f5b31c-339e-4eb1-9f89-dafe5eec57ea": {"__data__": {"text": "is reflected in\nvarious examples throughout the book.[ Team LiB ]\n\n[ Team LiB ]\n  \n\u00a0 \u00a0\n\u2022\u00a0 Table of Contents\nIntroduction to Parallel Computing, Second Edition\nBy Ananth\u00a0Grama , Anshul\u00a0Gupta , George\u00a0Karypis , Vipin\u00a0Kumar\n\u00a0\nPublisher : Addison Wesley\nPub Date : January 16, 2003\nISBN: 0-201-64865-2\nPages : 856\n\u00a0\u00a0\u00a0 Copyright\n\u00a0\u00a0\u00a0 Pearson Education\n\u00a0\u00a0\u00a0 Preface\n\u00a0\u00a0\u00a0 Acknowledgments\n\u00a0\u00a0\u00a0 Chapter 1.\u00a0 Introduction to Parallel Computing\n\u00a0\u00a0\u00a0\u00a0 Section 1.1.\u00a0 Motivating Parallelism\n\u00a0\u00a0\u00a0\u00a0 Section 1.2.\u00a0 Scope of Parallel Computing\n\u00a0\u00a0\u00a0\u00a0 Section 1.3.\u00a0 Organization and Contents of the Text\n\u00a0\u00a0\u00a0\u00a0 Section 1.4.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 2.\u00a0 Parallel Programming Platforms\n\u00a0\u00a0\u00a0\u00a0 Section 2.1.\u00a0 Implicit Parallelism: Trends in Microprocessor Architectures*\n\u00a0\u00a0\u00a0\u00a0 Section 2.2.\u00a0 Limitations of Memory System Performance*\n\u00a0\u00a0\u00a0\u00a0 Section 2.3.\u00a0 Dichotomy of Parallel Computing Platforms\n\u00a0\u00a0\u00a0\u00a0 Section 2.4.\u00a0 Physical Organization of Parallel Platforms\n\u00a0\u00a0\u00a0\u00a0 Section 2.5.\u00a0 Communication Costs in Parallel Machines\n\u00a0\u00a0\u00a0\u00a0 Section 2.6.\u00a0 Routing Mechanisms for Interconnection Networks\n\u00a0\u00a0\u00a0\u00a0 Section 2.7.\u00a0 Impact of Process-Processor Mapping and Mapping Techniques\n\u00a0\u00a0\u00a0\u00a0 Section 2.8.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 3.\u00a0 Principles of Parallel Algorithm Design\n\u00a0\u00a0\u00a0\u00a0 Section 3.1.\u00a0 Preliminaries\n\u00a0\u00a0\u00a0\u00a0 Section 3.2.\u00a0 Decomposition Techniques\n\u00a0\u00a0\u00a0\u00a0 Section 3.3.\u00a0 Characteristics of Tasks and Interactions\n\u00a0\u00a0\u00a0\u00a0 Section 3.4.\u00a0 Mapping Techniques for Load Balancing\n\u00a0\u00a0\u00a0\u00a0 Section 3.5.\u00a0 Methods for Containing Interaction Overheads\n\u00a0\u00a0\u00a0\u00a0 Section 3.6.\u00a0 Parallel Algorithm Models\n\u00a0\u00a0\u00a0\u00a0 Section 3.7.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 4.\u00a0 Basic Communication Operations\n\u00a0\u00a0\u00a0\u00a0 Section 4.1.\u00a0 One-to-All Broadcast and All-to-One Reduction\n\u00a0\u00a0\u00a0\u00a0 Section 4.2.\u00a0 All-to-All Broadcast and Reduction\n\u00a0\u00a0\u00a0\u00a0 Section 4.3.\u00a0 All-Reduce and Prefix-Sum Operations\n\u00a0\u00a0\u00a0\u00a0 Section 4.4.\u00a0 Scatter and Gather\n\u00a0\u00a0\u00a0\u00a0 Section 4.5.\u00a0 All-to-All Personalized Communication\n\u00a0\u00a0\u00a0\u00a0 Section 4.6.\u00a0 Circular Shift\n\u00a0\u00a0\u00a0\u00a0 Section 4.7.\u00a0 Improving the Speed of Some Communication Operations\n\u00a0\u00a0\u00a0\u00a0 Section 4.8.\u00a0 Summary\n\u00a0\u00a0\u00a0\u00a0 Section 4.9.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 5.\u00a0 Analytical Modeling of Parallel Programs\n\u00a0\u00a0\u00a0\u00a0 Section 5.1.\u00a0 Sources of Overhead in Parallel Programs\n\u00a0\u00a0\u00a0\u00a0 Section 5.2.\u00a0 Performance Metrics for Parallel Systems\n\u00a0\u00a0\u00a0\u00a0 Section 5.3.\u00a0 The Effect of Granularity on Performance\n\u00a0\u00a0\u00a0\u00a0 Section 5.4.\u00a0 Scalability of Parallel Systems\n\u00a0\u00a0\u00a0\u00a0 Section 5.5.\u00a0 Minimum Execution Time and Minimum Cost-Optimal Execution Time\n\u00a0\u00a0\u00a0\u00a0 Section 5.6.\u00a0 Asymptotic Analysis of Parallel Programs\n\u00a0\u00a0\u00a0\u00a0 Section 5.7.\u00a0 Other Scalability Metrics\n\u00a0\u00a0\u00a0\u00a0 Section 5.8.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 6.\u00a0 Programming Using the Message-Passing Paradigm\n\u00a0\u00a0\u00a0\u00a0 Section 6.1.\u00a0 Principles of Message-Passing Programming\n\u00a0\u00a0\u00a0\u00a0 Section 6.2.\u00a0 The Building Blocks: Send and Receive Operations\n\u00a0\u00a0\u00a0\u00a0 Section 6.3.\u00a0 MPI: the Message Passing Interface\n\u00a0\u00a0\u00a0\u00a0 Section 6.4.\u00a0 Topologies and Embedding\n\u00a0\u00a0\u00a0\u00a0 Section 6.5.\u00a0 Overlapping Communication with Computation\n\u00a0\u00a0\u00a0\u00a0 Section 6.6.\u00a0 Collective Communication and Computation Operations\n\u00a0\u00a0\u00a0\u00a0 Section 6.7.\u00a0 Groups and Communicators\n\u00a0\u00a0\u00a0\u00a0 Section 6.8.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 7.\u00a0 Programming Shared Address Space Platforms\n\u00a0\u00a0\u00a0\u00a0 Section 7.1.\u00a0 Thread Basics\n\u00a0\u00a0\u00a0\u00a0 Section 7.2.\u00a0 Why Threads?\n\u00a0\u00a0\u00a0\u00a0 Section 7.3.\u00a0 The POSIX Thread API\n\u00a0\u00a0\u00a0\u00a0 Section 7.4.\u00a0 Thread Basics: Creation and Termination\n\u00a0\u00a0\u00a0\u00a0 Section 7.5.\u00a0 Synchronization Primitives in Pthreads\n\u00a0\u00a0\u00a0\u00a0 Section 7.6.\u00a0 Controlling Thread and Synchronization", "doc_id": "e1f5b31c-339e-4eb1-9f89-dafe5eec57ea", "embedding": null, "doc_hash": "1297cc65159a1b303bcd132a8832d78d73ee7eec0d5e07b339e0569b15809109", "extra_info": null, "node_info": {"start": 3422, "end": 6935}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a9a40071-6002-475a-a7c5-c224caa05099", "3": "49f557cd-b836-4eba-8b4d-6e8b44ee7f5e"}}, "__type__": "1"}, "49f557cd-b836-4eba-8b4d-6e8b44ee7f5e": {"__data__": {"text": "Send and Receive Operations\n\u00a0\u00a0\u00a0\u00a0 Section 6.3.\u00a0 MPI: the Message Passing Interface\n\u00a0\u00a0\u00a0\u00a0 Section 6.4.\u00a0 Topologies and Embedding\n\u00a0\u00a0\u00a0\u00a0 Section 6.5.\u00a0 Overlapping Communication with Computation\n\u00a0\u00a0\u00a0\u00a0 Section 6.6.\u00a0 Collective Communication and Computation Operations\n\u00a0\u00a0\u00a0\u00a0 Section 6.7.\u00a0 Groups and Communicators\n\u00a0\u00a0\u00a0\u00a0 Section 6.8.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 7.\u00a0 Programming Shared Address Space Platforms\n\u00a0\u00a0\u00a0\u00a0 Section 7.1.\u00a0 Thread Basics\n\u00a0\u00a0\u00a0\u00a0 Section 7.2.\u00a0 Why Threads?\n\u00a0\u00a0\u00a0\u00a0 Section 7.3.\u00a0 The POSIX Thread API\n\u00a0\u00a0\u00a0\u00a0 Section 7.4.\u00a0 Thread Basics: Creation and Termination\n\u00a0\u00a0\u00a0\u00a0 Section 7.5.\u00a0 Synchronization Primitives in Pthreads\n\u00a0\u00a0\u00a0\u00a0 Section 7.6.\u00a0 Controlling Thread and Synchronization Attributes\n\u00a0\u00a0\u00a0\u00a0 Section 7.7.\u00a0 Thread Cancellation\n\u00a0\u00a0\u00a0\u00a0 Section 7.8.\u00a0 Composite Synchronization Constructs\n\u00a0\u00a0\u00a0\u00a0 Section 7.9.\u00a0 Tips for Designing Asynchronous Programs\n\u00a0\u00a0\u00a0\u00a0 Section 7.10.\u00a0 OpenMP: a Standard for Directive Based Parallel Programming\n\u00a0\u00a0\u00a0\u00a0 Section 7.11.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 8.\u00a0 Dense Matrix Algorithms\n\u00a0\u00a0\u00a0\u00a0 Section 8.1.\u00a0 Matrix-Vector Multiplication\n\u00a0\u00a0\u00a0\u00a0 Section 8.2.\u00a0 Matrix-Matrix Multiplication\n\u00a0\u00a0\u00a0\u00a0 Section 8.3.\u00a0 Solving a System of Linear Equations\n\u00a0\u00a0\u00a0\u00a0 Section 8.4.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 9.\u00a0 Sorting\n\u00a0\u00a0\u00a0\u00a0 Section 9.1.\u00a0 Issues in Sorting on Parallel Computers\n\u00a0\u00a0\u00a0\u00a0 Section 9.2.\u00a0 Sorting Networks\n\u00a0\u00a0\u00a0\u00a0 Section 9.3.\u00a0 Bubble Sort and its Variants\n\u00a0\u00a0\u00a0\u00a0 Section 9.4.\u00a0 Quicksort\n\u00a0\u00a0\u00a0\u00a0 Section 9.5.\u00a0 Bucket and Sample Sort\n\u00a0\u00a0\u00a0\u00a0 Section 9.6.\u00a0 Other Sorting Algorithms\n\u00a0\u00a0\u00a0\u00a0 Section 9.7.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 10.\u00a0 Graph Algorithms\n\u00a0\u00a0\u00a0\u00a0 Section 10.1.\u00a0 Definitions and Representation\n\u00a0\u00a0\u00a0\u00a0 Section 10.2.\u00a0 Minimum Spanning Tree: Prim's Algorithm\n\u00a0\u00a0\u00a0\u00a0 Section 10.3.\u00a0 Single-Source Shortest Paths: Dijkstra's Algorithm\n\u00a0\u00a0\u00a0\u00a0 Section 10.4.\u00a0 All-Pairs Shortest Paths\n\u00a0\u00a0\u00a0\u00a0 Section 10.5.\u00a0 Transitive Closure\n\u00a0\u00a0\u00a0\u00a0 Section 10.6.\u00a0 Connected Components\n\u00a0\u00a0\u00a0\u00a0 Section 10.7.\u00a0 Algorithms for Sparse Graphs\n\u00a0\u00a0\u00a0\u00a0 Section 10.8.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 11.\u00a0 Search Algorithms for Discrete Optimization Problems\n\u00a0\u00a0\u00a0\u00a0 Section 11.1.\u00a0 Definitions and Examples\n\u00a0\u00a0\u00a0\u00a0 Section 11.2.\u00a0 Sequential Search Algorithms\n\u00a0\u00a0\u00a0\u00a0 Section 11.3.\u00a0 Search Overhead Factor\n\u00a0\u00a0\u00a0\u00a0 Section 11.4.\u00a0 Parallel Depth-First Search\n\u00a0\u00a0\u00a0\u00a0 Section 11.5.\u00a0 Parallel Best-First Search\n\u00a0\u00a0\u00a0\u00a0 Section 11.6.\u00a0 Speedup Anomalies in Parallel Search Algorithms\n\u00a0\u00a0\u00a0\u00a0 Section 11.7.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 12.\u00a0 Dynamic Programming\n\u00a0\u00a0\u00a0\u00a0 Section 12.1.\u00a0 Overview of Dynamic Programming\n\u00a0\u00a0\u00a0\u00a0 Section 12.2.\u00a0 Serial Monadic DP Formulations\n\u00a0\u00a0\u00a0\u00a0 Section 12.3.\u00a0 Nonserial Monadic DP Formulations\n\u00a0\u00a0\u00a0\u00a0 Section 12.4.\u00a0 Serial Polyadic DP Formulations\n\u00a0\u00a0\u00a0\u00a0 Section 12.5.\u00a0 Nonserial Polyadic DP Formulations\n\u00a0\u00a0\u00a0\u00a0 Section 12.6.\u00a0 Summary and Discussion\n\u00a0\u00a0\u00a0\u00a0 Section 12.7.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 13.\u00a0 Fast Fourier Transform\n\u00a0\u00a0\u00a0\u00a0 Section 13.1.\u00a0 The Serial Algorithm\n\u00a0\u00a0\u00a0\u00a0 Section 13.2.\u00a0 The Binary-Exchange Algorithm\n\u00a0\u00a0\u00a0\u00a0 Section 13.3.\u00a0 The Transpose Algorithm\n\u00a0\u00a0\u00a0\u00a0 Section 13.4.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Appendix A.\u00a0 Complexity of Functions and Order Analysis\n\u00a0\u00a0\u00a0\u00a0 Section A.1.\u00a0 Complexity of Functions\n\u00a0\u00a0\u00a0\u00a0 Section A.2.\u00a0 Order Analysis of Functions\n\u00a0\u00a0\u00a0 Bibliography\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nCopyright\nPearson Education Limited\nEdinburgh Gate\nHarlow\nEssex CM20 2JE\nEngland\nand Associated Companies throughout the world\nVisit us on the World Wide Web at:  www.pearsoneduc.comFirst published by The", "doc_id": "49f557cd-b836-4eba-8b4d-6e8b44ee7f5e", "embedding": null, "doc_hash": "ca18f9d166988d5f12422564cc24ae3421dedabf0e0dccee133ad04e3deedae0", "extra_info": null, "node_info": {"start": 6886, "end": 10376}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e1f5b31c-339e-4eb1-9f89-dafe5eec57ea", "3": "716ba413-4a07-465c-930f-bd06482d55aa"}}, "__type__": "1"}, "716ba413-4a07-465c-930f-bd06482d55aa": {"__data__": {"text": "Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Chapter 13.\u00a0 Fast Fourier Transform\n\u00a0\u00a0\u00a0\u00a0 Section 13.1.\u00a0 The Serial Algorithm\n\u00a0\u00a0\u00a0\u00a0 Section 13.2.\u00a0 The Binary-Exchange Algorithm\n\u00a0\u00a0\u00a0\u00a0 Section 13.3.\u00a0 The Transpose Algorithm\n\u00a0\u00a0\u00a0\u00a0 Section 13.4.\u00a0 Bibliographic Remarks\n\u00a0\u00a0\u00a0\u00a0 Problems\n\u00a0\u00a0\u00a0 Appendix A.\u00a0 Complexity of Functions and Order Analysis\n\u00a0\u00a0\u00a0\u00a0 Section A.1.\u00a0 Complexity of Functions\n\u00a0\u00a0\u00a0\u00a0 Section A.2.\u00a0 Order Analysis of Functions\n\u00a0\u00a0\u00a0 Bibliography\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nCopyright\nPearson Education Limited\nEdinburgh Gate\nHarlow\nEssex CM20 2JE\nEngland\nand Associated Companies throughout the world\nVisit us on the World Wide Web at:  www.pearsoneduc.comFirst published by The Benjamin/Cummings Publishing Company, Inc. 1994\nSecond edition published 2003\n\u00a9 The Benjamin/Cummings Publishing Company, Inc. 1994\n\u00a9 Pearson Education Limited 2003\nThe rights of Ananth Grama, Anshul Gupta, George Karypis and Vipin Kumar to be identified as\nauthors of this work have been asserted by them in accordance with the Copyright, Designs and\nPatents Act 1988.\nAll rights reserved. No part of this publication may be reproduced, stored in a retrieval system,\nor transmitted in any form or by any means, electronic, mechanical, photocopying, recording or\notherwise, without either the prior written permission of the publisher or a licence permitting\nrestricted copying in the United Kingdom issued by the Copyright Licensing Agency Ltd, 90\nTottenham Court Road, London W1T 4LP.\nThe programs in this book have been included for their instructional value. They have been\ntested with care but are not guaranteed for any particular purpose. The publisher does not offer\nany warranties or representations nor does it accept any liabilities with respect to the programs.\nAll trademarks used herein are the property of their respective owners. The use of any\ntrademark in this text does not vest in the author or publisher any trademark ownership rights\nin such trademarks, nor does the use of such trademarks imply any affiliation with or\nendorsement of this book by such owners.\nBritish Library Cataloguing-in-Publication Data\nA catalogue record for this book is available from the British Library\nLibrary of Congress Cataloging-in-Publication Data\nA catalog record for this book is available from the Library of Congress\n10 9 8 7 6 5 4 3 2 1\n07 06 05 04 03\nPrinted and bound in the United States of America\nDedication\nTo Joanna, Rinku, Krista, and Renu\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nPearson Education\nWe work with leading authors to develop the strongest educational materials in computing,\nbringing cutting-edge thinking and best learning practice to a global market.\nUnder a range of well-known imprints, including Addison-Wesley, we craft high-quality print\nand electronic publications which help readers to understand and apply their content, whether\nstudying or at work.\nTo find out more about the complete range of our publishing, please visit us on the World Wide\nWeb at: www.pearsoneduc.com\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nPreface\nSince the 1994 release of the text \"Introduction to Parallel Computing: Design and Analysis of\nAlgorithms\" by the same authors, the field of parallel computing has undergone significant\nchanges. Whereas tightly coupled scalable message-passing platforms were the norm a decade\nago, a significant portion of the current generation of platforms consists of inexpensive clusters\nof workstations, and multiprocessor workstations and servers. Programming models for these\nplatforms have also evolved over this time. Whereas most machines a decade back relied on\ncustom APIs for messaging and loop-based parallelism, current models standardize these APIs\nacross platforms. Message passing libraries such as PVM and MPI, thread libraries such as\nPOSIX threads, and directive based models such as OpenMP are widely accepted as standards,\nand have been ported to a variety of", "doc_id": "716ba413-4a07-465c-930f-bd06482d55aa", "embedding": null, "doc_hash": "4526c06d5d9d1d1b18ccb2a558e98529dcfb60e6e146d610b8d37380ea121130", "extra_info": null, "node_info": {"start": 10403, "end": 14289}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "49f557cd-b836-4eba-8b4d-6e8b44ee7f5e", "3": "9711e810-b9dc-4513-9073-b88ed22273f4"}}, "__type__": "1"}, "9711e810-b9dc-4513-9073-b88ed22273f4": {"__data__": {"text": "Computing: Design and Analysis of\nAlgorithms\" by the same authors, the field of parallel computing has undergone significant\nchanges. Whereas tightly coupled scalable message-passing platforms were the norm a decade\nago, a significant portion of the current generation of platforms consists of inexpensive clusters\nof workstations, and multiprocessor workstations and servers. Programming models for these\nplatforms have also evolved over this time. Whereas most machines a decade back relied on\ncustom APIs for messaging and loop-based parallelism, current models standardize these APIs\nacross platforms. Message passing libraries such as PVM and MPI, thread libraries such as\nPOSIX threads, and directive based models such as OpenMP are widely accepted as standards,\nand have been ported to a variety of platforms.\nWith respect to applications, fluid dynamics, structural mechanics, and signal processing formed\ndominant applications a decade back. These applications continue to challenge the current\ngeneration of parallel platforms. However, a variety of new applications have also become\nimportant. These include data-intensive applications such as transaction processing and\ninformation retrieval, data mining and analysis, and multimedia services. Applications in\nemerging areas of computational biology and nanotechnology pose tremendous challenges for\nalgorithms and systems development. Changes in architectures, programming models, and\napplications are also being accompanied by changes in how parallel platforms are made\navailable to the users in the form of grid-based services.\nThis evolution has a profound impact on the process of design, analysis, and implementation of\nparallel algorithms. Whereas the emphasis of parallel algorithm design a decade back was on\nprecise mapping of tasks to specific topologies such as meshes and hypercubes, current\nemphasis is on programmability and portability, both from points of view of algorithm design\nand implementation. To this effect, where possible, this book employs an architecture\nindependent view of the underlying platforms and designs algorithms for an abstract model.\nWith respect to programming models, Message Passing Interface (MPI), POSIX threads, and\nOpenMP have been selected. The evolving application mix for parallel computing is also\nreflected in various examples in the book.\nThis book forms the basis for a single concentrated course on parallel computing or a two-part\nsequence. Some suggestions for such a two-part sequence are:\nIntroduction to Parallel Computing: Chapters 1\u20136. This course would provide the basics of\nalgorithm design and parallel programming.1.\nDesign and Analysis of Parallel Algorithms: Chapters 2 and 3 followed by Chapters 8\u201312.\nThis course would provide an in-depth coverage of design and analysis of various parallel\nalgorithms.2.\nThe material in this book has been tested in Parallel Algorithms and Parallel Computing courses\nat the University of Minnesota and Purdue University. These courses are taken primarily by\ngraduate students and senior-level undergraduate students in Computer Science. In addition,\nrelated courses in Scientific Computation, for which this material has also been tested, are\ntaken by graduate students in science and engineering, who are interested in solving\ncomputationally intensive problems.\nMost chapters of the book include (i) examples and illustrations; (ii) problems that supplement\nthe text and test students' understanding of the material; and (iii) bibliographic remarks to aid\nresearchers and students interested in learning more about related and advanced topics. The\ncomprehensive subject index helps the reader locate terms they might be interested in. The\npage number on which a term is defined is highlighted in boldface in the index. Furthermore,\nthe term itself appears in bold italics where it is defined. The sections that deal with relatively\ncomplex material are preceded by a '*'. An instructors' manual containing slides of the figures\nand solutions to selected problems is also available from the publisher\n(http://www.booksites.net/kumar).\nAs with our previous book, we view this book as a continually evolving resource. We thank all\nthe readers who have kindly shared critiques,", "doc_id": "9711e810-b9dc-4513-9073-b88ed22273f4", "embedding": null, "doc_hash": "660f0ea7de86f8dcb19844453be95fcb62f6f72edfd5ebf31543889ec26f69df", "extra_info": null, "node_info": {"start": 14185, "end": 18427}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "716ba413-4a07-465c-930f-bd06482d55aa", "3": "65ad5212-ed11-40f2-af99-0b576dbdc2b8"}}, "__type__": "1"}, "65ad5212-ed11-40f2-af99-0b576dbdc2b8": {"__data__": {"text": "text and test students' understanding of the material; and (iii) bibliographic remarks to aid\nresearchers and students interested in learning more about related and advanced topics. The\ncomprehensive subject index helps the reader locate terms they might be interested in. The\npage number on which a term is defined is highlighted in boldface in the index. Furthermore,\nthe term itself appears in bold italics where it is defined. The sections that deal with relatively\ncomplex material are preceded by a '*'. An instructors' manual containing slides of the figures\nand solutions to selected problems is also available from the publisher\n(http://www.booksites.net/kumar).\nAs with our previous book, we view this book as a continually evolving resource. We thank all\nthe readers who have kindly shared critiques, opinions, problems, code, and other information\nrelating to our first book. It is our sincere hope that we can continue this interaction centered\naround this new book. We encourage readers to address communication relating to this book to\nbook-vk@cs.umn.edu. All relevant reader input will be added to the information archived at the\nsite http://www.cs.umn.edu/~parbook  with due credit to (and permission of) the sender(s). An\non-line errata of the book will also be maintained at the site. We believe that in a highly\ndynamic field such as ours, a lot is to be gained from a healthy exchange of ideas and material\nin this manner.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nAcknowledgments\nWe would like to begin by acknowledging our spouses, Joanna, Rinku, Krista, and Renu to\nwhom this book is dedicated. Without their sacrifices this project would not have been seen\ncompletion. We also thank our parents, and family members, Akash, Avi, Chethan, Eleni, Larry,\nMary-Jo, Naina, Petros, Samir, Subhasish, Varun, Vibhav, and Vipasha for their affectionate\nsupport and encouragement throughout this project.\nOur respective institutions, Computer Sciences and Computing Research Institute (CRI) at\nPurdue University, Department of Computer Science & Engineering, the Army High Performance\nComputing Research Center (AHPCRC), and the Digital Technology Center (DTC) at the\nUniversity of Minnesota, and the IBM T. J. Watson Research Center at Yorktown Heights,\nprovided computing resources and active and nurturing environments for the completion of this\nproject.\nThis project evolved from our first book. We would therefore like to acknowledge all of the\npeople who helped us with both editions. Many people contributed to this project in different\nways. We would like to thank Ahmed Sameh for his constant encouragement and support, and\nDan Challou, Michael Heath, Dinesh Mehta, Tom Nurkkala, Paul Saylor, and Shang-Hua Teng for\nthe valuable input they provided to the various versions of the book. We thank the students of\nthe introduction to parallel computing classes at the University of Minnesota and Purdue\nuniversity for identifying and working through the errors in the early drafts of the book. In\nparticular, we acknowledge the patience and help of Jim Diehl and Rasit Eskicioglu, who worked\nthrough several early drafts of the manuscript to identify numerous errors. Ramesh Agarwal,\nDavid Bailey, Rupak Biswas, Jim Bottum, Thomas Downar, Rudolf Eigenmann, Sonia Fahmy,\nGreg Frederickson, John Gunnels, Fred Gustavson, Susanne Hambrusch, Bruce Hendrickson,\nChristoph Hoffmann, Kai Hwang, Ioannis Ioannidis, Chandrika Kamath, David Keyes, Mehmet\nKoyuturk, Piyush Mehrotra, Zhiyuan Li, Jens Palsberg, Voicu Popescu, Alex Pothen, Viktor\nPrasanna, Sanjay Ranka, Naren Ramakrishnan, Elisha Sacks, Vineet Singh, Sartaj Sahni, Vivek\nSarin, Wojciech Szpankowski,", "doc_id": "65ad5212-ed11-40f2-af99-0b576dbdc2b8", "embedding": null, "doc_hash": "6884e0e9016c5bfe7495bd87eb1d9a3c689c8b0f069ff2d81762776bc4bc2095", "extra_info": null, "node_info": {"start": 18429, "end": 22094}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9711e810-b9dc-4513-9073-b88ed22273f4", "3": "acaa7a76-8b5f-436a-a6fc-2cfda15d17ea"}}, "__type__": "1"}, "acaa7a76-8b5f-436a-a6fc-2cfda15d17ea": {"__data__": {"text": "errors. Ramesh Agarwal,\nDavid Bailey, Rupak Biswas, Jim Bottum, Thomas Downar, Rudolf Eigenmann, Sonia Fahmy,\nGreg Frederickson, John Gunnels, Fred Gustavson, Susanne Hambrusch, Bruce Hendrickson,\nChristoph Hoffmann, Kai Hwang, Ioannis Ioannidis, Chandrika Kamath, David Keyes, Mehmet\nKoyuturk, Piyush Mehrotra, Zhiyuan Li, Jens Palsberg, Voicu Popescu, Alex Pothen, Viktor\nPrasanna, Sanjay Ranka, Naren Ramakrishnan, Elisha Sacks, Vineet Singh, Sartaj Sahni, Vivek\nSarin, Wojciech Szpankowski, Srikanth Thirumalai, Jan Vitek, and David Yau have been great\ntechnical resources. It was a pleasure working with the cooperative and helpful staff at Pearson\nEducation. In particular, we would like to thank Keith Mansfield and Mary Lince for their\nprofessional handling of the project.\nThe Army Research Laboratory, ARO, DOE, NASA, and NSF provided parallel computing\nresearch support for Ananth Grama, George Karypis, and Vipin Kumar. In particular, Kamal\nAbdali, Michael Coyle, Jagdish Chandra, Frederica Darema, Stephen Davis, Wm Randolph\nFranklin, Richard Hirsch, Charles Koelbel, Raju Namburu, N. Radhakrishnan, John Van\nRosendale, Subhash Saini, and Xiaodong Zhang have been supportive of our research programs\nin the area of parallel computing. Andrew Conn, Brenda Dietrich, John Forrest, David Jensen,\nand Bill Pulleyblank at IBM supported the work of Anshul Gupta over the years.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 1. Introduction to Parallel\nComputing\nThe past decade has seen tremendous advances in microprocessor technology. Clock rates of\nprocessors have increased from about 40 MHz (e.g., a MIPS R3000, circa 1988) to over 2.0 GHz\n(e.g., a Pentium 4, circa 2002). At the same time, processors are now capable of executing\nmultiple instructions in the same cycle. The average number of cycles per instruction (CPI) of\nhigh end processors has improved by roughly an order of magnitude over the past 10 years. All\nthis translates to an increase in the peak floating point operation execution rate (floating point\noperations per second, or FLOPS) of several orders of magnitude. A variety of other issues have\nalso become important over the same period. Perhaps the most prominent of these is the ability\n(or lack thereof) of the memory system to feed data to the processor at the required rate.\nSignificant innovations in architecture and software have addressed the alleviation of\nbottlenecks posed by the datapath and the memory.\nThe role of concurrency in accelerating computing elements has been recognized for several\ndecades. However, their role in providing multiplicity of datapaths, increased access to storage\nelements (both memory and disk), scalable performance, and lower costs is reflected in the\nwide variety of applications of parallel computing. Desktop machines, engineering workstations,\nand compute servers with two, four, or even eight processors connected together are becoming\ncommon platforms for design applications. Large scale applications in science and engineering\nrely on larger configurations of parallel computers, often comprising hundreds of processors.\nData intensive platforms such as database or web servers and applications such as transaction\nprocessing and data mining often use clusters of workstations that provide high aggregate disk\nbandwidth. Applications in graphics and visualization use multiple rendering pipes and\nprocessing elements to compute and render realistic environments with millions of polygons in\nreal time. Applications requiring high availability rely on parallel and distributed platforms for\nredundancy. It is therefore extremely important, from the point of view of cost, performance,\nand application requirements, to understand the", "doc_id": "acaa7a76-8b5f-436a-a6fc-2cfda15d17ea", "embedding": null, "doc_hash": "adb1aa5061ed0bb41819d238659005460ec70ad966569f89439558fe76db4af5", "extra_info": null, "node_info": {"start": 22353, "end": 26063}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "65ad5212-ed11-40f2-af99-0b576dbdc2b8", "3": "39e4e540-2e7c-43eb-b2b2-c5ae930d8130"}}, "__type__": "1"}, "39e4e540-2e7c-43eb-b2b2-c5ae930d8130": {"__data__": {"text": "engineering workstations,\nand compute servers with two, four, or even eight processors connected together are becoming\ncommon platforms for design applications. Large scale applications in science and engineering\nrely on larger configurations of parallel computers, often comprising hundreds of processors.\nData intensive platforms such as database or web servers and applications such as transaction\nprocessing and data mining often use clusters of workstations that provide high aggregate disk\nbandwidth. Applications in graphics and visualization use multiple rendering pipes and\nprocessing elements to compute and render realistic environments with millions of polygons in\nreal time. Applications requiring high availability rely on parallel and distributed platforms for\nredundancy. It is therefore extremely important, from the point of view of cost, performance,\nand application requirements, to understand the principles, tools, and techniques for\nprogramming the wide variety of parallel platforms currently available.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n1.1 Motivating Parallelism\nDevelopment of parallel software has traditionally been thought of as time and effort intensive.\nThis can be largely attributed to the inherent complexity of specifying and coordinating\nconcurrent tasks, a lack of portable algorithms, standardized environments, and software\ndevelopment toolkits. When viewed in the context of the brisk rate of development of\nmicroprocessors, one is tempted to question the need for devoting significant effort towards\nexploiting parallelism as a means of accelerating applications. After all, if it takes two years to\ndevelop a parallel application, during which time the underlying hardware and/or software\nplatform has become obsolete, the development effort is clearly wasted. However, there are\nsome unmistakable trends in hardware design, which indicate that uniprocessor (or implicitly\nparallel) architectures may not be able to sustain the rate of realizable  performance increments\nin the future. This is a result of lack of implicit parallelism as well as other bottlenecks such as\nthe datapath and the memory. At the same time, standardized hardware interfaces have\nreduced the turnaround time from the development of a microprocessor to a parallel machine\nbased on the microprocessor. Furthermore, considerable progress has been made in\nstandardization of programming environments to ensure a longer life-cycle for parallel\napplications. All of these present compelling arguments in favor of parallel computing platforms.\n1.1.1 The Computational Power Argument \u2013 from Transistors to FLOPS\nIn 1965, Gordon Moore made the following simple observation:\n\"The complexity for minimum component costs has increased at a rate of roughly a factor\nof two per year. Certainly over the short term this rate can be expected to continue, if not\nto increase. Over the longer term, the rate of increase is a bit more uncertain, although\nthere is no reason to believe it will not remain nearly constant for at least 10 years. That\nmeans by 1975, the number of components per integrated circuit for minimum cost will be\n65,000.\"\nHis reasoning was based on an empirical log-linear relationship between device complexity and\ntime, observed over three data points. He used this to justify that by 1975, devices with as\nmany as 65,000 components would become feasible on a single silicon chip occupying an area\nof only about one-fourth of a square inch. This projection turned out to be accurate with the\nfabrication of a 16K CCD memory with about 65,000 components in 1975. In a subsequent\npaper in 1975, Moore attributed the log-linear relationship to exponential behavior of die sizes,\nfiner minimum dimensions, and \"circuit and device cleverness\". He went on to state that:\n\"There is no room left to squeeze anything out by being clever. Going forward from here\nwe have to depend on the two size factors - bigger dies and finer dimensions.\"\nHe revised his rate of circuit complexity doubling to 18 months and projected from 1975\nonwards at this reduced rate. This curve came to be known as \"Moore's Law\". Formally, Moore's\nLaw states that circuit complexity doubles every eighteen months.", "doc_id": "39e4e540-2e7c-43eb-b2b2-c5ae930d8130", "embedding": null, "doc_hash": "15034f0b0fbd656d38e7b82231532d88d4ffa5f312d9397461039ad0c59e9943", "extra_info": null, "node_info": {"start": 25699, "end": 29902}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "acaa7a76-8b5f-436a-a6fc-2cfda15d17ea", "3": "a8fae5e3-8c57-4db3-9a5a-f51a523527f2"}}, "__type__": "1"}, "a8fae5e3-8c57-4db3-9a5a-f51a523527f2": {"__data__": {"text": "one-fourth of a square inch. This projection turned out to be accurate with the\nfabrication of a 16K CCD memory with about 65,000 components in 1975. In a subsequent\npaper in 1975, Moore attributed the log-linear relationship to exponential behavior of die sizes,\nfiner minimum dimensions, and \"circuit and device cleverness\". He went on to state that:\n\"There is no room left to squeeze anything out by being clever. Going forward from here\nwe have to depend on the two size factors - bigger dies and finer dimensions.\"\nHe revised his rate of circuit complexity doubling to 18 months and projected from 1975\nonwards at this reduced rate. This curve came to be known as \"Moore's Law\". Formally, Moore's\nLaw states that circuit complexity doubles every eighteen months. This empirical relationship\nhas been amazingly resilient over the years both for microprocessors as well as for DRAMs. By\nrelating component density and increases in die-size to the computing power of a device,\nMoore's law has been extrapolated to state that the amount of computing power available at a\ngiven cost doubles approximately every 18 months.\nThe limits of Moore's law have been the subject of extensive debate in the past few years.\nStaying clear of this debate, the issue of translating transistors into useful OPS (operations per\nsecond) is the critical one. It is possible to fabricate devices with very large transistor counts.\nHow we use these transistors to achieve increasing rates of computation is the key architectural\nchallenge. A logical recourse to this is to rely on parallelism \u2013 both implicit and explicit. We will\nbriefly discuss implicit parallelism in Section 2.1 and devote the rest of this book to exploiting\nexplicit parallelism.\n1.1.2 The Memory/Disk Speed Argument\nThe overall speed of computation is determined not just by the speed of the processor, but also\nby the ability of the memory system to feed data to it. While clock rates of high-end processors\nhave increased at roughly 40% per year over the past decade, DRAM access times have only\nimproved at the rate of roughly 10% per year over this interval. Coupled with increases in\ninstructions executed per clock cycle, this gap between processor speed and memory presents a\ntremendous performance bottleneck. This growing mismatch between processor speed and\nDRAM latency is typically bridged by a hierarchy of successively faster memory devices called\ncaches that rely on locality of data reference to deliver higher memory system performance. In\naddition to the latency, the net effective bandwidth between DRAM and the processor poses\nother problems for sustained computation rates.\nThe overall performance of the memory system is determined by the fraction of the total\nmemory requests that can be satisfied from the cache. Memory system performance is\naddressed in greater detail in Section 2.2. Parallel platforms typically yield better memory\nsystem performance because they provide (i) larger aggregate caches, and (ii) higher\naggregate bandwidth to the memory system (both typically linear in the number of processors).\nFurthermore, the principles that are at the heart of parallel algorithms, namely locality of data\nreference, also lend themselves to cache-friendly serial algorithms. This argument can be\nextended to disks where parallel platforms can be used to achieve high aggregate bandwidth to\nsecondary storage. Here, parallel algorithms yield insights into the development of out-of-core\ncomputations. Indeed, some of the fastest growing application areas of parallel computing in\ndata servers (database servers, web servers) rely not so much on their high aggregate\ncomputation rates but rather on the ability to pump data out at a faster rate.\n1.1.3 The Data Communication Argument\nAs the networking infrastructure evolves, the vision of using the Internet as one large\nheterogeneous parallel/distributed computing environment has begun to take shape. Many\napplications lend themselves naturally to such computing paradigms. Some of the most\nimpressive applications of massively parallel computing have been in the context of wide-area\ndistributed platforms. The", "doc_id": "a8fae5e3-8c57-4db3-9a5a-f51a523527f2", "embedding": null, "doc_hash": "c86d0205823884a058009cf58a3854b32c77ef73a02111caf0c4724ca3bfca55", "extra_info": null, "node_info": {"start": 30058, "end": 34203}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "39e4e540-2e7c-43eb-b2b2-c5ae930d8130", "3": "3e2a41d5-0551-4bb1-be07-32866cfcf759"}}, "__type__": "1"}, "3e2a41d5-0551-4bb1-be07-32866cfcf759": {"__data__": {"text": "to achieve high aggregate bandwidth to\nsecondary storage. Here, parallel algorithms yield insights into the development of out-of-core\ncomputations. Indeed, some of the fastest growing application areas of parallel computing in\ndata servers (database servers, web servers) rely not so much on their high aggregate\ncomputation rates but rather on the ability to pump data out at a faster rate.\n1.1.3 The Data Communication Argument\nAs the networking infrastructure evolves, the vision of using the Internet as one large\nheterogeneous parallel/distributed computing environment has begun to take shape. Many\napplications lend themselves naturally to such computing paradigms. Some of the most\nimpressive applications of massively parallel computing have been in the context of wide-area\ndistributed platforms. The SETI (Search for Extra Terrestrial Intelligence) project utilizes the\npower of a large number of home computers to analyze electromagnetic signals from outer\nspace. Other such efforts have attempted to factor extremely large integers and to solve large\ndiscrete optimization problems.\nIn many applications there are constraints on the location of data and/or resources across the\nInternet. An example of such an application is mining of large commercial datasets distributed\nover a relatively low bandwidth network. In such applications, even if the computing power is\navailable to accomplish the required task without resorting to parallel computing, it is infeasible\nto collect the data at a central location. In these cases, the motivation for parallelism comes not\njust from the need for computing resources but also from the infeasibility or undesirability of\nalternate (centralized) approaches.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n1.2 Scope of Parallel Computing\nParallel computing has made a tremendous impact on a variety of areas ranging from\ncomputational simulations for scientific and engineering applications to commercial applications\nin data mining and transaction processing. The cost benefits of parallelism coupled with the\nperformance requirements of applications present compelling arguments in favor of parallel\ncomputing. We present a small sample of the diverse applications of parallel computing.\n1.2.1 Applications in Engineering and Design\nParallel computing has traditionally been employed with great success in the design of airfoils\n(optimizing lift, drag, stability), internal combustion engines (optimizing charge distribution,\nburn), high-speed circuits (layouts for delays and capacitive and inductive effects), and\nstructures (optimizing structural integrity, design parameters, cost, etc.), among others. More\nrecently, design of microelectromechanical and nanoelectromechanical systems (MEMS and\nNEMS) has attracted significant attention. While most applications in engineering and design\npose problems of multiple spatial and temporal scales and coupled physical phenomena, in the\ncase of MEMS/NEMS design these problems are particularly acute. Here, we often deal with a\nmix of quantum phenomena, molecular dynamics, and stochastic and continuum models with\nphysical processes such as conduction, convection, radiation, and structural mechanics, all in a\nsingle system. This presents formidable challenges for geometric modeling, mathematical\nmodeling, and algorithm development, all in the context of parallel computers.\nOther applications in engineering and design focus on optimization of a variety of processes.\nParallel computers have been used to solve a variety of discrete and continuous optimization\nproblems. Algorithms such as Simplex, Interior Point Method for linear optimization and\nBranch-and-bound, and Genetic programming for discrete optimization have been efficiently\nparallelized and are frequently used.\n1.2.2 Scientific Applications\nThe past few years have seen a revolution in high performance scientific computing\napplications. The sequencing of the human genome by the International Human Genome\nSequencing Consortium and Celera, Inc. has opened exciting new frontiers in bioinformatics.\nFunctional and structural characterization of genes and proteins hold the promise of\nunderstanding and fundamentally influencing biological", "doc_id": "3e2a41d5-0551-4bb1-be07-32866cfcf759", "embedding": null, "doc_hash": "fa96f7723b6d181121de28a6b2a4ca3da74c647c63840f46b73e110883fad152", "extra_info": null, "node_info": {"start": 34147, "end": 38345}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a8fae5e3-8c57-4db3-9a5a-f51a523527f2", "3": "8c728b39-1f32-4e93-92ed-c267e62a9580"}}, "__type__": "1"}, "8c728b39-1f32-4e93-92ed-c267e62a9580": {"__data__": {"text": "computers.\nOther applications in engineering and design focus on optimization of a variety of processes.\nParallel computers have been used to solve a variety of discrete and continuous optimization\nproblems. Algorithms such as Simplex, Interior Point Method for linear optimization and\nBranch-and-bound, and Genetic programming for discrete optimization have been efficiently\nparallelized and are frequently used.\n1.2.2 Scientific Applications\nThe past few years have seen a revolution in high performance scientific computing\napplications. The sequencing of the human genome by the International Human Genome\nSequencing Consortium and Celera, Inc. has opened exciting new frontiers in bioinformatics.\nFunctional and structural characterization of genes and proteins hold the promise of\nunderstanding and fundamentally influencing biological processes. Analyzing biological\nsequences with a view to developing new drugs and cures for diseases and medical conditions\nrequires innovative algorithms as well as large-scale computational power. Indeed, some of the\nnewest parallel computing technologies are targeted specifically towards applications in\nbioinformatics.\nAdvances in computational physics and chemistry have focused on understanding processes\nranging in scale from quantum phenomena to macromolecular structures. These have resulted\nin design of new materials, understanding of chemical pathways, and more efficient processes.\nApplications in astrophysics have explored the evolution of galaxies, thermonuclear processes,\nand the analysis of extremely large datasets from telescopes. Weather modeling, mineral\nprospecting, flood prediction, etc., rely heavily on parallel computers and have very significant\nimpact on day-to-day life.\nBioinformatics and astrophysics also present some of the most challenging problems with\nrespect to analyzing extremely large datasets. Protein and gene databases (such as PDB,\nSwissProt, and ENTREZ and NDB) along with Sky Survey datasets (such as the Sloan Digital\nSky Surveys) represent some of the largest scientific datasets. Effectively analyzing these\ndatasets requires tremendous computational power and holds the key to significant scientific\ndiscoveries.\n1.2.3 Commercial Applications\nWith the widespread use of the web and associated static and dynamic content, there is\nincreasing emphasis on cost-effective servers capable of providing scalable performance.\nParallel platforms ranging from multiprocessors to linux clusters are frequently used as web and\ndatabase servers. For instance, on heavy volume days, large brokerage houses on Wall Street\nhandle hundreds of thousands of simultaneous user sessions and millions of orders. Platforms\nsuch as IBMs SP supercomputers and Sun Ultra HPC servers power these business-critical sites.\nWhile not highly visible, some of the largest supercomputing networks are housed on Wall\nStreet.\nThe availability of large-scale transaction data has also sparked considerable interest in data\nmining and analysis for optimizing business and marketing decisions. The sheer volume and\ngeographically distributed nature of this data require the use of effective parallel algorithms for\nsuch problems as association rule mining, clustering, classification, and time-series analysis.\n1.2.4 Applications in Computer Systems\nAs computer systems become more pervasive and computation spreads over the network,\nparallel processing issues become engrained into a variety of applications. In computer security,\nintrusion detection is an outstanding challenge. In the case of network intrusion detection, data\nis collected at distributed sites and must be analyzed rapidly for signaling intrusion. The\ninfeasibility of collecting this data at a central location for analysis requires effective parallel and\ndistributed algorithms. In the area of cryptography, some of the most spectacular applications\nof Internet-based parallel computing have focused on factoring extremely large integers.\nEmbedded systems increasingly rely on distributed control algorithms for accomplishing a\nvariety of tasks. A modern automobile consists of tens of processors communicating to perform\ncomplex tasks for optimizing handling and performance. In such systems, traditional parallel\nand distributed algorithms for leader selection,", "doc_id": "8c728b39-1f32-4e93-92ed-c267e62a9580", "embedding": null, "doc_hash": "a6387833f80c12ac315e7da6266a607ccd6e9ae369acdab187d982a96ddc1222", "extra_info": null, "node_info": {"start": 38311, "end": 42605}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3e2a41d5-0551-4bb1-be07-32866cfcf759", "3": "276de109-608d-497f-a10f-d9958ab15824"}}, "__type__": "1"}, "276de109-608d-497f-a10f-d9958ab15824": {"__data__": {"text": "a variety of applications. In computer security,\nintrusion detection is an outstanding challenge. In the case of network intrusion detection, data\nis collected at distributed sites and must be analyzed rapidly for signaling intrusion. The\ninfeasibility of collecting this data at a central location for analysis requires effective parallel and\ndistributed algorithms. In the area of cryptography, some of the most spectacular applications\nof Internet-based parallel computing have focused on factoring extremely large integers.\nEmbedded systems increasingly rely on distributed control algorithms for accomplishing a\nvariety of tasks. A modern automobile consists of tens of processors communicating to perform\ncomplex tasks for optimizing handling and performance. In such systems, traditional parallel\nand distributed algorithms for leader selection, maximal independent set, etc., are frequently\nused.\nWhile parallel computing has traditionally confined itself to platforms with well behaved\ncompute and network elements in which faults and errors do not play a significant role, there\nare valuable lessons that extend to computations on ad-hoc, mobile, or faulty environments.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n1.3 Organization and Contents of the Text\nThis book provides a comprehensive and self-contained exposition of problem solving using\nparallel computers. Algorithms and metrics focus on practical and portable models of parallel\nmachines. Principles of algorithm design focus on desirable attributes of parallel algorithms and\ntechniques for achieving these in the contest of a large class of applications and architectures.\nProgramming techniques cover standard paradigms such as MPI and POSIX threads that are\navailable across a range of parallel platforms.\nChapters in this book can be grouped into four main parts as illustrated in Figure 1.1. These\nparts are as follows:\nFigure 1.1. Recommended sequence for reading the chapters.\nFundamentals  This section spans Chapters 2 through 4 of the book. Chapter 2 , Parallel\nProgramming Platforms, discusses the physical organization of parallel platforms. It establishes\ncost metrics that can be used for algorithm design. The objective of this chapter is not to\nprovide an exhaustive treatment of parallel architectures; rather, it aims to provide sufficient\ndetail required to use these machines efficiently. Chapter 3, Principles of Parallel Algorithm\nDesign , addresses key factors that contribute to efficient parallel algorithms and presents a\nsuite of techniques that can be applied across a wide range of applications. Chapter 4 , Basic\nCommunication Operations, presents a core set of operations that are used throughout the book\nfor facilitating efficient data transfer in parallel algorithms. Finally, Chapter 5, Analytical\nModeling of Parallel Programs, deals with metrics for quantifying the performance of a parallel\nalgorithm.\nParallel Programming  This section includes Chapters 6 and 7 of the book. Chapter 6 ,\nProgramming Using the Message-Passing Paradigm, focuses on the Message Passing Interface\n(MPI) for programming message passing platforms, including clusters. Chapter 7 , Programming\nShared Address Space Platforms, deals with programming paradigms such as threads and\ndirective based approaches. Using paradigms such as POSIX threads and OpenMP, it describes\nvarious features necessary for programming shared-address-space parallel machines. Both of\nthese chapters illustrate various programming concepts using a variety of examples of parallel\nprograms.\nNon-numerical Algorithms  Chapters 9\u201312 present parallel non-numerical algorithms. Chapter\n9 addresses sorting algorithms such as bitonic sort, bubble sort and its variants, quicksort,\nsample sort, and shellsort. Chapter 10  describes algorithms for various graph theory problems\nsuch as minimum spanning tree, shortest paths, and connected components. Algorithms for\nsparse graphs are also discussed. Chapter 11  addresses search-based methods such as branch-\nand-bound and heuristic search for combinatorial problems. Chapter 12  classifies and presents\nparallel formulations", "doc_id": "276de109-608d-497f-a10f-d9958ab15824", "embedding": null, "doc_hash": "df2311040adfee8cc93e356b0f14ddea8718d011de52b943015a927a6900a545", "extra_info": null, "node_info": {"start": 42601, "end": 46719}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8c728b39-1f32-4e93-92ed-c267e62a9580", "3": "b3015948-f2a2-47ca-b4ba-f915965c323d"}}, "__type__": "1"}, "b3015948-f2a2-47ca-b4ba-f915965c323d": {"__data__": {"text": "features necessary for programming shared-address-space parallel machines. Both of\nthese chapters illustrate various programming concepts using a variety of examples of parallel\nprograms.\nNon-numerical Algorithms  Chapters 9\u201312 present parallel non-numerical algorithms. Chapter\n9 addresses sorting algorithms such as bitonic sort, bubble sort and its variants, quicksort,\nsample sort, and shellsort. Chapter 10  describes algorithms for various graph theory problems\nsuch as minimum spanning tree, shortest paths, and connected components. Algorithms for\nsparse graphs are also discussed. Chapter 11  addresses search-based methods such as branch-\nand-bound and heuristic search for combinatorial problems. Chapter 12  classifies and presents\nparallel formulations for a variety of dynamic programming algorithms.\nNumerical Algorithms  Chapters 8 and 13 present parallel numerical algorithms. Chapter 8\ncovers basic operations on dense matrices such as matrix multiplication, matrix-vector\nmultiplication, and Gaussian elimination. This chapter is included before non-numerical\nalgorithms, as the techniques for partitioning and assigning matrices to processors are common\nto many non-numerical algorithms. Furthermore, matrix-vector and matrix-matrix\nmultiplication algorithms form the kernels of many graph algorithms. Chapter 13 describes\nalgorithms for computing Fast Fourier Transforms.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n1.4 Bibliographic Remarks\nMany books discuss aspects of parallel processing at varying levels of detail. Hardware aspects\nof parallel computers have been discussed extensively in several textbooks and monographs\n[CSG98, LW95 , HX98 , AG94 , Fly95 , AG94 , Sto93 , DeC89 , HB84 , RF89, Sie85 , Tab90 , Tab91 ,\nWF84 , Woo86 ]. A number of texts discuss paradigms and languages for programming parallel\ncomputers [ LB98, Pac98 , GLS99 , GSNL98 , CDK+00, WA98 , And91 , BA82 , Bab88 , Ble90 , Con89 ,\nCT92, Les93 , Per87 , Wal91 ]. Akl [ Akl97 ], Cole [ Col89 ], Gibbons and Rytter [ GR90 ], Foster\n[Fos95 ], Leighton [ Lei92 ], Miller and Stout [ MS96 ], and Quinn [ Qui94 ] discuss various aspects\nof parallel algorithm design and analysis. Buyya (Editor) [ Buy99 ] and Pfister [ Pfi98] discuss\nvarious aspects of parallel computing using clusters. Jaja [ Jaj92 ] covers parallel algorithms for\nthe PRAM model of computation. Hillis [ Hil85, HS86 ] and Hatcher and Quinn [ HQ91 ] discuss\ndata-parallel programming. Agha [ Agh86 ] discusses a model of concurrent computation based\non actors . Sharp [ Sha85 ] addresses data-flow computing. Some books provide a general\noverview of topics in parallel computing [ CL93, Fou94 , Zom96 , JGD87 , LER92 , Mol93 , Qui94 ].\nMany books address parallel processing applications in numerical analysis and scientific\ncomputing [ DDSV99, FJDS96 , GO93 , Car89 ]. Fox et al. [FJL+88] and Angus et al. [AFKW90 ]\nprovide an application-oriented view of algorithm design for problems in scientific computing.\nBertsekas and Tsitsiklis [ BT97] discuss parallel algorithms, with emphasis on numerical\napplications.\nAkl and Lyons [ AL93] discuss parallel algorithms in computational geometry. Ranka and Sahni\n[RS90b ] and Dew, Earnshaw, and Heywood [ DEH89 ] address parallel algorithms for use in\ncomputer vision. Green [ Gre91 ] covers parallel algorithms for graphics applications. Many\nbooks address the use of parallel processing in artificial intelligence applications [ Gup87 ,\nHD89b ,", "doc_id": "b3015948-f2a2-47ca-b4ba-f915965c323d", "embedding": null, "doc_hash": "67b1b32a63d6ba5fcf1e80c09e231ca92c625bf0eecede53eb45e1f6118412e0", "extra_info": null, "node_info": {"start": 46792, "end": 50237}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "276de109-608d-497f-a10f-d9958ab15824", "3": "e81df23c-18ec-4c19-83b4-aa0054488511"}}, "__type__": "1"}, "e81df23c-18ec-4c19-83b4-aa0054488511": {"__data__": {"text": "FJDS96 , GO93 , Car89 ]. Fox et al. [FJL+88] and Angus et al. [AFKW90 ]\nprovide an application-oriented view of algorithm design for problems in scientific computing.\nBertsekas and Tsitsiklis [ BT97] discuss parallel algorithms, with emphasis on numerical\napplications.\nAkl and Lyons [ AL93] discuss parallel algorithms in computational geometry. Ranka and Sahni\n[RS90b ] and Dew, Earnshaw, and Heywood [ DEH89 ] address parallel algorithms for use in\ncomputer vision. Green [ Gre91 ] covers parallel algorithms for graphics applications. Many\nbooks address the use of parallel processing in artificial intelligence applications [ Gup87 ,\nHD89b , KGK90 , KKKS94 , Kow88 , RZ89 ].\nA useful collection of reviews, bibliographies and indexes has been put together by the\nAssociation for Computing Machinery [ ACM91 ]. Messina and Murli [ MM91 ] present a collection\nof papers on various aspects of the application and potential of parallel computing. The scope of\nparallel processing and various aspects of US government support have also been discussed in\nNational Science Foundation reports [ NSF91, GOV99 ].\nA number of conferences address various aspects of parallel computing. A few important ones\nare the Supercomputing Conference, ACM Symposium on Parallel Algorithms and Architectures,\nthe International Conference on Parallel Processing, the International Parallel and Distributed\nProcessing Symposium, Parallel Computing, and the SIAM Conference on Parallel Processing.\nImportant journals in parallel processing include IEEE Transactions on Parallel and Distributed\nSystems, International Journal of Parallel Programming, Journal of Parallel and Distributed\nComputing, Parallel Computing, IEEE Concurrency, and Parallel Processing Letters. These\nproceedings and journals provide a rich source of information on the state of the art in parallel\nprocessing.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n1.1 Go to the Top 500 Supercomputers site ( http://www.top500.org/ ) and list the five\nmost powerful supercomputers along with their FLOPS rating.\n1.2 List three major problems requiring the use of supercomputing in the following\ndomains:\nStructural Mechanics.1.\nComputational Biology.2.\nCommercial Applications.3.\n1.3 Collect statistics on the number of components in state of the art integrated circuits\nover the years. Plot the number of components as a function of time and compare the\ngrowth rate to that dictated by Moore's law.\n1.4 Repeat the above experiment for the peak FLOPS rate of processors and compare the\nspeed to that inferred from Moore's law.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 2. Parallel Programming\nPlatforms\nThe traditional logical view of a sequential computer consists of a memory connected to a\nprocessor via a datapath. All three components \u2013 processor, memory, and datapath \u2013 present\nbottlenecks to the overall processing rate of a computer system. A number of architectural\ninnovations over the years have addressed these bottlenecks. One of the most important\ninnovations is multiplicity \u2013 in processing units, datapaths, and memory units. This multiplicity\nis either entirely hidden from the programmer, as in the case of implicit parallelism, or exposed\nto the programmer in different forms. In this chapter, we present an overview of important\narchitectural concepts as they relate to parallel processing. The objective is to provide sufficient\ndetail for programmers to be able to write efficient code on a variety of platforms. We develop\ncost models and abstractions for quantifying the performance of various parallel algorithms,\nand identify bottlenecks resulting from various programming constructs.\nWe start our discussion of parallel platforms with an overview of serial and implicitly parallel\narchitectures. This is necessitated by the fact that it is often possible to re-engineer codes to\nachieve significant speedups (2 x to 5 x unoptimized speed) using simple program\ntransformations.", "doc_id": "e81df23c-18ec-4c19-83b4-aa0054488511", "embedding": null, "doc_hash": "edb1045ed49951e33f069dd2f2f6864ea222d72d4c6ff689985af211b01cd015", "extra_info": null, "node_info": {"start": 50356, "end": 54297}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b3015948-f2a2-47ca-b4ba-f915965c323d", "3": "4ad116fd-94cb-4739-a52b-09ddde0c7ec0"}}, "__type__": "1"}, "4ad116fd-94cb-4739-a52b-09ddde0c7ec0": {"__data__": {"text": "either entirely hidden from the programmer, as in the case of implicit parallelism, or exposed\nto the programmer in different forms. In this chapter, we present an overview of important\narchitectural concepts as they relate to parallel processing. The objective is to provide sufficient\ndetail for programmers to be able to write efficient code on a variety of platforms. We develop\ncost models and abstractions for quantifying the performance of various parallel algorithms,\nand identify bottlenecks resulting from various programming constructs.\nWe start our discussion of parallel platforms with an overview of serial and implicitly parallel\narchitectures. This is necessitated by the fact that it is often possible to re-engineer codes to\nachieve significant speedups (2 x to 5 x unoptimized speed) using simple program\ntransformations. Parallelizing sub-optimal serial codes often has undesirable effects of\nunreliable speedups and misleading runtimes. For this reason, we advocate optimizing serial\nperformance of codes before attempting parallelization. As we shall demonstrate through this\nchapter, the tasks of serial and parallel optimization often have very similar characteristics.\nAfter discussing serial and implicitly parallel architectures, we devote the rest of this chapter to\norganization of parallel platforms, underlying cost models for algorithms, and platform\nabstractions for portable algorithm design. Readers wishing to delve directly into parallel\narchitectures may choose to skip Sections 2.1 and 2.2.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.1 Implicit Parallelism: Trends in Microprocessor\nArchitectures*\nWhile microprocessor technology has delivered significant improvements in clock speeds over\nthe past decade, it has also exposed a variety of other performance bottlenecks. To alleviate\nthese bottlenecks, microprocessor designers have explored alternate routes to cost-effective\nperformance gains. In this section, we will outline some of these trends with a view to\nunderstanding their limitations and how they impact algorithm and code development. The\nobjective here is not to provide a comprehensive description of processor architectures. There\nare several excellent texts referenced in the bibliography that address this topic.\nClock speeds of microprocessors have posted impressive gains - two to three orders of\nmagnitude over the past 20 years. However, these increments in clock speed are severely\ndiluted by the limitations of memory technology. At the same time, higher levels of device\nintegration have also resulted in a very large transistor count, raising the obvious issue of how\nbest to utilize them. Consequently, techniques that enable execution of multiple instructions in a\nsingle clock cycle have become popular. Indeed, this trend is evident in the current generation\nof microprocessors such as the Itanium, Sparc Ultra, MIPS, and Power4. In this section, we\nbriefly explore mechanisms used by various processors for supporting multiple instruction\nexecution.\n2.1.1 Pipelining and Superscalar Execution\nProcessors have long relied on pipelines for improving execution rates. By overlapping various\nstages in instruction execution (fetch, schedule, decode, operand fetch, execute, store, among\nothers), pipelining enables faster execution. The assembly-line analogy works well for\nunderstanding pipelines. If the assembly of a car, taking 100 time units, can be broken into 10\npipelined stages of 10 units each, a single assembly line can produce a car every 10 time units!\nThis represents a 10-fold speedup over producing cars entirely serially, one after the other. It is\nalso evident from this example that to increase the speed of a single pipeline, one would break\ndown the tasks into smaller and smaller units, thus lengthening the pipeline and increasing\noverlap in execution. In the context of processors, this enables faster clock rates since the tasks\nare now smaller. For example, the Pentium 4, which operates at 2.0 GHz, has a 20 stage\npipeline. Note that the speed of a single pipeline is ultimately limited by the largest atomic task\nin the", "doc_id": "4ad116fd-94cb-4739-a52b-09ddde0c7ec0", "embedding": null, "doc_hash": "ca9638e96914c02420d583c899de482cbc7e1b95f68872a52b45d4a5750dcefd", "extra_info": null, "node_info": {"start": 54129, "end": 58236}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e81df23c-18ec-4c19-83b4-aa0054488511", "3": "567ebb76-e3ca-4574-b4c6-b185fb5fdcd9"}}, "__type__": "1"}, "567ebb76-e3ca-4574-b4c6-b185fb5fdcd9": {"__data__": {"text": "pipelines. If the assembly of a car, taking 100 time units, can be broken into 10\npipelined stages of 10 units each, a single assembly line can produce a car every 10 time units!\nThis represents a 10-fold speedup over producing cars entirely serially, one after the other. It is\nalso evident from this example that to increase the speed of a single pipeline, one would break\ndown the tasks into smaller and smaller units, thus lengthening the pipeline and increasing\noverlap in execution. In the context of processors, this enables faster clock rates since the tasks\nare now smaller. For example, the Pentium 4, which operates at 2.0 GHz, has a 20 stage\npipeline. Note that the speed of a single pipeline is ultimately limited by the largest atomic task\nin the pipeline. Furthermore, in typical instruction traces, every fifth to sixth instruction is a\nbranch instruction. Long instruction pipelines therefore need effective techniques for predicting\nbranch destinations so that pipelines can be speculatively filled. The penalty of a misprediction\nincreases as the pipelines become deeper since a larger number of instructions need to be\nflushed. These factors place limitations on the depth of a processor pipeline and the resulting\nperformance gains.\nAn obvious way to improve instruction execution rate beyond this level is to use multiple\npipelines. During each clock cycle, multiple instructions are piped into the processor in parallel.\nThese instructions are executed on multiple functional units. We illustrate this process with the\nhelp of an example.\nExample 2.1 Superscalar execution\nConsider a processor with two pipelines and the ability to simultaneously issue two\ninstructions. These processors are sometimes also referred to as super-pipelined\nprocessors. The ability of a processor to issue multiple instructions in the same cycle is\nreferred to as superscalar execution. Since the architecture illustrated in Figure 2.1allows two issues per clock cycle, it is also referred to as two-way superscalar or dual\nissue execution.\nFigure 2.1. Example of a two-way superscalar execution of\ninstructions.\nConsider the execution of the first code fragment in Figure 2.1  for adding four\nnumbers. The first and second instructions are independent and therefore can be\nissued concurrently. This is illustrated in the simultaneous issue of the instructions\nload R1, @1000  and load R2, @1008  at t = 0. The instructions are fetched, decoded,\nand the operands are fetched. The next two instructions, add R1, @1004  and add R2,\n@100C  are also mutually independent, although they must be executed after the first\ntwo instructions. Consequently, they can be issued concurrently at t = 1 since the\nprocessors are pipelined. These instructions terminate at t = 5. The next two\ninstructions, add R1, R2 and store R1, @2000 cannot be executed concurrently\nsince the result of the former (contents of register R1) is used by the latter. Therefore,\nonly the add instruction is issued at t = 2 and the store  instruction at t = 3. Note that\nthe instruction add R1, R2 can be executed only after the previous two instructions\nhave been executed. The instruction schedule is illustrated in Figure 2.1(b) . The\nschedule assumes that each memory access takes a single cycle. In reality, this may\nnot be the case. The implications of this assumption are discussed in Section 2.2  on\nmemory system performance. \nIn principle, superscalar execution seems natural, even simple. However, a number of issues\nneed to be resolved. First, as illustrated in Example 2.1, instructions in a program may be\nrelated to each other. The results of an instruction may be required for subsequent instructions.\nThis is referred to as true data dependency . For instance, consider the second code fragment\nin Figure 2.1 for adding four numbers. There is a true data dependency between load R1,\n@1000  and add R1,", "doc_id": "567ebb76-e3ca-4574-b4c6-b185fb5fdcd9", "embedding": null, "doc_hash": "3e954d7d20ab2a339bb4bda8d79c84bd950e9bab518a90ca3d93fc5b6892dfba", "extra_info": null, "node_info": {"start": 58325, "end": 62210}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4ad116fd-94cb-4739-a52b-09ddde0c7ec0", "3": "116e8b02-7922-4d09-bad6-58f68db3ac02"}}, "__type__": "1"}, "116e8b02-7922-4d09-bad6-58f68db3ac02": {"__data__": {"text": "been executed. The instruction schedule is illustrated in Figure 2.1(b) . The\nschedule assumes that each memory access takes a single cycle. In reality, this may\nnot be the case. The implications of this assumption are discussed in Section 2.2  on\nmemory system performance. \nIn principle, superscalar execution seems natural, even simple. However, a number of issues\nneed to be resolved. First, as illustrated in Example 2.1, instructions in a program may be\nrelated to each other. The results of an instruction may be required for subsequent instructions.\nThis is referred to as true data dependency . For instance, consider the second code fragment\nin Figure 2.1 for adding four numbers. There is a true data dependency between load R1,\n@1000  and add R1, @1004 , and similarly between subsequent instructions. Dependencies of this\ntype must be resolved before simultaneous issue of instructions. This has two implications.\nFirst, since the resolution is done at runtime, it must be supported in hardware. The complexity\nof this hardware can be high. Second, the amount of instruction level parallelism in a program\nis often limited and is a function of coding technique. In the second code fragment, there can be\nno simultaneous issue, leading to poor resource utilization. The three code fragments in Figure2.1(a)  also illustrate that in many cases it is possible to extract more parallelism by reordering\nthe instructions and by altering the code. Notice that in this example the code reorganization\ncorresponds to exposing parallelism in a form that can be used by the instruction issue\nmechanism.\nAnother source of dependency between instructions results from the finite resources shared by\nvarious pipelines. As an example, consider the co-scheduling of two floating point operations on\na dual issue machine with a single floating point unit. Although there might be no data\ndependencies between the instructions, they cannot be scheduled together since both need the\nfloating point unit. This form of dependency in which two instructions compete for a single\nprocessor resource is referred to as resource dependency .\nThe flow of control through a program enforces a third form of dependency between\ninstructions. Consider the execution of a conditional branch instruction. Since the branch\ndestination is known only at the point of execution, scheduling instructions a priori  across\nbranches may lead to errors. These dependencies are referred to as branch dependencies  or\nprocedural dependencies  and are typically handled by speculatively scheduling across\nbranches and rolling back in case of errors. Studies of typical traces have shown that on\naverage, a branch instruction is encountered between every five to six instructions. Therefore,\njust as in populating instruction pipelines, accurate branch prediction is critical for efficient\nsuperscalar execution.\nThe ability of a processor to detect and schedule concurrent instructions is critical to superscalar\nperformance. For instance, consider the third code fragment in Figure 2.1 which also computes\nthe sum of four numbers. The reader will note that this is merely a semantically equivalent\nreordering of the first code fragment. However, in this case, there is a data dependency\nbetween the first two instructions \u2013 load R1, @1000  and add R1, @1004 . Therefore, these\ninstructions cannot be issued simultaneously. However, if the processor had the ability to look\nahead, it would realize that it is possible to schedule the third instruction \u2013 load R2, @1008  \u2013\nwith the first instruction. In the next issue cycle, instructions two and four can be scheduled,\nand so on. In this way, the same execution schedule can be derived for the first and third code\nfragments. However, the processor needs the ability to issue instructions out-of-order  to\naccomplish desired reordering. The parallelism available in in-order  issue of instructions can be\nhighly limited as illustrated by this example. Most current microprocessors are capable of out-\nof-order issue and completion. This model, also referred to as dynamic instruction issue", "doc_id": "116e8b02-7922-4d09-bad6-58f68db3ac02", "embedding": null, "doc_hash": "c03a0ed2a7b9b9646a400fa5cdc222affae6119e462f763d6dfc9494c169d7b6", "extra_info": null, "node_info": {"start": 62206, "end": 66313}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "567ebb76-e3ca-4574-b4c6-b185fb5fdcd9", "3": "d493e40e-d3e1-403d-b569-c423bb4380f2"}}, "__type__": "1"}, "d493e40e-d3e1-403d-b569-c423bb4380f2": {"__data__": {"text": ". Therefore, these\ninstructions cannot be issued simultaneously. However, if the processor had the ability to look\nahead, it would realize that it is possible to schedule the third instruction \u2013 load R2, @1008  \u2013\nwith the first instruction. In the next issue cycle, instructions two and four can be scheduled,\nand so on. In this way, the same execution schedule can be derived for the first and third code\nfragments. However, the processor needs the ability to issue instructions out-of-order  to\naccomplish desired reordering. The parallelism available in in-order  issue of instructions can be\nhighly limited as illustrated by this example. Most current microprocessors are capable of out-\nof-order issue and completion. This model, also referred to as dynamic instruction issue ,\nexploits maximum instruction level parallelism. The processor uses a window of instructions\nfrom which it selects instructions for simultaneous issue. This window corresponds to the look-\nahead of the scheduler.\nThe performance of superscalar architectures is limited by the available instruction level\nparallelism. Consider the example in Figure 2.1. For simplicity of discussion, let us ignore the\npipelining aspects of the example and focus on the execution aspects of the program. Assuming\ntwo execution units (multiply-add units), the figure illustrates that there are several zero-issue\ncycles (cycles in which the floating point unit is idle). These are essentially wasted cycles from\nthe point of view of the execution unit. If, during a particular cycle, no instructions are issued on\nthe execution units, it is referred to as vertical waste ; if only part of the execution units are\nused during a cycle, it is termed horizontal waste . In the example, we have two cycles of\nvertical waste and one cycle with horizontal waste. In all, only three of the eight available cycles\nare used for computation. This implies that the code fragment will yield no more than three-\neighths of the peak rated FLOP count of the processor. Often, due to limited parallelism,\nresource dependencies, or the inability of a processor to extract parallelism, the resources of\nsuperscalar processors are heavily under-utilized. Current microprocessors typically support up\nto four-issue superscalar execution.\n2.1.2 Very Long Instruction Word Processors\nThe parallelism extracted by superscalar processors is often limited by the instruction look-\nahead. The hardware logic for dynamic dependency analysis is typically in the range of 5-10%\nof the total logic on conventional microprocessors (about 5% on the four-way superscalar Sun\nUltraSPARC). This complexity grows roughly quadratically with the number of issues and can\nbecome a bottleneck. An alternate concept for exploiting instruction-level parallelism used in\nvery long instruction word (VLIW) processors relies on the compiler to resolve dependencies\nand resource availability at compile time. Instructions that can be executed concurrently are\npacked into groups and parceled off to the processor as a single long instruction word (thus the\nname) to be executed on multiple functional units at the same time.\nThe VLIW concept, first used in Multiflow Trace (circa 1984) and subsequently as a variant in\nthe Intel IA64 architecture, has both advantages and disadvantages compared to superscalar\nprocessors. Since scheduling is done in software, the decoding and instruction issue\nmechanisms are simpler in VLIW processors. The compiler has a larger context from which to\nselect instructions and can use a variety of transformations to optimize parallelism when\ncompared to a hardware issue unit. Additional parallel instructions are typically made available\nto the compiler to control parallel execution. However, compilers do not have the dynamic\nprogram state (e.g., the branch history buffer) available to make scheduling decisions. This\nreduces the accuracy of branch and memory prediction, but allows the use of more\nsophisticated static prediction schemes. Other runtime situations such as stalls on data fetch\nbecause of cache misses are extremely", "doc_id": "d493e40e-d3e1-403d-b569-c423bb4380f2", "embedding": null, "doc_hash": "fe316dcbf362c36e2cc6a9c85c8fd2c51f2598e75759a443026bd9ea74da167a", "extra_info": null, "node_info": {"start": 66291, "end": 70376}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "116e8b02-7922-4d09-bad6-58f68db3ac02", "3": "be7b177e-46d7-4369-ba4e-99e9de3f6e37"}}, "__type__": "1"}, "be7b177e-46d7-4369-ba4e-99e9de3f6e37": {"__data__": {"text": "has both advantages and disadvantages compared to superscalar\nprocessors. Since scheduling is done in software, the decoding and instruction issue\nmechanisms are simpler in VLIW processors. The compiler has a larger context from which to\nselect instructions and can use a variety of transformations to optimize parallelism when\ncompared to a hardware issue unit. Additional parallel instructions are typically made available\nto the compiler to control parallel execution. However, compilers do not have the dynamic\nprogram state (e.g., the branch history buffer) available to make scheduling decisions. This\nreduces the accuracy of branch and memory prediction, but allows the use of more\nsophisticated static prediction schemes. Other runtime situations such as stalls on data fetch\nbecause of cache misses are extremely difficult to predict accurately. This limits the scope and\nperformance of static compiler-based scheduling.\nFinally, the performance of VLIW processors is very sensitive to the compilers' ability to detect\ndata and resource dependencies and read and write hazards, and to schedule instructions for\nmaximum parallelism. Loop unrolling, branch prediction and speculative execution all play\nimportant roles in the performance of VLIW processors. While superscalar and VLIW processors\nhave been successful in exploiting implicit parallelism, they are generally limited to smaller\nscales of concurrency in the range of four- to eight-way parallelism.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.2 Limitations of Memory System Performance*\nThe effective performance of a program on a computer relies not just on the speed of the\nprocessor but also on the ability of the memory system to feed data to the processor. At the\nlogical level, a memory system, possibly consisting of multiple levels of caches, takes in a\nrequest for a memory word and returns a block of data of size b containing the requested word\nafter l nanoseconds. Here, l is referred to as the latency  of the memory. The rate at which data\ncan be pumped from the memory to the processor determines the bandwidth  of the memory\nsystem.\nIt is very important to understand the difference between latency and bandwidth since different,\noften competing, techniques are required for addressing these. As an analogy, if water comes\nout of the end of a fire hose 2 seconds after a hydrant is turned on, then the latency of the\nsystem is 2 seconds. Once the flow starts, if the hose pumps water at 1 gallon/second then the\n'bandwidth' of the hose is 1 gallon/second. If we need to put out a fire immediately, we might\ndesire a lower latency. This would typically require higher water pressure from the hydrant. On\nthe other hand, if we wish to fight bigger fires, we might desire a higher flow rate, necessitating\na wider hose and hydrant. As we shall see here, this analogy works well for memory systems as\nwell. Latency and bandwidth both play critical roles in determining memory system\nperformance. We examine these separately in greater detail using a few examples.\nTo study the effect of memory system latency, we assume in the following examples that a\nmemory block consists of one word. We later relax this assumption while examining the role of\nmemory bandwidth. Since we are primarily interested in maximum achievable performance, we\nalso assume the best case cache-replacement policy. We refer the reader to the bibliography for\na detailed discussion of memory system design.\nExample 2.2 Effect of memory latency on performance\nConsider a processor operating at 1 GHz (1 ns clock) connected to a DRAM with a\nlatency of 100 ns (no caches). Assume that the processor has two multiply-add units\nand is capable of executing four instructions in each cycle of 1 ns. The peak processor\nrating is therefore 4 GFLOPS. Since the memory latency is equal to 100 cycles and\nblock size is one word, every time a memory request is made, the processor must wait\n100 cycles before it can process the data. Consider the problem of computing the dot-\nproduct of two vectors on such a platform. A dot-product computation performs one\nmultiply-add on a single", "doc_id": "be7b177e-46d7-4369-ba4e-99e9de3f6e37", "embedding": null, "doc_hash": "de5a3f2dddfd903099ad10a28fdd7cd1b29945293958541e8a22fcb7fe0e3ddd", "extra_info": null, "node_info": {"start": 70331, "end": 74446}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d493e40e-d3e1-403d-b569-c423bb4380f2", "3": "f844c86e-ee8c-4b3b-aaf4-0c9c289086c5"}}, "__type__": "1"}, "f844c86e-ee8c-4b3b-aaf4-0c9c289086c5": {"__data__": {"text": "assume the best case cache-replacement policy. We refer the reader to the bibliography for\na detailed discussion of memory system design.\nExample 2.2 Effect of memory latency on performance\nConsider a processor operating at 1 GHz (1 ns clock) connected to a DRAM with a\nlatency of 100 ns (no caches). Assume that the processor has two multiply-add units\nand is capable of executing four instructions in each cycle of 1 ns. The peak processor\nrating is therefore 4 GFLOPS. Since the memory latency is equal to 100 cycles and\nblock size is one word, every time a memory request is made, the processor must wait\n100 cycles before it can process the data. Consider the problem of computing the dot-\nproduct of two vectors on such a platform. A dot-product computation performs one\nmultiply-add on a single pair of vector elements, i.e., each floating point operation\nrequires one data fetch. It is easy to see that the peak speed of this computation is\nlimited to one floating point operation every 100 ns, or a speed of 10 MFLOPS, a very\nsmall fraction of the peak processor rating. This example highlights the need for\neffective memory system performance in achieving high computation rates. \n2.2.1 Improving Effective Memory Latency Using Caches\nHandling the mismatch in processor and DRAM speeds has motivated a number of architectural\ninnovations in memory system design. One such innovation addresses the speed mismatch by\nplacing a smaller and faster memory between the processor and the DRAM. This memory,\nreferred to as the cache, acts as a low-latency high-bandwidth storage. The data needed by the\nprocessor is first fetched into the cache. All subsequent accesses to data items residing in the\ncache are serviced by the cache. Thus, in principle, if a piece of data is repeatedly used, the\neffective latency of this memory system can be reduced by the cache. The fraction of data\nreferences satisfied by the cache is called the cache hit ratio  of the computation on the system.\nThe effective computation rate of many applications is bounded not by the processing rate of\nthe CPU, but by the rate at which data can be pumped into the CPU. Such computations are\nreferred to as being memory bound . The performance of memory bound programs is critically\nimpacted by the cache hit ratio.\nExample 2.3 Impact of caches on memory system performance\nAs in the previous example, consider a 1 GHz processor with a 100 ns latency DRAM.\nIn this case, we introduce a cache of size 32 KB with a latency of 1 ns or one cycle\n(typically on the processor itself). We use this setup to multiply two matrices A  and B\nof dimensions 32 x 32. We have carefully chosen these numbers so that the cache is\nlarge enough to store matrices A and B, as well as the result matrix C. Once again, we\nassume an ideal cache placement strategy in which none of the data items are\noverwritten by others. Fetching the two matrices into the cache corresponds to\nfetching 2K words, which takes approximately 200 \u00b5s. We know from elementary\nalgorithmics that multiplying two n x n matrices takes 2 n3 operations. For our\nproblem, this corresponds to 64K operations, which can be performed in 16K cycles\n(or 16 \u00b5s) at four instructions per cycle. The total time for the computation is\ntherefore approximately the sum of time for load/store operations and the time for the\ncomputation itself, i.e., 200+16 \u00b5s. This corresponds to a peak computation rate of\n64K/216 or 303 MFLOPS. Note that this is a thirty-fold improvement over the previous\nexample, although it is still less than 10% of the peak processor performance. We see\nin this example that by placing a small cache memory, we are able to improve\nprocessor utilization considerably. \nThe improvement in performance resulting from the presence of the cache is based on the\nassumption that there is repeated reference to the same data item. This notion of repeated\nreference to a data item in a small time window is called temporal locality  of reference. In our\nexample, we had O(n2) data", "doc_id": "f844c86e-ee8c-4b3b-aaf4-0c9c289086c5", "embedding": null, "doc_hash": "f00d41d40b1ae494be9322ec3e12d970303ed88dc3ef56b598fb7b9729828bd2", "extra_info": null, "node_info": {"start": 74483, "end": 78496}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "be7b177e-46d7-4369-ba4e-99e9de3f6e37", "3": "25dca542-9b08-46c3-82e3-35f308dc2994"}}, "__type__": "1"}, "25dca542-9b08-46c3-82e3-35f308dc2994": {"__data__": {"text": "time for the computation is\ntherefore approximately the sum of time for load/store operations and the time for the\ncomputation itself, i.e., 200+16 \u00b5s. This corresponds to a peak computation rate of\n64K/216 or 303 MFLOPS. Note that this is a thirty-fold improvement over the previous\nexample, although it is still less than 10% of the peak processor performance. We see\nin this example that by placing a small cache memory, we are able to improve\nprocessor utilization considerably. \nThe improvement in performance resulting from the presence of the cache is based on the\nassumption that there is repeated reference to the same data item. This notion of repeated\nreference to a data item in a small time window is called temporal locality  of reference. In our\nexample, we had O(n2) data accesses and O(n3) computation. (See the Appendix for an\nexplanation of the O notation.) Data reuse is critical for cache performance because if each data\nitem is used only once, it would still have to be fetched once per use from the DRAM, and\ntherefore the DRAM latency would be paid for each operation.\n2.2.2 Impact of Memory Bandwidth\nMemory bandwidth refers to the rate at which data can be moved between the processor and\nmemory. It is determined by the bandwidth of the memory bus as well as the memory units.\nOne commonly used technique to improve memory bandwidth is to increase the size of the\nmemory blocks. For an illustration, let us relax our simplifying restriction on the size of the\nmemory block and assume that a single memory request returns a contiguous block of four\nwords. The single unit of four words in this case is also referred to as a cache line .\nConventional computers typically fetch two to eight words together into the cache. We will see\nhow this helps the performance of applications for which data reuse is limited.\nExample 2.4 Effect of block size: dot-product of two vectors\nConsider again a memory system with a single cycle cache and 100 cycle latency\nDRAM with the processor operating at 1 GHz. If the block size is one word, the\nprocessor takes 100 cycles to fetch each word. For each pair of words, the dot-product\nperforms one multiply-add, i.e., two FLOPs. Therefore, the algorithm performs one\nFLOP every 100 cycles for a peak speed of 10 MFLOPS as illustrated in Example 2.2.\nNow let us consider what happens if the block size is increased to four words, i.e., the\nprocessor can fetch a four-word cache line every 100 cycles. Assuming that the\nvectors are laid out linearly in memory, eight FLOPs (four multiply-adds) can be\nperformed in 200 cycles. This is because a single memory access fetches four\nconsecutive words in the vector. Therefore, two accesses can fetch four elements of\neach of the vectors. This corresponds to a FLOP every 25 ns, for a peak speed of 40\nMFLOPS. Note that increasing the block size from one to four words did not change the\nlatency of the memory system. However, it increased the bandwidth four-fold. In this\ncase, the increased bandwidth of the memory system enabled us to accelerate the\ndot-product algorithm which has no data reuse at all.\nAnother way of quickly estimating performance bounds is to estimate the cache hit\nratio, using it to compute mean access time per word, and relating this to the FLOP\nrate via the underlying algorithm. For example, in this example, there are two DRAM\naccesses (cache misses) for every eight data accesses required by the algorithm. This\ncorresponds to a cache hit ratio of 75%. Assuming that the dominant overhead is\nposed by the cache misses, the average memory access time contributed by the\nmisses is 25% at 100 ns (or 25 ns/word). Since the dot-product has one\noperation/word, this corresponds to a computation rate of 40 MFLOPS as before. A\nmore accurate estimate of this rate would compute the average memory access time\nas 0.75 x 1 + 0.25 x 100 or 25.75 ns/word. The corresponding computation rate is\n38.8 MFLOPS. \nPhysically, the scenario", "doc_id": "25dca542-9b08-46c3-82e3-35f308dc2994", "embedding": null, "doc_hash": "e14207cb5388733f6f81a3e1754e404d3cccb0c8d6170e93d181a95b6cd236f5", "extra_info": null, "node_info": {"start": 78507, "end": 82461}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f844c86e-ee8c-4b3b-aaf4-0c9c289086c5", "3": "e82226ce-d75c-4cde-960d-11ce4a21c944"}}, "__type__": "1"}, "e82226ce-d75c-4cde-960d-11ce4a21c944": {"__data__": {"text": "per word, and relating this to the FLOP\nrate via the underlying algorithm. For example, in this example, there are two DRAM\naccesses (cache misses) for every eight data accesses required by the algorithm. This\ncorresponds to a cache hit ratio of 75%. Assuming that the dominant overhead is\nposed by the cache misses, the average memory access time contributed by the\nmisses is 25% at 100 ns (or 25 ns/word). Since the dot-product has one\noperation/word, this corresponds to a computation rate of 40 MFLOPS as before. A\nmore accurate estimate of this rate would compute the average memory access time\nas 0.75 x 1 + 0.25 x 100 or 25.75 ns/word. The corresponding computation rate is\n38.8 MFLOPS. \nPhysically, the scenario illustrated in Example 2.4  corresponds to a wide data bus (4 words or\n128 bits) connected to multiple memory banks. In practice, such wide buses are expensive to\nconstruct. In a more practical system, consecutive words are sent on the memory bus on\nsubsequent bus cycles after the first word is retrieved. For example, with a 32 bit data bus, the\nfirst word is put on the bus after 100 ns (the associated latency) and one word is put on each\nsubsequent bus cycle. This changes our calculations above slightly since the entire cache line\nbecomes available only after 100 + 3 x (memory bus cycle) ns. Assuming a data bus operating\nat 200 MHz, this adds 15 ns to the cache line access time. This does not change our bound on\nthe execution rate significantly.\nThe above examples clearly illustrate how increased bandwidth results in higher peak\ncomputation rates. They also make certain assumptions that have significance for the\nprogrammer. The data layouts were assumed to be such that consecutive data words in\nmemory were used by successive instructions. In other words, if we take a computation-centric\nview, there is a spatial locality  of memory access. If we take a data-layout centric point of\nview, the computation is ordered so that successive computations require contiguous data. If\nthe computation (or access pattern) does not have spatial locality, then effective bandwidth can\nbe much smaller than the peak bandwidth.\nAn example of such an access pattern is in reading a dense matrix column-wise when the matrix\nhas been stored in a row-major fashion in memory. Compilers can often be relied on to do a\ngood job of restructuring computation to take advantage of spatial locality.\nExample 2.5 Impact of strided access\nConsider the following code fragment:\n1  for (i = 0; i < 1000; i++) \n2          column_sum[i] = 0.0; \n3          for (j = 0; j < 1000; j++) \n4                  column_sum[i] += b[j][i]; \nThe code fragment sums columns of the matrix b into a vector column_sum . There are\ntwo observations that can be made: (i) the vector column_sum  is small and easily fits\ninto the cache; and (ii) the matrix b is accessed in a column order as illustrated in\nFigure 2.2(a) . For a matrix of size 1000 x 1000, stored in a row-major order, this\ncorresponds to accessing every 1000th entry. Therefore, it is likely that only one word\nin each cache line fetched from memory will be used. Consequently, the code\nfragment as written above is likely to yield poor performance. \nFigure 2.2. Multiplying a matrix with a vector: (a) multiplying\ncolumn-by-column, keeping a running sum; (b) computing each\nelement of the result as a dot product of a row of the matrix\nwith the vector.\nThe above example illustrates problems with strided access (with strides greater than one). The\nlack of spatial locality in computation causes poor memory system performance. Often it is\npossible to restructure the computation to remove strided access. In the case of our example, a\nsimple rewrite of", "doc_id": "e82226ce-d75c-4cde-960d-11ce4a21c944", "embedding": null, "doc_hash": "64038843427ea61781572138bc07dc1e690dab5617bebdc1a6527a952abcd2eb", "extra_info": null, "node_info": {"start": 82520, "end": 86229}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "25dca542-9b08-46c3-82e3-35f308dc2994", "3": "29237465-ef73-4ed8-9907-6b4c04e43b9e"}}, "__type__": "1"}, "29237465-ef73-4ed8-9907-6b4c04e43b9e": {"__data__": {"text": "stored in a row-major order, this\ncorresponds to accessing every 1000th entry. Therefore, it is likely that only one word\nin each cache line fetched from memory will be used. Consequently, the code\nfragment as written above is likely to yield poor performance. \nFigure 2.2. Multiplying a matrix with a vector: (a) multiplying\ncolumn-by-column, keeping a running sum; (b) computing each\nelement of the result as a dot product of a row of the matrix\nwith the vector.\nThe above example illustrates problems with strided access (with strides greater than one). The\nlack of spatial locality in computation causes poor memory system performance. Often it is\npossible to restructure the computation to remove strided access. In the case of our example, a\nsimple rewrite of the loops is possible as follows:\nExample 2.6 Eliminating strided access\nConsider the following restructuring of the column-sum fragment:\n1  for (i = 0; i < 1000; i++) \n2          column_sum[i] = 0.0; \n3  for (j = 0; j < 1000; j++) \n4          for (i = 0; i < 1000; i++) \n5                  column_sum[i] += b[j][i]; \nIn this case, the matrix is traversed in a row-order as illustrated in Figure 2.2(b) .\nHowever, the reader will note that this code fragment relies on the fact that the vector\ncolumn_sum  can be retained in the cache through the loops. Indeed, for this particular\nexample, our assumption is reasonable. If the vector is larger, we would have to\nbreak the iteration space into blocks and compute the product one block at a time.\nThis concept is also called tiling  an iteration space. The improved performance of this\nloop is left as an exercise for the reader. \nSo the next question is whether we have effectively solved the problems posed by memory\nlatency and bandwidth. While peak processor rates have grown significantly over the past\ndecades, memory latency and bandwidth have not kept pace with this increase. Consequently,\nfor typical computers, the ratio of peak FLOPS rate to peak memory bandwidth is anywhere\nbetween 1 MFLOPS/MBs (the ratio signifies FLOPS per megabyte/second of bandwidth) to 100\nMFLOPS/MBs. The lower figure typically corresponds to large scale vector supercomputers and\nthe higher figure to fast microprocessor based computers. This figure is very revealing in that it\ntells us that on average, a word must be reused 100 times after being fetched into the full\nbandwidth storage (typically L1 cache) to be able to achieve full processor utilization. Here, we\ndefine full-bandwidth as the rate of data transfer required by a computation to make it\nprocessor bound.\nThe series of examples presented in this section illustrate the following concepts:\nExploiting spatial and temporal locality in applications is critical for amortizing memory\nlatency and increasing effective memory bandwidth.Certain applications have inherently greater temporal locality than others, and thus have\ngreater tolerance to low memory bandwidth. The ratio of the number of operations to\nnumber of memory accesses is a good indicator of anticipated tolerance to memory\nbandwidth.\nMemory layouts and organizing computation appropriately can make a significant impact\non the spatial and temporal locality.\n2.2.3 Alternate Approaches for Hiding Memory Latency\nImagine sitting at your computer browsing the web during peak network traffic hours. The lack\nof response from your browser can be alleviated using one of three simple approaches:\n(i) we anticipate which pages we are going to browse ahead of time and issue requests for them\nin advance; (ii) we open multiple browsers and access different pages in each browser, thus\nwhile we are waiting for one page to load, we could be reading others; or (iii) we access a\nwhole bunch of pages in one go \u2013 amortizing the latency across various accesses. The first\napproach is called prefetching", "doc_id": "29237465-ef73-4ed8-9907-6b4c04e43b9e", "embedding": null, "doc_hash": "2437393f560625fe773c760867f0f0d4c24d3db1549fc14d49676ecb5eeeb223", "extra_info": null, "node_info": {"start": 86185, "end": 90011}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e82226ce-d75c-4cde-960d-11ce4a21c944", "3": "a32107d5-5a9d-41ec-84e1-8c2b418ca93a"}}, "__type__": "1"}, "a32107d5-5a9d-41ec-84e1-8c2b418ca93a": {"__data__": {"text": "to memory\nbandwidth.\nMemory layouts and organizing computation appropriately can make a significant impact\non the spatial and temporal locality.\n2.2.3 Alternate Approaches for Hiding Memory Latency\nImagine sitting at your computer browsing the web during peak network traffic hours. The lack\nof response from your browser can be alleviated using one of three simple approaches:\n(i) we anticipate which pages we are going to browse ahead of time and issue requests for them\nin advance; (ii) we open multiple browsers and access different pages in each browser, thus\nwhile we are waiting for one page to load, we could be reading others; or (iii) we access a\nwhole bunch of pages in one go \u2013 amortizing the latency across various accesses. The first\napproach is called prefetching , the second multithreading , and the third one corresponds to\nspatial locality in accessing memory words. Of these three approaches, spatial locality of\nmemory accesses has been discussed before. We focus on prefetching and multithreading as\ntechniques for latency hiding in this section.\nMultithreading for Latency Hiding\nA thread is a single stream of control in the flow of a program. We illustrate threads with a\nsimple example:\nExample 2.7 Threaded execution of matrix multiplication\nConsider the following code segment for multiplying an n x n matrix a by a vector b to\nget vector c.\n1  for(i=0;i<n;i++) \n2     c[i] = dot_product(get_row(a, i), b); \nThis code computes each element of c as the dot product of the corresponding row of\na with the vector b. Notice that each dot-product is independent of the other, and\ntherefore represents a concurrent unit of execution. We can safely rewrite the above\ncode segment as:\n1 for(i=0;i<n;i++) \n2    c[i] = create_thread(dot_product, get_row(a, i), b); \nThe only difference between the two code segments is that we have explicitly specified\neach instance of the dot-product computation as being a thread. (As we shall learn in\nChapter 7, there are a number of APIs for specifying threads. We have simply chosen\nan intuitive name for a function to create threads.) Now, consider the execution of\neach instance of the function dot_product. The first instance of this function accesses\na pair of vector elements and waits for them. In the meantime, the second instance of\nthis function can access two other vector elements in the next cycle, and so on. After l\nunits of time, where l is the latency of the memory system, the first function instance\ngets the requested data from memory and can perform the required computation. In\nthe next cycle, the data items for the next function instance arrive, and so on. In this\nway, in every clock cycle, we can perform a computation. \nThe execution schedule in Example 2.7  is predicated upon two assumptions: the memory\nsystem is capable of servicing multiple outstanding requests, and the processor is capable of\nswitching threads at every cycle. In addition, it also requires the program to have an explicit\nspecification of concurrency in the form of threads. Multithreaded processors are capable of\nmaintaining the context of a number of threads of computation with outstanding requests\n(memory accesses, I/O, or communication requests) and execute them as the requests are\nsatisfied. Machines such as the HEP and Tera rely on multithreaded processors that can switch\nthe context of execution in every cycle. Consequently, they are able to hide latency effectively,\nprovided there is enough concurrency (threads) to keep the processor from idling. The tradeoffs\nbetween concurrency and latency will be a recurring theme through many chapters of this text.\nPrefetching for Latency Hiding\nIn a typical program, a data item is loaded and used by a processor in a small time window. If\nthe load results in a cache miss, then the use stalls. A simple solution to this problem is to\nadvance the load operation so that even if there is a cache miss, the", "doc_id": "a32107d5-5a9d-41ec-84e1-8c2b418ca93a", "embedding": null, "doc_hash": "4f7f69b360e7795ef0517f82e936397805dc88aac34314a0fe310a974dc77f6a", "extra_info": null, "node_info": {"start": 90001, "end": 93921}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "29237465-ef73-4ed8-9907-6b4c04e43b9e", "3": "48bff303-73dd-4ee5-8ceb-7cc7523fa8b5"}}, "__type__": "1"}, "48bff303-73dd-4ee5-8ceb-7cc7523fa8b5": {"__data__": {"text": "of computation with outstanding requests\n(memory accesses, I/O, or communication requests) and execute them as the requests are\nsatisfied. Machines such as the HEP and Tera rely on multithreaded processors that can switch\nthe context of execution in every cycle. Consequently, they are able to hide latency effectively,\nprovided there is enough concurrency (threads) to keep the processor from idling. The tradeoffs\nbetween concurrency and latency will be a recurring theme through many chapters of this text.\nPrefetching for Latency Hiding\nIn a typical program, a data item is loaded and used by a processor in a small time window. If\nthe load results in a cache miss, then the use stalls. A simple solution to this problem is to\nadvance the load operation so that even if there is a cache miss, the data is likely to have\narrived by the time it is used. However, if the data item has been overwritten between load and\nuse, a fresh load is issued. Note that this is no worse than the situation in which the load had\nnot been advanced. A careful examination of this technique reveals that prefetching works for\nmuch the same reason as multithreading. In advancing the loads, we are trying to identify\nindependent threads of execution that have no resource dependency (i.e., use the same\nregisters) with respect to other threads. Many compilers aggressively try to advance loads to\nmask memory system latency.\nExample 2.8 Hiding latency by prefetching\nConsider the problem of adding two vectors a and b using a single for loop. In the\nfirst iteration of the loop, the processor requests a[0] and b[0]. Since these are not in\nthe cache, the processor must pay the memory latency. While these requests are\nbeing serviced, the processor also requests a[1] and b[1]. Assuming that each\nrequest is generated in one cycle (1 ns) and memory requests are satisfied in 100 ns,\nafter 100 such requests the first set of data items is returned by the memory system.\nSubsequently, one pair of vector components will be returned every cycle. In this way,\nin each subsequent cycle, one addition can be performed and processor cycles are not\nwasted. \n2.2.4 Tradeoffs of Multithreading and Prefetching\nWhile it might seem that multithreading and prefetching solve all the problems related to\nmemory system performance, they are critically impacted by the memory bandwidth.\nExample 2.9 Impact of bandwidth on multithreaded programs\nConsider a computation running on a machine with a 1 GHz clock, 4-word cache line,\nsingle cycle access to the cache, and 100 ns latency to DRAM. The computation has a\ncache hit ratio at 1 KB of 25% and at 32 KB of 90%. Consider two cases: first, a single\nthreaded execution in which the entire cache is available to the serial context, and\nsecond, a multithreaded execution with 32 threads where each thread has a cache\nresidency of 1 KB. If the computation makes one data request in every cycle of 1 ns, in\nthe first case the bandwidth requirement to DRAM is one word every 10 ns since the\nother words come from the cache (90% cache hit ratio). This corresponds to a\nbandwidth of 400 MB/s. In the second case, the bandwidth requirement to DRAM\nincreases to three words every four cycles of each thread (25% cache hit ratio).\nAssuming that all threads exhibit similar cache behavior, this corresponds to 0.75\nwords/ns, or 3 GB/s. \n\nExample 2.9  illustrates a very important issue, namely that the bandwidth requirements of a\nmultithreaded system may increase very significantly because of the smaller cache residency of\neach thread. In the example, while a sustained DRAM bandwidth of 400 MB/s is reasonable, 3.0\nGB/s is more than most systems currently offer. At this point, multithreaded systems become\nbandwidth bound instead of latency bound. It is important to realize that multithreading and\nprefetching only address the latency problem and may often exacerbate the bandwidth problem.\nAnother issue relates to the additional hardware resources required to effectively use\nprefetching and", "doc_id": "48bff303-73dd-4ee5-8ceb-7cc7523fa8b5", "embedding": null, "doc_hash": "fcdf9a614ce8e938fa33d27f43a02ea821dc015f16e507d4373b3a0a33bdbd7b", "extra_info": null, "node_info": {"start": 93908, "end": 97914}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a32107d5-5a9d-41ec-84e1-8c2b418ca93a", "3": "db4936b0-33d4-4d46-999c-613255922792"}}, "__type__": "1"}, "db4936b0-33d4-4d46-999c-613255922792": {"__data__": {"text": "hit ratio).\nAssuming that all threads exhibit similar cache behavior, this corresponds to 0.75\nwords/ns, or 3 GB/s. \n\nExample 2.9  illustrates a very important issue, namely that the bandwidth requirements of a\nmultithreaded system may increase very significantly because of the smaller cache residency of\neach thread. In the example, while a sustained DRAM bandwidth of 400 MB/s is reasonable, 3.0\nGB/s is more than most systems currently offer. At this point, multithreaded systems become\nbandwidth bound instead of latency bound. It is important to realize that multithreading and\nprefetching only address the latency problem and may often exacerbate the bandwidth problem.\nAnother issue relates to the additional hardware resources required to effectively use\nprefetching and multithreading. Consider a situation in which we have advanced 10 loads into\nregisters. These loads require 10 registers to be free for the duration. If an intervening\ninstruction overwrites the registers, we would have to load the data again. This would not\nincrease the latency of the fetch any more than the case in which there was no prefetching.\nHowever, now we are fetching the same data item twice, resulting in doubling of the bandwidth\nrequirement from the memory system. This situation is similar to the one due to cache\nconstraints as illustrated in Example 2.9. It can be alleviated by supporting prefetching and\nmultithreading with larger register files and caches.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.3 Dichotomy of Parallel Computing Platforms\nIn the preceding sections, we pointed out various factors that impact the performance of a serial\nor implicitly parallel program. The increasing gap in peak and sustainable performance of\ncurrent microprocessors, the impact of memory system performance, and the distributed nature\nof many problems present overarching motivations for parallelism. We now introduce, at a high\nlevel, the elements of parallel computing platforms that are critical for performance oriented\nand portable parallel programming. To facilitate our discussion of parallel platforms, we first\nexplore a dichotomy based on the logical and physical organization of parallel platforms. The\nlogical organization refers to a programmer's view of the platform while the physical\norganization refers to the actual hardware organization of the platform. The two critical\ncomponents of parallel computing from a programmer's perspective are ways of expressing\nparallel tasks and mechanisms for specifying interaction between these tasks. The former is\nsometimes also referred to as the control structure and the latter as the communication model.\n2.3.1 Control Structure of Parallel Platforms\nParallel tasks can be specified at various levels of granularity. At one extreme, each program in\na set of programs can be viewed as one parallel task. At the other extreme, individual\ninstructions within a program can be viewed as parallel tasks. Between these extremes lie a\nrange of models for specifying the control structure of programs and the corresponding\narchitectural support for them.\nExample 2.10 Parallelism from single instruction on multiple\nprocessors\nConsider the following code segment that adds two vectors:1  for (i = 0; i < 1000; i++) \n2          c[i] = a[i] + b[i]; \nIn this example, various iterations of the loop are independent of each other; i.e.,\nc[0] = a[0] + b[0]; c[1] = a[1] + b[1]; , etc., can all be executed\nindependently of each other. Consequently, if there is a mechanism for executing the\nsame instruction, in this case add on all the processors with appropriate data, we\ncould execute this loop much faster. \nProcessing units in parallel computers either operate under the centralized control of a single\ncontrol unit or work independently. In architectures referred to as single instruction stream,\nmultiple data stream  (SIMD), a single control unit dispatches instructions to each processing\nunit. Figure 2.3(a)  illustrates a", "doc_id": "db4936b0-33d4-4d46-999c-613255922792", "embedding": null, "doc_hash": "01f88189de0ad0b2745b2d0d4b9f25e0fc6e9a94f95262c614b73469efb82a66", "extra_info": null, "node_info": {"start": 97919, "end": 101882}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "48bff303-73dd-4ee5-8ceb-7cc7523fa8b5", "3": "0d62c003-d614-436f-9552-c3f9934ff5ef"}}, "__type__": "1"}, "0d62c003-d614-436f-9552-c3f9934ff5ef": {"__data__": {"text": "= a[i] + b[i]; \nIn this example, various iterations of the loop are independent of each other; i.e.,\nc[0] = a[0] + b[0]; c[1] = a[1] + b[1]; , etc., can all be executed\nindependently of each other. Consequently, if there is a mechanism for executing the\nsame instruction, in this case add on all the processors with appropriate data, we\ncould execute this loop much faster. \nProcessing units in parallel computers either operate under the centralized control of a single\ncontrol unit or work independently. In architectures referred to as single instruction stream,\nmultiple data stream  (SIMD), a single control unit dispatches instructions to each processing\nunit. Figure 2.3(a)  illustrates a typical SIMD architecture. In an SIMD parallel computer, the\nsame instruction is executed synchronously by all processing units. In Example 2.10 , the add\ninstruction is dispatched to all processors and executed concurrently by them. Some of the\nearliest parallel computers such as the Illiac IV, MPP, DAP, CM-2, and MasPar MP-1 belonged to\nthis class of machines. More recently, variants of this concept have found use in co-processing\nunits such as the MMX units in Intel processors and DSP chips such as the Sharc. The Intel\nPentium processor with its SSE (Streaming SIMD Extensions) provides a number of instructions\nthat execute the same instruction on multiple data items. These architectural enhancements rely\non the highly structured (regular) nature of the underlying computations, for example in image\nprocessing and graphics, to deliver improved performance.\nFigure 2.3. A typical SIMD architecture (a) and a typical MIMD\narchitecture (b).\nWhile the SIMD concept works well for structured computations on parallel data structures such\nas arrays, often it is necessary to selectively turn off operations on certain data items. For this\nreason, most SIMD programming paradigms allow for an \"activity mask\". This is a binary mask\nassociated with each data item and operation that specifies whether it should participate in the\noperation or not. Primitives such as where (condition)  then <stmnt> <elsewhere stmnt>\nare used to support selective execution. Conditional execution can be detrimental to the\nperformance of SIMD processors and therefore must be used with care.\nIn contrast to SIMD architectures, computers in which each processing element is capable of\nexecuting a different program independent of the other processing elements are called multiple\ninstruction stream, multiple data stream  (MIMD) computers. Figure 2.3(b) depicts a typical\nMIMD computer. A simple variant of this model, called the single program multiple data\n(SPMD) model, relies on multiple instances of the same program executing on different data. It\nis easy to see that the SPMD model has the same expressiveness as the MIMD model since each\nof the multiple programs can be inserted into one large if-else  block with conditions specified\nby the task identifiers. The SPMD model is widely used by many parallel platforms and requires\nminimal architectural support. Examples of such platforms include the Sun Ultra Servers,\nmultiprocessor PCs, workstation clusters, and the IBM SP.\nSIMD computers require less hardware than MIMD computers because they have only one\nglobal control unit. Furthermore, SIMD computers require less memory because only one copy\nof the program needs to be stored. In contrast, MIMD computers store the program and\noperating system at each processor. However, the relative unpopularity of SIMD processors as\ngeneral purpose compute engines can be attributed to their specialized hardware architectures,\neconomic factors, design constraints, product life-cycle, and application characteristics. In\ncontrast, platforms supporting the SPMD paradigm can be built from inexpensive off-the-shelf\ncomponents with relatively little effort in a short amount of time. SIMD computers require\nextensive design effort resulting in longer product development times. Since the underlying\nserial processors change so", "doc_id": "0d62c003-d614-436f-9552-c3f9934ff5ef", "embedding": null, "doc_hash": "28a60317951a17545cbe973c14509e04642a502dab1b5751c140ea280848b473", "extra_info": null, "node_info": {"start": 101966, "end": 105979}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "db4936b0-33d4-4d46-999c-613255922792", "3": "48b10adc-552a-470e-8364-7dfdc0c110fe"}}, "__type__": "1"}, "48b10adc-552a-470e-8364-7dfdc0c110fe": {"__data__": {"text": "SP.\nSIMD computers require less hardware than MIMD computers because they have only one\nglobal control unit. Furthermore, SIMD computers require less memory because only one copy\nof the program needs to be stored. In contrast, MIMD computers store the program and\noperating system at each processor. However, the relative unpopularity of SIMD processors as\ngeneral purpose compute engines can be attributed to their specialized hardware architectures,\neconomic factors, design constraints, product life-cycle, and application characteristics. In\ncontrast, platforms supporting the SPMD paradigm can be built from inexpensive off-the-shelf\ncomponents with relatively little effort in a short amount of time. SIMD computers require\nextensive design effort resulting in longer product development times. Since the underlying\nserial processors change so rapidly, SIMD computers suffer from fast obsolescence. The\nirregular nature of many applications also makes SIMD architectures less suitable. Example2.11 illustrates a case in which SIMD architectures yield poor resource utilization in the case of\nconditional execution.\nExample 2.11 Execution of conditional statements on a SIMD\narchitecture\nConsider the execution of a conditional statement illustrated in Figure 2.4 . The\nconditional statement in Figure 2.4(a)  is executed in two steps. In the first step, all\nprocessors that have B equal to zero execute the instruction C = A. All other\nprocessors are idle. In the second step, the 'else' part of the instruction ( C = A/B) is\nexecuted. The processors that were active in the first step now become idle. This\nillustrates one of the drawbacks of SIMD architectures. \nFigure 2.4. Executing a conditional statement on an SIMD\ncomputer with four processors: (a) the conditional statement;\n(b) the execution of the statement in two steps.\n2.3.2 Communication Model of Parallel Platforms\nThere are two primary forms of data exchange between parallel tasks \u2013 accessing a shared data\nspace and exchanging messages.\nShared-Address-Space Platforms\nThe \"shared-address-space\" view of a parallel platform supports a common data space that is\naccessible to all processors. Processors interact by modifying data objects stored in this shared-\naddress-space. Shared-address-space platforms supporting SPMD programming are also\nreferred to as multiprocessors . Memory in shared-address-space platforms can be local\n(exclusive to a processor) or global (common to all processors). If the time taken by a\nprocessor to access any memory word in the system (global or local) is identical, the platform is\nclassified as a uniform memory access  (UMA) multicomputer. On the other hand, if the time\ntaken to access certain memory words is longer than others, the platform is called a non-\nuniform memory access  (NUMA) multicomputer. Figures 2.5(a) and (b) illustrate UMA\nplatforms, whereas Figure 2.5(c)  illustrates a NUMA platform. An interesting case is illustrated\nin Figure 2.5(b) . Here, it is faster to access a memory word in cache than a location in memory.\nHowever, we still classify this as a UMA architecture. The reason for this is that all current\nmicroprocessors have cache hierarchies. Consequently, even a uniprocessor would not be\ntermed UMA if cache access times are considered. For this reason, we define NUMA and UMA\narchitectures only in terms of memory access times and not cache access times. Machines such\nas the SGI Origin 2000 and Sun Ultra HPC servers belong to the class of NUMA multiprocessors.\nThe distinction between UMA and NUMA platforms is important. If accessing local memory is\ncheaper than accessing global memory, algorithms must build locality and structure data and\ncomputation accordingly.\nFigure 2.5. Typical shared-address-space architectures: (a) Uniform-\nmemory-access shared-address-space computer; (b) Uniform-\nmemory-access shared-address-space computer with caches and\nmemories; (c)", "doc_id": "48b10adc-552a-470e-8364-7dfdc0c110fe", "embedding": null, "doc_hash": "080732a9ff18ae611fd0c494393cb2f4fcb53deb218b6f9c8b63c3da5b5440de", "extra_info": null, "node_info": {"start": 105828, "end": 109743}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0d62c003-d614-436f-9552-c3f9934ff5ef", "3": "04a50d25-cc33-42ba-a91b-b5f8471a413c"}}, "__type__": "1"}, "04a50d25-cc33-42ba-a91b-b5f8471a413c": {"__data__": {"text": "Consequently, even a uniprocessor would not be\ntermed UMA if cache access times are considered. For this reason, we define NUMA and UMA\narchitectures only in terms of memory access times and not cache access times. Machines such\nas the SGI Origin 2000 and Sun Ultra HPC servers belong to the class of NUMA multiprocessors.\nThe distinction between UMA and NUMA platforms is important. If accessing local memory is\ncheaper than accessing global memory, algorithms must build locality and structure data and\ncomputation accordingly.\nFigure 2.5. Typical shared-address-space architectures: (a) Uniform-\nmemory-access shared-address-space computer; (b) Uniform-\nmemory-access shared-address-space computer with caches and\nmemories; (c) Non-uniform-memory-access shared-address-space\ncomputer with local memory only.\nThe presence of a global memory space makes programming such platforms much easier. All\nread-only interactions are invisible to the programmer, as they are coded no differently than in\na serial program. This greatly eases the burden of writing parallel programs. Read/write\ninteractions are, however, harder to program than the read-only interactions, as these\noperations require mutual exclusion for concurrent accesses. Shared-address-space\nprogramming paradigms such as threads (POSIX, NT) and directives (OpenMP) therefore\nsupport synchronization using locks  and related mechanisms.\nThe presence of caches on processors also raises the issue of multiple copies of a single memory\nword being manipulated by two or more processors at the same time. Supporting a shared-\naddress-space in this context involves two major tasks: providing an address translation\nmechanism that locates a memory word in the system, and ensuring that concurrent operations\non multiple copies of the same memory word have well-defined semantics. The latter is also\nreferred to as the cache coherence  mechanism. This mechanism and its implementation are\ndiscussed in greater detail in Section 2.4.6. Supporting cache coherence requires considerable\nhardware support. Consequently, some shared-address-space machines only support an\naddress translation mechanism and leave the task of ensuring coherence to the programmer.\nThe native programming model for such platforms consists of primitives such as get  and put.\nThese primitives allow a processor to get (and put) variables stored at a remote processor.\nHowever, if one of the copies of this variable is changed, the other copies are not automatically\nupdated or invalidated.\nIt is important to note the difference between two commonly used and often misunderstood\nterms \u2013 shared-address-space and shared-memory computers. The term shared-memory\ncomputer is historically used for architectures in which the memory is physically shared among\nvarious processors, i.e., each processor has equal access to any memory segment. This is\nidentical to the UMA model we just discussed. This is in contrast to a distributed-memory\ncomputer, in which different segments of the memory are physically associated with different\nprocessing elements. The dichotomy of shared- versus distributed-memory computers pertains\nto the physical organization of the machine and is discussed in greater detail in Section 2.4.\nEither of these physical models, shared or distributed memory, can present the logical view of a\ndisjoint or shared-address-space platform. A distributed-memory shared-address-space\ncomputer is identical to a NUMA machine.\nMessage-Passing Platforms\nThe logical machine view of a message-passing platform consists of p processing nodes, each\nwith its own exclusive address space. Each of these processing nodes can either be single\nprocessors or a shared-address-space multiprocessor \u2013 a trend that is fast gaining momentum\nin modern message-passing parallel computers. Instances of such a view come naturally from\nclustered workstations and non-shared-address-space multicomputers. On such platforms,\ninteractions between processes running on different nodes must be accomplished using\nmessages, hence the name message passing . This exchange of messages is used to transfer\ndata, work,", "doc_id": "04a50d25-cc33-42ba-a91b-b5f8471a413c", "embedding": null, "doc_hash": "18adb0f60238b6289dda7842f149e67e09a53c6029d07d8bb28377ef48b20437", "extra_info": null, "node_info": {"start": 109848, "end": 113977}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "48b10adc-552a-470e-8364-7dfdc0c110fe", "3": "a228ab89-3a5d-48a4-8a62-3696e9c1a2c6"}}, "__type__": "1"}, "a228ab89-3a5d-48a4-8a62-3696e9c1a2c6": {"__data__": {"text": "a\ndisjoint or shared-address-space platform. A distributed-memory shared-address-space\ncomputer is identical to a NUMA machine.\nMessage-Passing Platforms\nThe logical machine view of a message-passing platform consists of p processing nodes, each\nwith its own exclusive address space. Each of these processing nodes can either be single\nprocessors or a shared-address-space multiprocessor \u2013 a trend that is fast gaining momentum\nin modern message-passing parallel computers. Instances of such a view come naturally from\nclustered workstations and non-shared-address-space multicomputers. On such platforms,\ninteractions between processes running on different nodes must be accomplished using\nmessages, hence the name message passing . This exchange of messages is used to transfer\ndata, work, and to synchronize actions among the processes. In its most general form,\nmessage-passing paradigms support execution of a different program on each of the p nodes.\nSince interactions are accomplished by sending and receiving messages, the basic operations in\nthis programming paradigm are send and receive  (the corresponding calls may differ across\nAPIs but the semantics are largely identical). In addition, since the send and receive operations\nmust specify target addresses, there must be a mechanism to assign a unique identification or\nID to each of the multiple processes executing a parallel program. This ID is typically made\navailable to the program using a function such as whoami, which returns to a calling process its\nID. There is one other function that is typically needed to complete the basic set of message-\npassing operations \u2013 numprocs , which specifies the number of processes participating in the\nensemble. With these four basic operations, it is possible to write any message-passing\nprogram. Different message-passing APIs, such as the Message Passing Interface (MPI) and\nParallel Virtual Machine (PVM), support these basic operations and a variety of higher level\nfunctionality under different function names. Examples of parallel platforms that support the\nmessage-passing paradigm include the IBM SP, SGI Origin 2000, and workstation clusters.\nIt is easy to emulate a message-passing architecture containing p nodes on a shared-address-\nspace computer with an identical number of nodes. Assuming uniprocessor nodes, this can be\ndone by partitioning the shared-address-space into p disjoint parts and assigning one such\npartition exclusively to each processor. A processor can then \"send\" or \"receive\" messages by\nwriting to or reading from another processor's partition while using appropriate synchronization\nprimitives to inform its communication partner when it has finished reading or writing the data.\nHowever, emulating a shared-address-space architecture on a message-passing computer is\ncostly, since accessing another node's memory requires sending and receiving messages.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.4 Physical Organization of Parallel Platforms\nIn this section, we discuss the physical architecture of parallel machines. We start with an ideal\narchitecture, outline practical difficulties associated with realizing this model, and discuss some\nconventional architectures.\n2.4.1 Architecture of an Ideal Parallel Computer\nA natural extension of the serial model of computation (the Random Access Machine, or RAM)\nconsists of p processors and a global memory of unbounded size that is uniformly accessible to\nall processors. All processors access the same address space. Processors share a common clock\nbut may execute different instructions in each cycle. This ideal model is also referred to as a\nparallel random access machine (PRAM) . Since PRAMs allow concurrent access to various\nmemory locations, depending on how simultaneous memory accesses are handled, PRAMs can\nbe divided into four subclasses.\nExclusive-read, exclusive-write (EREW) PRAM.  In this class, access to a memory\nlocation is exclusive. No concurrent read or write operations are allowed. This is the\nweakest PRAM model, affording minimum concurrency in memory access.1.\nConcurrent-read, exclusive-write (CREW) PRAM. ", "doc_id": "a228ab89-3a5d-48a4-8a62-3696e9c1a2c6", "embedding": null, "doc_hash": "13e01c9007be7eb20e003cc62927f01302707848591ac1a433651b8c71f8df5d", "extra_info": null, "node_info": {"start": 113920, "end": 118045}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "04a50d25-cc33-42ba-a91b-b5f8471a413c", "3": "cdef0c6c-3e87-4201-896e-670ae3dff634"}}, "__type__": "1"}, "cdef0c6c-3e87-4201-896e-670ae3dff634": {"__data__": {"text": "or RAM)\nconsists of p processors and a global memory of unbounded size that is uniformly accessible to\nall processors. All processors access the same address space. Processors share a common clock\nbut may execute different instructions in each cycle. This ideal model is also referred to as a\nparallel random access machine (PRAM) . Since PRAMs allow concurrent access to various\nmemory locations, depending on how simultaneous memory accesses are handled, PRAMs can\nbe divided into four subclasses.\nExclusive-read, exclusive-write (EREW) PRAM.  In this class, access to a memory\nlocation is exclusive. No concurrent read or write operations are allowed. This is the\nweakest PRAM model, affording minimum concurrency in memory access.1.\nConcurrent-read, exclusive-write (CREW) PRAM.  In this class, multiple read accesses\nto a memory location are allowed. However, multiple write accesses to a memory location\nare serialized.2.\nExclusive-read, concurrent-write (ERCW) PRAM.  Multiple write accesses are allowed\nto a memory location, but multiple read accesses are serialized.3.\nConcurrent-read, concurrent-write (CRCW) PRAM.  This class allows multiple read and\nwrite accesses to a common memory location. This is the most powerful PRAM model.4.\nAllowing concurrent read access does not create any semantic discrepancies in the program.\nHowever, concurrent write access to a memory location requires arbitration. Several protocols\nare used to resolve concurrent writes. The most frequently used protocols are as follows:\nCommon , in which the concurrent write is allowed if all the values that the processors are\nattempting to write are identical.Arbitrary , in which an arbitrary processor is allowed to proceed with the write operation\nand the rest fail.\nPriority , in which all processors are organized into a predefined prioritized list, and the\nprocessor with the highest priority succeeds and the rest fail.\nSum, in which the sum of all the quantities is written (the sum-based write conflict\nresolution model can be extended to any associative operator defined on the quantities\nbeing written).\nArchitectural Complexity of the Ideal Model\nConsider the implementation of an EREW PRAM as a shared-memory computer with p\nprocessors and a global memory of m words. The processors are connected to the memory\nthrough a set of switches. These switches determine the memory word being accessed by each\nprocessor. In an EREW PRAM, each of the p processors in the ensemble can access any of the\nmemory words, provided that a word is not accessed by more than one processor\nsimultaneously. To ensure such connectivity, the total number of switches must be Q(mp). (See\nthe Appendix for an explanation of the Q notation.) For a reasonable memory size, constructing\na switching network of this complexity is very expensive. Thus, PRAM models of computation\nare impossible to realize in practice.\n2.4.2 Interconnection Networks for Parallel Computers\nInterconnection networks provide mechanisms for data transfer between processing nodes or\nbetween processors and memory modules. A blackbox view of an interconnection network\nconsists of n inputs and m outputs. The outputs may or may not be distinct from the inputs.\nTypical interconnection networks are built using links and switches. A link corresponds to\nphysical media such as a set of wires or fibers capable of carrying information. A variety of\nfactors influence link characteristics. For links based on conducting media, the capacitive\ncoupling between wires limits the speed of signal propagation. This capacitive coupling and\nattenuation of signal strength are functions of the length of the link.\nInterconnection networks can be classified as static  or dynamic . Static networks consist of\npoint-to-point communication links among processing nodes and are also referred to as direct\nnetworks. Dynamic networks, on the other hand, are built using switches and communication\nlinks. Communication links are connected to one another dynamically by the switches to\nestablish paths among processing nodes and memory banks. Dynamic networks are also\nreferred to as indirect  networks. Figure 2.6(a) illustrates a simple static network of", "doc_id": "cdef0c6c-3e87-4201-896e-670ae3dff634", "embedding": null, "doc_hash": "c85d43e5032bed9e9628dacca7d0876171ca324cb688f54c730dc4e20ee95e31", "extra_info": null, "node_info": {"start": 118066, "end": 122252}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a228ab89-3a5d-48a4-8a62-3696e9c1a2c6", "3": "234999d6-7dd5-41af-8767-1e1dfd63f0c5"}}, "__type__": "1"}, "234999d6-7dd5-41af-8767-1e1dfd63f0c5": {"__data__": {"text": "fibers capable of carrying information. A variety of\nfactors influence link characteristics. For links based on conducting media, the capacitive\ncoupling between wires limits the speed of signal propagation. This capacitive coupling and\nattenuation of signal strength are functions of the length of the link.\nInterconnection networks can be classified as static  or dynamic . Static networks consist of\npoint-to-point communication links among processing nodes and are also referred to as direct\nnetworks. Dynamic networks, on the other hand, are built using switches and communication\nlinks. Communication links are connected to one another dynamically by the switches to\nestablish paths among processing nodes and memory banks. Dynamic networks are also\nreferred to as indirect  networks. Figure 2.6(a) illustrates a simple static network of four\nprocessing elements or nodes. Each processing node is connected via a network interface to two\nother nodes in a mesh configuration. Figure 2.6(b) illustrates a dynamic network of four nodes\nconnected via a network of switches to other nodes.\nFigure 2.6. Classification of interconnection networks: (a) a static\nnetwork; and (b) a dynamic network.\n\nA single switch in an interconnection network consists of a set of input ports and a set of output\nports. Switches provide a range of functionality. The minimal functionality provided by a switch\nis a mapping from the input to the output ports. The total number of ports on a switch is also\ncalled the degree  of the switch. Switches may also provide support for internal buffering (when\nthe requested output port is busy), routing (to alleviate congestion on the network), and\nmulticast (same output on multiple ports). The mapping from input to output ports can be\nprovided using a variety of mechanisms based on physical crossbars, multi-ported memories,\nmultiplexor-demultiplexors, and multiplexed buses. The cost of a switch is influenced by the\ncost of the mapping hardware, the peripheral hardware and packaging costs. The mapping\nhardware typically grows as the square of the degree of the switch, the peripheral hardware\nlinearly as the degree, and the packaging costs linearly as the number of pins.\nThe connectivity between the nodes and the network is provided by a network interface. The\nnetwork interface has input and output ports that pipe data into and out of the network. It\ntypically has the responsibility of packetizing data, computing routing information, buffering\nincoming and outgoing data for matching speeds of network and processing elements, and error\nchecking. The position of the interface between the processing element and the network is also\nimportant. While conventional network interfaces hang off the I/O buses, interfaces in tightly\ncoupled parallel machines hang off the memory bus. Since I/O buses are typically slower than\nmemory buses, the latter can support higher bandwidth.\n2.4.3 Network Topologies\nA wide variety of network topologies have been used in interconnection networks. These\ntopologies try to trade off cost and scalability with performance. While pure topologies have\nattractive mathematical properties, in practice interconnection networks tend to be\ncombinations or modifications of the pure topologies discussed in this section.\nBus-Based Networks\nA bus-based network is perhaps the simplest network consisting of a shared medium that is\ncommon to all the nodes. A bus has the desirable property that the cost of the network scales\nlinearly as the number of nodes, p. This cost is typically associated with bus interfaces.\nFurthermore, the distance between any two nodes in the network is constant (O (1)). Buses are\nalso ideal for broadcasting information among nodes. Since the transmission medium is shared,\nthere is little overhead associated with broadcast compared to point-to-point message transfer.\nHowever, the bounded bandwidth of a bus places limitations on the overall performance of the\nnetwork as the number of nodes increases. Typical bus based machines are limited to dozens of\nnodes. Sun Enterprise servers and Intel Pentium based shared-bus multiprocessors are\nexamples of such architectures.\nThe demands on bus bandwidth can be reduced by making use of the property that in", "doc_id": "234999d6-7dd5-41af-8767-1e1dfd63f0c5", "embedding": null, "doc_hash": "13377817025df7b4c4befd80307c159a7510728fd2407da166075f734482c0ef", "extra_info": null, "node_info": {"start": 122200, "end": 126449}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "cdef0c6c-3e87-4201-896e-670ae3dff634", "3": "2e7ae1e2-3b5f-4ac1-a5ff-d3d9dfe587bf"}}, "__type__": "1"}, "2e7ae1e2-3b5f-4ac1-a5ff-d3d9dfe587bf": {"__data__": {"text": "A bus has the desirable property that the cost of the network scales\nlinearly as the number of nodes, p. This cost is typically associated with bus interfaces.\nFurthermore, the distance between any two nodes in the network is constant (O (1)). Buses are\nalso ideal for broadcasting information among nodes. Since the transmission medium is shared,\nthere is little overhead associated with broadcast compared to point-to-point message transfer.\nHowever, the bounded bandwidth of a bus places limitations on the overall performance of the\nnetwork as the number of nodes increases. Typical bus based machines are limited to dozens of\nnodes. Sun Enterprise servers and Intel Pentium based shared-bus multiprocessors are\nexamples of such architectures.\nThe demands on bus bandwidth can be reduced by making use of the property that in typical\nprograms, a majority of the data accessed is local to the node. For such programs, it is possible\nto provide a cache for each node. Private data is cached at the node and only remote data is\naccessed through the bus.\nExample 2.12 Reducing shared-bus bandwidth using caches\nFigure 2.7(a) illustrates p processors sharing a bus to the memory. Assuming that\neach processor accesses k data items, and each data access takes time tcycle, the\nexecution time is lower bounded by tcycle x kp seconds. Now consider the hardware\norganization of Figure 2.7(b) . Let us assume that 50% of the memory accesses (0.5 k)\nare made to local data. This local data resides in the private memory of the processor.\nWe assume that access time to the private memory is identical to the global memory,\ni.e., tcycle. In this case, the total execution time is lower bounded by 0.5 x tcycle x k +\n0.5 x tcycle x kp. Here, the first term results from accesses to local data and the second\nterm from access to shared data. It is easy to see that as p becomes large, the\norganization of Figure 2.7(b) results in a lower bound that approaches 0.5 x tcycle x\nkp. This time is a 50% improvement in lower bound on execution time compared to\nthe organization of Figure 2.7(a) . \nFigure 2.7. Bus-based interconnects (a) with no local caches;\n(b) with local memory/caches.\nIn practice, shared and private data is handled in a more sophisticated manner. This is briefly\naddressed with cache coherence issues in Section 2.4.6.\nCrossbar Networks\nA simple way to connect p processors to b memory banks is to use a crossbar network. A\ncrossbar network employs a grid of switches or switching nodes as shown in Figure 2.8 . The\ncrossbar network is a non-blocking network in the sense that the connection of a processing\nnode to a memory bank does not block the connection of any other processing nodes to other\nmemory banks.\nFigure 2.8. A completely non-blocking crossbar network connecting p\nprocessors to b memory banks.\nThe total number of switching nodes required to implement such a network is Q (pb). It is\nreasonable to assume that the number of memory banks b is at least p; otherwise, at any given\ntime, there will be some processing nodes that will be unable to access any memory banks.\nTherefore, as the value of p is increased, the complexity (component count) of the switching\nnetwork grows as W(p2). (See the Appendix for an explanation of the W notation.) As the\nnumber of processing nodes becomes large, this switch complexity is difficult to realize at high\ndata rates. Consequently, crossbar networks are not very scalable in terms of cost.\nMultistage Networks\nThe crossbar interconnection network is scalable in terms of performance but unscalable in\nterms of cost. Conversely, the shared bus network is scalable in terms of cost but unscalable in\nterms of performance. An intermediate class of networks called multistage interconnection\nnetworks  lies between these two extremes. It is more scalable than the bus in terms of\nperformance and more scalable than the crossbar in terms of cost.\nThe general schematic of a multistage network consisting", "doc_id": "2e7ae1e2-3b5f-4ac1-a5ff-d3d9dfe587bf", "embedding": null, "doc_hash": "3332f3a2678ef7d56b98dbaaab23f9c9c3d459e7b81de0b41f52d4ed35b329ff", "extra_info": null, "node_info": {"start": 126470, "end": 130431}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "234999d6-7dd5-41af-8767-1e1dfd63f0c5", "3": "0fe82c31-64f0-424f-866f-c3dd2e4acfbf"}}, "__type__": "1"}, "0fe82c31-64f0-424f-866f-c3dd2e4acfbf": {"__data__": {"text": "of the switching\nnetwork grows as W(p2). (See the Appendix for an explanation of the W notation.) As the\nnumber of processing nodes becomes large, this switch complexity is difficult to realize at high\ndata rates. Consequently, crossbar networks are not very scalable in terms of cost.\nMultistage Networks\nThe crossbar interconnection network is scalable in terms of performance but unscalable in\nterms of cost. Conversely, the shared bus network is scalable in terms of cost but unscalable in\nterms of performance. An intermediate class of networks called multistage interconnection\nnetworks  lies between these two extremes. It is more scalable than the bus in terms of\nperformance and more scalable than the crossbar in terms of cost.\nThe general schematic of a multistage network consisting of p processing nodes and b memory\nbanks is shown in Figure 2.9. A commonly used multistage connection network is the omega\nnetwork . This network consists of log p stages, where p is the number of inputs (processing\nnodes) and also the number of outputs (memory banks). Each stage of the omega network\nconsists of an interconnection pattern that connects p inputs and p outputs; a link exists\nbetween input i and output j if the following is true:\nEquation 2.1\n\nFigure 2.9. The schematic of a typical multistage interconnection\nnetwork.\nEquation 2.1  represents a left-rotation operation on the binary representation of i to obtain j.\nThis interconnection pattern is called a perfect shuffle . Figure 2.10  shows a perfect shuffle\ninterconnection pattern for eight inputs and outputs. At each stage of an omega network, a\nperfect shuffle interconnection pattern feeds into a set of p/2 switches or switching nodes. Each\nswitch is in one of two connection modes. In one mode, the inputs are sent straight through to\nthe outputs, as shown in Figure 2.11(a). This is called the pass-through  connection. In the\nother mode, the inputs to the switching node are crossed over and then sent out, as shown in\nFigure 2.11(b). This is called the cross-over  connection.\nFigure 2.10. A perfect shuffle interconnection for eight inputs and\noutputs.\nFigure 2.11. Two switching configurations of the 2 x 2 switch: (a)\nPass-through; (b) Cross-over.\nAn omega network has p/2 x log p switching nodes, and the cost of such a network grows as\nQ(p log p). Note that this cost is less than the Q(p2) cost of a complete crossbar network. Figure\n2.12 shows an omega network for eight processors (denoted by the binary numbers on the left)\nand eight memory banks (denoted by the binary numbers on the right). Routing data in an\nomega network is accomplished using a simple scheme. Let s be the binary representation of a\nprocessor that needs to write some data into memory bank t. The data traverses the link to the\nfirst switching node. If the most significant bits of s and t are the same, then the data is routed\nin pass-through mode by the switch. If these bits are different, then the data is routed through\nin crossover mode. This scheme is repeated at the next switching stage using the next most\nsignificant bit. Traversing log p stages uses all log p bits in the binary representations of s and\nt.\nFigure 2.12. A complete omega network connecting eight inputs and\neight outputs.\nFigure 2.13  shows data routing over an omega network from processor two (010) to memory\nbank seven (111) and from processor six (110) to memory bank four (100). This figure also\nillustrates an important property of this network. When processor two (010) is communicating\nwith memory bank seven (111), it blocks the path from processor six (110) to memory bank\nfour (100). Communication link AB is used by both communication paths. Thus, in an omega\nnetwork, access to a memory bank by a processor may disallow access to another memory bank\nby another processor. Networks with this property are referred to as blocking networks .\nFigure 2.13. An example of blocking in omega network: one of the\nmessages (010 to 111 or", "doc_id": "0fe82c31-64f0-424f-866f-c3dd2e4acfbf", "embedding": null, "doc_hash": "d6707d7cd8218924f469ec96e5f377db5ad7e342d6ece105e14467345b7b57f0", "extra_info": null, "node_info": {"start": 130461, "end": 134439}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2e7ae1e2-3b5f-4ac1-a5ff-d3d9dfe587bf", "3": "c3db5b8c-d519-4543-b88e-78269d2a1add"}}, "__type__": "1"}, "c3db5b8c-d519-4543-b88e-78269d2a1add": {"__data__": {"text": "and\nt.\nFigure 2.12. A complete omega network connecting eight inputs and\neight outputs.\nFigure 2.13  shows data routing over an omega network from processor two (010) to memory\nbank seven (111) and from processor six (110) to memory bank four (100). This figure also\nillustrates an important property of this network. When processor two (010) is communicating\nwith memory bank seven (111), it blocks the path from processor six (110) to memory bank\nfour (100). Communication link AB is used by both communication paths. Thus, in an omega\nnetwork, access to a memory bank by a processor may disallow access to another memory bank\nby another processor. Networks with this property are referred to as blocking networks .\nFigure 2.13. An example of blocking in omega network: one of the\nmessages (010 to 111 or 110 to 100) is blocked at link AB.\nCompletely-Connected Network\nIn a completely-connected network , each node has a direct communication link to every\nother node in the network. Figure 2.14(a)  illustrates a completely-connected network of eight\nnodes. This network is ideal in the sense that a node can send a message to another node in a\nsingle step, since a communication link exists between them. Completely-connected networks\nare the static counterparts of crossbar switching networks, since in both networks, the\ncommunication between any input/output pair does not block communication between any\nother pair.\nFigure 2.14. (a) A completely-connected network of eight nodes; (b) a\nstar connected network of nine nodes.\nStar-Connected Network\nIn a star-connected network , one processor acts as the central processor. Every other\nprocessor has a communication link connecting it to this processor. Figure 2.14(b)  shows a star-\nconnected network of nine processors. The star-connected network is similar to bus-based\nnetworks. Communication between any pair of processors is routed through the central\nprocessor, just as the shared bus forms the medium for all communication in a bus-based\nnetwork. The central processor is the bottleneck in the star topology.\nLinear Arrays, Meshes, and k-d Meshes\nDue to the large number of links in completely connected networks, sparser networks are\ntypically used to build parallel computers. A family of such networks spans the space of linear\narrays and hypercubes. A linear array is a static network in which each node (except the two\nnodes at the ends) has two neighbors, one each to its left and right. A simple extension of the\nlinear array ( Figure 2.15(a)) is the ring or a 1-D torus ( Figure 2.15(b) ). The ring has a\nwraparound connection between the extremities of the linear array. In this case, each node has\ntwo neighbors.\nFigure 2.15. Linear arrays: (a) with no wraparound links; (b) with\nwraparound link.\nA two-dimensional mesh illustrated in Figure 2.16(a)  is an extension of the linear array to two-\ndimensions. Each dimension has \n  nodes with a node identified by a two-tuple ( i, j). Every\nnode (except those on the periphery) is connected to four other nodes whose indices differ in\nany dimension by one. A 2-D mesh has the property that it can be laid out in 2-D space, making\nit attractive from a wiring standpoint. Furthermore, a variety of regularly structured\ncomputations map very naturally to a 2-D mesh. For this reason, 2-D meshes were often used\nas interconnects in parallel machines. Two dimensional meshes can be augmented with\nwraparound links to form two dimensional tori illustrated in Figure 2.16(b). The three-\ndimensional cube is a generalization of the 2-D mesh to three dimensions, as illustrated in\nFigure 2.16(c). Each node element in a 3-D cube, with the exception of those on the periphery,\nis connected to six other nodes, two along each of the three dimensions. A variety of physical\nsimulations commonly executed on parallel computers (for example, 3-D weather modeling,\nstructural modeling, etc.) can be mapped naturally to 3-D network topologies.", "doc_id": "c3db5b8c-d519-4543-b88e-78269d2a1add", "embedding": null, "doc_hash": "e755be219706f2b855e73b56aef448ce4d25f241beddd1954b1e26081e743a6f", "extra_info": null, "node_info": {"start": 134437, "end": 138384}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0fe82c31-64f0-424f-866f-c3dd2e4acfbf", "3": "b296c0bc-15d8-4e8d-9094-83e37a8dec57"}}, "__type__": "1"}, "b296c0bc-15d8-4e8d-9094-83e37a8dec57": {"__data__": {"text": "structured\ncomputations map very naturally to a 2-D mesh. For this reason, 2-D meshes were often used\nas interconnects in parallel machines. Two dimensional meshes can be augmented with\nwraparound links to form two dimensional tori illustrated in Figure 2.16(b). The three-\ndimensional cube is a generalization of the 2-D mesh to three dimensions, as illustrated in\nFigure 2.16(c). Each node element in a 3-D cube, with the exception of those on the periphery,\nis connected to six other nodes, two along each of the three dimensions. A variety of physical\nsimulations commonly executed on parallel computers (for example, 3-D weather modeling,\nstructural modeling, etc.) can be mapped naturally to 3-D network topologies. For this reason,\n3-D cubes are used commonly in interconnection networks for parallel computers (for example,\nin the Cray T3E).\nFigure 2.16. Two and three dimensional meshes: (a) 2-D mesh with no\nwraparound; (b) 2-D mesh with wraparound link (2-D torus); and (c)\na 3-D mesh with no wraparound.\nThe general class of k-d meshes refers to the class of topologies consisting of d dimensions with\nk nodes along each dimension. Just as a linear array forms one extreme of the k-d mesh family,\nthe other extreme is formed by an interesting topology called the hypercube. The hypercube\ntopology has two nodes along each dimension and log p dimensions. The construction of a\nhypercube is illustrated in Figure 2.17. A zero-dimensional hypercube consists of 20, i.e., one\nnode. A one-dimensional hypercube is constructed from two zero-dimensional hypercubes by\nconnecting them. A two-dimensional hypercube of four nodes is constructed from two one-\ndimensional hypercubes by connecting corresponding nodes. In general a d-dimensional\nhypercube is constructed by connecting corresponding nodes of two ( d - 1) dimensional\nhypercubes. Figure 2.17 illustrates this for up to 16 nodes in a 4-D hypercube.\nFigure 2.17. Construction of hypercubes from hypercubes of lower\ndimension.\nIt is useful to derive a numbering scheme for nodes in a hypercube. A simple numbering\nscheme can be derived from the construction of a hypercube. As illustrated in Figure 2.17, if we\nhave a numbering of two subcubes of p/2 nodes, we can derive a numbering scheme for the\ncube of p nodes by prefixing the labels of one of the subcubes with a \"0\" and the labels of the\nother subcube with a \"1\". This numbering scheme has the useful property that the minimum\ndistance between two nodes is given by the number of bits that are different in the two labels.\nFor example, nodes labeled 0110 and 0101 are two links apart, since they differ at two bit\npositions. This property is useful for deriving a number of parallel algorithms for the hypercube\narchitecture.\nTree-Based Networks\nA tree network  is one in which there is only one path between any pair of nodes. Both linear\narrays and star-connected networks are special cases of tree networks. Figure 2.18  shows\nnetworks based on complete binary trees. Static tree networks have a processing element at\neach node of the tree ( Figure 2.18(a) ). Tree networks also have a dynamic counterpart. In a\ndynamic tree network, nodes at intermediate levels are switching nodes and the leaf nodes are\nprocessing elements ( Figure 2.18(b)).\nFigure 2.18. Complete binary tree networks: (a) a static tree network;\nand (b) a dynamic tree network.\nTo route a message in a tree, the source node sends the message up the tree until it reaches\nthe node at the root of the smallest subtree containing both the source and destination nodes.\nThen the message is routed down the tree towards the destination node.\nTree networks suffer from a communication bottleneck at higher levels of the tree. For example,\nwhen many nodes in the left subtree of a node", "doc_id": "b296c0bc-15d8-4e8d-9094-83e37a8dec57", "embedding": null, "doc_hash": "035f11172ba3b0d333273af296d74fcefd6d06c9b06a2e7f563c1a4759c873c6", "extra_info": null, "node_info": {"start": 138450, "end": 142220}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c3db5b8c-d519-4543-b88e-78269d2a1add", "3": "ef8b209e-d15b-493b-abfe-857f5a6e3417"}}, "__type__": "1"}, "ef8b209e-d15b-493b-abfe-857f5a6e3417": {"__data__": {"text": "Static tree networks have a processing element at\neach node of the tree ( Figure 2.18(a) ). Tree networks also have a dynamic counterpart. In a\ndynamic tree network, nodes at intermediate levels are switching nodes and the leaf nodes are\nprocessing elements ( Figure 2.18(b)).\nFigure 2.18. Complete binary tree networks: (a) a static tree network;\nand (b) a dynamic tree network.\nTo route a message in a tree, the source node sends the message up the tree until it reaches\nthe node at the root of the smallest subtree containing both the source and destination nodes.\nThen the message is routed down the tree towards the destination node.\nTree networks suffer from a communication bottleneck at higher levels of the tree. For example,\nwhen many nodes in the left subtree of a node communicate with nodes in the right subtree, the\nroot node must handle all the messages. This problem can be alleviated in dynamic tree\nnetworks by increasing the number of communication links and switching nodes closer to the\nroot. This network, also called a fat tree , is illustrated in Figure 2.19.\nFigure 2.19. A fat tree network of 16 processing nodes.\n2.4.4 Evaluating Static Interconnection Networks\nWe now discuss various criteria used to characterize the cost and performance of static\ninterconnection networks. We use these criteria to evaluate static networks introduced in the\nprevious subsection.\nDiameter  The diameter  of a network is the maximum distance between any two processing\nnodes in the network. The distance between two processing nodes is defined as the shortest\npath (in terms of number of links) between them. The diameter of a completely-connected\nnetwork is one, and that of a star-connected network is two. The diameter of a ring network is\n. The diameter of a two-dimensional mesh without wraparound connections is \nfor the two nodes at diagonally opposed corners, and that of a wraparound mesh is \n .\nThe diameter of a hypercube-connected network is log p since two node labels can differ in at\nmost log p positions. The diameter of a complete binary tree is 2 log(( p + 1)/2) because the\ntwo communicating nodes may be in separate subtrees of the root node, and a message might\nhave to travel all the way to the root and then down the other subtree.\nConnectivity  The connectivity  of a network is a measure of the multiplicity of paths between\nany two processing nodes. A network with high connectivity is desirable, because it lowers\ncontention for communication resources. One measure of connectivity is the minimum number\nof arcs that must be removed from the network to break it into two disconnected networks. This\nis called the arc connectivity  of the network. The arc connectivity is one for linear arrays, as\nwell as tree and star networks. It is two for rings and 2-D meshes without wraparound, four for\n2-D wraparound meshes, and d for d-dimensional hypercubes.\nBisection Width and Bisection Bandwidth  The bisection width  of a network is defined as\nthe minimum number of communication links that must be removed to partition the network\ninto two equal halves. The bisection width of a ring is two, since any partition cuts across only\ntwo communication links. Similarly, the bisection width of a two-dimensional p-node mesh\nwithout wraparound connections is \n and with wraparound connections is \n . The bisection\nwidth of a tree and a star is one, and that of a completely-connected network of p nodes is p2/4.\nThe bisection width of a hypercube can be derived from its construction. We construct a d-\ndimensional hypercube by connecting corresponding links of two ( d - 1)-dimensional\nhypercubes. Since each of these subcubes contains 2(d-1) or p/2 nodes, at least p/2\ncommunication links must cross any partition of a hypercube into two subcubes (Problem 2.15).\nThe number of bits that can be communicated simultaneously over a link connecting two nodes\nis called the channel width . Channel width is equal to the number of physical wires in each\ncommunication link. The peak rate at which a single physical wire can deliver bits is called the\nchannel rate", "doc_id": "ef8b209e-d15b-493b-abfe-857f5a6e3417", "embedding": null, "doc_hash": "04652e543bd12728340d7cff83cf633dbabcb8b98478f1c0e6db25ef5384318e", "extra_info": null, "node_info": {"start": 142182, "end": 146270}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b296c0bc-15d8-4e8d-9094-83e37a8dec57", "3": "208daeff-7428-42df-95ae-4ff36e07868a"}}, "__type__": "1"}, "208daeff-7428-42df-95ae-4ff36e07868a": {"__data__": {"text": "connections is \n . The bisection\nwidth of a tree and a star is one, and that of a completely-connected network of p nodes is p2/4.\nThe bisection width of a hypercube can be derived from its construction. We construct a d-\ndimensional hypercube by connecting corresponding links of two ( d - 1)-dimensional\nhypercubes. Since each of these subcubes contains 2(d-1) or p/2 nodes, at least p/2\ncommunication links must cross any partition of a hypercube into two subcubes (Problem 2.15).\nThe number of bits that can be communicated simultaneously over a link connecting two nodes\nis called the channel width . Channel width is equal to the number of physical wires in each\ncommunication link. The peak rate at which a single physical wire can deliver bits is called the\nchannel rate . The peak rate at which data can be communicated between the ends of a\ncommunication link is called channel bandwidth . Channel bandwidth is the product of channel\nrate and channel width.\nTable 2.1. A summary of the characteristics of various static network\ntopologies connecting p nodes.\nNetwork Diameter Bisection\nWidthArc\nConnectivityCost (No. of\nlinks)\nCompletely-connected 1 p2/4 p - 1 p(p - 1)/2\nStar 2 1 1 p - 1\nComplete binary tree 2 log(( p +\n1)/2)1 1 p - 1\nLinear array p - 1 1 1 p - 1\n2-D mesh, no\nwraparound\n2\n2-D wraparound mesh\n 4 2p\nNetwork Diameter Bisection\nWidthArc\nConnectivityCost (No. of\nlinks)\nHypercube log p p/2 logp (p log p)/2\nWraparound k-ary d-\ncube\n2kd-12d dp\nThe bisection bandwidth  of a network is defined as the minimum volume of communication\nallowed between any two halves of the network. It is the product of the bisection width and the\nchannel bandwidth. Bisection bandwidth of a network is also sometimes referred to as cross-\nsection bandwidth .\nCost Many criteria can be used to evaluate the cost of a network. One way of defining the cost\nof a network is in terms of the number of communication links or the number of wires required\nby the network. Linear arrays and trees use only p - 1 links to connect p nodes. A d-dimensional\nwraparound mesh has dp links. A hypercube-connected network has ( p log p)/2 links.\nThe bisection bandwidth of a network can also be used as a measure of its cost, as it provides a\nlower bound on the area in a two-dimensional packaging or the volume in a three-dimensional\npackaging. If the bisection width of a network is w, the lower bound on the area in a two-\ndimensional packaging is Q(w2), and the lower bound on the volume in a three-dimensional\npackaging is Q(w3/2). According to this criterion, hypercubes and completely connected\nnetworks are more expensive than the other networks.\nWe summarize the characteristics of various static networks in Table 2.1, which highlights the\nvarious cost-performance tradeoffs.\n2.4.5 Evaluating Dynamic Interconnection Networks\nA number of evaluation metrics for dynamic networks follow from the corresponding metrics for\nstatic networks. Since a message traversing a switch must pay an overhead, it is logical to think\nof each switch as a node in the network, in addition to the processing nodes. The diameter of\nthe network can now be defined as the maximum distance between any two nodes in the\nnetwork. This is indicative of the maximum delay that a message will encounter in being\ncommunicated between the selected pair of nodes. In reality, we would like the metric to be the\nmaximum distance between any two processing nodes; however, for all networks of interest,\nthis is equivalent to the maximum distance between any (processing or switching) pair of\nnodes.\nThe connectivity of a dynamic network can be defined in terms of node or edge connectivity.\nThe node connectivity is the minimum number of nodes that must fail (be removed from the\nnetwork) to fragment the network into two parts. As before, we should consider only switching\nnodes (as opposed to all nodes). However, considering all nodes gives", "doc_id": "208daeff-7428-42df-95ae-4ff36e07868a", "embedding": null, "doc_hash": "8ab3055a2dd3ed88697e95bc04d5c418bee5225cee8e4736241dfaafac2629ed", "extra_info": null, "node_info": {"start": 146273, "end": 150182}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ef8b209e-d15b-493b-abfe-857f5a6e3417", "3": "762f3d1a-04ee-477e-a629-a1c6769bbab6"}}, "__type__": "1"}, "762f3d1a-04ee-477e-a629-a1c6769bbab6": {"__data__": {"text": "addition to the processing nodes. The diameter of\nthe network can now be defined as the maximum distance between any two nodes in the\nnetwork. This is indicative of the maximum delay that a message will encounter in being\ncommunicated between the selected pair of nodes. In reality, we would like the metric to be the\nmaximum distance between any two processing nodes; however, for all networks of interest,\nthis is equivalent to the maximum distance between any (processing or switching) pair of\nnodes.\nThe connectivity of a dynamic network can be defined in terms of node or edge connectivity.\nThe node connectivity is the minimum number of nodes that must fail (be removed from the\nnetwork) to fragment the network into two parts. As before, we should consider only switching\nnodes (as opposed to all nodes). However, considering all nodes gives a good approximation to\nthe multiplicity of paths in a dynamic network. The arc connectivity of the network can be\nsimilarly defined as the minimum number of edges that must fail (be removed from the\nnetwork) to fragment the network into two unreachable parts.\nThe bisection width of a dynamic network must be defined more precisely than diameter and\nconnectivity. In the case of bisection width, we consider any possible partitioning of the p\nprocessing nodes into two equal parts. Note that this does not restrict the partitioning of the\nswitching nodes. For each such partition, we select an induced partitioning of the switching\nnodes such that the number of edges crossing this partition is minimized. The minimum number\nof edges for any such partition is the bisection width of the dynamic network. Another intuitive\nway of thinking of bisection width is in terms of the minimum number of edges that must beHypercube log p p/2 logp (p log p)/2\nWraparound k-ary d-\ncube\n2kd-12d dp\nThe bisection bandwidth  of a network is defined as the minimum volume of communication\nallowed between any two halves of the network. It is the product of the bisection width and the\nchannel bandwidth. Bisection bandwidth of a network is also sometimes referred to as cross-\nsection bandwidth .\nCost Many criteria can be used to evaluate the cost of a network. One way of defining the cost\nof a network is in terms of the number of communication links or the number of wires required\nby the network. Linear arrays and trees use only p - 1 links to connect p nodes. A d-dimensional\nwraparound mesh has dp links. A hypercube-connected network has ( p log p)/2 links.\nThe bisection bandwidth of a network can also be used as a measure of its cost, as it provides a\nlower bound on the area in a two-dimensional packaging or the volume in a three-dimensional\npackaging. If the bisection width of a network is w, the lower bound on the area in a two-\ndimensional packaging is Q(w2), and the lower bound on the volume in a three-dimensional\npackaging is Q(w3/2). According to this criterion, hypercubes and completely connected\nnetworks are more expensive than the other networks.\nWe summarize the characteristics of various static networks in Table 2.1, which highlights the\nvarious cost-performance tradeoffs.\n2.4.5 Evaluating Dynamic Interconnection Networks\nA number of evaluation metrics for dynamic networks follow from the corresponding metrics for\nstatic networks. Since a message traversing a switch must pay an overhead, it is logical to think\nof each switch as a node in the network, in addition to the processing nodes. The diameter of\nthe network can now be defined as the maximum distance between any two nodes in the\nnetwork. This is indicative of the maximum delay that a message will encounter in being\ncommunicated between the selected pair of nodes. In reality, we would like the metric to be the\nmaximum distance between any two processing nodes; however, for all networks of interest,\nthis is equivalent to the maximum distance between any (processing or switching) pair of\nnodes.\nThe connectivity of a dynamic network can be defined in terms of node or edge connectivity.\nThe node connectivity is the minimum number of nodes that must fail (be removed from the\nnetwork) to fragment the network into two parts. As before,", "doc_id": "762f3d1a-04ee-477e-a629-a1c6769bbab6", "embedding": null, "doc_hash": "9b2e265303602b3937931666fb14172de654d5e4e08e70f698f095f75f2f00c7", "extra_info": null, "node_info": {"start": 150120, "end": 154291}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "208daeff-7428-42df-95ae-4ff36e07868a", "3": "8cb8a8e5-25f0-42d7-87ed-c3663bf7d5ee"}}, "__type__": "1"}, "8cb8a8e5-25f0-42d7-87ed-c3663bf7d5ee": {"__data__": {"text": "a switch must pay an overhead, it is logical to think\nof each switch as a node in the network, in addition to the processing nodes. The diameter of\nthe network can now be defined as the maximum distance between any two nodes in the\nnetwork. This is indicative of the maximum delay that a message will encounter in being\ncommunicated between the selected pair of nodes. In reality, we would like the metric to be the\nmaximum distance between any two processing nodes; however, for all networks of interest,\nthis is equivalent to the maximum distance between any (processing or switching) pair of\nnodes.\nThe connectivity of a dynamic network can be defined in terms of node or edge connectivity.\nThe node connectivity is the minimum number of nodes that must fail (be removed from the\nnetwork) to fragment the network into two parts. As before, we should consider only switching\nnodes (as opposed to all nodes). However, considering all nodes gives a good approximation to\nthe multiplicity of paths in a dynamic network. The arc connectivity of the network can be\nsimilarly defined as the minimum number of edges that must fail (be removed from the\nnetwork) to fragment the network into two unreachable parts.\nThe bisection width of a dynamic network must be defined more precisely than diameter and\nconnectivity. In the case of bisection width, we consider any possible partitioning of the p\nprocessing nodes into two equal parts. Note that this does not restrict the partitioning of the\nswitching nodes. For each such partition, we select an induced partitioning of the switching\nnodes such that the number of edges crossing this partition is minimized. The minimum number\nof edges for any such partition is the bisection width of the dynamic network. Another intuitive\nway of thinking of bisection width is in terms of the minimum number of edges that must be\nremoved from the network so as to partition the network into two halves with identical number\nof processing nodes. We illustrate this concept further in the following example:\nExample 2.13 Bisection width of dynamic networks\nConsider the network illustrated in Figure 2.20 . We illustrate here three bisections, A,\nB, and C, each of which partitions the network into two groups of two processing\nnodes each. Notice that these partitions need not partition the network nodes equally.\nIn the example, each partition results in an edge cut of four. We conclude that the\nbisection width of this graph is four. \nFigure 2.20. Bisection width of a dynamic network is computed\nby examining various equi-partitions of the processing nodes\nand selecting the minimum number of edges crossing the\npartition. In this case, each partition yields an edge cut of four.\nTherefore, the bisection width of this graph is four.\nThe cost of a dynamic network is determined by the link cost, as is the case with static\nnetworks, as well as the switch cost. In typical dynamic networks, the degree of a switch is\nconstant. Therefore, the number of links and switches is asymptotically identical. Furthermore,\nin typical networks, switch cost exceeds link cost. For this reason, the cost of dynamic networks\nis often determined by the number of switching nodes in the network.\nWe summarize the characteristics of various dynamic networks in Table 2.2.\n2.4.6 Cache Coherence in Multiprocessor Systems\nWhile interconnection networks provide basic mechanisms for communicating messages (data),\nin the case of shared-address-space computers additional hardware is required to keep multiple\ncopies of data consistent with each other. Specifically, if there exist two copies of the data (in\ndifferent caches/memory elements), how do we ensure that different processors operate on\nthese in a manner that follows predefined semantics?\nTable 2.2. A summary of the characteristics of various dynamic\nnetwork topologies connecting p processing nodes.\nNetwork Diameter Bisection Width Arc Connectivity Cost (No. of links)\nCrossbar 1 p 1 p2\nOmega Network log p p/2 2 p/2\nDynamic Tree 2 log p 1 2 p - 1\nThe problem of keeping caches in multiprocessor systems coherent", "doc_id": "8cb8a8e5-25f0-42d7-87ed-c3663bf7d5ee", "embedding": null, "doc_hash": "7a7455d1fa4975722e9c52b6cb0e4b312f34883ce9ec5c678996a48d28cfa9e0", "extra_info": null, "node_info": {"start": 154303, "end": 158392}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "762f3d1a-04ee-477e-a629-a1c6769bbab6", "3": "e8939af1-e442-451e-a761-b9061a868b86"}}, "__type__": "1"}, "e8939af1-e442-451e-a761-b9061a868b86": {"__data__": {"text": "Systems\nWhile interconnection networks provide basic mechanisms for communicating messages (data),\nin the case of shared-address-space computers additional hardware is required to keep multiple\ncopies of data consistent with each other. Specifically, if there exist two copies of the data (in\ndifferent caches/memory elements), how do we ensure that different processors operate on\nthese in a manner that follows predefined semantics?\nTable 2.2. A summary of the characteristics of various dynamic\nnetwork topologies connecting p processing nodes.\nNetwork Diameter Bisection Width Arc Connectivity Cost (No. of links)\nCrossbar 1 p 1 p2\nOmega Network log p p/2 2 p/2\nDynamic Tree 2 log p 1 2 p - 1\nThe problem of keeping caches in multiprocessor systems coherent is significantly more\ncomplex than in uniprocessor systems. This is because in addition to multiple copies as in\nuniprocessor systems, there may also be multiple processors modifying these copies. Consider a\nsimple scenario illustrated in Figure 2.21. Two processors P0 and P1 are connected over a\nshared bus to a globally accessible memory. Both processors load the same variable. There are\nnow three copies of the variable. The coherence mechanism must now ensure that all operations\nperformed on these copies are serializable (i.e., there exists some serial order of instruction\nexecution that corresponds to the parallel schedule). When a processor changes the value of its\ncopy of the variable, one of two things must happen: the other copies must be invalidated, or\nthe other copies must be updated. Failing this, other processors may potentially work with\nincorrect (stale) values of the variable. These two protocols are referred to as invalidate  and\nupdate  protocols and are illustrated in Figure 2.21(a) and (b).\nFigure 2.21. Cache coherence in multiprocessor systems: (a)\nInvalidate protocol; (b) Update protocol for shared variables.\n\nIn an update protocol, whenever a data item is written, all of its copies in the system are\nupdated. For this reason, if a processor simply reads a data item once and never uses it,\nsubsequent updates to this item at other processors cause excess overhead in terms of latency\nat source and bandwidth on the network. On the other hand, in this situation, an invalidate\nprotocol invalidates the data item on the first update at a remote processor and subsequent\nupdates need not be performed on this copy.\nAnother important factor affecting the performance of these protocols is false sharing . False\nsharing refers to the situation in which different processors update different parts of of the same\ncache-line. Thus, although the updates are not performed on shared variables, the system does\nnot detect this. In an invalidate protocol, when a processor updates its part of the cache-line,\nthe other copies of this line are invalidated. When other processors try to update their parts of\nthe cache-line, the line must actually be fetched from the remote processor. It is easy to see\nthat false-sharing can cause a cache-line to be ping-ponged between various processors. In an\nupdate protocol, this situation is slightly better since all reads can be performed locally and the\nwrites must be updated. This saves an invalidate operation that is otherwise wasted.\nThe tradeoff between invalidate and update schemes is the classic tradeoff between\ncommunication overhead (updates) and idling (stalling in invalidates). Current generation cache\ncoherent machines typically rely on invalidate protocols. The rest of our discussion of\nmultiprocessor cache systems therefore assumes invalidate protocols.\nMaintaining Coherence Using Invalidate Protocols  Multiple copies of a single data item are\nkept consistent by keeping track of the number of copies and the state of each of these copies.\nWe discuss here one possible set of states associated with data items and events that trigger\ntransitions among these states. Note that this set of states and transitions is not unique. It is\npossible to define other states and associated transitions as well.\nLet us revisit the example in Figure", "doc_id": "e8939af1-e442-451e-a761-b9061a868b86", "embedding": null, "doc_hash": "7ad90d0060325f9e9cd2278ca4a81ae154fcbb4f12b075002b32bf1a354f969c", "extra_info": null, "node_info": {"start": 158443, "end": 162536}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8cb8a8e5-25f0-42d7-87ed-c3663bf7d5ee", "3": "3515e404-84b9-4929-b3de-4284ef0a87df"}}, "__type__": "1"}, "3515e404-84b9-4929-b3de-4284ef0a87df": {"__data__": {"text": "is otherwise wasted.\nThe tradeoff between invalidate and update schemes is the classic tradeoff between\ncommunication overhead (updates) and idling (stalling in invalidates). Current generation cache\ncoherent machines typically rely on invalidate protocols. The rest of our discussion of\nmultiprocessor cache systems therefore assumes invalidate protocols.\nMaintaining Coherence Using Invalidate Protocols  Multiple copies of a single data item are\nkept consistent by keeping track of the number of copies and the state of each of these copies.\nWe discuss here one possible set of states associated with data items and events that trigger\ntransitions among these states. Note that this set of states and transitions is not unique. It is\npossible to define other states and associated transitions as well.\nLet us revisit the example in Figure 2.21. Initially the variable x resides in the global memory.\nThe first step executed by both processors is a load operation on this variable. At this point, the\nstate of the variable is said to be shared , since it is shared by multiple processors. When\nprocessor P0 executes a store on this variable, it marks all other copies of this variable as\ninvalid . It must also mark its own copy as modified or dirty . This is done to ensure that all\nsubsequent accesses to this variable at other processors will be serviced by processor P0 and\nnot from the memory. At this point, say, processor P1 executes another load operation on x .\nProcessor P1 attempts to fetch this variable and, since the variable was marked dirty by\nprocessor P0, processor P0 services the request. Copies of this variable at processor P1 and the\nglobal memory are updated and the variable re-enters the shared state. Thus, in this simple\nmodel, there are three states - shared , invalid , and dirty  - that a cache line goes through.\nThe complete state diagram of a simple three-state protocol is illustrated in Figure 2.22. The\nsolid lines depict processor actions and the dashed lines coherence actions. For example, when\na processor executes a read on an invalid block, the block is fetched and a transition is made\nfrom invalid to shared. Similarly, if a processor does a write on a shared block, the coherence\nprotocol propagates a C_write (a coherence write) on the block. This triggers a transition from\nshared to invalid at all the other blocks.\nFigure 2.22. State diagram of a simple three-state coherence protocol.\nExample 2.14 Maintaining coherence using a simple three-state\nprotocol\nConsider an example of two program segments being executed by processor P0 and P1\nas illustrated in Figure 2.23 . The system consists of local memories (or caches) at\nprocessors P0 and P1, and a global memory. The three-state protocol assumed in this\nexample corresponds to the state diagram illustrated in Figure 2.22 . Cache lines in\nthis system can be either shared, invalid, or dirty. Each data item (variable) is\nassumed to be on a different cache line. Initially, the two variables x and y are tagged\ndirty and the only copies of these variables exist in the global memory. Figure 2.23\nillustrates state transitions along with values of copies of the variables with each\ninstruction execution. \nFigure 2.23. Example of parallel program execution with the\nsimple three-state coherence protocol discussed in Section\n2.4.6 .\nThe implementation of coherence protocols can be carried out using a variety of hardware\nmechanisms \u2013 snoopy systems, directory based systems, or combinations thereof.\nSnoopy Cache Systems\nSnoopy caches are typically associated with multiprocessor systems based on broadcast\ninterconnection networks such as a bus or a ring. In such systems, all processors snoop on\n(monitor) the bus for transactions. This allows the processor to make state transitions for its\ncache-blocks. Figure 2.24 illustrates a typical snoopy bus based system. Each processor's cache\nhas a set of tag bits associated with it that determine the state of the cache blocks. These tags\nare updated according to the state diagram associated with the coherence protocol. For\ninstance, when the snoop hardware detects that a", "doc_id": "3515e404-84b9-4929-b3de-4284ef0a87df", "embedding": null, "doc_hash": "d2490d7e17c1fa8fdc4e51b827c53a9ec28b700ee71217c5338b0b00dad716ce", "extra_info": null, "node_info": {"start": 162469, "end": 166597}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e8939af1-e442-451e-a761-b9061a868b86", "3": "8d8431af-bc1b-452a-8e06-3e5a3e345ad7"}}, "__type__": "1"}, "8d8431af-bc1b-452a-8e06-3e5a3e345ad7": {"__data__": {"text": "implementation of coherence protocols can be carried out using a variety of hardware\nmechanisms \u2013 snoopy systems, directory based systems, or combinations thereof.\nSnoopy Cache Systems\nSnoopy caches are typically associated with multiprocessor systems based on broadcast\ninterconnection networks such as a bus or a ring. In such systems, all processors snoop on\n(monitor) the bus for transactions. This allows the processor to make state transitions for its\ncache-blocks. Figure 2.24 illustrates a typical snoopy bus based system. Each processor's cache\nhas a set of tag bits associated with it that determine the state of the cache blocks. These tags\nare updated according to the state diagram associated with the coherence protocol. For\ninstance, when the snoop hardware detects that a read has been issued to a cache block that it\nhas a dirty copy of, it asserts control of the bus and puts the data out. Similarly, when the\nsnoop hardware detects that a write operation has been issued on a cache block that it has a\ncopy of, it invalidates the block. Other state transitions are made in this fashion locally.\nFigure 2.24. A simple snoopy bus based cache coherence system.\nPerformance of Snoopy Caches  Snoopy protocols have been extensively studied and used in\ncommercial systems. This is largely because of their simplicity and the fact that existing bus\nbased systems can be upgraded to accommodate snoopy protocols. The performance gains of\nsnoopy systems are derived from the fact that if different processors operate on different data\nitems, these items can be cached. Once these items are tagged dirty, all subsequent operations\ncan be performed locally on the cache without generating external traffic. Similarly, if a data\nitem is read by a number of processors, it transitions to the shared state in the cache and all\nsubsequent read operations become local. In both cases, the coherence protocol does not add\nany overhead. On the other hand, if multiple processors read and update the same data item,\nthey generate coherence functions across processors. Since a shared bus has a finite bandwidth,\nonly a constant number of such coherence operations can execute in unit time. This presents a\nfundamental bottleneck for snoopy bus based systems.\nSnoopy protocols are intimately tied to multicomputers based on broadcast networks such as\nbuses. This is because all processors must snoop all the messages. Clearly, broadcasting all of a\nprocessor's memory operations to all the processors is not a scalable solution. An obvious\nsolution to this problem is to propagate coherence operations only to those processors that\nmust participate in the operation (i.e., processors that have relevant copies of the data). This\nsolution requires us to keep track of which processors have copies of various data items and\nalso the relevant state information for these data items. This information is stored in a\ndirectory, and the coherence mechanism based on such information is called a directory-based\nsystem.\nDirectory Based Systems\nConsider a simple system in which the global memory is augmented with a directory that\nmaintains a bitmap representing cache-blocks and the processors at which they are cached\n(Figure 2.25). These bitmap entries are sometimes referred to as the presence bits . As before,\nwe assume a three-state protocol with the states labeled invalid , dirty , and shared . The key to\nthe performance of directory based schemes is the simple observation that only processors that\nhold a particular block (or are reading it) participate in the state transitions due to coherence\noperations. Note that there may be other state transitions triggered by processor read, write, or\nflush (retiring a line from cache) but these transitions can be handled locally with the operation\nreflected in the presence bits and state in the directory.\nFigure 2.25. Architecture of typical directory based systems: (a) a\ncentralized directory; and (b) a distributed directory.\nRevisiting the code segment in Figure 2.21 , when processors P0 and P1 access the block\ncorresponding to variable x , the state of the block is changed to shared, and the presence", "doc_id": "8d8431af-bc1b-452a-8e06-3e5a3e345ad7", "embedding": null, "doc_hash": "e1c412cbde3727bb3fe21f8867a575e6dabf056e361625f9dd776718ff19d899", "extra_info": null, "node_info": {"start": 166646, "end": 170805}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3515e404-84b9-4929-b3de-4284ef0a87df", "3": "f102d4a0-01a3-4448-920c-f5d4596d1c39"}}, "__type__": "1"}, "f102d4a0-01a3-4448-920c-f5d4596d1c39": {"__data__": {"text": "with the states labeled invalid , dirty , and shared . The key to\nthe performance of directory based schemes is the simple observation that only processors that\nhold a particular block (or are reading it) participate in the state transitions due to coherence\noperations. Note that there may be other state transitions triggered by processor read, write, or\nflush (retiring a line from cache) but these transitions can be handled locally with the operation\nreflected in the presence bits and state in the directory.\nFigure 2.25. Architecture of typical directory based systems: (a) a\ncentralized directory; and (b) a distributed directory.\nRevisiting the code segment in Figure 2.21 , when processors P0 and P1 access the block\ncorresponding to variable x , the state of the block is changed to shared, and the presence bits\nupdated to indicate that processors P0 and P1 share the block. When P0 executes a store on the\nvariable, the state in the directory is changed to dirty and the presence bit of P1 is reset. All\nsubsequent operations on this variable performed at processor P0 can proceed locally. If another\nprocessor reads the value, the directory notices the dirty tag and uses the presence bits to\ndirect the request to the appropriate processor. Processor P0 updates the block in the memory,\nand sends it to the requesting processor. The presence bits are modified to reflect this and the\nstate transitions to shared.\nPerformance of Directory Based Schemes  As is the case with snoopy protocols, if different\nprocessors operate on distinct data blocks, these blocks become dirty in the respective caches\nand all operations after the first one can be performed locally. Furthermore, if multiple\nprocessors read (but do not update) a single data block, the data block gets replicated in the\ncaches in the shared state and subsequent reads can happen without triggering any coherence\noverheads.\nCoherence actions are initiated when multiple processors attempt to update the same data item.\nIn this case, in addition to the necessary data movement, coherence operations add to the\noverhead in the form of propagation of state updates (invalidates or updates) and generation of\nstate information from the directory. The former takes the form of communication overhead and\nthe latter adds contention. The communication overhead is a function of the number of\nprocessors requiring state updates and the algorithm for propagating state information. The\ncontention overhead is more fundamental in nature. Since the directory is in memory and the\nmemory system can only service a bounded number of read/write operations in unit time, the\nnumber of state updates is ultimately bounded by the directory. If a parallel program requires a\nlarge number of coherence actions (large number of read/write shared data blocks) the\ndirectory will ultimately bound its parallel performance.\nFinally, from the point of view of cost, the amount of memory required to store the directory\nmay itself become a bottleneck as the number of processors increases. Recall that the directory\nsize grows as O(mp), where m is the number of memory blocks and p the number of\nprocessors. One solution would be to make the memory block larger (thus reducing m for a\ngiven memory size). However, this adds to other overheads such as false sharing, where two\nprocessors update distinct data items in a program but the data items happen to lie in the same\nmemory block. This phenomenon is discussed in greater detail in Chapter 7.\nSince the directory forms a central point of contention, it is natural to break up the task of\nmaintaining coherence across multiple processors. The basic principle is to let each processor\nmaintain coherence of its own memory blocks, assuming a physical (or logical) partitioning of\nthe memory blocks across processors. This is the principle of a distributed directory system.\nDistributed Directory Schemes  In scalable architectures, memory is physically distributed\nacross processors. The corresponding presence bits of the blocks are also distributed. Each\nprocessor is responsible for maintaining the coherence of its own memory blocks. The\narchitecture of such a system is illustrated in Figure 2.25(b). Since each memory block has an\nowner (which can typically be computed from the block address), its directory location is\nimplicitly known to all", "doc_id": "f102d4a0-01a3-4448-920c-f5d4596d1c39", "embedding": null, "doc_hash": "2e8c58865b5bab6c6eebb00c4aa0c85ddba888a9bb4e71743d61ad41605ad786", "extra_info": null, "node_info": {"start": 170789, "end": 175143}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8d8431af-bc1b-452a-8e06-3e5a3e345ad7", "3": "6cab64a2-7131-45cf-99f9-b4b42dd32f96"}}, "__type__": "1"}, "6cab64a2-7131-45cf-99f9-b4b42dd32f96": {"__data__": {"text": "the directory forms a central point of contention, it is natural to break up the task of\nmaintaining coherence across multiple processors. The basic principle is to let each processor\nmaintain coherence of its own memory blocks, assuming a physical (or logical) partitioning of\nthe memory blocks across processors. This is the principle of a distributed directory system.\nDistributed Directory Schemes  In scalable architectures, memory is physically distributed\nacross processors. The corresponding presence bits of the blocks are also distributed. Each\nprocessor is responsible for maintaining the coherence of its own memory blocks. The\narchitecture of such a system is illustrated in Figure 2.25(b). Since each memory block has an\nowner (which can typically be computed from the block address), its directory location is\nimplicitly known to all processors. When a processor attempts to read a block for the first time,\nit requests the owner for the block. The owner suitably directs this request based on presence\nand state information locally available. Similarly, when a processor writes into a memory block,\nit propagates an invalidate to the owner, which in turn forwards the invalidate to all processors\nthat have a cached copy of the block. In this way, the directory is decentralized and the\ncontention associated with the central directory is alleviated. Note that the communication\noverhead associated with state update messages is not reduced.\nPerformance of Distributed Directory Schemes  As is evident, distributed directories permit\nO(p) simultaneous coherence operations, provided the underlying network can sustain the\nassociated state update messages. From this point of view, distributed directories are inherently\nmore scalable than snoopy systems or centralized directory systems. The latency and bandwidth\nof the network become fundamental performance bottlenecks for such systems.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.5 Communication Costs in Parallel Machines\nOne of the major overheads in the execution of parallel programs arises from communication of\ninformation between processing elements. The cost of communication is dependent on a variety\nof features including the programming model semantics, the network topology, data handling\nand routing, and associated software protocols. These issues form the focus of our discussion\nhere.\n2.5.1 Message Passing Costs in Parallel Computers\nThe time taken to communicate a message between two nodes in a network is the sum of the\ntime to prepare a message for transmission and the time taken by the message to traverse the\nnetwork to its destination. The principal parameters that determine the communication latency\nare as follows:\nStartup time  (ts): The startup time is the time required to handle a message at the\nsending and receiving nodes. This includes the time to prepare the message (adding\nheader, trailer, and error correction information), the time to execute the routing\nalgorithm, and the time to establish an interface between the local node and the router.\nThis delay is incurred only once for a single message transfer.1.\nPer-hop time  (th): After a message leaves a node, it takes a finite amount of time to\nreach the next node in its path. The time taken by the header of a message to travel\nbetween two directly-connected nodes in the network is called the per-hop time. It is also\nknown as node latency . The per-hop time is directly related to the latency within the\nrouting switch for determining which output buffer or channel the message should be\nforwarded to.2.\nPer-word transfer time  (tw): If the channel bandwidth is r words per second, then each\nword takes time tw = 1/r to traverse the link. This time is called the per-word transfer\ntime. This time includes network as well as buffering overheads.3.\nWe now discuss two routing techniques that have been used in parallel computers \u2013 store-and-\nforward routing and cut-through routing.\nStore-and-Forward Routing\nIn store-and-forward routing, when a message is traversing a path with multiple links, each\nintermediate node on the path forwards the message to the next node after it has received and\nstored the entire message. Figure 2.26(a) shows the", "doc_id": "6cab64a2-7131-45cf-99f9-b4b42dd32f96", "embedding": null, "doc_hash": "d3110852743593568241c5f29bb31685ea63b0a7e01208d2a2497768889eb524", "extra_info": null, "node_info": {"start": 175108, "end": 179309}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f102d4a0-01a3-4448-920c-f5d4596d1c39", "3": "a325fd42-232b-44a0-bf14-53c527fab714"}}, "__type__": "1"}, "a325fd42-232b-44a0-bf14-53c527fab714": {"__data__": {"text": "related to the latency within the\nrouting switch for determining which output buffer or channel the message should be\nforwarded to.2.\nPer-word transfer time  (tw): If the channel bandwidth is r words per second, then each\nword takes time tw = 1/r to traverse the link. This time is called the per-word transfer\ntime. This time includes network as well as buffering overheads.3.\nWe now discuss two routing techniques that have been used in parallel computers \u2013 store-and-\nforward routing and cut-through routing.\nStore-and-Forward Routing\nIn store-and-forward routing, when a message is traversing a path with multiple links, each\nintermediate node on the path forwards the message to the next node after it has received and\nstored the entire message. Figure 2.26(a) shows the communication of a message through a\nstore-and-forward network.\nFigure 2.26. Passing a message from node P0 to P3 (a) through a\nstore-and-forward communication network; (b) and (c) extending the\nconcept to cut-through routing. The shaded regions represent the time\nthat the message is in transit. The startup time associated with this\nmessage transfer is assumed to be zero.\nSuppose that a message of size m is being transmitted through such a network. Assume that it\ntraverses l links. At each link, the message incurs a cost th for the header and twm for the rest\nof the message to traverse the link. Since there are l such links, the total time is ( th + twm)l.\nTherefore, for store-and-forward routing, the total communication cost for a message of size m\nwords to traverse l communication links is\nEquation 2.2\nIn current parallel computers, the per-hop time th is quite small. For most parallel algorithms, it\nis less than twm even for small values of m and thus can be ignored. For parallel platforms\nusing store-and-forward routing, the time given by Equation 2.2  can be simplified to\nPacket Routing\nStore-and-forward routing makes poor use of communication resources. A message is sent from\none node to the next only after the entire message has been received ( Figure 2.26(a) ). Consider\nthe scenario shown in Figure 2.26(b) , in which the original message is broken into two equal\nsized parts before it is sent. In this case, an intermediate node waits for only half of the original\nmessage to arrive before passing it on. The increased utilization of communication resources\nand reduced communication time is apparent from Figure 2.26(b). Figure 2.26(c)  goes a step\nfurther and breaks the message into four parts. In addition to better utilization of\ncommunication resources, this principle offers other advantages \u2013 lower overhead from packet\nloss (errors), possibility of packets taking different paths, and better error correction capability.\nFor these reasons, this technique is the basis for long-haul communication networks such as the\nInternet, where error rates, number of hops, and variation in network state can be higher. Of\ncourse, the overhead here is that each packet must carry routing, error correction, and\nsequencing information.\nConsider the transfer of an m word message through the network. The time taken for\nprogramming the network interfaces and computing the routing information, etc., is\nindependent of the message length. This is aggregated into the startup time ts of the message\ntransfer. We assume a scenario in which routing tables are static over the time of message\ntransfer (i.e., all packets traverse the same path). While this is not a valid assumption under all\ncircumstances, it serves the purpose of motivating a cost model for message transfer. The\nmessage is broken into packets, and packets are assembled with their error, routing, and\nsequencing fields. The size of a packet is now given by r + s, where r is the original message\nand s is the additional information carried in the packet. The time for packetizing the message\nis proportional to the length of the message. We denote this time by mtw1. If the network is\ncapable of communicating one word every tw2 seconds, incurs a delay of th on each hop, and if\nthe first packet", "doc_id": "a325fd42-232b-44a0-bf14-53c527fab714", "embedding": null, "doc_hash": "e4cfaae1c68c3cc0f62abe65ffb04683fc37a8220c6316c2bdf2309b3cbf5f73", "extra_info": null, "node_info": {"start": 179377, "end": 183439}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6cab64a2-7131-45cf-99f9-b4b42dd32f96", "3": "f88a87a0-a62c-4b61-8e2a-981bdef3012f"}}, "__type__": "1"}, "f88a87a0-a62c-4b61-8e2a-981bdef3012f": {"__data__": {"text": "time ts of the message\ntransfer. We assume a scenario in which routing tables are static over the time of message\ntransfer (i.e., all packets traverse the same path). While this is not a valid assumption under all\ncircumstances, it serves the purpose of motivating a cost model for message transfer. The\nmessage is broken into packets, and packets are assembled with their error, routing, and\nsequencing fields. The size of a packet is now given by r + s, where r is the original message\nand s is the additional information carried in the packet. The time for packetizing the message\nis proportional to the length of the message. We denote this time by mtw1. If the network is\ncapable of communicating one word every tw2 seconds, incurs a delay of th on each hop, and if\nthe first packet traverses l hops, then this packet takes time thl + tw2(r + s) to reach the\ndestination. After this time, the destination node receives an additional packet every tw2(r + s)\nseconds. Since there are m/r - 1 additional packets, the total communication time is given by:\nwhere\nPacket routing is suited to networks with highly dynamic states and higher error rates, such as\nlocal- and wide-area networks. This is because individual packets may take different routes and\nretransmissions can be localized to lost packets.\nCut-Through Routing\nIn interconnection networks for parallel computers, additional restrictions can be imposed on\nmessage transfers to further reduce the overheads associated with packet switching. By forcing\nall packets to take the same path, we can eliminate the overhead of transmitting routing\ninformation with each packet. By forcing in-sequence delivery, sequencing information can be\neliminated. By associating error information at message level rather than packet level, the\noverhead associated with error detection and correction can be reduced. Finally, since error\nrates in interconnection networks for parallel machines are extremely low, lean error detection\nmechanisms can be used instead of expensive error correction schemes.\nThe routing scheme resulting from these optimizations is called cut-through routing. In cut-\nthrough routing, a message is broken into fixed size units called flow control digits  or flits.\nSince flits do not contain the overheads of packets, they can be much smaller than packets. A\ntracer is first sent from the source to the destination node to establish a connection. Once a\nconnection has been established, the flits are sent one after the other. All flits follow the same\npath in a dovetailed fashion. An intermediate node does not wait for the entire message to\narrive before forwarding it. As soon as a flit is received at an intermediate node, the flit is\npassed on to the next node. Unlike store-and-forward routing, it is no longer necessary to have\nbuffer space at each intermediate node to store the entire message. Therefore, cut-through\nrouting uses less memory and memory bandwidth at intermediate nodes, and is faster.\nConsider a message that is traversing such a network. If the message traverses l links, and th is\nthe per-hop time, then the header of the message takes time lth to reach the destination. If the\nmessage is m words long, then the entire message arrives in time twm after the arrival of the\nheader of the message. Therefore, the total communication time for cut-through routing is\nEquation 2.3\nThis time is an improvement over store-and-forward routing since terms corresponding to\nnumber of hops and number of words are additive as opposed to multiplicative in the former.\nNote that if the communication is between nearest neighbors (that is, l = 1), or if the message\nsize is small, then the communication time is similar for store-and-forward and cut-through\nrouting schemes.\nMost current parallel computers and many local area networks support cut-through routing. The\nsize of a flit is determined by a variety of network parameters. The control circuitry must\noperate at the flit rate. Therefore, if we select a very small flit size, for a given link bandwidth,\nthe required flit rate becomes large. This poses considerable", "doc_id": "f88a87a0-a62c-4b61-8e2a-981bdef3012f", "embedding": null, "doc_hash": "68d2963853c53315b330fd3a96ae01e98bfab62fd2874a599fbb3ec356b1face", "extra_info": null, "node_info": {"start": 183446, "end": 187555}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a325fd42-232b-44a0-bf14-53c527fab714", "3": "03f73cd0-c882-40d9-a6a2-7e725ad5baf0"}}, "__type__": "1"}, "03f73cd0-c882-40d9-a6a2-7e725ad5baf0": {"__data__": {"text": "the total communication time for cut-through routing is\nEquation 2.3\nThis time is an improvement over store-and-forward routing since terms corresponding to\nnumber of hops and number of words are additive as opposed to multiplicative in the former.\nNote that if the communication is between nearest neighbors (that is, l = 1), or if the message\nsize is small, then the communication time is similar for store-and-forward and cut-through\nrouting schemes.\nMost current parallel computers and many local area networks support cut-through routing. The\nsize of a flit is determined by a variety of network parameters. The control circuitry must\noperate at the flit rate. Therefore, if we select a very small flit size, for a given link bandwidth,\nthe required flit rate becomes large. This poses considerable challenges for designing routers as\nit requires the control circuitry to operate at a very high speed. On the other hand, as flit sizes\nbecome large, internal buffer sizes increase, so does the latency of message transfer. Both of\nthese are undesirable. Flit sizes in recent cut-through interconnection networks range from four\nbits to 32 bytes. In many parallel programming paradigms that rely predominantly on short\nmessages (such as cache lines), the latency of messages is critical. For these, it is unreasonable\nfor a long message traversing a link to hold up a short message. Such scenarios are addressed\nin routers using multilane cut-through routing. In multilane cut-through routing, a single\nphysical channel is split into a number of virtual channels.\nMessaging constants ts, tw, and th are determined by hardware characteristics, software layers,\nand messaging semantics. Messaging semantics associated with paradigms such as message\npassing are best served by variable length messages, others by fixed length short messages.\nWhile effective bandwidth may be critical for the former, reducing latency is more important for\nthe latter. Messaging layers for these paradigms are tuned to reflect these requirements.\nWhile traversing the network, if a message needs to use a link that is currently in use, then the\nmessage is blocked. This may lead to deadlock. Figure 2.27 illustrates deadlock in a cut-through\nrouting network. The destinations of messages 0, 1, 2, and 3 are A, B, C, and D, respectively. A\nflit from message 0 occupies the link CB (and the associated buffers). However, since link BA is\noccupied by a flit from message 3, the flit from message 0 is blocked. Similarly, the flit from\nmessage 3 is blocked since link AD is in use. We can see that no messages can progress in the\nnetwork and the network is deadlocked. Deadlocks can be avoided in cut-through networks by\nusing appropriate routing techniques and message buffers. These are discussed in Section 2.6 .\nFigure 2.27. An example of deadlock in a cut-through routing network.\nA Simplified Cost Model for Communicating Messages\nAs we have just seen in Section 2.5.1 , the cost of communicating a message between two nodes\nl hops away using cut-through routing is given by\nThis equation implies that in order to optimize the cost of message transfers, we would need to:\nCommunicate in bulk.  That is, instead of sending small messages and paying a startup\ncost ts for each, we want to aggregate small messages into a single large message and\namortize the startup latency across a larger message. This is because on typical platforms\nsuch as clusters and message-passing machines, the value of ts is much larger than those\nof th or tw.1.\nMinimize the volume of data.  To minimize the overhead paid in terms of per-word\ntransfer time tw, it is desirable to reduce the volume of data communicated as much as2.\n3.\npossible.2.\nMinimize distance of data transfer.  Minimize the number of hops l that a message must\ntraverse.3.\nWhile the first two objectives are relatively easy to achieve, the task of minimizing distance of\ncommunicating nodes is difficult, and in many cases an unnecessary burden on the", "doc_id": "03f73cd0-c882-40d9-a6a2-7e725ad5baf0", "embedding": null, "doc_hash": "fe82140a0a4da082a1658e314d0950f581fbd2c5407beb7d5499bb16686f9b1b", "extra_info": null, "node_info": {"start": 187526, "end": 191511}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f88a87a0-a62c-4b61-8e2a-981bdef3012f", "3": "8997d91b-9461-4613-8f1e-7cfad75c3dd6"}}, "__type__": "1"}, "8997d91b-9461-4613-8f1e-7cfad75c3dd6": {"__data__": {"text": "ts for each, we want to aggregate small messages into a single large message and\namortize the startup latency across a larger message. This is because on typical platforms\nsuch as clusters and message-passing machines, the value of ts is much larger than those\nof th or tw.1.\nMinimize the volume of data.  To minimize the overhead paid in terms of per-word\ntransfer time tw, it is desirable to reduce the volume of data communicated as much as2.\n3.\npossible.2.\nMinimize distance of data transfer.  Minimize the number of hops l that a message must\ntraverse.3.\nWhile the first two objectives are relatively easy to achieve, the task of minimizing distance of\ncommunicating nodes is difficult, and in many cases an unnecessary burden on the algorithm\ndesigner. This is a direct consequence of the following characteristics of parallel platforms and\nparadigms:\nIn many message-passing libraries such as MPI, the programmer has little control on the\nmapping of processes onto physical processors. In such paradigms, while tasks might have\nwell defined topologies and may communicate only among neighbors in the task topology,\nthe mapping of processes to nodes might destroy this structure.Many architectures rely on randomized (two-step) routing, in which a message is first sent\nto a random node from source and from this intermediate node to the destination. This\nalleviates hot-spots and contention on the network. Minimizing number of hops in a\nrandomized routing network yields no benefits.\nThe per-hop time ( th ) is typically dominated either by the startup latency ( ts )for small\nmessages or by per-word component ( twm) for large messages. Since the maximum\nnumber of hops ( l) in most networks is relatively small, the per-hop time can be ignored\nwith little loss in accuracy.\nAll of these point to a simpler cost model in which the cost of transferring a message between\ntwo nodes on a network is given by:\nEquation 2.4\nThis expression has significant implications for architecture-independent algorithm design as\nwell as for the accuracy of runtime predictions. Since this cost model implies that it takes the\nsame amount of time to communicate between any pair of nodes, it corresponds to a completely\nconnected network. Instead of designing algorithms for each specific architecture (for example,\na mesh, hypercube, or tree), we can design algorithms with this cost model in mind and port it\nto any target parallel computer.\nThis raises the important issue of loss of accuracy (or fidelity) of prediction when the algorithm\nis ported from our simplified model (which assumes a completely connected network) to an\nactual machine architecture. If our initial assumption that the th term is typically dominated by\nthe ts or tw terms is valid, then the loss in accuracy should be minimal.\nHowever, it is important to note that our basic cost model is valid only for uncongested\nnetworks. Architectures have varying thresholds for when they get congested; i.e., a linear\narray has a much lower threshold for congestion than a hypercube. Furthermore, different\ncommunication patterns congest a given network to different extents. Consequently, our\nsimplified cost model is valid only as long as the underlying communication pattern does not\ncongest the network.\nExample 2.15 Effect of congestion on communication cost\nConsider a \n  mesh in which each node is only communicating with its nearest\nneighbor. Since no links in the network are used for more than one communication,\nthe time for this operation is ts + twm, where m is the number of words\ncommunicated. This time is consistent with our simplified model.\nConsider an alternate scenario in which each node is communicating with a randomly\nselected node. This randomness implies that there are p/2 communications (or p/4 bi-\ndirectional communications) occurring across any equi-partition of the machine (since\nthe node being communicated with could be in either half with equal probability).\nFrom our discussion of bisection width, we know that a 2-D mesh has a bisection\nwidth of \n. From these two, we can infer that some links would now have to carry", "doc_id": "8997d91b-9461-4613-8f1e-7cfad75c3dd6", "embedding": null, "doc_hash": "9174bf9c3f00e7ac06ae06ede5ddaae9a27cac6539edd1fdb99c6821140ce2c4", "extra_info": null, "node_info": {"start": 191575, "end": 195690}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "03f73cd0-c882-40d9-a6a2-7e725ad5baf0", "3": "e267c7ee-ad0c-4fd9-b430-65e2513aaa0e"}}, "__type__": "1"}, "e267c7ee-ad0c-4fd9-b430-65e2513aaa0e": {"__data__": {"text": "on communication cost\nConsider a \n  mesh in which each node is only communicating with its nearest\nneighbor. Since no links in the network are used for more than one communication,\nthe time for this operation is ts + twm, where m is the number of words\ncommunicated. This time is consistent with our simplified model.\nConsider an alternate scenario in which each node is communicating with a randomly\nselected node. This randomness implies that there are p/2 communications (or p/4 bi-\ndirectional communications) occurring across any equi-partition of the machine (since\nthe node being communicated with could be in either half with equal probability).\nFrom our discussion of bisection width, we know that a 2-D mesh has a bisection\nwidth of \n. From these two, we can infer that some links would now have to carry at\nleast \n  messages, assuming bi-directional communication channels. These\nmessages must be serialized over the link. If each message is of size m, the time for\nthis operation is at least \n . This time is not in conformity with our\nsimplified model. \nThe above example illustrates that for a given architecture, some communication patterns can\nbe non-congesting and others may be congesting. This makes the task of modeling\ncommunication costs dependent not just on the architecture, but also on the communication\npattern. To address this, we introduce the notion of effective bandwidth . For communication\npatterns that do not congest the network, the effective bandwidth is identical to the link\nbandwidth. However, for communication operations that congest the network, the effective\nbandwidth is the link bandwidth scaled down by the degree of congestion on the most\ncongested link. This is often difficult to estimate since it is a function of process to node\nmapping, routing algorithms, and communication schedule. Therefore, we use a lower bound on\nthe message communication time. The associated link bandwidth is scaled down by a factor p/b,\nwhere b is the bisection width of the network.\nIn the rest of this text, we will work with the simplified communication model for message\npassing with effective per-word time tw because it allows us to design algorithms in an\narchitecture-independent manner. We will also make specific notes on when a communication\noperation within an algorithm congests the network and how its impact is factored into parallel\nruntime. The communication times in the book apply to the general class of k-d meshes. While\nthese times may be realizable on other architectures as well, this is a function of the underlying\narchitecture.\n2.5.2 Communication Costs in Shared-Address-Space Machines\nThe primary goal of associating communication costs with parallel programs is to associate a\nfigure of merit with a program to guide program development. This task is much more difficult\nfor cache-coherent shared-address-space machines than for message-passing or non-cache-\ncoherent architectures. The reasons for this are as follows:\nMemory layout is typically determined by the system. The programmer has minimal\ncontrol on the location of specific data items over and above permuting data structures to\noptimize access. This is particularly important in distributed memory shared-address-\nspace architectures because it is difficult to identify local and remote accesses. If the\naccess times for local and remote data items are significantly different, then the cost of\ncommunication can vary greatly depending on the data layout.\nFinite cache sizes can result in cache thrashing. Consider a scenario in which a node needs\na certain fraction of the total data to compute its results. If this fraction is smaller than\nlocally available cache, the data can be fetched on first access and computed on. However,\nif the fraction exceeds available cache, then certain portions of this data might get\noverwritten, and consequently accessed several times. This overhead can cause sharp\ndegradation in program performance as the problem size is increased. To remedy this, the\nprogrammer must alter execution schedules (e.g., blocking loops as illustrated in serial\nmatrix multiplication in Problem 2.5) for minimizing working set size. While this problem is\ncommon to both serial and multiprocessor platforms, the penalty is much higher in the\ncase of multiprocessors", "doc_id": "e267c7ee-ad0c-4fd9-b430-65e2513aaa0e", "embedding": null, "doc_hash": "157e2613ec68c1c2d7af130cb4bd0a84db26aac9f908709411201a8362b1932e", "extra_info": null, "node_info": {"start": 195627, "end": 199933}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8997d91b-9461-4613-8f1e-7cfad75c3dd6", "3": "29232692-2ecd-4ca2-86de-53a41d5e7b21"}}, "__type__": "1"}, "29232692-2ecd-4ca2-86de-53a41d5e7b21": {"__data__": {"text": "can result in cache thrashing. Consider a scenario in which a node needs\na certain fraction of the total data to compute its results. If this fraction is smaller than\nlocally available cache, the data can be fetched on first access and computed on. However,\nif the fraction exceeds available cache, then certain portions of this data might get\noverwritten, and consequently accessed several times. This overhead can cause sharp\ndegradation in program performance as the problem size is increased. To remedy this, the\nprogrammer must alter execution schedules (e.g., blocking loops as illustrated in serial\nmatrix multiplication in Problem 2.5) for minimizing working set size. While this problem is\ncommon to both serial and multiprocessor platforms, the penalty is much higher in the\ncase of multiprocessors since each miss might now involve coherence operations and\ninterprocessor communication.Overheads associated with invalidate and update operations are difficult to quantify. After\na data item has been fetched by a processor into cache, it may be subject to a variety of\noperations at another processor. For example, in an invalidate protocol, the cache line\nmight be invalidated by a write operation at a remote processor. In this case, the next\nread operation on the data item must pay a remote access latency cost again. Similarly,\nthe overhead associated with an update protocol might vary significantly depending on the\nnumber of copies of a data item. The number of concurrent copies of a data item and the\nschedule of instruction execution are typically beyond the control of the programmer.Spatial locality is difficult to model. Since cache lines are generally longer than one word\n(anywhere from four to 128 words), different words might have different access latencies\nassociated with them even for the first access. Accessing a neighbor of a previously fetched\nword might be extremely fast, if the cache line has not yet been overwritten. Once again,\nthe programmer has minimal control over this, other than to permute data structures to\nmaximize spatial locality of data reference.Prefetching can play a role in reducing the overhead associated with data access.\nCompilers can advance loads and, if sufficient resources exist, the overhead associated\nwith these loads may be completely masked. Since this is a function of the compiler, the\nunderlying program, and availability of resources (registers/cache), it is very difficult to\nmodel accurately.False sharing is often an important overhead in many programs. Two words used by\n(threads executing on) different processor may reside on the same cache line. This may\ncause coherence actions and communication overheads, even though none of the data\nmight be shared. The programmer must adequately pad data structures used by various\nprocessors to minimize false sharing.Contention in shared accesses is often a major contributing overhead in shared address\nspace machines. Unfortunately, contention is a function of execution schedule and\nconsequently very difficult to model accurately (independent of the scheduling algorithm).\nWhile it is possible to get loose asymptotic estimates by counting the number of shared\naccesses, such a bound is often not very meaningful.Any cost model for shared-address-space machines must account for all of these overheads.\nBuilding these into a single cost model results in a model that is too cumbersome to design\nprograms for and too specific to individual machines to be generally applicable.\nAs a first-order model, it is easy to see that accessing a remote word results in a cache line\nbeing fetched into the local cache. The time associated with this includes the coherence\noverheads, network overheads, and memory overheads. The coherence and network overheads\nare functions of the underlying interconnect (since a coherence operation must be potentially\npropagated to remote processors and the data item must be fetched). In the absence of\nknowledge of what coherence operations are associated with a specific access and where the\nword is coming from, we associate a constant overhead to accessing a cache line of the shared\ndata. For the sake of uniformity with the message-passing model, we refer to this cost as ts.\nBecause of various latency-hiding protocols, such as prefetching, implemented in modern\nprocessor architectures, we assume that a constant cost of ts", "doc_id": "29232692-2ecd-4ca2-86de-53a41d5e7b21", "embedding": null, "doc_hash": "ed3ad4602f182d2995ae8bc8131e5bbb42811a53f045fe31accd450a1b098dc5", "extra_info": null, "node_info": {"start": 199930, "end": 204315}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e267c7ee-ad0c-4fd9-b430-65e2513aaa0e", "3": "617d6efb-3459-4844-b5cf-41c894dbc945"}}, "__type__": "1"}, "617d6efb-3459-4844-b5cf-41c894dbc945": {"__data__": {"text": "see that accessing a remote word results in a cache line\nbeing fetched into the local cache. The time associated with this includes the coherence\noverheads, network overheads, and memory overheads. The coherence and network overheads\nare functions of the underlying interconnect (since a coherence operation must be potentially\npropagated to remote processors and the data item must be fetched). In the absence of\nknowledge of what coherence operations are associated with a specific access and where the\nword is coming from, we associate a constant overhead to accessing a cache line of the shared\ndata. For the sake of uniformity with the message-passing model, we refer to this cost as ts.\nBecause of various latency-hiding protocols, such as prefetching, implemented in modern\nprocessor architectures, we assume that a constant cost of ts is associated with initiating access\nto a contiguous chunk of m words of shared data, even if m is greater than the cache line size.\nWe further assume that accessing shared data is costlier than accessing local data (for instance,\non a NUMA machine, local data is likely to reside in a local memory module, while data shared\nby p processors will need to be fetched from a nonlocal module for at least p - 1 processors).\nTherefore, we assign a per-word access cost of tw to shared data.\nFrom the above discussion, it follows that we can use the same expression t s + twm to account\nfor the cost of sharing a single chunk of m words between a pair of processors in both shared-\nmemory and message-passing paradigms ( Equation 2.4) with the difference that the value of\nthe constant ts relative to tw is likely to be much smaller on a shared-memory machine than on\na distributed memory machine ( tw is likely to be near zero for a UMA machine). Note that the\ncost ts + twm assumes read-only access without contention. If multiple processes access the\nsame data, then the cost is multiplied by the number of processes, just as in the message-\npassing where the process that owns the data will need to send a message to each receiving\nprocess. If the access is read-write, then the cost will be incurred again for subsequent access\nby processors other than the one writing. Once again, there is an equivalence with the\nmessage-passing model. If a process modifies the contents of a message that it receives, then it\nmust send it back to processes that subsequently need access to the refreshed data. While this\nmodel seems overly simplified in the context of shared-address-space machines, we note that\nthe model provides a good estimate of the cost of sharing an array of m words between a pair\nof processors.\nThe simplified model presented above accounts primarily for remote data access but does not\nmodel a variety of other overheads. Contention for shared data access must be explicitly\naccounted for by counting the number of accesses to shared data between co-scheduled tasks.\nThe model does not explicitly include many of the other overheads. Since different machines\nhave caches of varying sizes, it is difficult to identify the point at which working set size exceeds\nthe cache size resulting in cache thrashing, in an architecture independent manner. For this\nreason, effects arising from finite caches are ignored in this cost model. Maximizing spatial\nlocality (cache line effects) is not explicitly included in the cost. False sharing is a function of the\ninstruction schedules as well as data layouts. The cost model assumes that shared data\nstructures are suitably padded and, therefore, does not include false sharing costs. Finally, the\ncost model does not account for overlapping communication and computation. Other models\nhave been proposed to model overlapped communication. However, designing even simple\nalgorithms for these models is cumbersome. The related issue of multiple concurrent\ncomputations (threads) on a single processor is not modeled in the expression. Instead, each\nprocessor is assumed to execute a single concurrent unit of computation.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.6 Routing Mechanisms for Interconnection Networks\nEfficient algorithms for routing a message to its destination", "doc_id": "617d6efb-3459-4844-b5cf-41c894dbc945", "embedding": null, "doc_hash": "1b62d9631b6fd37fbb21b6008385f23881d8f205008d1615923cb469c176204e", "extra_info": null, "node_info": {"start": 204288, "end": 208449}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "29232692-2ecd-4ca2-86de-53a41d5e7b21", "3": "463e9172-94ff-4eca-a179-9447806f5407"}}, "__type__": "1"}, "463e9172-94ff-4eca-a179-9447806f5407": {"__data__": {"text": "explicitly included in the cost. False sharing is a function of the\ninstruction schedules as well as data layouts. The cost model assumes that shared data\nstructures are suitably padded and, therefore, does not include false sharing costs. Finally, the\ncost model does not account for overlapping communication and computation. Other models\nhave been proposed to model overlapped communication. However, designing even simple\nalgorithms for these models is cumbersome. The related issue of multiple concurrent\ncomputations (threads) on a single processor is not modeled in the expression. Instead, each\nprocessor is assumed to execute a single concurrent unit of computation.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.6 Routing Mechanisms for Interconnection Networks\nEfficient algorithms for routing a message to its destination are critical to the performance of\nparallel computers. A routing mechanism  determines the path a message takes through the\nnetwork to get from the source to the destination node. It takes as input a message's source\nand destination nodes. It may also use information about the state of the network. It returns\none or more paths through the network from the source to the destination node.\nRouting mechanisms can be classified as minimal  or non-minimal . A minimal routing\nmechanism always selects one of the shortest paths between the source and the destination. In\na minimal routing scheme, each link brings a message closer to its destination, but the scheme\ncan lead to congestion in parts of the network. A non-minimal routing scheme, in contrast, may\nroute the message along a longer path to avoid network congestion.\nRouting mechanisms can also be classified on the basis of how they use information regarding\nthe state of the network. A deterministic routing  scheme determines a unique path for a\nmessage, based on its source and destination. It does not use any information regarding the\nstate of the network. Deterministic schemes may result in uneven use of the communication\nresources in a network. In contrast, an adaptive routing  scheme uses information regarding\nthe current state of the network to determine the path of the message. Adaptive routing detects\ncongestion in the network and routes messages around it.\nOne commonly used deterministic minimal routing technique is called dimension-ordered\nrouting . Dimension-ordered routing assigns successive channels for traversal by a message\nbased on a numbering scheme determined by the dimension of the channel. The dimension-\nordered routing technique for a two-dimensional mesh is called XY-routing  and that for a\nhypercube is called E-cube routing .\nConsider a two-dimensional mesh without wraparound connections. In the XY-routing scheme,\na message is sent first along the X dimension until it reaches the column of the destination node\nand then along the Y dimension until it reaches its destination. Let PSy,Sx represent the position\nof the source node and PDy,Dx represent that of the destination node. Any minimal routing\nscheme should return a path of length | Sx - Dx| + |Sy - Dy|. Assume that Dx \n Sx and Dy \nSy. In the XY-routing scheme, the message is passed through intermediate nodes PSy,Sx+1,\nPSy,Sx+2, ..., PSy,Dx along the X dimension and then through nodes PSy+1,Dx, PSy+2,Dx, ..., PDy,Dx\nalong the Y dimension to reach the destination. Note that the length of this path is indeed | Sx -\nDx| + |Sy - Dy|.\nE-cube routing for hypercube-connected networks works similarly. Consider a d-dimensional\nhypercube of p nodes. Let Ps and Pd be the labels of the source and destination nodes. We know\nfrom Section 2.4.3  that the binary representations of these labels are d bits long. Furthermore,\nthe minimum distance between these nodes is given by the number of ones in Ps \n Pd (where\n represents the bitwise exclusive-OR operation). In the E-cube algorithm, node Ps computes\nPs \n Pd and", "doc_id": "463e9172-94ff-4eca-a179-9447806f5407", "embedding": null, "doc_hash": "d39b0923f90983b8ad6357b0c52efdfffe8e2d6cbdf271170f7f284fd6c0ff9f", "extra_info": null, "node_info": {"start": 208463, "end": 212356}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "617d6efb-3459-4844-b5cf-41c894dbc945", "3": "ac21383f-f7ff-4093-b0ed-1d511681136b"}}, "__type__": "1"}, "ac21383f-f7ff-4093-b0ed-1d511681136b": {"__data__": {"text": "PSy,Dx along the X dimension and then through nodes PSy+1,Dx, PSy+2,Dx, ..., PDy,Dx\nalong the Y dimension to reach the destination. Note that the length of this path is indeed | Sx -\nDx| + |Sy - Dy|.\nE-cube routing for hypercube-connected networks works similarly. Consider a d-dimensional\nhypercube of p nodes. Let Ps and Pd be the labels of the source and destination nodes. We know\nfrom Section 2.4.3  that the binary representations of these labels are d bits long. Furthermore,\nthe minimum distance between these nodes is given by the number of ones in Ps \n Pd (where\n represents the bitwise exclusive-OR operation). In the E-cube algorithm, node Ps computes\nPs \n Pd and sends the message along dimension k, where k is the position of the least\nsignificant nonzero bit in Ps \n Pd . At each intermediate step, node Pi , which receives the\nmessage, computes Pi \n Pd and forwards the message along the dimension corresponding to\nthe least significant nonzero bit. This process continues until the message reaches its\ndestination. Example 2.16 illustrates E-cube routing in a three-dimensional hypercube network.\nExample 2.16 E-cube routing in a hypercube network\nConsider the three-dimensional hypercube shown in Figure 2.28 . Let P s = 010 and Pd\n= 111 represent the source and destination nodes for a message. Node Ps computes\n010 \n  111 = 101. In the first step, Ps forwards the message along the dimension\ncorresponding to the least significant bit to node 011. Node 011 sends the message\nalong the dimension corresponding to the most significant bit (011 \n  111 = 100).\nThe message reaches node 111, which is the destination of the message. \nFigure 2.28. Routing a message from node Ps (010) to node Pd\n(111) in a three-dimensional hypercube using E-cube routing.\nIn the rest of this book we assume deterministic and minimal message routing for analyzing\nparallel algorithms.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.7 Impact of Process-Processor Mapping and Mapping\nTechniques\nAs we have discussed in Section 2.5.1 , a programmer often does not have control over how\nlogical processes are mapped to physical nodes in a network. For this reason, even\ncommunication patterns that are not inherently congesting may congest the network. We\nillustrate this with the following example:\nExample 2.17 Impact of process mapping\nConsider the scenario illustrated in Figure 2.29. The underlying architecture is a 16-\nnode mesh with nodes labeled from 1 to 16 ( Figure 2.29(a) ) and the algorithm has\nbeen implemented as 16 processes, labeled 'a' through 'p' ( Figure 2.29(b) ). The\nalgorithm has been tuned for execution on a mesh in such a way that there are no\ncongesting communication operations. We now consider two mappings of the\nprocesses to nodes as illustrated in Figures 2.29(c) and (d). Figure 2.29(c)  is an\nintuitive mapping and is such that a single link in the underlying architecture only\ncarries data corresponding to a single communication channel between processes.\nFigure 2.29(d), on the other hand, corresponds to a situation in which processes have\nbeen mapped randomly to processing nodes. In this case, it is easy to see that each\nlink in the machine carries up to six channels of data between processes. This may\npotentially result in considerably larger communication times if the required data rates\non communication channels between processes is high. \nFigure 2.29. Impact of process mapping on performance: (a)\nunderlying architecture; (b) processes and their interactions;\n(c) an intuitive mapping of processes to nodes; and (d) a\nrandom mapping of processes to nodes.\nIt is evident from the above example that while an algorithm may be fashioned out of non-\ncongesting communication operations, the mapping of processes to nodes may in fact induce\ncongestion on the", "doc_id": "ac21383f-f7ff-4093-b0ed-1d511681136b", "embedding": null, "doc_hash": "75b6a9bde8a901034b072cef76f2a4ea2c4c35df273456a9258336738888199b", "extra_info": null, "node_info": {"start": 212497, "end": 216284}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "463e9172-94ff-4eca-a179-9447806f5407", "3": "669c5a06-a208-4579-abf6-39fe1d563d2e"}}, "__type__": "1"}, "669c5a06-a208-4579-abf6-39fe1d563d2e": {"__data__": {"text": "on the other hand, corresponds to a situation in which processes have\nbeen mapped randomly to processing nodes. In this case, it is easy to see that each\nlink in the machine carries up to six channels of data between processes. This may\npotentially result in considerably larger communication times if the required data rates\non communication channels between processes is high. \nFigure 2.29. Impact of process mapping on performance: (a)\nunderlying architecture; (b) processes and their interactions;\n(c) an intuitive mapping of processes to nodes; and (d) a\nrandom mapping of processes to nodes.\nIt is evident from the above example that while an algorithm may be fashioned out of non-\ncongesting communication operations, the mapping of processes to nodes may in fact induce\ncongestion on the network and cause degradation in performance.\n2.7.1 Mapping Techniques for Graphs\nWhile the programmer generally does not have control over process-processor mapping, it is\nimportant to understand algorithms for such mappings. This is because these mappings can be\nused to determine degradation in the performance of an algorithm. Given two graphs, G(V, E)\nand G'(V', E'), mapping graph G into graph G' maps each vertex in the set V onto a vertex (or a\nset of vertices) in set V' and each edge in the set E onto an edge (or a set of edges) in E'. When\nmapping graph G(V, E) into G'(V', E'), three parameters are important. First, it is possible that\nmore than one edge in E is mapped onto a single edge in E'. The maximum number of edges\nmapped onto any edge in E' is called the congestion  of the mapping. In Example 2.17, the\nmapping in Figure 2.29(c)  has a congestion of one and that in Figure 2.29(d)  has a congestion\nof six. Second, an edge in E may be mapped onto multiple contiguous edges in E'. This is\nsignificant because traffic on the corresponding communication link must traverse more than\none link, possibly contributing to congestion on the network. The maximum number of links in\nE' that any edge in E is mapped onto is called the dilation  of the mapping. Third, the sets V and\nV' may contain different numbers of vertices. In this case, a node in V corresponds to more than\none node in V'. The ratio of the number of nodes in the set V' to that in set V is called the\nexpansion  of the mapping. In the context of process-processor mapping, we want the\nexpansion of the mapping to be identical to the ratio of virtual and physical processors.\nIn this section, we discuss embeddings of some commonly encountered graphs such as 2-D\nmeshes (matrix operations illustrated in Chapter 8 ), hypercubes (sorting and FFT algorithms in\nChapters 9 and 13, respectively), and trees (broadcast, barriers in Chapter 4 ). We limit the\nscope of the discussion to cases in which sets V and V' contain an equal number of nodes (i.e.,\nan expansion of one).\nEmbedding a Linear Array into a Hypercube\nA linear array (or a ring) composed of 2d nodes (labeled 0 through 2d -1) can be embedded into\na d-dimensional hypercube by mapping node i of the linear array onto node G(i, d) of the\nhypercube. The function G(i, x) is defined as follows:\nThe function G is called the binary reflected Gray code  (RGC). The entry G(i, d) denotes the i\nth entry in the sequence of Gray codes of d bits. Gray codes of d + 1 bits are derived from a\ntable of Gray codes of d bits by reflecting the table and prefixing the reflected entries with a 1\nand the original entries with a 0. This process is illustrated in Figure 2.30(a) .\nFigure 2.30. (a) A three-bit reflected Gray code ring; and (b) its\nembedding into a three-dimensional hypercube.\nA careful look at the Gray code table reveals that two adjoining entries (", "doc_id": "669c5a06-a208-4579-abf6-39fe1d563d2e", "embedding": null, "doc_hash": "b7626d69243d19701ee7043120dc4f2940d64161a548effc89e061dc01b747ff", "extra_info": null, "node_info": {"start": 216173, "end": 219866}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ac21383f-f7ff-4093-b0ed-1d511681136b", "3": "3d4652a7-6e62-4fff-ab3c-7be155cf8eac"}}, "__type__": "1"}, "3d4652a7-6e62-4fff-ab3c-7be155cf8eac": {"__data__": {"text": "d-dimensional hypercube by mapping node i of the linear array onto node G(i, d) of the\nhypercube. The function G(i, x) is defined as follows:\nThe function G is called the binary reflected Gray code  (RGC). The entry G(i, d) denotes the i\nth entry in the sequence of Gray codes of d bits. Gray codes of d + 1 bits are derived from a\ntable of Gray codes of d bits by reflecting the table and prefixing the reflected entries with a 1\nand the original entries with a 0. This process is illustrated in Figure 2.30(a) .\nFigure 2.30. (a) A three-bit reflected Gray code ring; and (b) its\nembedding into a three-dimensional hypercube.\nA careful look at the Gray code table reveals that two adjoining entries ( G(i, d) and G(i + 1, d))\ndiffer from each other at only one bit position. Since node i in the linear array is mapped to\nnode G(i, d), and node i + 1 is mapped to G(i + 1, d), there is a direct link in the hypercube\nthat corresponds to each direct link in the linear array. (Recall that two nodes whose labels\ndiffer at only one bit position have a direct link in a hypercube.) Therefore, the mapping\nspecified by the function G has a dilation of one and a congestion of one. Figure 2.30(b)\nillustrates the embedding of an eight-node ring into a three-dimensional hypercube.\nEmbedding a Mesh into a Hypercube\nEmbedding a mesh into a hypercube is a natural extension of embedding a ring into a\nhypercube. We can embed a 2r x 2s wraparound mesh into a 2r+s -node hypercube by\nmapping node ( i, j) of the mesh onto node G(i, r - 1)|| G( j, s - 1) of the hypercube (where ||\ndenotes concatenation of the two Gray codes). Note that immediate neighbors in the mesh are\nmapped to hypercube nodes whose labels differ in exactly one bit position. Therefore, this\nmapping has a dilation of one and a congestion of one.\nFor example, consider embedding a 2 x 4 mesh into an eight-node hypercube. The values of r\nand s are 1 and 2, respectively. Node ( i, j) of the mesh is mapped to node G(i, 1)|| G( j, 2) of\nthe hypercube. Therefore, node (0, 0) of the mesh is mapped to node 000 of the hypercube,\nbecause G(0, 1) is 0 and G(0, 2) is 00; concatenating the two yields the label 000 for the\nhypercube node. Similarly, node (0, 1) of the mesh is mapped to node 001 of the hypercube,\nand so on. Figure 2.31 illustrates embedding meshes into hypercubes.\nFigure 2.31. (a) A 4 x 4 mesh illustrating the mapping of mesh nodes\nto the nodes in a four-dimensional hypercube; and (b) a 2 x 4 mesh\nembedded into a three-dimensional hypercube.\nThis mapping of a mesh into a hypercube has certain useful properties. All nodes in\nthe same row of the mesh are mapped to hypercube nodes whose labels have r identical most\nsignificant bits. We know from Section 2.4.3  that fixing any r bits in the node label of an ( r +\ns)-dimensional hypercube yields a subcube of dimension s with 2s nodes. Since each mesh node\nis mapped onto a unique node in the hypercube, and each row in the mesh has 2s nodes, every\nrow in the mesh is mapped to a distinct subcube in the hypercube. Similarly, each column in the\nmesh is mapped to a distinct subcube in the hypercube.\nEmbedding a Mesh into a Linear Array\nWe have, up until this point, considered embeddings of sparser networks into denser networks.\nA 2-D mesh has 2 x p links. In contrast, a p-node linear array has p links. Consequently, there\nmust be a congestion associated with this mapping.\nConsider first the mapping", "doc_id": "3d4652a7-6e62-4fff-ab3c-7be155cf8eac", "embedding": null, "doc_hash": "eb66736adcdd03e8b67368ccd6cade60a5f022bb6652d06eea03037e5cf7f6bc", "extra_info": null, "node_info": {"start": 219966, "end": 223400}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "669c5a06-a208-4579-abf6-39fe1d563d2e", "3": "f1e04342-19f3-46d6-9531-9b2ab49aa145"}}, "__type__": "1"}, "f1e04342-19f3-46d6-9531-9b2ab49aa145": {"__data__": {"text": " that fixing any r bits in the node label of an ( r +\ns)-dimensional hypercube yields a subcube of dimension s with 2s nodes. Since each mesh node\nis mapped onto a unique node in the hypercube, and each row in the mesh has 2s nodes, every\nrow in the mesh is mapped to a distinct subcube in the hypercube. Similarly, each column in the\nmesh is mapped to a distinct subcube in the hypercube.\nEmbedding a Mesh into a Linear Array\nWe have, up until this point, considered embeddings of sparser networks into denser networks.\nA 2-D mesh has 2 x p links. In contrast, a p-node linear array has p links. Consequently, there\nmust be a congestion associated with this mapping.\nConsider first the mapping of a linear array into a mesh. We assume that neither the mesh nor\nthe linear array has wraparound connections. An intuitive mapping of a linear array into a mesh\nis illustrated in Figure 2.32. Here, the solid lines correspond to links in the linear array and\nnormal lines to links in the mesh. It is easy to see from Figure 2.32(a)  that a congestion-one,\ndilation-one mapping of a linear array to a mesh is possible.\nFigure 2.32. (a) Embedding a 16 node linear array into a 2-D mesh;\nand (b) the inverse of the mapping. Solid lines correspond to links in\nthe linear array and normal lines to links in the mesh.\nConsider now the inverse of this mapping, i.e., we are given a mesh and we map vertices of the\nmesh to those in a linear array using the inverse of the same mapping function. This mapping is\nillustrated in Figure 2.32(b). As before, the solid lines correspond to edges in the linear array\nand normal lines to edges in the mesh. As is evident from the figure, the congestion of the\nmapping in this case is five \u2013 i.e., no solid line carries more than five normal lines. In general, it\nis easy to show that the congestion of this (inverse) mapping is \n for a general p -node\nmapping (one for each of the \n  edges to the next row, and one additional edge).\nWhile this is a simple mapping, the question at this point is whether we can do better. To\nanswer this question, we use the bisection width of the two networks. We know that the\nbisection width of a 2-D mesh without wraparound links is \n , and that of a linear array is 1.\nAssume that the best mapping of a 2-D mesh into a linear array has a congestion of r. This\nimplies that if we take the linear array and cut it in half (at the middle), we will cut only one\nlinear array link, or no more than r mesh links. We claim that r must be at least equal to the\nbisection width of the mesh. This follows from the fact that an equi-partition of the linear array\ninto two also partitions the mesh into two. Therefore, at least \n  mesh links must cross the\npartition, by definition of bisection width. Consequently, the one linear array link connecting the\ntwo halves must carry at least \n mesh links. Therefore, the congestion of any mapping is\nlower bounded by \n . This is almost identical to the simple (inverse) mapping we have\nillustrated in Figure 2.32(b) .\nThe lower bound established above has a more general applicability when mapping denser\nnetworks to sparser ones. One may reasonably believe that the lower bound on congestion of a\nmapping of network S with x links into network Q with y links is x/y. In the case of the mapping\nfrom a mesh to a linear array, this would be 2 p/p, or 2. However, this lower bound is overly\nconservative. A tighter lower bound is in fact possible by examining the bisection width of the\ntwo networks. We illustrate this further in the next section.\nEmbedding a Hypercube into a 2-D Mesh\nConsider the embedding of a p-node hypercube into a p-node 2-D mesh. For the sake", "doc_id": "f1e04342-19f3-46d6-9531-9b2ab49aa145", "embedding": null, "doc_hash": "4a83424da2c00d5fe151c281d1ab43207316afbd5e0b181ffe667e028be25367", "extra_info": null, "node_info": {"start": 223405, "end": 227071}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3d4652a7-6e62-4fff-ab3c-7be155cf8eac", "3": "fb8c4189-ee99-4e03-bdd2-23a0bda2cc76"}}, "__type__": "1"}, "fb8c4189-ee99-4e03-bdd2-23a0bda2cc76": {"__data__": {"text": "simple (inverse) mapping we have\nillustrated in Figure 2.32(b) .\nThe lower bound established above has a more general applicability when mapping denser\nnetworks to sparser ones. One may reasonably believe that the lower bound on congestion of a\nmapping of network S with x links into network Q with y links is x/y. In the case of the mapping\nfrom a mesh to a linear array, this would be 2 p/p, or 2. However, this lower bound is overly\nconservative. A tighter lower bound is in fact possible by examining the bisection width of the\ntwo networks. We illustrate this further in the next section.\nEmbedding a Hypercube into a 2-D Mesh\nConsider the embedding of a p-node hypercube into a p-node 2-D mesh. For the sake of\nconvenience, we assume that p is an even power of two. In this scenario, it is possible to\nvisualize the hypercube as \n subcubes, each with \n  nodes. We do this as follows: let d =\nlog p be the dimension of the hypercube. From our assumption, we know that d is even. We\ntake the d/2 least significant bits and use them to define individual subcubes of \n  nodes. For\nexample, in the case of a 4D hypercube, we use the lower two bits to define the subcubes as\n(0000, 0001, 0011, 0010), (0100, 0101, 0111, 0110), (1100, 1101, 1111, 1110), and (1000,\n1001, 1011, 1010). Note at this point that if we fix the d/2 least significant bits across all of\nthese subcubes, we will have another subcube as defined by the d/2 most significant bits. For\nexample, if we fix the lower two bits across the subcubes to 10, we get the nodes (0010, 0110,\n1110, 1010). The reader can verify that this corresponds to a 2-D subcube.\nThe mapping from a hypercube to a mesh can now be defined as follows: each \n node\nsubcube is mapped to a \n  node row of the mesh. We do this by simply inverting the linear-\narray to hypercube mapping. The bisection width of the \n  node hypercube is \n . The\ncorresponding bisection width of a \n  node row is 1. Therefore the congestion of this subcube-\nto-row mapping is \n  (at the edge that connects the two halves of the row). This is\nillustrated for the cases of p = 16 and p = 32 in Figure 2.33(a)  and (b). In this fashion, we can\nmap each subcube to a different row in the mesh. Note that while we have computed the\ncongestion resulting from the subcube-to-row mapping, we have not addressed the congestion\nresulting from the column mapping. We map the hypercube nodes into the mesh in such a way\nthat nodes with identical d/2 least significant bits in the hypercube are mapped to the same\ncolumn. This results in a subcube-to-column mapping, where each subcube/column has \nnodes. Using the same argument as in the case of subcube-to-row mapping, this results in a\ncongestion of \n . Since the congestion from the row and column mappings affects disjoint\nsets of edges, the total congestion of this mapping is \n .\nFigure 2.33. Embedding a hypercube into a 2-D mesh.\nTo establish a lower bound on the congestion, we follow the same argument as in Section 2.7.1 .\nSince the bisection width of a hypercube is p/2 and that of a mesh is \n , the lower bound on\ncongestion is the ratio of these, i.e., \n . We notice that our mapping yields this lower bound\non congestion.\nProcess-Processor Mapping and Design of Interconnection Networks\nOur analysis in previous sections reveals that it is possible to map denser networks into sparser\nnetworks with associated congestion overheads. This implies that a sparser network whose link\nbandwidth is increased to compensate for the congestion can be expected to perform as well as\nthe denser network", "doc_id": "fb8c4189-ee99-4e03-bdd2-23a0bda2cc76", "embedding": null, "doc_hash": "1bce3064a4ad4d86ee333ab698634ec1f59b2e23ccfb43830f9384040db28b44", "extra_info": null, "node_info": {"start": 227052, "end": 230616}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f1e04342-19f3-46d6-9531-9b2ab49aa145", "3": "2a34daaf-f887-4d1c-b2a1-519bc874b642"}}, "__type__": "1"}, "2a34daaf-f887-4d1c-b2a1-519bc874b642": {"__data__": {"text": "mapping is \n .\nFigure 2.33. Embedding a hypercube into a 2-D mesh.\nTo establish a lower bound on the congestion, we follow the same argument as in Section 2.7.1 .\nSince the bisection width of a hypercube is p/2 and that of a mesh is \n , the lower bound on\ncongestion is the ratio of these, i.e., \n . We notice that our mapping yields this lower bound\non congestion.\nProcess-Processor Mapping and Design of Interconnection Networks\nOur analysis in previous sections reveals that it is possible to map denser networks into sparser\nnetworks with associated congestion overheads. This implies that a sparser network whose link\nbandwidth is increased to compensate for the congestion can be expected to perform as well as\nthe denser network (modulo dilation effects). For example, a mesh whose links are faster by a\nfactor of \n will yield comparable performance to a hypercube. We call such a mesh a fat-\nmesh. A fat-mesh has the same bisection-bandwidth as a hypercube; however it has a higher\ndiameter. As we have seen in Section 2.5.1 , by using appropriate message routing techniques,\nthe effect of node distance can be minimized. It is important to note that higher dimensional\nnetworks involve more complicated layouts, wire crossings, and variable wire-lengths. For these\nreasons, fattened lower dimensional networks provide attractive alternate approaches to\ndesigning interconnects. We now do a more formal examination of the cost-performance\ntradeoffs of parallel architectures.\n2.7.2 Cost-Performance Tradeoffs\nWe now examine how various cost metrics can be used to investigate cost-performance\ntradeoffs in interconnection networks. We illustrate this by analyzing the performance of a mesh\nand a hypercube network with identical costs.\nIf the cost of a network is proportional to the number of wires, then a square p-node\nwraparound mesh with (log p)/4 wires per channel costs as much as a p-node hypercube with\none wire per channel. Let us compare the average communication times of these two networks.\nThe average distance lav between any two nodes in a two-dimensional wraparound mesh is\n and that in a hypercube is (log p)/2. The time for sending a message of size m between\nnodes that are lav hops apart is given by ts + thlav + twm in networks that use cut-through\nrouting. Since the channel width of the mesh is scaled up by a factor of (log p)/4, the per-word\ntransfer time is reduced by the same factor. Hence, if the per-word transfer time on the\nhypercube is tw, then the same time on a mesh with fattened channels is given by 4 tw/(log p).\nHence, the average communication latency for a hypercube is given by ts + th (log p)/2 + twm\nand that for a wraparound mesh of the same cost is \n .\nLet us now investigate the behavior of these expressions. For a fixed number of nodes, as the\nmessage size is increased, the communication term due to tw dominates. Comparing tw for the\ntwo networks, we see that the time for a wraparound mesh (4 twm/(log p))is less than the time\nfor a hypercube ( twm)if p is greater than 16 and the message size m is sufficiently large. Under\nthese circumstances, point-to-point communication of large messages between random pairs of\nnodes takes less time on a wraparound mesh with cut-through routing than on a hypercube of\nthe same cost. Furthermore, for algorithms in which communication is suited to a mesh, the\nextra bandwidth of each channel results in better performance. Note that, with store-and-\nforward routing, the mesh is no longer more cost-efficient than a hypercube. Similar cost-\nperformance tradeoffs can be analyzed for the general case of k-ary d-cubes (Problems\n2.25\u20132.29).\nThe communication times above are computed under light load conditions in the network. As\nthe number of messages increases, there is contention on the network. Contention affects the\nmesh network more adversely", "doc_id": "2a34daaf-f887-4d1c-b2a1-519bc874b642", "embedding": null, "doc_hash": "b473762957bb4ddc21ec9af7cf92b02c2c474367ec1c6349ca197f7a0b684fee", "extra_info": null, "node_info": {"start": 230593, "end": 234442}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "fb8c4189-ee99-4e03-bdd2-23a0bda2cc76", "3": "b02f7298-f2b7-4eb6-932b-e8b8dbe913e1"}}, "__type__": "1"}, "b02f7298-f2b7-4eb6-932b-e8b8dbe913e1": {"__data__": {"text": "sufficiently large. Under\nthese circumstances, point-to-point communication of large messages between random pairs of\nnodes takes less time on a wraparound mesh with cut-through routing than on a hypercube of\nthe same cost. Furthermore, for algorithms in which communication is suited to a mesh, the\nextra bandwidth of each channel results in better performance. Note that, with store-and-\nforward routing, the mesh is no longer more cost-efficient than a hypercube. Similar cost-\nperformance tradeoffs can be analyzed for the general case of k-ary d-cubes (Problems\n2.25\u20132.29).\nThe communication times above are computed under light load conditions in the network. As\nthe number of messages increases, there is contention on the network. Contention affects the\nmesh network more adversely than the hypercube network. Therefore, if the network is heavily\nloaded, the hypercube will outperform the mesh.\nIf the cost of a network is proportional to its bisection width, then a p-node wraparound mesh\nwith \n wires per channel has a cost equal to a p -node hypercube with one wire per channel.\nLet us perform an analysis similar to the one above to investigate cost-performance tradeoffs\nusing this cost metric. Since the mesh channels are wider by a factor of \n , the per-word\ntransfer time will be lower by an identical factor. Therefore, the communication times for the\nhypercube and the mesh networks of the same cost are given by ts + th (log p)/2 + twm and\n, respectively. Once again, as the message size m becomes large for a\ngiven number of nodes, the tw term dominates. Comparing this term for the two networks, we\nsee that for p > 16 and sufficiently large message sizes, a mesh outperforms a hypercube of the\nsame cost. Therefore, for large enough messages, a mesh is always better than a hypercube of\nthe same cost, provided the network is lightly loaded. Even when the network is heavily loaded,\nthe performance of a mesh is similar to that of a hypercube of the same cost.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n2.8 Bibliographic Remarks\nSeveral textbooks discuss various aspects of high-performance architectures [ PH90 , PH96 ,\nSto93 ]. Parallel architectures and interconnection networks have been well described [ CSG98 ,\nLW95 , HX98 , Fly95 , AG94 , DeC89 , HB84 , Lil92, Sie85 , Sto93 ]. Historically, the classification of\nparallel computers as SISD, SIMD, and MIMD was introduced by Flynn [ Fly72 ]. He also\nproposed the MISD (multiple instruction stream, single data stream) model. MISD is less\nnatural than the other classes, although it can be viewed as a model for pipelining. Darema\n[DRGNP] introduced the Single Program Multiple Data (SPMD) paradigm. Ni [ Ni91] provides a\nlayered classification of parallel computers based on hardware architecture, address space,\ncommunication model, language, programming environment, and applications.\nInterconnection networks have been an area of active interest for decades. Feng [ Fen81]\nprovides a tutorial on static and dynamic interconnection networks. The perfect shuffle\ninterconnection pattern was introduced by Stone [ Sto71 ]. Omega networks were introduced by\nLawrie [ Law75 ]. Other multistage networks have also been proposed. These include the Flip\nnetwork [ Bat76 ] and the Baseline network [ WF80 ]. Mesh of trees and pyramidal mesh are\ndiscussed by Leighton [ Lei92 ]. Leighton [ Lei92 ] also provides a detailed discussion of many\nrelated networks.\nThe C.mmp was an early research prototype MIMD shared-address-space parallel computer\nbased on the Crossbar switch [ WB72 ]. The Sun Ultra HPC Server and Fujitsu VPP 500 are\nexamples of crossbar-based parallel computers or their variants. Several parallel computers\nwere based on multistage interconnection networks including the BBN Butterfly [ BBN89], the\nNYU Ultracomputer [ GGK+83], and the IBM", "doc_id": "b02f7298-f2b7-4eb6-932b-e8b8dbe913e1", "embedding": null, "doc_hash": "07440754a125c37e6963823083af9e12577b2a86342ff0549ff468612dbf32ce", "extra_info": null, "node_info": {"start": 234380, "end": 238201}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2a34daaf-f887-4d1c-b2a1-519bc874b642", "3": "faed50ec-03e0-42a4-a96b-75080474835c"}}, "__type__": "1"}, "faed50ec-03e0-42a4-a96b-75080474835c": {"__data__": {"text": "networks were introduced by\nLawrie [ Law75 ]. Other multistage networks have also been proposed. These include the Flip\nnetwork [ Bat76 ] and the Baseline network [ WF80 ]. Mesh of trees and pyramidal mesh are\ndiscussed by Leighton [ Lei92 ]. Leighton [ Lei92 ] also provides a detailed discussion of many\nrelated networks.\nThe C.mmp was an early research prototype MIMD shared-address-space parallel computer\nbased on the Crossbar switch [ WB72 ]. The Sun Ultra HPC Server and Fujitsu VPP 500 are\nexamples of crossbar-based parallel computers or their variants. Several parallel computers\nwere based on multistage interconnection networks including the BBN Butterfly [ BBN89], the\nNYU Ultracomputer [ GGK+83], and the IBM RP-3 [ PBG+85]. The SGI Origin 2000, Stanford\nDash [ LLG+92] and the KSR-1 [ Ken90 ] are NUMA shared-address-space computers.\nThe Cosmic Cube [ Sei85 ] was among the first message-passing parallel computers based on a\nhypercube-connected network. These were followed by the nCUBE 2 [ nCU90 ] and the Intel\niPSC-1, iPSC-2, and iPSC/860. More recently, the SGI Origin 2000 uses a network similar to a\nhypercube. Saad and Shultz [ SS88, SS89a ] derive interesting properties of the hypercube-\nconnected network and a variety of other static networks [ SS89b ]. Many parallel computers,\nsuch as the Cray T3E, are based on the mesh network. The Intel Paragon XP/S [ Sup91 ] and the\nMosaic C [ Sei92 ] are earlier examples of two-dimensional mesh-based computers. The MIT J-\nMachine [ D+92] was based on a three-dimensional mesh network. The performance of mesh-\nconnected computers can be improved by augmenting the mesh network with broadcast buses\n[KR87a]. The reconfigurable mesh architecture ( Figure 2.35  in Problem 2.16) was introduced by\nMiller et al. [ MKRS88 ]. Other examples of reconfigurable meshes include the TRAC and PCHIP.\nThe DADO parallel computer was based on a tree network [SM86]. It used a complete binary\ntree of depth 10. Leiserson [ Lei85b] introduced the fat-tree interconnection network and proved\nseveral interesting characteristics of it. He showed that for a given volume of hardware, no\nnetwork has much better performance than a fat tree. The Thinking Machines CM-5 [ Thi91]\nparallel computer was based on a fat tree interconnection network.\nThe Illiac IV [ Bar68 ] was among the first SIMD parallel computers. Other SIMD computers\ninclude the Goodyear MPP [ Bat80 ], the DAP 610, and the CM-2 [ Thi90 ], MasPar MP-1, and\nMasPar MP-2 [ Nic90 ]. The CM-5 and DADO incorporate both SIMD and MIMD features. Both are\nMIMD computers but have extra hardware for fast synchronization, which enables them to\noperate in SIMD mode. The CM-5 had a control network to augment the data network. The\ncontrol network provides such functions as broadcast, reduction, combining, and other global\noperations.\nLeighton [ Lei92 ] and Ranka and Sahni [ RS90b ] discuss embedding one interconnection network\ninto another. Gray codes, used in embedding linear array and mesh topologies, are discussed\nby Reingold [ RND77 ]. Ranka and Sahni [ RS90b ] discuss the concepts of congestion, dilation,\nand expansion.\nA comprehensive survey of cut-through routing techniques is provided by Ni and McKinley\n[NM93 ]. The wormhole routing technique was proposed by Dally and Seitz [ DS86 ]. A related\ntechnique called virtual cut-through , in which communication buffers are provided at\nintermediate nodes, was described by Kermani and Kleinrock [ KK79 ]. Dally and Seitz [ DS87 ]\ndiscuss deadlock-free wormhole routing based on channel dependence graphs.", "doc_id": "faed50ec-03e0-42a4-a96b-75080474835c", "embedding": null, "doc_hash": "967feffc76d230c60460703d7a6a9b6649cd55954507fe1dcb76b391f5c025a3", "extra_info": null, "node_info": {"start": 238268, "end": 241841}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b02f7298-f2b7-4eb6-932b-e8b8dbe913e1", "3": "ee834d83-ccca-426c-a7d1-70bde5dc3cac"}}, "__type__": "1"}, "ee834d83-ccca-426c-a7d1-70bde5dc3cac": {"__data__": {"text": "[ Lei92 ] and Ranka and Sahni [ RS90b ] discuss embedding one interconnection network\ninto another. Gray codes, used in embedding linear array and mesh topologies, are discussed\nby Reingold [ RND77 ]. Ranka and Sahni [ RS90b ] discuss the concepts of congestion, dilation,\nand expansion.\nA comprehensive survey of cut-through routing techniques is provided by Ni and McKinley\n[NM93 ]. The wormhole routing technique was proposed by Dally and Seitz [ DS86 ]. A related\ntechnique called virtual cut-through , in which communication buffers are provided at\nintermediate nodes, was described by Kermani and Kleinrock [ KK79 ]. Dally and Seitz [ DS87 ]\ndiscuss deadlock-free wormhole routing based on channel dependence graphs. Deterministic\nrouting schemes based on dimension ordering are often used to avoid deadlocks. Cut-through\nrouting has been used in several parallel computers. The E-cube routing scheme for hypercubes\nwas proposed by [ SB77].\nDally [ Dal90b ] discusses cost-performance tradeoffs of networks for message-passing\ncomputers. Using the bisection bandwidth of a network as a measure of the cost of the network,\nhe shows that low-dimensional networks (such as two-dimensional meshes) are more cost-\neffective than high-dimensional networks (such as hypercubes) [Dal87, Dal90b , Dal90a].\nKreeger and Vempaty [ KV92 ] derive the bandwidth equalization factor for a mesh with respect\nto a hypercube-connected computer for all-to-all personalized communication ( Section 4.5 ).\nGupta and Kumar [ GK93b ] analyze the cost-performance tradeoffs of FFT computations on\nmesh and hypercube networks.\nThe properties of PRAMs have been studied extensively [ FW78 , KR88 , LY86, Sni82 , Sni85 ].\nBooks by Akl [ Akl89 ], Gibbons [ GR90 ], and Jaja [ Jaj92 ] address PRAM algorithms. Our\ndiscussion of PRAM is based upon the book by Jaja [ Jaj92 ]. A number of processor networks\nhave been proposed to simulate PRAM models [ AHMP87 , HP89 , LPP88 , LPP89 , MV84 , Upf84 ,\nUW84 ]. Mehlhorn and Vishkin [ MV84 ] propose the module parallel computer  (MPC) to\nsimulate PRAM models. The MPC is a message-passing parallel computer composed of p\nprocessors, each with a fixed amount of memory and connected by a completely-connected\nnetwork. The MPC is capable of probabilistically simulating T steps of a PRAM in T log p steps if\nthe total memory is increased by a factor of log p. The main drawback of the MPC model is that\na completely-connected network is difficult to construct for a large number of processors. Alt et\nal. [AHMP87 ] propose another model called the bounded-degree network  (BDN). In this\nnetwork, each processor is connected to a fixed number of other processors. Karlin and Upfal\n[KU86] describe an O(T log p) time probabilistic simulation of a PRAM on a BDN. Hornick and\nPreparata [ HP89 ] propose a bipartite network that connects sets of processors and memory\npools. They investigate both the message-passing MPC and BDN based on a mesh of trees.\nMany modifications of the PRAM model have been proposed that attempt to bring it closer to\npractical parallel computers. Aggarwal, Chandra, and Snir [ ACS89b ] propose the LPRAM (local-\nmemory PRAM) model and the BPRAM (block PRAM) model [ ACS89b ]. They also introduce a\nhierarchical memory model of computation [ ACS89a ]. In this model, memory units at different\nlevels are accessed in different times. Parallel algorithms for this model induce locality by\nbringing data into faster memory units before using them and returning them to the slower\nmemory units. Other PRAM models such as phase PRAM [ Gib89], XPRAM [", "doc_id": "ee834d83-ccca-426c-a7d1-70bde5dc3cac", "embedding": null, "doc_hash": "b131a246bbfcd24a890f2fb9a974b86886d05edfb1750a7a6c6657a10a52a40b", "extra_info": null, "node_info": {"start": 241843, "end": 245430}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "faed50ec-03e0-42a4-a96b-75080474835c", "3": "23d5fc6d-5572-4622-a226-cb511aaf1f20"}}, "__type__": "1"}, "23d5fc6d-5572-4622-a226-cb511aaf1f20": {"__data__": {"text": "and memory\npools. They investigate both the message-passing MPC and BDN based on a mesh of trees.\nMany modifications of the PRAM model have been proposed that attempt to bring it closer to\npractical parallel computers. Aggarwal, Chandra, and Snir [ ACS89b ] propose the LPRAM (local-\nmemory PRAM) model and the BPRAM (block PRAM) model [ ACS89b ]. They also introduce a\nhierarchical memory model of computation [ ACS89a ]. In this model, memory units at different\nlevels are accessed in different times. Parallel algorithms for this model induce locality by\nbringing data into faster memory units before using them and returning them to the slower\nmemory units. Other PRAM models such as phase PRAM [ Gib89], XPRAM [ Val90b ], and the\ndelay model [ PY88] have also been proposed. Many researchers have investigated abstract\nuniversal models for parallel computers [ CKP+93a, Sny86 , Val90a ]. Models such as BSP\n[Val90a ], Postal model [ BNK92 ], LogP [ CKP+93b], A3 [GKRS96 ], C3 [HK96 ], CGM [ DFRC96 ],\nand QSM [ Ram97 ] have been proposed with similar objectives.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n2.1 Design an experiment (i.e., design and write programs and take measurements) to\ndetermine the memory bandwidth of your computer and to estimate the caches at various\nlevels of the hierarchy. Use this experiment to estimate the bandwidth and L1 cache of\nyour computer. Justify your answer. (Hint: To test bandwidth, you do not want reuse. To\ntest cache size, you want reuse to see the effect of the cache and to increase this size until\nthe reuse decreases sharply.)\n2.2 Consider a memory system with a level 1 cache of 32 KB and DRAM of 512 MB with\nthe processor operating at 1 GHz. The latency to L1 cache is one cycle and the latency to\nDRAM is 100 cycles. In each memory cycle, the processor fetches four words (cache line\nsize is four words). What is the peak achievable performance of a dot product of two\nvectors? Note: Where necessary, assume an optimal cache placement policy.\n1        /* dot product loop */ \n2        for (i = 0; i < dim; i++) \n3                dot_prod += a[i] * b[i]; \n2.3 Now consider the problem of multiplying a dense matrix with a vector using a two-\nloop dot-product formulation. The matrix is of dimension 4 K x 4K. (Each row of the matrix\ntakes 16 KB of storage.) What is the peak achievable performance of this technique using\na two-loop dot-product based matrix-vector product?\n1        /* matrix-vector product loop */ \n2        for (i = 0; i < dim; i++) \n3                for (j = 0; i < dim; j++) \n4                        c[i] += a[i][j] * b[j]; \n2.4 Extending this further, consider the problem of multiplying two dense matrices of\ndimension 4 K x 4K. What is the peak achievable performance using a three-loop dot-\nproduct based formulation? (Assume that matrices are laid out in a row-major fashion.)\n1        /* matrix-matrix product loop */ \n2        for (i = 0; i < dim; i++) \n3                for (j = 0; i < dim; j++) \n4                        for (k = 0; k < dim; k++) \n5         ", "doc_id": "23d5fc6d-5572-4622-a226-cb511aaf1f20", "embedding": null, "doc_hash": "2a96764fb368d0093c4e8c885ba39a4358c716d885355025f39af6cc5c4dfb36", "extra_info": null, "node_info": {"start": 245441, "end": 248485}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ee834d83-ccca-426c-a7d1-70bde5dc3cac", "3": "c0dff789-15af-4a97-93d4-fbca5385752e"}}, "__type__": "1"}, "c0dff789-15af-4a97-93d4-fbca5385752e": {"__data__": {"text": "+= a[i][j] * b[j]; \n2.4 Extending this further, consider the problem of multiplying two dense matrices of\ndimension 4 K x 4K. What is the peak achievable performance using a three-loop dot-\nproduct based formulation? (Assume that matrices are laid out in a row-major fashion.)\n1        /* matrix-matrix product loop */ \n2        for (i = 0; i < dim; i++) \n3                for (j = 0; i < dim; j++) \n4                        for (k = 0; k < dim; k++) \n5                                c[i][j] += a[i][k] * b[k][j]; \n2.5 Restructure the matrix multiplication algorithm to achieve better cache performance.\nThe most obvious cause of the poor performance of matrix multiplication was the absence\nof spatial locality. In some cases, we were wasting three of the four words fetched from\nmemory. To fix this problem, we compute the elements of the result matrix four at a time.\nUsing this approach, we can increase our FLOP count with a simple restructuring of the\nprogram. However, it is possible to achieve much higher performance from this problem.\nThis is possible by viewing the matrix multiplication problem as a cube in which each\ninternal grid point corresponds to a multiply-add operation. Matrix multiplication\nalgorithms traverse this cube in different ways, which induce different partitions of the\ncube. The data required for computing a partition grows as the surface area of the input\nfaces of the partition and the computation as the volume of the partition. For the\nalgorithms discussed above, we were slicing thin partitions of the cube for which the area\nand volume were comparable (thus achieving poor cache performance). To remedy this,\nwe restructure the computation by partitioning the cube into subcubes of size k x k x k.\nThe data associated with this is 3 x k2 (k2 data for each of the three matrices) and the\ncomputation is k3. To maximize performance, we would like 3 x k2 to be equal to 8 K since\nthat is the amount of cache available (assuming the same machine parameters as in\nProblem 2.2). This corresponds to k = 51. The computation associated with a cube of this\ndimension is 132651 multiply-add operations or 265302 FLOPs. To perform this\ncomputation, we needed to fetch two submatrices of size 51 x 51. This corresponds to\n5202 words or 1301 cache lines. Accessing these cache lines takes 130100 ns. Since\n265302 FLOPs are performed in 130100 ns, the peak computation rate of this formulation\nis 2.04 GFLOPS. Code this example and plot the performance as a function of k. (Code on\nany conventional microprocessor. Make sure you note the clock speed, the microprocessor\nand the cache available at each level.)\n2.6 Consider an SMP with a distributed shared-address-space. Consider a simple cost\nmodel in which it takes 10 ns to access local cache, 100 ns to access local memory, and\n400 ns to access remote memory. A parallel program is running on this machine. The\nprogram is perfectly load balanced with 80% of all accesses going to local cache, 10% to\nlocal memory, and 10% to remote memory. What is the effective memory access time for\nthis computation? If the computation is memory bound, what is the peak computation\nrate?\nNow consider the same computation running on one processor. Here, the processor hits\nthe cache 70% of the time and local memory 30% of the time. What is the effective peak\ncomputation rate for one processor? What is the fractional computation rate of a processor\nin a parallel configuration as compared to the serial configuration?\nHint:  Notice that the cache hit for multiple processors is higher than that for one\nprocessor. This is typically because the aggregate cache available on multiprocessors", "doc_id": "c0dff789-15af-4a97-93d4-fbca5385752e", "embedding": null, "doc_hash": "99b31d51392a7867403196677e64d1bf5a5681e7f66a017b4c759cd31969d87b", "extra_info": null, "node_info": {"start": 248765, "end": 252424}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "23d5fc6d-5572-4622-a226-cb511aaf1f20", "3": "ea1005ed-503b-44c1-8df9-f561f494b4e3"}}, "__type__": "1"}, "ea1005ed-503b-44c1-8df9-f561f494b4e3": {"__data__": {"text": "ns to access remote memory. A parallel program is running on this machine. The\nprogram is perfectly load balanced with 80% of all accesses going to local cache, 10% to\nlocal memory, and 10% to remote memory. What is the effective memory access time for\nthis computation? If the computation is memory bound, what is the peak computation\nrate?\nNow consider the same computation running on one processor. Here, the processor hits\nthe cache 70% of the time and local memory 30% of the time. What is the effective peak\ncomputation rate for one processor? What is the fractional computation rate of a processor\nin a parallel configuration as compared to the serial configuration?\nHint:  Notice that the cache hit for multiple processors is higher than that for one\nprocessor. This is typically because the aggregate cache available on multiprocessors is\nlarger than on single processor systems.\n2.7 What are the major differences between message-passing and shared-address-space\ncomputers? Also outline the advantages and disadvantages of the two.\n2.8 Why is it difficult to construct a true shared-memory computer? What is the minimum\nnumber of switches for connecting p processors to a shared memory with b words (where\neach word can be accessed independently)?\n2.9 Of the four PRAM models (EREW, CREW, ERCW, and CRCW), which model is the most\npowerful? Why?\n2.10 [ Lei92] The Butterfly network  is an interconnection network composed of log p\nlevels (as the omega network). In a Butterfly network, each switching node i at a level l is\nconnected to the identically numbered element at level l + 1 and to a switching node\nwhose number differs from itself only at the lth most significant bit. Therefore, switching\nnode Si is connected to element S j at level l if j = i or j = i \n (2log p-l ).\nFigure 2.34  illustrates a Butterfly network with eight processing nodes. Show the\nequivalence of a Butterfly network and an omega network.\nFigure 2.34. A Butterfly network with eight processing nodes.\nHint:  Rearrange the switches of an omega network so that it looks like a Butterfly\nnetwork.\n2.11 Consider the omega network described in Section 2.4.3 . As shown there, this\nnetwork is a blocking network (that is, a processor that uses the network to access a\nmemory location might prevent another processor from accessing another memory\nlocation). Consider an omega network that connects p processors. Define a function f  that\nmaps P = [0, 1, ..., p - 1] onto a permutation P' of P (that is, P'[i] = f(P[i]) and P'[i] \n P\nfor all 0 \n  i < p). Think of this function as mapping communication requests by the\nprocessors so that processor P[i] requests communication with processor P'[i].\nHow many distinct permutation functions exist?1.\nHow many of these functions result in non-blocking communication?2.\nWhat is the probability that an arbitrary function will result in non-blocking\ncommunication?3.\n2.12 A cycle in a graph is defined as a path originating and terminating at the same node.\nThe length of a cycle is the number of edges in the cycle. Show that there are no odd-\nlength cycles in a d-dimensional hypercube.\n2.13 The labels in a d-dimensional hypercube use d bits. Fixing any k of these bits, show\nthat the nodes whose labels differ in the remaining d - k bit positions form a ( d - k)-\ndimensional subcube composed of 2(d-k) nodes.\n2.14 Let A and B be two nodes in a d-dimensional hypercube. Define H(A, B) to be the\nHamming distance between A and B, and P(A, B) to be the number of distinct paths\nconnecting A and B. These paths are called parallel paths and have no common nodes\nother than A and B. Prove the following:\nThe minimum distance in terms of communication links between A and B is given by\nH(A, B).1.\nThe total number of parallel paths between any two nodes is P(A,", "doc_id": "ea1005ed-503b-44c1-8df9-f561f494b4e3", "embedding": null, "doc_hash": "bc3043c130be1856523f7c19ffba1261712aeeaaa25c4af568d56291bd939df1", "extra_info": null, "node_info": {"start": 252037, "end": 255824}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c0dff789-15af-4a97-93d4-fbca5385752e", "3": "b05a4bb3-8bb4-46d0-8e8a-e301900c8088"}}, "__type__": "1"}, "b05a4bb3-8bb4-46d0-8e8a-e301900c8088": {"__data__": {"text": "labels in a d-dimensional hypercube use d bits. Fixing any k of these bits, show\nthat the nodes whose labels differ in the remaining d - k bit positions form a ( d - k)-\ndimensional subcube composed of 2(d-k) nodes.\n2.14 Let A and B be two nodes in a d-dimensional hypercube. Define H(A, B) to be the\nHamming distance between A and B, and P(A, B) to be the number of distinct paths\nconnecting A and B. These paths are called parallel paths and have no common nodes\nother than A and B. Prove the following:\nThe minimum distance in terms of communication links between A and B is given by\nH(A, B).1.\nThe total number of parallel paths between any two nodes is P(A, B) = d . 2.\n3.\n4.\n2.\nThe number of parallel paths between A and B of length H(A, B) is Plength =H(A,B)(A, B)\n= H(A, B).3.\nThe length of the remaining d - H(A, B) parallel paths is H(A, B) + 2. 4.\n2.15 In the informal derivation of the bisection width of a hypercube, we used the\nconstruction of a hypercube to show that a d-dimensional hypercube is formed from two ( d\n- 1)-dimensional hypercubes. We argued that because corresponding nodes in each of\nthese subcubes have a direct communication link, there are 2d - 1 links across the\npartition. However, it is possible to partition a hypercube into two parts such that neither\nof the partitions is a hypercube. Show that any such partitions will have more than 2d - 1\ndirect links between them.\n2.16 [ MKRS88] A \n  reconfigurable mesh  consists of a \n  array of\nprocessing nodes connected to a grid-shaped reconfigurable broadcast bus. A 4 x 4\nreconfigurable mesh is shown in Figure 2.35 . Each node has locally-controllable bus\nswitches. The internal connections among the four ports, north (N), east (E), west (W),\nand south (S), of a node can be configured during the execution of an algorithm. Note that\nthere are 15 connection patterns. For example, {SW, EN} represents the configuration in\nwhich port S is connected to port W and port N is connected to port E. Each bit of the bus\ncarries one of 1-signal  or 0-signal  at any time. The switches allow the broadcast bus to\nbe divided into subbuses, providing smaller reconfigurable meshes. For a given set of\nswitch settings, a subbus  is a maximally-connected subset of the nodes. Other than the\nbuses and the switches, the reconfigurable mesh is similar to the standard two-\ndimensional mesh. Assume that only one node is allowed to broadcast on a subbus  shared\nby multiple nodes at any time.\nFigure 2.35. Switch connection patterns in a reconfigurable mesh.\nDetermine the bisection width, the diameter, and the number of switching nodes and\ncommunication links for a reconfigurable mesh of \n  processing nodes. What are\nthe advantages and disadvantages of a reconfigurable mesh as compared to a wraparound\nmesh?\n2.17 [ Lei92 ] A mesh of trees  is a network that imposes a tree interconnection on a grid\nof processing nodes. A \n  mesh of trees is constructed as follows. Starting with a\n grid, a complete binary tree is imposed on each row of the grid. Then a\ncomplete binary tree is imposed on each column of the grid. Figure 2.36  illustrates the\nconstruction of a 4 x 4 mesh of trees. Assume that the nodes at intermediate levels are\nswitching nodes. Determine the bisection width, diameter, and total number of switching\nnodes in a \n  mesh.\nFigure 2.36. The construction of a 4 x 4 mesh of trees: (a) a 4 x 4\ngrid, (b) complete binary trees imposed over individual rows, (c)\ncomplete binary trees imposed over each column, and (d)", "doc_id": "b05a4bb3-8bb4-46d0-8e8a-e301900c8088", "embedding": null, "doc_hash": "da67b18609b0fb6ce46300c3e5c08d866cb72dea19daf60b3e79393db16c40bb", "extra_info": null, "node_info": {"start": 255994, "end": 259500}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ea1005ed-503b-44c1-8df9-f561f494b4e3", "3": "d52c0555-15da-47c4-9298-ccffe77d2ca7"}}, "__type__": "1"}, "d52c0555-15da-47c4-9298-ccffe77d2ca7": {"__data__": {"text": "mesh of trees  is a network that imposes a tree interconnection on a grid\nof processing nodes. A \n  mesh of trees is constructed as follows. Starting with a\n grid, a complete binary tree is imposed on each row of the grid. Then a\ncomplete binary tree is imposed on each column of the grid. Figure 2.36  illustrates the\nconstruction of a 4 x 4 mesh of trees. Assume that the nodes at intermediate levels are\nswitching nodes. Determine the bisection width, diameter, and total number of switching\nnodes in a \n  mesh.\nFigure 2.36. The construction of a 4 x 4 mesh of trees: (a) a 4 x 4\ngrid, (b) complete binary trees imposed over individual rows, (c)\ncomplete binary trees imposed over each column, and (d) the\ncomplete 4 x 4 mesh of trees.\n2.18 [ Lei92 ] Extend the two-dimensional mesh of trees (Problem 2.17) to d dimensions to\nconstruct a p1/d x p1/d x \u00b7\u00b7\u00b7 x p1/d mesh of trees. We can do this by fixing grid positions in\nall dimensions to different values and imposing a complete binary tree on the one\ndimension that is being varied.\nDerive the total number of switching nodes in a p1/d x p1/d x \u00b7\u00b7\u00b7 x p1/d mesh of trees.\nCalculate the diameter, bisection width, and wiring cost in terms of the total number of\nwires. What are the advantages and disadvantages of a mesh of trees as compared to a\nwraparound mesh?\n2.19 [ Lei92] A network related to the mesh of trees is the d-dimensional pyramidal\nmesh . A d-dimensional pyramidal mesh imposes a pyramid on the underlying grid of\nprocessing nodes (as opposed to a complete tree in the mesh of trees). The generalization\nis as follows. In the mesh of trees, all dimensions except one are fixed and a tree is\nimposed on the remaining dimension. In a pyramid, all but two dimensions are fixed and a\npyramid is imposed on the mesh formed by these two dimensions. In a tree, each node i\nat level k is connected to node i/2 at level k - 1. Similarly, in a pyramid, a node ( i, j) at\nlevel k is connected to a node ( i/2, j/2) at level k - 1. Furthermore, the nodes at each level\nare connected in a mesh. A two-dimensional pyramidal mesh is illustrated in Figure 2.37.\nFigure 2.37. A 4 x 4 pyramidal mesh.\nFor a \n  pyramidal mesh, assume that the intermediate nodes are switching\nnodes, and derive the diameter, bisection width, arc connectivity, and cost in terms of the\nnumber of communication links and switching nodes. What are the advantages and\ndisadvantages of a pyramidal mesh as compared to a mesh of trees?\n2.20 [ Lei92] One of the drawbacks of a hypercube-connected network is that different\nwires in the network are of different lengths. This implies that data takes different times to\ntraverse different communication links. It appears that two-dimensional mesh networks\nwith wraparound connections suffer from this drawback too. However, it is possible to\nfabricate a two-dimensional wraparound mesh using wires of fixed length. Illustrate this\nlayout by drawing such a 4 x 4 wraparound mesh.\n2.21 Show how to embed a p-node three-dimensional mesh into a p-node hypercube.\nWhat are the allowable values of p for your embedding?\n2.22 Show how to embed a p-node mesh of trees into a p-node hypercube.\n2.23 Consider a complete binary tree of 2d - 1 nodes in which each node is a processing\nnode. What is the minimum-dilation mapping of such a tree onto a d-dimensional\nhypercube?\n2.24 The concept of a minimum congestion mapping  is very useful. Consider two\nparallel computers with different interconnection networks such that a congestion- r\nmapping of the first into the second exists.", "doc_id": "d52c0555-15da-47c4-9298-ccffe77d2ca7", "embedding": null, "doc_hash": "71b3a8d915edc5a853278b3962e273d1065335308a096562812a37c1da228c01", "extra_info": null, "node_info": {"start": 259466, "end": 263011}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b05a4bb3-8bb4-46d0-8e8a-e301900c8088", "3": "3e1b813f-97fd-4309-947d-c1e78493ed22"}}, "__type__": "1"}, "3e1b813f-97fd-4309-947d-c1e78493ed22": {"__data__": {"text": "two-dimensional wraparound mesh using wires of fixed length. Illustrate this\nlayout by drawing such a 4 x 4 wraparound mesh.\n2.21 Show how to embed a p-node three-dimensional mesh into a p-node hypercube.\nWhat are the allowable values of p for your embedding?\n2.22 Show how to embed a p-node mesh of trees into a p-node hypercube.\n2.23 Consider a complete binary tree of 2d - 1 nodes in which each node is a processing\nnode. What is the minimum-dilation mapping of such a tree onto a d-dimensional\nhypercube?\n2.24 The concept of a minimum congestion mapping  is very useful. Consider two\nparallel computers with different interconnection networks such that a congestion- r\nmapping of the first into the second exists. Ignoring the dilation of the mapping, if each\ncommunication link in the second computer is more than r times faster than the first\ncomputer, the second computer is strictly superior to the first.\nNow consider mapping a d-dimensional hypercube onto a 2d-node mesh. Ignoring the\ndilation of the mapping, what is the minimum-congestion mapping of the hypercube onto\nthe mesh? Use this result to determine whether a 1024-node mesh with communication\nlinks operating at 25 million bytes per second is strictly better than a 1024-node\nhypercube (whose nodes are identical to those used in the mesh) with communication links\noperating at two million bytes per second.\n2.25 Derive the diameter, number of links, and bisection width of a k-ary d-cube with p\nnodes. Define lav to be the average distance between any two nodes in the network. Derive\nlav for a k-ary d-cube.\n2.26 Consider the routing of messages in a parallel computer that uses store-and-forward\nrouting. In such a network, the cost of sending a single message of size m from Psource to\nPdestination  via a path of length d is ts + tw x d x m. An alternate way of sending a message\nof size m is as follows. The user breaks the message into k  parts each of size m/k, and\nthen sends these k distinct messages one by one from Psource to Pdestination . For this new\nmethod, derive the expression for time to transfer a message of size m to a node d hops\naway under the following two cases:\nAssume that another message can be sent from Psource as soon as the previous\nmessage has reached the next node in the path.1.\nAssume that another message can be sent from Psource only after the previous\nmessage has reached Pdestination .2.\nFor each case, comment on the value of this expression as the value of k varies between 1\nand m. Also, what is the optimal value of k if ts is very large, or if ts = 0?\n2.27 Consider a hypercube network of p nodes. Assume that the channel width of each\ncommunication link is one. The channel width of the links in a k-ary d-cube (for d < log p)\ncan be increased by equating the cost of this network with that of a hypercube. Two\ndistinct measures can be used to evaluate the cost of a network.\nThe cost can be expressed in terms of the total number of wires in the network (the\ntotal number of wires is a product of the number of communication links and the\nchannel width).1.\nThe bisection bandwidth can be used as a measure of cost.2.\nUsing each of these cost metrics and equating the cost of a k-ary d-cube with a hypercube,\nwhat is the channel width of a k-ary d-cube with an identical number of nodes, channel\nrate, and cost?\n2.28 The results from Problems 2.25 and 2.27 can be used in a cost-performance analysis\nof static interconnection networks. Consider a k-ary d-cube network of p nodes with cut-\nthrough routing. Assume a hypercube-connected network of p nodes with channel width\none. The channel width of other networks in the family is scaled up so that their cost is\nidentical to that of the hypercube. Let s and s' be the scaling factors for the channel width\nderived by equating the costs specified by the two cost metrics in Problem", "doc_id": "3e1b813f-97fd-4309-947d-c1e78493ed22", "embedding": null, "doc_hash": "6719eff684487a46c2c1f693fb757c625209e3483654dfc67757fbd0301b14c2", "extra_info": null, "node_info": {"start": 262986, "end": 266836}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d52c0555-15da-47c4-9298-ccffe77d2ca7", "3": "1fa7f339-a178-4a49-b5a5-63fe8985e293"}}, "__type__": "1"}, "1fa7f339-a178-4a49-b5a5-63fe8985e293": {"__data__": {"text": "be used as a measure of cost.2.\nUsing each of these cost metrics and equating the cost of a k-ary d-cube with a hypercube,\nwhat is the channel width of a k-ary d-cube with an identical number of nodes, channel\nrate, and cost?\n2.28 The results from Problems 2.25 and 2.27 can be used in a cost-performance analysis\nof static interconnection networks. Consider a k-ary d-cube network of p nodes with cut-\nthrough routing. Assume a hypercube-connected network of p nodes with channel width\none. The channel width of other networks in the family is scaled up so that their cost is\nidentical to that of the hypercube. Let s and s' be the scaling factors for the channel width\nderived by equating the costs specified by the two cost metrics in Problem 2.27.\nFor each of the two scaling factors s and s', express the average communication time\nbetween any two nodes as a function of the dimensionality ( d)of a k-ary d-cube and the\nnumber of nodes. Plot the communication time as a function of the dimensionality for p =\n256, 512, and 1024, message size m = 512 bytes, ts = 50.0 \u00b5s, and th = tw = 0.5 \u00b5s (for\nthe hypercube). For these values of p and m, what is the dimensionality of the network\nthat yields the best performance for a given cost?\n2.29 Repeat Problem 2.28 for a k-ary d-cube with store-and-forward routing.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 3. Principles of Parallel\nAlgorithm Design\nAlgorithm development is a critical component of problem solving using computers. A sequential\nalgorithm is essentially a recipe or a sequence of basic steps for solving a given problem using a\nserial computer. Similarly, a parallel algorithm is a recipe that tells us how to solve a given\nproblem using multiple processors. However, specifying a parallel algorithm involves more than\njust specifying the steps. At the very least, a parallel algorithm has the added dimension of\nconcurrency and the algorithm designer must specify sets of steps that can be executed\nsimultaneously. This is essential for obtaining any performance benefit from the use of a parallel\ncomputer. In practice, specifying a nontrivial parallel algorithm may include some or all of the\nfollowing:\nIdentifying portions of the work that can be performed concurrently.Mapping the concurrent pieces of work onto multiple processes running in parallel.\nDistributing the input, output, and intermediate data associated with the program.\nManaging accesses to data shared by multiple processors.\nSynchronizing the processors at various stages of the parallel program execution.\nTypically, there are several choices for each of the above steps, but usually, relatively few\ncombinations of choices lead to a parallel algorithm that yields performance commensurate with\nthe computational and storage resources employed to solve the problem. Often, different\nchoices yield the best performance on different parallel architectures or under different parallel\nprogramming paradigms.\nIn this chapter, we methodically discuss the process of designing and implementing parallel\nalgorithms. We shall assume that the onus of providing a complete description of a parallel\nalgorithm or program lies on the programmer or the algorithm designer. Tools and compilers\nfor automatic parallelization at the current state of the art seem to work well only for highly\nstructured programs or portions of programs. Therefore, we do not consider these in this\nchapter or elsewhere in this book.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n3.1 Preliminaries\nDividing a computation into smaller computations and assigning them to different processors for\nparallel execution are the two key steps in the design of parallel algorithms. In this section, we\npresent some basic terminology and introduce these two key steps in parallel algorithm design\nusing matrix-vector multiplication and database query processing as examples.\n3.1.1 Decomposition, Tasks, and Dependency Graphs\nThe process of dividing a computation into smaller parts, some or all of which may potentially\nbe executed in parallel, is called", "doc_id": "1fa7f339-a178-4a49-b5a5-63fe8985e293", "embedding": null, "doc_hash": "f5fcd403d69d1bb87985958e7ae97f1c8a63cc49c7ff4802fa938db6c124cf0a", "extra_info": null, "node_info": {"start": 266821, "end": 270856}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3e1b813f-97fd-4309-947d-c1e78493ed22", "3": "900c711d-cb41-42ae-8395-07ec2a4765ed"}}, "__type__": "1"}, "900c711d-cb41-42ae-8395-07ec2a4765ed": {"__data__": {"text": "the art seem to work well only for highly\nstructured programs or portions of programs. Therefore, we do not consider these in this\nchapter or elsewhere in this book.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n3.1 Preliminaries\nDividing a computation into smaller computations and assigning them to different processors for\nparallel execution are the two key steps in the design of parallel algorithms. In this section, we\npresent some basic terminology and introduce these two key steps in parallel algorithm design\nusing matrix-vector multiplication and database query processing as examples.\n3.1.1 Decomposition, Tasks, and Dependency Graphs\nThe process of dividing a computation into smaller parts, some or all of which may potentially\nbe executed in parallel, is called decomposition . Tasks  are programmer-defined units of\ncomputation into which the main computation is subdivided by means of decomposition.\nSimultaneous execution of multiple tasks is the key to reducing the time required to solve the\nentire problem. Tasks can be of arbitrary size, but once defined, they are regarded as indivisible\nunits of computation. The tasks into which a problem is decomposed may not all be of the same\nsize.\nExample 3.1 Dense matrix-vector multiplication\nConsider the multiplication of a dense n x n matrix A with a vector b to yield another\nvector y. The ith element y[i] of the product vector is the dot-product of the ith row of\nA with the input vector b; i.e., \n. As shown later in Figure 3.1 ,\nthe computation of each y[i] can be regarded as a task. Alternatively, as shown later\nin Figure 3.4 , the computation could be decomposed into fewer, say four, tasks where\neach task computes roughly n/4 of the entries of the vector y. \nFigure 3.1. Decomposition of dense matrix-vector multiplication\ninto n tasks, where n is the number of rows in the matrix. The\nportions of the matrix and the input and output vectors\naccessed by Task 1 are highlighted.\n\nNote that all tasks in Figure 3.1  are independent and can be performed all together or in any\nsequence. However, in general, some tasks may use data produced by other tasks and thus\nmay need to wait for these tasks to finish execution. An abstraction used to express such\ndependencies among tasks and their relative order of execution is known as a task-\ndependency graph . A task-dependency graph is a directed acyclic graph in which the nodes\nrepresent tasks and the directed edges indicate the dependencies amongst them. The task\ncorresponding to a node can be executed when all tasks connected to this node by incoming\nedges have completed. Note that task-dependency graphs can be disconnected and the edge-\nset of a task-dependency graph can be empty. This is the case for matrix-vector multiplication,\nwhere each task computes a subset of the entries of the product vector. To see a more\ninteresting task-dependency graph, consider the following database query processing example.\nExample 3.2 Database query processing\nTable 3.1 shows a relational database of vehicles. Each row of the table is a record\nthat contains data corresponding to a particular vehicle, such as its ID, model, year,\ncolor, etc. in various fields. Consider the computations performed in processing the\nfollowing query:\nMODEL=\"Civic\" AND YEAR=\"2001\" AND (COLOR=\"Green\" OR COLOR=\"White\")\nThis query looks for all 2001 Civics whose color is either Green or White. On a\nrelational database, this query is processed by creating a number of intermediate\ntables. One possible way is to first create the following four tables: a table containing\nall Civics, a table containing all 2001-model cars, a table containing all green-colored\ncars, and a table containing all white-colored cars. Next, the computation proceeds by\ncombining these tables by computing their pairwise intersections or unions. In\nparticular, it computes the intersection of the Civic-table with the 2001-model year\ntable, to", "doc_id": "900c711d-cb41-42ae-8395-07ec2a4765ed", "embedding": null, "doc_hash": "23d70e6819961c0f91c227eedddd2aa4376e53fb1a80c10bd16258b5a1652c59", "extra_info": null, "node_info": {"start": 270828, "end": 274740}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1fa7f339-a178-4a49-b5a5-63fe8985e293", "3": "171d2d18-69c4-47af-9e93-9c6ae5c0d055"}}, "__type__": "1"}, "171d2d18-69c4-47af-9e93-9c6ae5c0d055": {"__data__": {"text": "etc. in various fields. Consider the computations performed in processing the\nfollowing query:\nMODEL=\"Civic\" AND YEAR=\"2001\" AND (COLOR=\"Green\" OR COLOR=\"White\")\nThis query looks for all 2001 Civics whose color is either Green or White. On a\nrelational database, this query is processed by creating a number of intermediate\ntables. One possible way is to first create the following four tables: a table containing\nall Civics, a table containing all 2001-model cars, a table containing all green-colored\ncars, and a table containing all white-colored cars. Next, the computation proceeds by\ncombining these tables by computing their pairwise intersections or unions. In\nparticular, it computes the intersection of the Civic-table with the 2001-model year\ntable, to construct a table of all 2001-model Civics. Similarly, it computes the union of\nthe green- and white-colored tables to compute a table storing all cars whose color is\neither green or white. Finally, it computes the intersection of the table containing all\nthe 2001 Civics with the table containing all the green or white vehicles, and returns\nthe desired list. \nTable 3.1. A database storing information about used\nvehicles.\nID# Model Year Color Dealer Price\n4523 Civic 2002 Blue MN $18,000\n3476 Corolla 1999 White IL $15,000\n7623 Camry 2001 Green NY $21,000\n9834 Prius 2001 Green CA $18,000\n6734 Civic 2001 White OR $17,000\n5342 Altima 2001 Green FL $19,000\nID# Model Year Color Dealer Price\n3845 Maxima 2001 Blue NY $22,000\n8354 Accord 2000 Green VT $18,000\n4395 Civic 2001 Red CA $17,000\n7352 Civic 2002 Red WA $18,000\nThe various computations involved in processing the query in Example 3.2  can be visualized by\nthe task-dependency graph shown in Figure 3.2 . Each node in this figure is a task that\ncorresponds to an intermediate table that needs to be computed and the arrows between nodes\nindicate dependencies between the tasks. For example, before we can compute the table that\ncorresponds to the 2001 Civics, we must first compute the table of all the Civics and a table of\nall the 2001-model cars.\nFigure 3.2. The different tables and their dependencies in a query\nprocessing operation.\nNote that often there are multiple ways of expressing certain computations, especially those\ninvolving associative operators such as addition, multiplication, and logical AND or OR. Different\nways of arranging computations can lead to different task-dependency graphs with different\ncharacteristics. For instance, the database query in Example 3.2 can be solved by first\ncomputing a table of all green or white cars, then performing an intersection with a table of all\n2001 model cars, and finally combining the results with the table of all Civics. This sequence of\ncomputation results in the task-dependency graph shown in Figure 3.3.\nFigure 3.3. An alternate data-dependency graph for the query\nprocessing operation.3845 Maxima 2001 Blue NY $22,000\n8354 Accord 2000 Green VT $18,000\n4395 Civic 2001 Red CA $17,000\n7352 Civic 2002 Red WA $18,000\nThe various computations involved in processing the query in Example 3.2  can be visualized by\nthe task-dependency graph shown in Figure 3.2 . Each node in this figure is a task that\ncorresponds to an intermediate table that needs to be computed and the arrows between nodes\nindicate dependencies between the tasks. For example, before we can compute the table that\ncorresponds to the 2001 Civics, we must first compute the table of all the Civics and a table of\nall the 2001-model cars.\nFigure 3.2. The different tables and their dependencies in a query\nprocessing operation.\nNote that often there are multiple ways of expressing certain computations, especially those\ninvolving associative operators such as addition, multiplication, and logical AND or OR. Different\nways of arranging computations can lead to different task-dependency graphs with different\ncharacteristics. For instance, the database query in Example", "doc_id": "171d2d18-69c4-47af-9e93-9c6ae5c0d055", "embedding": null, "doc_hash": "8192b6cb080f610a57d5aa879c27a4d35c50da5d8d73955666f06c42071f0a6c", "extra_info": null, "node_info": {"start": 274732, "end": 278663}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "900c711d-cb41-42ae-8395-07ec2a4765ed", "3": "0418c5d6-d9e3-46c6-8fc9-dd0be3a6dda2"}}, "__type__": "1"}, "0418c5d6-d9e3-46c6-8fc9-dd0be3a6dda2": {"__data__": {"text": "by\nthe task-dependency graph shown in Figure 3.2 . Each node in this figure is a task that\ncorresponds to an intermediate table that needs to be computed and the arrows between nodes\nindicate dependencies between the tasks. For example, before we can compute the table that\ncorresponds to the 2001 Civics, we must first compute the table of all the Civics and a table of\nall the 2001-model cars.\nFigure 3.2. The different tables and their dependencies in a query\nprocessing operation.\nNote that often there are multiple ways of expressing certain computations, especially those\ninvolving associative operators such as addition, multiplication, and logical AND or OR. Different\nways of arranging computations can lead to different task-dependency graphs with different\ncharacteristics. For instance, the database query in Example 3.2 can be solved by first\ncomputing a table of all green or white cars, then performing an intersection with a table of all\n2001 model cars, and finally combining the results with the table of all Civics. This sequence of\ncomputation results in the task-dependency graph shown in Figure 3.3.\nFigure 3.3. An alternate data-dependency graph for the query\nprocessing operation.\n3.1.2 Granularity, Concurrency, and Task-Interaction\nThe number and size of tasks into which a problem is decomposed determines the granularity\nof the decomposition. A decomposition into a large number of small tasks is called fine-grained\nand a decomposition into a small number of large tasks is called coarse-grained . For example,\nthe decomposition for matrix-vector multiplication shown in Figure 3.1  would usually be\nconsidered fine-grained because each of a large number of tasks performs a single dot-product.\nFigure 3.4  shows a coarse-grained decomposition of the same problem into four tasks, where\neach tasks computes n/4 of the entries of the output vector of length n.\nFigure 3.4. Decomposition of dense matrix-vector multiplication into\nfour tasks. The portions of the matrix and the input and output vectors\naccessed by Task 1 are highlighted.\n\nA concept related to granularity is that of degree of concurrency . The maximum number of\ntasks that can be executed simultaneously in a parallel program at any given time is known as\nits maximum degree of concurrency . In most cases, the maximum degree of concurrency is less\nthan the total number of tasks due to dependencies among the tasks. For example, the\nmaximum degree of concurrency in the task-graphs of Figures 3.2 and 3.3 is four. In these\ntask-graphs, maximum concurrency is available right at the beginning when tables for Model,\nYear, Color Green, and Color White can be computed simultaneously. In general, for task-\ndependency graphs that are trees, the maximum degree of concurrency is always equal to the\nnumber of leaves in the tree.\nA more useful indicator of a parallel program's performance is the average degree of\nconcurrency , which is the average number of tasks that can run concurrently over the entire\nduration of execution of the program.\nBoth the maximum and the average degrees of concurrency usually increase as the granularity\nof tasks becomes smaller (finer). For example, the decomposition of matrix-vector\nmultiplication shown in Figure 3.1 has a fairly small granularity and a large degree of\nconcurrency. The decomposition for the same problem shown in Figure 3.4  has a larger\ngranularity and a smaller degree of concurrency.\nThe degree of concurrency also depends on the shape of the task-dependency graph and the\nsame granularity, in general, does not guarantee the same degree of concurrency. For example,\nconsider the two task graphs in Figure 3.5, which are abstractions of the task graphs of Figures\n3.2 and 3.3, respectively (Problem 3.1). The number inside each node represents the amount of\nwork required to complete the task corresponding to that node. The average degree of\nconcurrency of the task graph in Figure 3.5(a) is 2.33 and that of the task graph in Figure\n3.5(b)  is 1.88 (Problem 3.1), although both", "doc_id": "0418c5d6-d9e3-46c6-8fc9-dd0be3a6dda2", "embedding": null, "doc_hash": "7b05c8d78b89fd9436a615978c16ac123563221ae7cd62a99cd0c7042618fc0d", "extra_info": null, "node_info": {"start": 278613, "end": 282642}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "171d2d18-69c4-47af-9e93-9c6ae5c0d055", "3": "ea90e577-361c-48de-b0b9-3d168655d1a4"}}, "__type__": "1"}, "ea90e577-361c-48de-b0b9-3d168655d1a4": {"__data__": {"text": "large degree of\nconcurrency. The decomposition for the same problem shown in Figure 3.4  has a larger\ngranularity and a smaller degree of concurrency.\nThe degree of concurrency also depends on the shape of the task-dependency graph and the\nsame granularity, in general, does not guarantee the same degree of concurrency. For example,\nconsider the two task graphs in Figure 3.5, which are abstractions of the task graphs of Figures\n3.2 and 3.3, respectively (Problem 3.1). The number inside each node represents the amount of\nwork required to complete the task corresponding to that node. The average degree of\nconcurrency of the task graph in Figure 3.5(a) is 2.33 and that of the task graph in Figure\n3.5(b)  is 1.88 (Problem 3.1), although both task-dependency graphs are based on the same\ndecomposition.\nFigure 3.5. Abstractions of the task graphs of Figures 3.2 and 3.3,\nrespectively.\nA feature of a task-dependency graph that determines the average degree of concurrency for a\ngiven granularity is its critical path . In a task-dependency graph, let us refer to the nodes with\nno incoming edges by start nodes  and the nodes with no outgoing edges by finish nodes . The\nlongest directed path between any pair of start and finish nodes is known as the critical path.\nThe sum of the weights of nodes along this path is known as the critical path length , where\nthe weight of a node is the size or the amount of work associated with the corresponding task.\nThe ratio of the total amount of work to the critical-path length is the average degree of\nconcurrency. Therefore, a shorter critical path favors a higher degree of concurrency. For\nexample, the critical path length is 27 in the task-dependency graph shown in Figure 3.5(a) and\nis 34 in the task-dependency graph shown in Figure 3.5(b) . Since the total amount of work\nrequired to solve the problems using the two decompositions is 63 and 64, respectively, the\naverage degree of concurrency of the two task-dependency graphs is 2.33 and 1.88,\nrespectively.\nAlthough it may appear that the time required to solve a problem can be reduced simply by\nincreasing the granularity of decomposition and utilizing the resulting concurrency to perform\nmore and more tasks in parallel, this is not the case in most practical scenarios. Usually, there\nis an inherent bound on how fine-grained a decomposition a problem permits. For instance,\nthere are n2 multiplications and additions in matrix-vector multiplication considered in Example3.1 and the problem cannot be decomposed into more than O(n2) tasks even by using the most\nfine-grained decomposition.\nOther than limited granularity and degree of concurrency, there is another important practical\nfactor that limits our ability to obtain unbounded speedup (ratio of serial to parallel execution\ntime) from parallelization. This factor is the interaction  among tasks running on different\nphysical processors. The tasks that a problem is decomposed into often share input, output, or\nintermediate data. The dependencies in a task-dependency graph usually result from the fact\nthat the output of one task is the input for another. For example, in the database query\nexample, tasks share intermediate data; the table generated by one task is often used by\nanother task as input. Depending on the definition of the tasks and the parallel programming\nparadigm, there may be interactions among tasks that appear to be independent in a task-\ndependency graph. For example, in the decomposition for matrix-vector multiplication, although\nall tasks are independent, they all need access to the entire input vector b. Since originally\nthere is only one copy of the vector b, tasks may have to send and receive messages for all of\nthem to access the entire vector in the distributed-memory paradigm.\nThe pattern of interaction among tasks is captured by what is known as a task-interaction\ngraph . The nodes in a task-interaction graph represent tasks and the edges connect tasks that\ninteract with each other. The nodes and edges of a task-interaction graph can", "doc_id": "ea90e577-361c-48de-b0b9-3d168655d1a4", "embedding": null, "doc_hash": "b33e0d460126366dbc5bb2e0bba3c27e683e851b532fe0f44761e7db06338ef7", "extra_info": null, "node_info": {"start": 282724, "end": 286777}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0418c5d6-d9e3-46c6-8fc9-dd0be3a6dda2", "3": "69cd95da-af06-49cc-a1c5-9fb5a8404bfe"}}, "__type__": "1"}, "69cd95da-af06-49cc-a1c5-9fb5a8404bfe": {"__data__": {"text": "often used by\nanother task as input. Depending on the definition of the tasks and the parallel programming\nparadigm, there may be interactions among tasks that appear to be independent in a task-\ndependency graph. For example, in the decomposition for matrix-vector multiplication, although\nall tasks are independent, they all need access to the entire input vector b. Since originally\nthere is only one copy of the vector b, tasks may have to send and receive messages for all of\nthem to access the entire vector in the distributed-memory paradigm.\nThe pattern of interaction among tasks is captured by what is known as a task-interaction\ngraph . The nodes in a task-interaction graph represent tasks and the edges connect tasks that\ninteract with each other. The nodes and edges of a task-interaction graph can be assigned\nweights proportional to the amount of computation a task performs and the amount of\ninteraction that occurs along an edge, if this information is known. The edges in a task-\ninteraction graph are usually undirected, but directed edges can be used to indicate the\ndirection of flow of data, if it is unidirectional. The edge-set of a task-interaction graph is\nusually a superset of the edge-set of the task-dependency graph. In the database query\nexample discussed earlier, the task-interaction graph is the same as the task-dependency\ngraph. We now give an example of a more interesting task-interaction graph that results from\nthe problem of sparse matrix-vector multiplication.\nExample 3.3 Sparse matrix-vector multiplication\nConsider the problem of computing the product y = Ab of a sparse n x n matrix A with\na dense n x 1 vector b. A matrix is considered sparse when a significant number of\nentries in it are zero and the locations of the non-zero entries do not conform to a\npredefined structure or pattern. Arithmetic operations involving sparse matrices can\noften be optimized significantly by avoiding computations involving the zeros. For\ninstance, while computing the ith entry \n of the product\nvector, we need to compute the products A[i, j] x b[j] for only those values of j  for\nwhich A[i, j] \n 0. For example, y[0] = A[0, 0]. b[0] + A[0, 1]. b[1] + A[0, 4]. b[4] +\nA[0, 8]. b[8].\nOne possible way of decomposing this computation is to partition the output vector y\nand have each task compute an entry in it. Figure 3.6(a)  illustrates this\ndecomposition. In addition to assigning the computation of the element y[i] of the\noutput vector to Task i, we also make it the \"owner\" of row A[i, *] of the matrix and\nthe element b[i] of the input vector. Note that the computation of y[i] requires access\nto many elements of b that are owned by other tasks. So Task i must get these\nelements from the appropriate locations. In the message-passing paradigm, with the\nownership of b[i],Task i also inherits the responsibility of sending b[i] to all the other\ntasks that need it for their computation. For example, Task 4 must send b[4] to Tasks\n0, 5, 8, and 9 and must get b[0], b[5], b[8], and b[9] to perform its own\ncomputation. The resulting task-interaction graph is shown in Figure 3.6(b) . \nFigure 3.6. A decomposition for sparse matrix-vector\nmultiplication and the corresponding task-interaction graph. In\nthe decomposition Task i computes \n .\nChapter 5  contains detailed quantitative analysis of overheads due to interaction and limited\nconcurrency and their effect on the performance and scalability of parallel algorithm-\narchitecture combinations. In this section, we have provided a basic introduction to these\nfactors because they require important consideration in designing parallel algorithms.\n3.1.3 Processes and Mapping\nThe tasks, into which a problem is decomposed, run on", "doc_id": "69cd95da-af06-49cc-a1c5-9fb5a8404bfe", "embedding": null, "doc_hash": "9f3bf6c05f074fe879fc6496db836e916fb02050b3a43c7ffd1645970e090a43", "extra_info": null, "node_info": {"start": 286720, "end": 290444}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ea90e577-361c-48de-b0b9-3d168655d1a4", "3": "df4379a1-2ab5-4e37-a18c-26b16996ab7f"}}, "__type__": "1"}, "df4379a1-2ab5-4e37-a18c-26b16996ab7f": {"__data__": {"text": "8, and 9 and must get b[0], b[5], b[8], and b[9] to perform its own\ncomputation. The resulting task-interaction graph is shown in Figure 3.6(b) . \nFigure 3.6. A decomposition for sparse matrix-vector\nmultiplication and the corresponding task-interaction graph. In\nthe decomposition Task i computes \n .\nChapter 5  contains detailed quantitative analysis of overheads due to interaction and limited\nconcurrency and their effect on the performance and scalability of parallel algorithm-\narchitecture combinations. In this section, we have provided a basic introduction to these\nfactors because they require important consideration in designing parallel algorithms.\n3.1.3 Processes and Mapping\nThe tasks, into which a problem is decomposed, run on physical processors. However, for\nreasons that we shall soon discuss, we will use the term process  in this chapter to refer to a\nprocessing or computing agent that performs tasks. In this context, the term process does not\nadhere to the rigorous operating system definition of a process. Instead, it is an abstract entity\nthat uses the code and data corresponding to a task to produce the output of that task within a\nfinite amount of time after the task is activated by the parallel program. During this time, in\naddition to performing computations, a process may synchronize or communicate with other\nprocesses, if needed. In order to obtain any speedup over a sequential implementation, a\nparallel program must have several processes active simultaneously, working on different tasks.\nThe mechanism by which tasks are assigned to processes for execution is called mapping . For\nexample, four processes could be assigned the task of computing one submatrix of C each in the\nmatrix-multiplication computation of Example 3.5.\nThe task-dependency and task-interaction graphs that result from a choice of decomposition\nplay an important role in the selection of a good mapping for a parallel algorithm. A good\nmapping should seek to maximize the use of concurrency by mapping independent tasks onto\ndifferent processes, it should seek to minimize the total completion time by ensuring that\nprocesses are available to execute the tasks on the critical path as soon as such tasks become\nexecutable, and it should seek to minimize interaction among processes by mapping tasks with\na high degree of mutual interaction onto the same process. In most nontrivial parallel\nalgorithms, these tend to be conflicting goals. For instance, the most efficient decomposition-\nmapping combination is a single task mapped onto a single process. It wastes no time in idling\nor interacting, but achieves no speedup either. Finding a balance that optimizes the overall\nparallel performance is the key to a successful parallel algorithm. Therefore, mapping of tasks\nonto processes plays an important role in determining how efficient the resulting parallel\nalgorithm is. Even though the degree of concurrency is determined by the decomposition, it is\nthe mapping that determines how much of that concurrency is actually utilized, and how\nefficiently.\nFor example, Figure 3.7 shows efficient mappings for the decompositions and the task-\ninteraction graphs of Figure 3.5  onto four processes. Note that, in this case, a maximum of four\nprocesses can be employed usefully, although the total number of tasks is seven. This is\nbecause the maximum degree of concurrency is only four. The last three tasks can be mapped\narbitrarily among the processes to satisfy the constraints of the task-dependency graph.\nHowever, it makes more sense to map the tasks connected by an edge onto the same process\nbecause this prevents an inter-task interaction from becoming an inter-processes interaction.\nFor example, in Figure 3.7(b), if Task 5 is mapped onto process P2, then both processes P0 and\nP1 will need to interact with P2. In the current mapping, only a single interaction between P0 and\nP1 suffices.\nFigure 3.7. Mappings of the task graphs of Figure 3.5  onto four\nprocesses.\n3.1.4 Processes versus Processors\nIn the context of parallel", "doc_id": "df4379a1-2ab5-4e37-a18c-26b16996ab7f", "embedding": null, "doc_hash": "65035fa58e3275e1fcf1a4042ba879903f0c35e429457acc39a2131d19dfda70", "extra_info": null, "node_info": {"start": 290490, "end": 294542}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "69cd95da-af06-49cc-a1c5-9fb5a8404bfe", "3": "03e2429d-7fdd-454a-a561-efe7c38a6dce"}}, "__type__": "1"}, "03e2429d-7fdd-454a-a561-efe7c38a6dce": {"__data__": {"text": "is seven. This is\nbecause the maximum degree of concurrency is only four. The last three tasks can be mapped\narbitrarily among the processes to satisfy the constraints of the task-dependency graph.\nHowever, it makes more sense to map the tasks connected by an edge onto the same process\nbecause this prevents an inter-task interaction from becoming an inter-processes interaction.\nFor example, in Figure 3.7(b), if Task 5 is mapped onto process P2, then both processes P0 and\nP1 will need to interact with P2. In the current mapping, only a single interaction between P0 and\nP1 suffices.\nFigure 3.7. Mappings of the task graphs of Figure 3.5  onto four\nprocesses.\n3.1.4 Processes versus Processors\nIn the context of parallel algorithm design, processes are logical computing agents that perform\ntasks. Processors are the hardware units that physically perform computations. In this text, we\nchoose to express parallel algorithms and programs in terms of processes. In most cases, when\nwe refer to processes in the context of a parallel algorithm, there is a one-to-one\ncorrespondence between processes and processors and it is appropriate to assume that there\nare as many processes as the number of physical CPUs on the parallel computer. However,\nsometimes a higher level of abstraction may be required to express a parallel algorithm,\nespecially if it is a complex algorithm with multiple stages or with different forms of parallelism.\nTreating processes and processors separately is also useful when designing parallel programs\nfor hardware that supports multiple programming paradigms. For instance, consider a parallel\ncomputer that consists of multiple computing nodes that communicate with each other via\nmessage passing. Now each of these nodes could be a shared-address-space module with\nmultiple CPUs. Consider implementing matrix multiplication on such a parallel computer. The\nbest way to design a parallel algorithm is to do so in two stages. First, develop a decomposition\nand mapping strategy suitable for the message-passing paradigm and use this to exploit\nparallelism among the nodes. Each task that the original matrix multiplication problem\ndecomposes into is a matrix multiplication computation itself. The next step is to develop a\ndecomposition and mapping strategy suitable for the shared-memory paradigm and use this to\nimplement each task on the multiple CPUs of a node.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n3.2 Decomposition Techniques\nAs mentioned earlier, one of the fundamental steps that we need to undertake to solve a\nproblem in parallel is to split the computations to be performed into a set of tasks for\nconcurrent execution defined by the task-dependency graph. In this section, we describe some\ncommonly used decomposition techniques for achieving concurrency. This is not an exhaustive\nset of possible decomposition techniques. Also, a given decomposition is not always guaranteed\nto lead to the best parallel algorithm for a given problem. Despite these shortcomings, the\ndecomposition techniques described in this section often provide a good starting point for many\nproblems and one or a combination of these techniques can be used to obtain effective\ndecompositions for a large variety of problems.\nThese techniques are broadly classified as recursive decomposition , data-decomposition ,\nexploratory decomposition , and speculative decomposition . The recursive- and data-\ndecomposition techniques are relatively general purpose  as they can be used to decompose a\nwide variety of problems. On the other hand, speculative- and exploratory-decomposition\ntechniques are more of a special purpose  nature because they apply to specific classes of\nproblems.\n3.2.1 Recursive Decomposition\nRecursive decomposition is a method for inducing concurrency in problems that can be solved\nusing the divide-and-conquer strategy. In this technique, a problem is solved by first dividing it\ninto a set of independent subproblems. Each one of these subproblems is solved by recursively\napplying a similar division into smaller", "doc_id": "03e2429d-7fdd-454a-a561-efe7c38a6dce", "embedding": null, "doc_hash": "7a4ac30e04900b5d3c02cfadabd365a4987e079b82f5ec7ddea57ee8cdde8211", "extra_info": null, "node_info": {"start": 294571, "end": 298620}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "df4379a1-2ab5-4e37-a18c-26b16996ab7f", "3": "3cce9c40-4cc8-4963-a5ca-a11d32fe5b4b"}}, "__type__": "1"}, "3cce9c40-4cc8-4963-a5ca-a11d32fe5b4b": {"__data__": {"text": "classified as recursive decomposition , data-decomposition ,\nexploratory decomposition , and speculative decomposition . The recursive- and data-\ndecomposition techniques are relatively general purpose  as they can be used to decompose a\nwide variety of problems. On the other hand, speculative- and exploratory-decomposition\ntechniques are more of a special purpose  nature because they apply to specific classes of\nproblems.\n3.2.1 Recursive Decomposition\nRecursive decomposition is a method for inducing concurrency in problems that can be solved\nusing the divide-and-conquer strategy. In this technique, a problem is solved by first dividing it\ninto a set of independent subproblems. Each one of these subproblems is solved by recursively\napplying a similar division into smaller subproblems followed by a combination of their results.\nThe divide-and-conquer strategy results in natural concurrency, as different subproblems can be\nsolved concurrently.\nExample 3.4 Quicksort\nConsider the problem of sorting a sequence A of n elements using the commonly used\nquicksort algorithm. Quicksort is a divide and conquer algorithm that starts by\nselecting a pivot element x and then partitions the sequence A into two subsequences\nA0 and A1 such that all the elements in A0 are smaller than x and all the elements in\nA1 are greater than or equal to x. This partitioning step forms the divide  step of the\nalgorithm. Each one of the subsequences A0 and A1 is sorted by recursively calling\nquicksort. Each one of these recursive calls further partitions the sequences. This is\nillustrated in Figure 3.8 for a sequence of 12 numbers. The recursion terminates when\neach subsequence contains only a single element. \nFigure 3.8. The quicksort task-dependency graph based on\nrecursive decomposition for sorting a sequence of 12 numbers.\nIn Figure 3.8 , we define a task as the work of partitioning a given subsequence. Therefore,\nFigure 3.8  also represents the task graph for the problem. Initially, there is only one sequence\n(i.e., the root of the tree), and we can use only a single process to partition it. The completion\nof the root task results in two subsequences ( A0 and A1, corresponding to the two nodes at the\nfirst level of the tree) and each one can be partitioned in parallel. Similarly, the concurrency\ncontinues to increase as we move down the tree.\nSometimes, it is possible to restructure a computation to make it amenable to recursive\ndecomposition even if the commonly used algorithm for the problem is not based on the divide-\nand-conquer strategy. For example, consider the problem of finding the minimum element in an\nunordered sequence A of n elements. The serial algorithm for solving this problem scans the\nentire sequence A, recording at each step the minimum element found so far as illustrated in\nAlgorithm 3.1. It is easy to see that this serial algorithm exhibits no concurrency.\nAlgorithm 3.1 A serial program for finding the minimum in an array of\nnumbers A of length n.\n1.   procedure  SERIAL_MIN ( A, n) \n2.   begin \n3.   min = A[0]; \n4.   for i := 1 to n - 1 do \n5.       if (A[i] < min) min := A[i]; \n6.   endfor; \n7.   return min; \n8.   end SERIAL_MIN \nOnce we restructure this computation as a divide-and-conquer algorithm, we can use recursive\ndecomposition to extract concurrency. Algorithm 3.2  is a divide-and-conquer algorithm for\nfinding the minimum element in an array. In this algorithm, we split the sequence A into two\nsubsequences, each of size n/2, and we find the minimum for each of these subsequences by\nperforming a recursive call. Now the overall minimum element is found by selecting the\nminimum of these two subsequences. The recursion terminates when there is only one element\nleft in each", "doc_id": "3cce9c40-4cc8-4963-a5ca-a11d32fe5b4b", "embedding": null, "doc_hash": "fc6e6b56ced58bff0719d1167bdbd5a472fd85f6d23bfd278742fbc783ac90c3", "extra_info": null, "node_info": {"start": 298557, "end": 302295}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "03e2429d-7fdd-454a-a561-efe7c38a6dce", "3": "738cefda-8b6a-4215-96cb-454154292b8b"}}, "__type__": "1"}, "738cefda-8b6a-4215-96cb-454154292b8b": {"__data__": {"text": "1 to n - 1 do \n5.       if (A[i] < min) min := A[i]; \n6.   endfor; \n7.   return min; \n8.   end SERIAL_MIN \nOnce we restructure this computation as a divide-and-conquer algorithm, we can use recursive\ndecomposition to extract concurrency. Algorithm 3.2  is a divide-and-conquer algorithm for\nfinding the minimum element in an array. In this algorithm, we split the sequence A into two\nsubsequences, each of size n/2, and we find the minimum for each of these subsequences by\nperforming a recursive call. Now the overall minimum element is found by selecting the\nminimum of these two subsequences. The recursion terminates when there is only one element\nleft in each subsequence. Having restructured the serial computation in this manner, it is easy\nto construct a task-dependency graph for this problem. Figure 3.9 illustrates such a task-\ndependency graph for finding the minimum of eight numbers where each task is assigned the\nwork of finding the minimum of two numbers.\nFigure 3.9. The task-dependency graph for finding the minimum\nnumber in the sequence {4, 9, 1, 7, 8, 11, 2, 12}. Each node in the\ntree represents the task of finding the minimum of a pair of numbers.\nAlgorithm 3.2 A recursive program for finding the minimum in an array\nof numbers A of length n.\n1.   procedure  RECURSIVE_MIN ( A, n) \n2.   begin \n3.   if (n = 1) then \n4.       min := A[0]; \n5.   else \n6.       lmin := RECURSIVE_MIN ( A, n/2); \n7.       rmin := RECURSIVE_MIN (&( A[n/2]), n - n/2); \n8.       if (lmin < rmin) then \n9.           min := lmin; \n10.      else \n11.          min := rmin; \n12.      endelse; \n13.  endelse; \n14.  return min; \n15.  end RECURSIVE_MIN \n3.2.2 Data Decomposition\nData decomposition is a powerful and commonly used method for deriving concurrency in\nalgorithms that operate on large data structures. In this method, the decomposition of\ncomputations is done in two steps. In the first step, the data on which the computations are\nperformed is partitioned, and in the second step, this data partitioning is used to induce a\npartitioning of the computations into tasks. The operations that these tasks perform on different\ndata partitions are usually similar (e.g., matrix multiplication introduced in Example 3.5) or are\nchosen from a small set of operations (e.g., LU factorization introduced in Example 3.10 ).\nThe partitioning of data can be performed in many possible ways as discussed next. In general,\none must explore and evaluate all possible ways of partitioning the data and determine which\none yields a natural and efficient computational decomposition.\nPartitioning Output Data  In many computations, each element of the output can be computed\nindependently of others as a function of the input. In such computations, a partitioning of the\noutput data automatically induces a decomposition of the problems into tasks, where each task\nis assigned the work of computing a portion of the output. We introduce the problem of matrix-\nmultiplication in Example 3.5  to illustrate a decomposition based on partitioning output data.\nExample 3.5 Matrix multiplication\nConsider the problem of multiplying two n x n matrices A and B to yield a matrix C.\nFigure 3.10  shows a decomposition of this problem into four tasks. Each matrix is\nconsidered to be composed of four blocks or submatrices defined by splitting each\ndimension of the matrix into half. The four", "doc_id": "738cefda-8b6a-4215-96cb-454154292b8b", "embedding": null, "doc_hash": "10d67cfbab86077f7d74ac2f56d882993999237032571878903c3231eff384c2", "extra_info": null, "node_info": {"start": 302427, "end": 305801}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3cce9c40-4cc8-4963-a5ca-a11d32fe5b4b", "3": "043da2e4-f3fe-42c5-ae21-3069c40ecec0"}}, "__type__": "1"}, "043da2e4-f3fe-42c5-ae21-3069c40ecec0": {"__data__": {"text": " In many computations, each element of the output can be computed\nindependently of others as a function of the input. In such computations, a partitioning of the\noutput data automatically induces a decomposition of the problems into tasks, where each task\nis assigned the work of computing a portion of the output. We introduce the problem of matrix-\nmultiplication in Example 3.5  to illustrate a decomposition based on partitioning output data.\nExample 3.5 Matrix multiplication\nConsider the problem of multiplying two n x n matrices A and B to yield a matrix C.\nFigure 3.10  shows a decomposition of this problem into four tasks. Each matrix is\nconsidered to be composed of four blocks or submatrices defined by splitting each\ndimension of the matrix into half. The four submatrices of C, roughly of size n /2 x n/2\neach, are then independently computed by four tasks as the sums of the appropriate\nproducts of submatrices of A and B. \nFigure 3.10. (a) Partitioning of input and output matrices into 2\nx 2 submatrices. (b) A decomposition of matrix multiplication\ninto four tasks based on the partitioning of the matrices in (a).\nMost matrix algorithms, including matrix-vector and matrix-matrix multiplication, can be\nformulated in terms of block matrix operations. In such a formulation, the matrix is viewed as\ncomposed of blocks or submatrices and the scalar arithmetic operations on its elements are\nreplaced by the equivalent matrix operations on the blocks. The results of the element and the\nblock versions of the algorithm are mathematically equivalent (Problem 3.10). Block versions of\nmatrix algorithms are often used to aid decomposition.\nThe decomposition shown in Figure 3.10 is based on partitioning the output matrix C into four\nsubmatrices and each of the four tasks computes one of these submatrices. The reader must\nnote that data-decomposition is distinct from the decomposition of the computation into tasks.\nAlthough the two are often related and the former often aids the latter, a given data-\ndecomposition does not result in a unique decomposition into tasks. For example, Figure 3.11shows two other decompositions of matrix multiplication, each into eight tasks, corresponding\nto the same data-decomposition as used in Figure 3.10(a).\nFigure 3.11. Two examples of decomposition of matrix multiplication\ninto eight tasks.\nWe now introduce another example to illustrate decompositions based on data partitioning.\nExample 3.6  describes the problem of computing the frequency of a set of itemsets in a\ntransaction database, which can be decomposed based on the partitioning of output data.\nExample 3.6 Computing frequencies of itemsets in a transaction\ndatabase\nConsider the problem of computing the frequency of a set of itemsets in a transaction\ndatabase. In this problem we are given a set T containing n transactions and a set I\ncontaining m itemsets. Each transaction and itemset contains a small number of\nitems, out of a possible set of items. For example, T could be a grocery stores\ndatabase of customer sales with each transaction being an individual grocery list of a\nshopper and each itemset could be a group of items in the store. If the store desires to\nfind out how many customers bought each of the designated groups of items, then it\nwould need to find the number of times that each itemset in I appears in all the\ntransactions; i.e., the number of transactions of which each itemset is a subset of.\nFigure 3.12(a) shows an example of this type of computation. The database shown in\nFigure 3.12  consists of 10 transactions, and we are interested in computing the\nfrequency of the eight itemsets shown in the second column. The actual frequencies of\nthese itemsets in the database, which are the output of the frequency-computing\nprogram, are shown in the third column. For instance, itemset {D, K} appears twice,\nonce in the second and once in the ninth transaction. \nFigure 3.12. Computing itemset", "doc_id": "043da2e4-f3fe-42c5-ae21-3069c40ecec0", "embedding": null, "doc_hash": "a4f83c70aebafd8a14394143262258a47bf9792d655b6ff54e76a441b0c84cec", "extra_info": null, "node_info": {"start": 305698, "end": 309640}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "738cefda-8b6a-4215-96cb-454154292b8b", "3": "0a70a199-afd7-4524-b3f2-4f677155b355"}}, "__type__": "1"}, "0a70a199-afd7-4524-b3f2-4f677155b355": {"__data__": {"text": "If the store desires to\nfind out how many customers bought each of the designated groups of items, then it\nwould need to find the number of times that each itemset in I appears in all the\ntransactions; i.e., the number of transactions of which each itemset is a subset of.\nFigure 3.12(a) shows an example of this type of computation. The database shown in\nFigure 3.12  consists of 10 transactions, and we are interested in computing the\nfrequency of the eight itemsets shown in the second column. The actual frequencies of\nthese itemsets in the database, which are the output of the frequency-computing\nprogram, are shown in the third column. For instance, itemset {D, K} appears twice,\nonce in the second and once in the ninth transaction. \nFigure 3.12. Computing itemset frequencies in a transaction\ndatabase.\nFigure 3.12(b)  shows how the computation of frequencies of the itemsets can be decomposed\ninto two tasks by partitioning the output into two parts and having each task compute its half of\nthe frequencies. Note that, in the process, the itemsets input has also been partitioned, but the\nprimary motivation for the decomposition of Figure 3.12(b) is to have each task independently\ncompute the subset of frequencies assigned to it.\nPartitioning Input Data Partitioning of output data can be performed only if each output can\nbe naturally computed as a function of the input. In many algorithms, it is not possible or\ndesirable to partition the output data. For example, while finding the minimum, maximum, or\nthe sum of a set of numbers, the output is a single unknown value. In a sorting algorithm, the\nindividual elements of the output cannot be efficiently determined in isolation. In such cases, it\nis sometimes possible to partition the input data, and then use this partitioning to induce\nconcurrency. A task is created for each partition of the input data and this task performs as\nmuch computation as possible using these local data. Note that the solutions to tasks induced\nby input partitions may not directly solve the original problem. In such cases, a follow-up\ncomputation is needed to combine the results. For example, while finding the sum of a sequence\nof N numbers using p processes ( N > p), we can partition the input into p subsets of nearly\nequal sizes. Each task then computes the sum of the numbers in one of the subsets. Finally, the\np partial results can be added up to yield the final result.\nThe problem of computing the frequency of a set of itemsets in a transaction database described\nin Example 3.6 can also be decomposed based on a partitioning of input data. Figure 3.13(a)\nshows a decomposition based on a partitioning of the input set of transactions. Each of the two\ntasks computes the frequencies of all the itemsets in its respective subset of transactions. The\ntwo sets of frequencies, which are the independent outputs of the two tasks, represent\nintermediate results. Combining the intermediate results by pairwise addition yields the final\nresult.\nFigure 3.13. Some decompositions for computing itemset frequencies\nin a transaction database.\nPartitioning both Input and Output Data  In some cases, in which it is possible to partition\nthe output data, partitioning of input data can offer additional concurrency. For example,\nconsider the 4-way decomposition shown in Figure 3.13(b)  for computing itemset frequencies.\nHere, both the transaction set and the frequencies are divided into two parts and a different one\nof the four possible combinations is assigned to each of the four tasks. Each task then computes\na local set of frequencies. Finally, the outputs of Tasks 1 and 3 are added together, as are the\noutputs of Tasks 2 and 4.\nPartitioning Intermediate Data  Algorithms are often structured as multi-stage computations\nsuch that the output of one stage is the input to the subsequent stage. A decomposition of such\nan algorithm can be derived by partitioning the input or the output data of an intermediate\nstage of the algorithm. Partitioning intermediate", "doc_id": "0a70a199-afd7-4524-b3f2-4f677155b355", "embedding": null, "doc_hash": "661d83fd2c38f6bc5d344e5cb2982c89022cf63ca7cb9aeb69ec3672f3f88faf", "extra_info": null, "node_info": {"start": 309644, "end": 313664}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "043da2e4-f3fe-42c5-ae21-3069c40ecec0", "3": "93312774-8a01-4736-8f9d-d1b0c3fb013b"}}, "__type__": "1"}, "93312774-8a01-4736-8f9d-d1b0c3fb013b": {"__data__": {"text": "can offer additional concurrency. For example,\nconsider the 4-way decomposition shown in Figure 3.13(b)  for computing itemset frequencies.\nHere, both the transaction set and the frequencies are divided into two parts and a different one\nof the four possible combinations is assigned to each of the four tasks. Each task then computes\na local set of frequencies. Finally, the outputs of Tasks 1 and 3 are added together, as are the\noutputs of Tasks 2 and 4.\nPartitioning Intermediate Data  Algorithms are often structured as multi-stage computations\nsuch that the output of one stage is the input to the subsequent stage. A decomposition of such\nan algorithm can be derived by partitioning the input or the output data of an intermediate\nstage of the algorithm. Partitioning intermediate data can sometimes lead to higher concurrency\nthan partitioning input or output data. Often, the intermediate data are not generated explicitly\nin the serial algorithm for solving the problem and some restructuring of the original algorithm\nmay be required to use intermediate data partitioning to induce a decomposition.\nLet us revisit matrix multiplication to illustrate a decomposition based on partitioning\nintermediate data. Recall that the decompositions induced by a 2 x 2 partitioning of the output\nmatrix C, as shown in Figures 3.10 and 3.11, have a maximum degree of concurrency of four.\nWe can increase the degree of concurrency by introducing an intermediate stage in which eight\ntasks compute their respective product submatrices and store the results in a temporary three-\ndimensional matrix D, as shown in Figure 3.14. The submatrix Dk,i,j is the product of Ai,k and\nBk,j.\nFigure 3.14. Multiplication of matrices A and B with partitioning of the\nthree-dimensional intermediate matrix D.\nA partitioning of the intermediate matrix D induces a decomposition into eight tasks. Figure\n3.15 shows this decomposition. After the multiplication phase, a relatively inexpensive matrix\naddition step can compute the result matrix C. All submatrices D*,i,j with the same second and\nthird dimensions i and j are added to yield Ci,j. The eight tasks numbered 1 through 8 in Figure\n3.15 perform O(n3/8) work each in multiplying n/2 x n/2 submatrices of A and B. Then, four\ntasks numbered 9 through 12 spend O(n2/4) time each in adding the appropriate n/2 x n/2\nsubmatrices of the intermediate matrix D to yield the final result matrix C. Figure 3.16  shows\nthe task-dependency graph corresponding to the decomposition shown in Figure 3.15 .\nFigure 3.15. A decomposition of matrix multiplication based on\npartitioning the intermediate three-dimensional matrix.\nFigure 3.16. The task-dependency graph of the decomposition shown\nin Figure 3.15 .\nNote that all elements of D are computed implicitly in the original decomposition shown in\nFigure 3.11 , but are not explicitly stored. By restructuring the original algorithm and by\nexplicitly storing D, we have been able to devise a decomposition with higher concurrency. This,\nhowever, has been achieved at the cost of extra aggregate memory usage.\nThe Owner-Computes Rule  A decomposition based on partitioning output or input data is also\nwidely referred to as the owner-computes  rule. The idea behind this rule is that each partition\nperforms all the computations involving data that it owns. Depending on the nature of the data\nor the type of data-partitioning, the owner-computes rule may mean different things. For\ninstance, when we assign partitions of the input data to tasks, then the owner-computes rule\nmeans that a task performs all the computations that can be done using these data. On the\nother hand, if we partition the output data, then the owner-computes rule means that a task\ncomputes all the data in the partition assigned to it.\n3.2.3 Exploratory Decomposition\nExploratory decomposition  is", "doc_id": "93312774-8a01-4736-8f9d-d1b0c3fb013b", "embedding": null, "doc_hash": "464547cb076dbad527a207050f1628678e234dc0926f373991cdb3991dbb7e56", "extra_info": null, "node_info": {"start": 313646, "end": 317493}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0a70a199-afd7-4524-b3f2-4f677155b355", "3": "3843aa89-ab0c-42a9-80fb-4b531e8d2868"}}, "__type__": "1"}, "3843aa89-ab0c-42a9-80fb-4b531e8d2868": {"__data__": {"text": "Rule  A decomposition based on partitioning output or input data is also\nwidely referred to as the owner-computes  rule. The idea behind this rule is that each partition\nperforms all the computations involving data that it owns. Depending on the nature of the data\nor the type of data-partitioning, the owner-computes rule may mean different things. For\ninstance, when we assign partitions of the input data to tasks, then the owner-computes rule\nmeans that a task performs all the computations that can be done using these data. On the\nother hand, if we partition the output data, then the owner-computes rule means that a task\ncomputes all the data in the partition assigned to it.\n3.2.3 Exploratory Decomposition\nExploratory decomposition  is used to decompose problems whose underlying computations\ncorrespond to a search of a space for solutions. In exploratory decomposition, we partition the\nsearch space into smaller parts, and search each one of these parts concurrently, until the\ndesired solutions are found. For an example of exploratory decomposition, consider the 15-\npuzzle problem.\nExample 3.7 The 15-puzzle problem\nThe 15-puzzle consists of 15 tiles numbered 1 through 15 and one blank tile placed in\na 4 x 4 grid. A tile can be moved into the blank position from a position adjacent to it,\nthus creating a blank in the tile's original position. Depending on the configuration of\nthe grid, up to four moves are possible: up, down, left, and right. The initial and final\nconfigurations of the tiles are specified. The objective is to determine any sequence or\na shortest sequence of moves that transforms the initial configuration to the final\nconfiguration. Figure 3.17 illustrates sample initial and final configurations and a\nsequence of moves leading from the initial configuration to the final configuration. \nFigure 3.17. A 15-puzzle problem instance showing the initial\nconfiguration (a), the final configuration (d), and a sequence of\nmoves leading from the initial to the final configuration.\nThe 15-puzzle is typically solved using tree-search techniques. Starting from the initial\nconfiguration, all possible successor configurations are generated. A configuration may have 2,\n3, or 4 possible successor configurations, each corresponding to the occupation of the empty\nslot by one of its neighbors. The task of finding a path from initial to final configuration now\ntranslates to finding a path from one of these newly generated configurations to the final\nconfiguration. Since one of these newly generated configurations must be closer to the solution\nby one move (if a solution exists), we have made some progress towards finding the solution.\nThe configuration space generated by the tree search is often referred to as a state space graph.\nEach node of the graph is a configuration and each edge of the graph connects configurations\nthat can be reached from one another by a single move of a tile.\nOne method for solving this problem in parallel is as follows. First, a few levels of configurations\nstarting from the initial configuration are generated serially until the search tree has a sufficient\nnumber of leaf nodes (i.e., configurations of the 15-puzzle). Now each node is assigned to a\ntask to explore further until at least one of them finds a solution. As soon as one of the\nconcurrent tasks finds a solution it can inform the others to terminate their searches. Figure3.18 illustrates one such decomposition into four tasks in which task 4 finds the solution.\nFigure 3.18. The states generated by an instance of the 15-puzzle\nproblem.\n\n\nNote that even though exploratory decomposition may appear similar to data-decomposition\n(the search space can be thought of as being the data that get partitioned) it is fundamentally\ndifferent in the following way. The tasks induced by data-decomposition are performed in their\nentirety and each task performs useful computations towards the solution of the problem. On\nthe other hand, in exploratory decomposition, unfinished tasks can be terminated as soon as an\noverall solution is found. Hence, the portion of the search space", "doc_id": "3843aa89-ab0c-42a9-80fb-4b531e8d2868", "embedding": null, "doc_hash": "38e8286c7b54720bfd7b4cab3cfa557cd48dbfab2bcf896f09f791c7c6504064", "extra_info": null, "node_info": {"start": 317531, "end": 321643}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "93312774-8a01-4736-8f9d-d1b0c3fb013b", "3": "198b11ef-d048-4c97-9c8b-ec61394940d4"}}, "__type__": "1"}, "198b11ef-d048-4c97-9c8b-ec61394940d4": {"__data__": {"text": "solution it can inform the others to terminate their searches. Figure3.18 illustrates one such decomposition into four tasks in which task 4 finds the solution.\nFigure 3.18. The states generated by an instance of the 15-puzzle\nproblem.\n\n\nNote that even though exploratory decomposition may appear similar to data-decomposition\n(the search space can be thought of as being the data that get partitioned) it is fundamentally\ndifferent in the following way. The tasks induced by data-decomposition are performed in their\nentirety and each task performs useful computations towards the solution of the problem. On\nthe other hand, in exploratory decomposition, unfinished tasks can be terminated as soon as an\noverall solution is found. Hence, the portion of the search space searched (and the aggregate\namount of work performed) by a parallel formulation can be very different from that searched\nby a serial algorithm. The work performed by the parallel formulation can be either smaller or\ngreater than that performed by the serial algorithm. For example, consider a search space that\nhas been partitioned into four concurrent tasks as shown in Figure 3.19. If the solution lies right\nat the beginning of the search space corresponding to task 3 ( Figure 3.19(a) ), then it will be\nfound almost immediately by the parallel formulation. The serial algorithm would have found\nthe solution only after performing work equivalent to searching the entire space corresponding\nto tasks 1 and 2. On the other hand, if the solution lies towards the end of the search space\ncorresponding to task 1 ( Figure 3.19(b)), then the parallel formulation will perform almost four\ntimes the work of the serial algorithm and will yield no speedup.\nFigure 3.19. An illustration of anomalous speedups resulting from\nexploratory decomposition.\n3.2.4 Speculative Decomposition\nSpeculative decomposition  is used when a program may take one of many possible\ncomputationally significant branches depending on the output of other computations that\nprecede it. In this situation, while one task is performing the computation whose output is used\nin deciding the next computation, other tasks can concurrently start the computations of the\nnext stage. This scenario is similar to evaluating one or more of the branches of a switch\nstatement in C in parallel before the input for the switch  is available. While one task is\nperforming the computation that will eventually resolve the switch, other tasks could pick up the\nmultiple branches of the switch in parallel. When the input for the switch  has finally been\ncomputed, the computation corresponding to the correct branch would be used while that\ncorresponding to the other branches would be discarded. The parallel run time is smaller than\nthe serial run time by the amount of time required to evaluate the condition on which the next\ntask depends because this time is utilized to perform a useful computation for the next stage in\nparallel. However, this parallel formulation of a switch guarantees at least some wasteful\ncomputation. In order to minimize the wasted computation, a slightly different formulation of\nspeculative decomposition could be used, especially in situations where one of the outcomes of\nthe switch is more likely than the others. In this case, only the most promising branch is taken\nup a task in parallel with the preceding computation. In case the outcome of the switch is\ndifferent from what was anticipated, the computation is rolled back and the correct branch of\nthe switch is taken.\nThe speedup due to speculative decomposition can add up if there are multiple speculative\nstages. An example of an application in which speculative decomposition is useful is discrete\nevent simulation . A detailed description of discrete event simulation is beyond the scope of\nthis chapter; however, we give a simplified description of the problem.\nExample 3.8 Parallel discrete event simulation\nConsider the simulation of a system that is represented as a network or a directed\ngraph. The nodes of this network represent components. Each component has an input\nbuffer of jobs. The initial state of each component or node", "doc_id": "198b11ef-d048-4c97-9c8b-ec61394940d4", "embedding": null, "doc_hash": "9f587e26b490b469dfb3626eda6346ed51c3d5b2b9e55bd2c10541dfdc48c002", "extra_info": null, "node_info": {"start": 321615, "end": 325774}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3843aa89-ab0c-42a9-80fb-4b531e8d2868", "3": "32f6b2fd-c9dd-4e3f-b53b-bae5ed7b2f05"}}, "__type__": "1"}, "32f6b2fd-c9dd-4e3f-b53b-bae5ed7b2f05": {"__data__": {"text": "is taken\nup a task in parallel with the preceding computation. In case the outcome of the switch is\ndifferent from what was anticipated, the computation is rolled back and the correct branch of\nthe switch is taken.\nThe speedup due to speculative decomposition can add up if there are multiple speculative\nstages. An example of an application in which speculative decomposition is useful is discrete\nevent simulation . A detailed description of discrete event simulation is beyond the scope of\nthis chapter; however, we give a simplified description of the problem.\nExample 3.8 Parallel discrete event simulation\nConsider the simulation of a system that is represented as a network or a directed\ngraph. The nodes of this network represent components. Each component has an input\nbuffer of jobs. The initial state of each component or node is idle. An idle component\npicks up a job from its input queue, if there is one, processes that job in some finite\namount of time, and puts it in the input buffer of the components which are connected\nto it by outgoing edges. A component has to wait if the input buffer of one of its\noutgoing neighbors if full, until that neighbor picks up a job to create space in the\nbuffer. There is a finite number of input job types. The output of a component (and\nhence the input to the components connected to it) and the time it takes to process a\njob is a function of the input job. The problem is to simulate the functioning of the\nnetwork for a given sequence or a set of sequences of input jobs and compute the\ntotal completion time and possibly other aspects of system behavior. Figure 3.20shows a simple network for a discrete event solution problem. \nFigure 3.20. A simple network for discrete event simulation.\nThe problem of simulating a sequence of input jobs on the network described in Example 3.8\nappears inherently sequential because the input of a typical component is the output of another.\nHowever, we can define speculative tasks that start simulating a subpart of the network, each\nassuming one of several possible inputs to that stage. When an actual input to a certain stage\nbecomes available (as a result of the completion of another selector task from a previous\nstage), then all or part of the work required to simulate this input would have already been\nfinished if the speculation was correct, or the simulation of this stage is restarted with the most\nrecent correct input if the speculation was incorrect.\nSpeculative decomposition is different from exploratory decomposition in the following way. In\nspeculative decomposition, the input at a branch leading to multiple parallel tasks is unknown,\nwhereas in exploratory decomposition, the output of the multiple tasks originating at a branch\nis unknown. In speculative decomposition, the serial algorithm would strictly perform only one\nof the tasks at a speculative stage because when it reaches the beginning of that stage, it knows\nexactly which branch to take. Therefore, by preemptively computing for multiple possibilities\nout of which only one materializes, a parallel program employing speculative decomposition\nperforms more aggregate work than its serial counterpart. Even if only one of the possibilities is\nexplored speculatively, the parallel algorithm may perform more or the same amount of work as\nthe serial algorithm. On the other hand, in exploratory decomposition, the serial algorithm too\nmay explore different alternatives one after the other, because the branch that may lead to the\nsolution is not known beforehand. Therefore, the parallel program may perform more, less, or\nthe same amount of aggregate work compared to the serial algorithm depending on the location\nof the solution in the search space.\n3.2.5 Hybrid Decompositions\nSo far we have discussed a number of decomposition methods that can be used to derive\nconcurrent formulations of many algorithms. These decomposition techniques are not exclusive,\nand can often be combined together. Often, a computation is structured into multiple stages\nand it is sometimes necessary to apply different types of decomposition in different stages. For\nexample, while finding the minimum of a large set of n numbers, a purely recursive\ndecomposition may result in far more", "doc_id": "32f6b2fd-c9dd-4e3f-b53b-bae5ed7b2f05", "embedding": null, "doc_hash": "024b082a093d7d7ff2e3e772d68c105551096079dd0eb85b1c3bb920f226578c", "extra_info": null, "node_info": {"start": 325723, "end": 329975}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "198b11ef-d048-4c97-9c8b-ec61394940d4", "3": "62df075e-1639-49fb-bbca-e3e3465ec132"}}, "__type__": "1"}, "62df075e-1639-49fb-bbca-e3e3465ec132": {"__data__": {"text": "explore different alternatives one after the other, because the branch that may lead to the\nsolution is not known beforehand. Therefore, the parallel program may perform more, less, or\nthe same amount of aggregate work compared to the serial algorithm depending on the location\nof the solution in the search space.\n3.2.5 Hybrid Decompositions\nSo far we have discussed a number of decomposition methods that can be used to derive\nconcurrent formulations of many algorithms. These decomposition techniques are not exclusive,\nand can often be combined together. Often, a computation is structured into multiple stages\nand it is sometimes necessary to apply different types of decomposition in different stages. For\nexample, while finding the minimum of a large set of n numbers, a purely recursive\ndecomposition may result in far more tasks than the number of processes, P, available. An\nefficient decomposition would partition the input into P roughly equal parts and have each task\ncompute the minimum of the sequence assigned to it. The final result can be obtained by finding\nthe minimum of the P intermediate results by using the recursive decomposition shown in\nFigure 3.21.\nFigure 3.21. Hybrid decomposition for finding the minimum of an array\nof size 16 using four tasks.\nAs another example of an application of hybrid decomposition, consider performing quicksort in\nparallel. In Example 3.4, we used a recursive decomposition to derive a concurrent formulation\nof quicksort. This formulation results in O(n) tasks for the problem of sorting a sequence of size\nn. But due to the dependencies among these tasks and due to uneven sizes of the tasks, the\neffective concurrency is quite limited. For example, the first task for splitting the input list into\ntwo parts takes O(n) time, which puts an upper limit on the performance gain possible via\nparallelization. But the step of splitting lists performed by tasks in parallel quicksort can also be\ndecomposed using the input decomposition technique discussed in Section 9.4.1. The resulting\nhybrid decomposition that combines recursive decomposition and the input data-decomposition\nleads to a highly concurrent formulation of quicksort.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n3.3 Characteristics of Tasks and Interactions\nThe various decomposition techniques described in the previous section allow us to identify the\nconcurrency that is available in a problem and decompose it into tasks that can be executed in\nparallel. The next step in the process of designing a parallel algorithm is to take these tasks and\nassign (i.e., map) them onto the available processes. While devising a mapping scheme to\nconstruct a good parallel algorithm, we often take a cue from the decomposition. The nature of\nthe tasks and the interactions among them has a bearing on the mapping. In this section, we\nshall discuss the various properties of tasks and inter-task interactions that affect the choice of\na good mapping.\n3.3.1 Characteristics of Tasks\nThe following four characteristics of the tasks have a large influence on the suitability of a\nmapping scheme.\nTask Generation  The tasks that constitute a parallel algorithm may be generated either\nstatically or dynamically. Static task generation  refers to the scenario where all the tasks are\nknown before the algorithm starts execution. Data decomposition usually leads to static task\ngeneration. Examples of data-decomposition leading to a static task generation include matrix-\nmultiplication and LU factorization (Problem 3.5). Recursive decomposition can also lead to a\nstatic task-dependency graph. Finding the minimum of a list of numbers ( Figure 3.9) is an\nexample of a static recursive task-dependency graph.\nCertain decompositions lead to a dynamic task generation  during the execution of the\nalgorithm. In such decompositions, the actual tasks and the task-dependency graph are not\nexplicitly available a priori , although the high level rules or guidelines governing task generation\nare known as a part of the algorithm. Recursive decomposition can lead to dynamic task\ngeneration. For example, consider the recursive decomposition in", "doc_id": "62df075e-1639-49fb-bbca-e3e3465ec132", "embedding": null, "doc_hash": "60e7d8010fb4debe506c3eb86ddd5827bdc16dc3053e20fd70893d3015bbb55d", "extra_info": null, "node_info": {"start": 329977, "end": 334113}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "32f6b2fd-c9dd-4e3f-b53b-bae5ed7b2f05", "3": "6f845ed0-c9c2-4243-8211-a25fb2ee4977"}}, "__type__": "1"}, "6f845ed0-c9c2-4243-8211-a25fb2ee4977": {"__data__": {"text": "the algorithm starts execution. Data decomposition usually leads to static task\ngeneration. Examples of data-decomposition leading to a static task generation include matrix-\nmultiplication and LU factorization (Problem 3.5). Recursive decomposition can also lead to a\nstatic task-dependency graph. Finding the minimum of a list of numbers ( Figure 3.9) is an\nexample of a static recursive task-dependency graph.\nCertain decompositions lead to a dynamic task generation  during the execution of the\nalgorithm. In such decompositions, the actual tasks and the task-dependency graph are not\nexplicitly available a priori , although the high level rules or guidelines governing task generation\nare known as a part of the algorithm. Recursive decomposition can lead to dynamic task\ngeneration. For example, consider the recursive decomposition in quicksort ( Figure 3.8). The\ntasks are generated dynamically, and the size and shape of the task tree is determined by the\nvalues in the input array to be sorted. An array of the same size can lead to task-dependency\ngraphs of different shapes and with a different total number of tasks.\nExploratory decomposition can be formulated to generate tasks either statically or dynamically.\nFor example, consider the 15-puzzle problem discussed in Section 3.2.3. One way to generate a\nstatic task-dependency graph using exploratory decomposition is as follows. First, a\npreprocessing task starts with the initial configuration and expands the search tree in a\nbreadth-first manner until a predefined number of configurations are generated. These\nconfiguration now represent independent tasks, which can be mapped onto different processes\nand run independently. A different decomposition that generates tasks dynamically would be\none in which a task takes a state as input, expands it through a predefined number of steps of\nbreadth-first search and spawns new tasks to perform the same computation on each of the\nresulting states (unless it has found the solution, in which case the algorithm terminates).\nTask Sizes  The size of a task is the relative amount of time required to complete it. The\ncomplexity of mapping schemes often depends on whether or not the tasks are uniform ; i.e.,\nwhether or not they require roughly the same amount of time. If the amount of time required\nby the tasks varies significantly, then they are said to be non-uniform . For example, the tasks\nin the decompositions for matrix multiplication shown in Figures 3.10 and 3.11 would be\nconsidered uniform. On the other hand, the tasks in quicksort in Figure 3.8  are non-uniform.\nKnowledge of Task Sizes  The third characteristic that influences the choice of mapping\nscheme is knowledge of the task size. If the size of all the tasks is known, then this information\ncan often be used in mapping of tasks to processes. For example, in the various decompositions\nfor matrix multiplication discussed so far, the computation time for each task is known before\nthe parallel program starts. On the other hand, the size of a typical task in the 15-puzzle\nproblem is unknown. We do not know a priori  how many moves will lead to the solution from a\ngiven state.\nSize of Data Associated with Tasks  Another important characteristic of a task is the size of\ndata associated with it. The reason this is an important consideration for mapping is that the\ndata associated with a task must be available to the process performing that task, and the size\nand the location of these data may determine the process that can perform the task without\nincurring excessive data-movement overheads.\nDifferent types of data associated with a task may have different sizes. For instance, the input\ndata may be small but the output may be large, or vice versa. For example, the input to a task\nin the 15-puzzle problem may be just one state of the puzzle. This is a small input relative to\nthe amount of computation that may be required to find a sequence of moves from this state to\na solution state. In the problem of computing the minimum of a sequence, the size of the input\nis proportional to the amount of computation, but the output is just one number. In the", "doc_id": "6f845ed0-c9c2-4243-8211-a25fb2ee4977", "embedding": null, "doc_hash": "cdff8b5db70e45b2ba5678be05e981db5bf3bb0149dc6e0d967b2fdf3f0ce97b", "extra_info": null, "node_info": {"start": 334097, "end": 338253}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "62df075e-1639-49fb-bbca-e3e3465ec132", "3": "4e1262d7-aec6-4254-943b-67598d88b6f9"}}, "__type__": "1"}, "4e1262d7-aec6-4254-943b-67598d88b6f9": {"__data__": {"text": "is an important consideration for mapping is that the\ndata associated with a task must be available to the process performing that task, and the size\nand the location of these data may determine the process that can perform the task without\nincurring excessive data-movement overheads.\nDifferent types of data associated with a task may have different sizes. For instance, the input\ndata may be small but the output may be large, or vice versa. For example, the input to a task\nin the 15-puzzle problem may be just one state of the puzzle. This is a small input relative to\nthe amount of computation that may be required to find a sequence of moves from this state to\na solution state. In the problem of computing the minimum of a sequence, the size of the input\nis proportional to the amount of computation, but the output is just one number. In the parallel\nformulation of the quick sort, the size of both the input and the output data is of the same order\nas the sequential time needed to solve the task.\n3.3.2 Characteristics of Inter-Task Interactions\nIn any nontrivial parallel algorithm, tasks need to interact with each other to share data, work,\nor synchronization information. Different parallel algorithms require different types of\ninteractions among concurrent tasks. The nature of these interactions makes them more\nsuitable for certain programming paradigms and mapping schemes, and less suitable for others.\nThe types of inter-task interactions can be described along different dimensions, each\ncorresponding to a distinct characteristic of the underlying computations.\nStatic versus Dynamic One way of classifying the type of interactions that take place among\nconcurrent tasks is to consider whether or not these interactions have a static  or dynamic\npattern. An interaction pattern is static if for each task, the interactions happen at\npredetermined times, and if the set of tasks to interact with at these times is known prior to the\nexecution of the algorithm. In other words, in a static interaction pattern, not only is the task-\ninteraction graph known a priori , but the stage of the computation at which each interaction\noccurs is also known. An interaction pattern is dynamic if the timing of interactions or the set of\ntasks to interact with cannot be determined prior to the execution of the algorithm.\nStatic interactions can be programmed easily in the message-passing paradigm, but dynamic\ninteractions are harder to program. The reason is that interactions in message-passing require\nactive involvement of both interacting tasks \u2013 the sender and the receiver of information. The\nunpredictable nature of dynamic iterations makes it hard for both the sender and the receiver to\nparticipate in the interaction at the same time. Therefore, when implementing a parallel\nalgorithm with dynamic interactions in the message-passing paradigm, the tasks must be\nassigned additional synchronization or polling responsibility. Shared-address space\nprogramming can code both types of interactions equally easily.\nThe decompositions for parallel matrix multiplication presented earlier in this chapter exhibit\nstatic inter-task interactions. For an example of dynamic interactions, consider solving the 15-\npuzzle problem in which tasks are assigned different states to explore after an initial step that\ngenerates the desirable number of states by applying breadth-first search on the initial state. It\nis possible that a certain state leads to all dead ends and a task exhausts its search space\nwithout reaching the goal state, while other tasks are still busy trying to find a solution. The\ntask that has exhausted its work can pick up an unexplored state from the queue of another\nbusy task and start exploring it. The interactions involved in such a transfer of work from one\ntask to another are dynamic.\nRegular versus Irregular  Another way of classifying the interactions is based upon their\nspatial structure. An interaction pattern is considered to be regular  if it has some structure that\ncan be exploited for efficient implementation. On the other hand, an interaction pattern is called\nirregular  if no such regular pattern exists. Irregular and dynamic communications are harder\nto handle, particularly in the message-passing programming paradigm. An example of", "doc_id": "4e1262d7-aec6-4254-943b-67598d88b6f9", "embedding": null, "doc_hash": "0298dff07a19d29e91a1e9a97237bcf30a0d63323cf22fd8f7f03f47c88097c6", "extra_info": null, "node_info": {"start": 338275, "end": 342572}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6f845ed0-c9c2-4243-8211-a25fb2ee4977", "3": "311050d6-4a8d-4889-9d1c-c56f0a44a873"}}, "__type__": "1"}, "311050d6-4a8d-4889-9d1c-c56f0a44a873": {"__data__": {"text": "a certain state leads to all dead ends and a task exhausts its search space\nwithout reaching the goal state, while other tasks are still busy trying to find a solution. The\ntask that has exhausted its work can pick up an unexplored state from the queue of another\nbusy task and start exploring it. The interactions involved in such a transfer of work from one\ntask to another are dynamic.\nRegular versus Irregular  Another way of classifying the interactions is based upon their\nspatial structure. An interaction pattern is considered to be regular  if it has some structure that\ncan be exploited for efficient implementation. On the other hand, an interaction pattern is called\nirregular  if no such regular pattern exists. Irregular and dynamic communications are harder\nto handle, particularly in the message-passing programming paradigm. An example of a\ndecomposition with a regular interaction pattern is the problem of image dithering.\nExample 3.9 Image dithering\nIn image dithering, the color of each pixel in the image is determined as the weighted\naverage of its original value and the values of its neighboring pixels. We can easily\ndecompose this computation, by breaking the image into square regions and using a\ndifferent task to dither each one of these regions. Note that each task needs to access\nthe pixel values of the region assigned to it as well as the values of the image\nsurrounding its region. Thus, if we regard the tasks as nodes of a graph with an edge\nlinking a pair of interacting tasks, the resulting pattern is a two-dimensional mesh, as\nshown in Figure 3.22. \nFigure 3.22. The regular two-dimensional task-interaction\ngraph for image dithering. The pixels with dotted outline\nrequire color values from the boundary pixels of the\nneighboring tasks.\nSparse matrix-vector multiplication discussed in Section 3.1.2  provides a good example of\nirregular interaction, which is shown in Figure 3.6 . In this decomposition, even though each\ntask, by virtue of the decomposition, knows a priori  which rows of matrix A it needs to access,\nwithout scanning the row(s) of A assigned to it, a task cannot know which entries of vector b it\nrequires. The reason is that the access pattern for b depends on the structure of the sparse\nmatrix A.\nRead-only versus Read-Write  We have already learned that sharing of data among tasks\nleads to inter-task interaction. However, the type of sharing may impact the choice of the\nmapping. Data sharing interactions can be categorized as either read-only  or read-write\ninteractions. As the name suggests, in read-only interactions, tasks require only a read-access\nto the data shared among many concurrent tasks. For example, in the decomposition for\nparallel matrix multiplication shown in Figure 3.10, the tasks only need to read the shared input\nmatrices A and B. In read-write interactions, multiple tasks need to read and write on some\nshared data. For example, consider the problem of solving the 15-puzzle. The parallel\nformulation method proposed in Section 3.2.3 uses an exhaustive search to find a solution. In\nthis formulation, each state is considered an equally suitable candidate for further expansion.\nThe search can be made more efficient if the states that appear to be closer to the solution are\ngiven a priority for further exploration. An alternative search technique known as heuristic\nsearch implements such a strategy. In heuristic search, we use a heuristic to provide a relative\napproximate indication of the distance of each state from the solution (i.e. the potential number\nof moves required to reach the solution). In the case of the 15-puzzle, the number of tiles that\nare out of place in a given state could serve as such a heuristic. The states that need to be\nexpanded further are stored in a priority queue based on the value of this heuristic. While\nchoosing the states to expand, we give preference to more promising states, i.e. the ones that\nhave fewer out-of-place tiles and hence, are more likely to lead to a quick solution. In", "doc_id": "311050d6-4a8d-4889-9d1c-c56f0a44a873", "embedding": null, "doc_hash": "3101ff2ac743f987c6cd0890c77f9da44317af464ac272cf7cfe181d5581218b", "extra_info": null, "node_info": {"start": 342557, "end": 346584}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4e1262d7-aec6-4254-943b-67598d88b6f9", "3": "7c2e5da7-0380-40e9-abdb-4a04e47c8f25"}}, "__type__": "1"}, "7c2e5da7-0380-40e9-abdb-4a04e47c8f25": {"__data__": {"text": "the solution are\ngiven a priority for further exploration. An alternative search technique known as heuristic\nsearch implements such a strategy. In heuristic search, we use a heuristic to provide a relative\napproximate indication of the distance of each state from the solution (i.e. the potential number\nof moves required to reach the solution). In the case of the 15-puzzle, the number of tiles that\nare out of place in a given state could serve as such a heuristic. The states that need to be\nexpanded further are stored in a priority queue based on the value of this heuristic. While\nchoosing the states to expand, we give preference to more promising states, i.e. the ones that\nhave fewer out-of-place tiles and hence, are more likely to lead to a quick solution. In this\nsituation, the priority queue constitutes shared data and tasks need both read and write access\nto it; they need to put the states resulting from an expansion into the queue and they need to\npick up the next most promising state for the next expansion.\nOne-way versus Two-way  In some interactions, the data or work needed by a task or a\nsubset of tasks is explicitly supplied by another task or subset of tasks. Such interactions are\ncalled two-way  interactions and usually involve predefined producer and consumer tasks. In\nother interactions, only one of a pair of communicating tasks initiates the interaction and\ncompletes it without interrupting the other one. Such an interaction is called a one-way\ninteraction. All read-only interactions can be formulated as one-way interactions. Read-write\ninteractions can be either one-way or two-way.\nThe shared-address-space programming paradigms can handle both one-way and two-way\ninteractions equally easily. However, one-way interactions cannot be directly programmed in\nthe message-passing paradigm because the source of the data to be transferred must explicitly\nsend the data to the recipient. In the message-passing paradigm, all one-way interactions must\nbe converted to two-way interactions via program restructuring. Static one-way interactions can\nbe easily converted to two-way communications. Since the time and the location in the program\nof a static one-way interaction is known a priori , introducing a matching interaction operation in\nthe partner task is enough to convert a one-way static interaction to a two-way static\ninteraction. On the other hand, dynamic one-way interactions can require some nontrivial\nprogram restructuring to be converted to two-way interactions. The most common such\nrestructuring involves polling. Each task checks for pending requests from other tasks after\nregular intervals, and services such requests, if any.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n3.4 Mapping Techniques for Load Balancing\nOnce a computation has been decomposed into tasks, these tasks are mapped onto processes\nwith the objective that all tasks complete in the shortest amount of elapsed time. In order to\nachieve a small execution time, the overheads  of executing the tasks in parallel must be\nminimized. For a given decomposition, there are two key sources of overhead. The time spent in\ninter-process interaction is one source of overhead. Another important source of overhead is the\ntime that some processes may spend being idle. Some processes can be idle even before the\noverall computation is finished for a variety of reasons. Uneven load distribution may cause\nsome processes to finish earlier than others. At times, all the unfinished tasks mapped onto a\nprocess may be waiting for tasks mapped onto other processes to finish in order to satisfy the\nconstraints imposed by the task-dependency graph. Both interaction and idling are often a\nfunction of mapping. Therefore, a good mapping of tasks onto processes must strive to achieve\nthe twin objectives of (1) reducing the amount of time processes spend in interacting with each\nother, and (2) reducing the total amount of time some processes are idle while the others are\nengaged in", "doc_id": "7c2e5da7-0380-40e9-abdb-4a04e47c8f25", "embedding": null, "doc_hash": "9ab481bd6c9d5225c500c3f7427d5ae28c92dba1c99b101830712cca71585c07", "extra_info": null, "node_info": {"start": 346661, "end": 350646}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "311050d6-4a8d-4889-9d1c-c56f0a44a873", "3": "c3565267-9c7d-4510-85c8-30ca726e1d3b"}}, "__type__": "1"}, "c3565267-9c7d-4510-85c8-30ca726e1d3b": {"__data__": {"text": "is the\ntime that some processes may spend being idle. Some processes can be idle even before the\noverall computation is finished for a variety of reasons. Uneven load distribution may cause\nsome processes to finish earlier than others. At times, all the unfinished tasks mapped onto a\nprocess may be waiting for tasks mapped onto other processes to finish in order to satisfy the\nconstraints imposed by the task-dependency graph. Both interaction and idling are often a\nfunction of mapping. Therefore, a good mapping of tasks onto processes must strive to achieve\nthe twin objectives of (1) reducing the amount of time processes spend in interacting with each\nother, and (2) reducing the total amount of time some processes are idle while the others are\nengaged in performing some tasks.\nThese two objectives often conflict with each other. For example, the objective of minimizing the\ninteractions can be easily achieved by assigning sets of tasks that need to interact with each\nother onto the same process. In most cases, such a mapping will result in a highly unbalanced\nworkload among the processes. In fact, following this strategy to the limit will often map all\ntasks onto a single process. As a result, the processes with a lighter load will be idle when those\nwith a heavier load are trying to finish their tasks. Similarly, to balance the load among\nprocesses, it may be necessary to assign tasks that interact heavily to different processes. Due\nto the conflicts between these objectives, finding a good mapping is a nontrivial problem.\nIn this section, we will discuss various schemes for mapping tasks onto processes with the\nprimary view of balancing the task workload of processes and minimizing their idle time.\nReducing inter-process interaction is the topic of Section 3.5. The reader should be aware that\nassigning a balanced aggregate load of tasks to each process is a necessary but not sufficient\ncondition for reducing process idling. Recall that the tasks resulting from a decomposition are\nnot all ready for execution at the same time. A task-dependency graph determines which tasks\ncan execute in parallel and which must wait for some others to\nfinish at a given stage in the execution of a parallel algorithm. Therefore, it is possible in a\ncertain parallel formulation that although all processes perform the same aggregate amount of\nwork, at different times, only a fraction of the processes are active while the remainder contain\ntasks that must wait for other tasks to finish. Similarly, poor synchronization among interacting\ntasks can lead to idling if one of the tasks has to wait to send or receive data from another task.\nA good mapping must ensure that the computations and interactions among processes at each\nstage of the execution of the parallel algorithm are well balanced. Figure 3.23 shows two\nmappings of 12-task decomposition in which the last four tasks can be started only after the\nfirst eight are finished due to dependencies among tasks. As the figure shows, two mappings,\neach with an overall balanced workload, can result in different completion times.\nFigure 3.23. Two mappings of a hypothetical decomposition with a\nsynchronization.\nMapping techniques used in parallel algorithms can be broadly classified into two categories:\nstatic  and dynamic . The parallel programming paradigm and the characteristics of tasks and\nthe interactions among them determine whether a static or a dynamic mapping is more\nsuitable.\nStatic Mapping:  Static mapping techniques distribute the tasks among processes prior to\nthe execution of the algorithm. For statically generated tasks, either static or dynamic\nmapping can be used. The choice of a good mapping in this case depends on several\nfactors, including the knowledge of task sizes, the size of data associated with tasks, the\ncharacteristics of inter-task interactions, and even the parallel programming paradigm.\nEven when task sizes are known, in general, the problem of obtaining an optimal mapping\nis an NP-complete problem for non-uniform tasks. However, for many practical cases,\nrelatively inexpensive heuristics", "doc_id": "c3565267-9c7d-4510-85c8-30ca726e1d3b", "embedding": null, "doc_hash": "c789e0561a4b89c007ccfa4ce658e5a83d547a11b5e08e7d71f231985f570709", "extra_info": null, "node_info": {"start": 350648, "end": 354763}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7c2e5da7-0380-40e9-abdb-4a04e47c8f25", "3": "5fc4c8c9-d1cd-49a9-92e0-8a5975243395"}}, "__type__": "1"}, "5fc4c8c9-d1cd-49a9-92e0-8a5975243395": {"__data__": {"text": "programming paradigm and the characteristics of tasks and\nthe interactions among them determine whether a static or a dynamic mapping is more\nsuitable.\nStatic Mapping:  Static mapping techniques distribute the tasks among processes prior to\nthe execution of the algorithm. For statically generated tasks, either static or dynamic\nmapping can be used. The choice of a good mapping in this case depends on several\nfactors, including the knowledge of task sizes, the size of data associated with tasks, the\ncharacteristics of inter-task interactions, and even the parallel programming paradigm.\nEven when task sizes are known, in general, the problem of obtaining an optimal mapping\nis an NP-complete problem for non-uniform tasks. However, for many practical cases,\nrelatively inexpensive heuristics provide fairly acceptable approximate solutions to the\noptimal static mapping problem.\nAlgorithms that make use of static mapping are in general easier to design and program.Dynamic Mapping:  Dynamic mapping techniques distribute the work among processes\nduring the execution of the algorithm. If tasks are generated dynamically, then they must\nbe mapped dynamically too. If task sizes are unknown, then a static mapping can\npotentially lead to serious load-imbalances and dynamic mappings are usually more\neffective. If the amount of data associated with tasks is large relative to the computation,\nthen a dynamic mapping may entail moving this data among processes. The cost of this\ndata movement may outweigh some other advantages of dynamic mapping and may\nrender a static mapping more suitable. However, in a shared-address-space paradigm,\ndynamic mapping may work well even with large data associated with tasks if the\ninteraction is read-only. The reader should be aware that the shared-address-space\nprogramming paradigm does not automatically provide immunity against data-movement\ncosts. If the underlying hardware is NUMA ( Section 2.3.2), then the data may physically\nmove from a distant memory. Even in a cc-UMA architecture, the data may have to move\nfrom one cache to another.\nAlgorithms that require dynamic mapping are usually more complicated, particularly in the\nmessage-passing programming paradigm.\nHaving discussed the guidelines for choosing between static and dynamic mappings, we now\ndescribe various schemes of these two types of mappings in detail.\n3.4.1 Schemes for Static Mapping\nStatic mapping is often, though not exclusively, used in conjunction with a decomposition based\non data partitioning. Static mapping is also used for mapping certain problems that are\nexpressed naturally by a static task-dependency graph. In the following subsections, we will\ndiscuss mapping schemes based on data partitioning and task partitioning.\nMappings Based on Data Partitioning\nIn this section, we will discuss mappings based on partitioning two of the most common ways of\nrepresenting data in algorithms, namely, arrays and graphs. The data-partitioning actually\ninduces a decomposition, but the partitioning or the decomposition is selected with the final\nmapping in mind.\nArray Distribution Schemes  In a decomposition based on partitioning data, the tasks are\nclosely associated with portions of data by the owner-computes rule. Therefore, mapping the\nrelevant data onto the processes is equivalent to mapping tasks onto processes. We now study\nsome commonly used techniques of distributing arrays or matrices among processes.\nBlock Distributions\nBlock distributions  are some of the simplest ways to distribute an array and assign uniform\ncontiguous portions of the array to different processes. In these distributions, a d-dimensional\narray is distributed among the processes such that each process receives a contiguous block of\narray entries along a specified subset of array dimensions. Block distributions of arrays are\nparticularly suitable when there is a locality of interaction, i.e., computation of an element of an\narray requires other nearby elements in the array.\nFor example, consider an n x n two-dimensional array A with n rows and n columns. We can\nnow select one", "doc_id": "5fc4c8c9-d1cd-49a9-92e0-8a5975243395", "embedding": null, "doc_hash": "5bfc01268f8a04bb1035237a4675602b5687fc9379f41715b52f5df7c1b421aa", "extra_info": null, "node_info": {"start": 354722, "end": 358823}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c3565267-9c7d-4510-85c8-30ca726e1d3b", "3": "c694d345-37f9-4c9c-bab5-c3305e0cd38d"}}, "__type__": "1"}, "c694d345-37f9-4c9c-bab5-c3305e0cd38d": {"__data__": {"text": "onto the processes is equivalent to mapping tasks onto processes. We now study\nsome commonly used techniques of distributing arrays or matrices among processes.\nBlock Distributions\nBlock distributions  are some of the simplest ways to distribute an array and assign uniform\ncontiguous portions of the array to different processes. In these distributions, a d-dimensional\narray is distributed among the processes such that each process receives a contiguous block of\narray entries along a specified subset of array dimensions. Block distributions of arrays are\nparticularly suitable when there is a locality of interaction, i.e., computation of an element of an\narray requires other nearby elements in the array.\nFor example, consider an n x n two-dimensional array A with n rows and n columns. We can\nnow select one of these dimensions, e.g., the first dimension, and partition the array into p\nparts such that the kth part contains rows kn/p...(k + 1)n/p - 1, where 0 \n k < p. That is, each\npartition contains a block of n/p consecutive rows of A. Similarly, if we partition A along the\nsecond dimension, then each partition contains a block of n/p consecutive columns. These row-\nand column-wise array distributions are illustrated in Figure 3.24 .\nFigure 3.24. Examples of one-dimensional partitioning of an array\namong eight processes.\nSimilarly, instead of selecting a single dimension, we can select multiple dimensions to\npartition. For instance, in the case of array A we can select both dimensions and partition the\nmatrix into blocks such that each block corresponds to a n/p1 x n/p2 section of the matrix, with\np = p1 x p2 being the number of processes. Figure 3.25  illustrates two different two-dimensional\ndistributions, on a 4 x 4 and 2x 8 process grid, respectively. In general, given a d-dimensional\narray, we can distribute it using up to a d-dimensional block distribution.\nFigure 3.25. Examples of two-dimensional distributions of an array,\n(a) on a 4 x 4 process grid, and (b) on a 2 x 8 process grid.\nUsing these block distributions we can load-balance a variety of parallel computations that\noperate on multi-dimensional arrays. For example, consider the n x n matrix multiplication C =\nA x B , as discussed in Section 3.2.2 . One way of decomposing this computation is to partition\nthe output matrix C . Since each entry of C requires the same amount of computation, we can\nbalance the computations by using either a one- or two-dimensional block distribution to\npartition C uniformly among the p available processes. In the first case, each process will get a\nblock of n/p rows (or columns) of C, whereas in the second case, each process will get a block of\nsize \n. In either case, the process will be responsible for computing the entries of\nthe partition of C assigned to it.\nAs the matrix-multiplication example illustrates, quite often we have the choice of mapping the\ncomputations using either a one- or a two-dimensional distribution (and even more choices in\nthe case of higher dimensional arrays). In general, higher dimensional distributions allow us to\nuse more processes. For example, in the case of matrix-matrix multiplication, a one-\ndimensional distribution will allow us to use up to n processes by assigning a single row of C to\neach process. On the other hand, a two-dimensional distribution will allow us to use up to n2\nprocesses by assigning a single element of C to each process.\nIn addition to allowing a higher degree of concurrency, higher dimensional distributions also\nsometimes help in reducing the amount of interactions among the different processes for many\nproblems. Figure 3.26 illustrates this in the case of dense matrix-multiplication. With a one-\ndimensional partitioning along the rows, each process needs to access the corresponding n/p\nrows of matrix A and the entire matrix B, as shown in Figure 3.26(a)  for process P5. Thus the\ntotal amount of data that needs to be accessed", "doc_id": "c694d345-37f9-4c9c-bab5-c3305e0cd38d", "embedding": null, "doc_hash": "ae41c7309b2e36eb3cc391d6ec5fe1e4d55e2a87a7752c67d8ee3175fb7c6626", "extra_info": null, "node_info": {"start": 358815, "end": 362758}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5fc4c8c9-d1cd-49a9-92e0-8a5975243395", "3": "de556126-45e4-45f9-a592-a2513214bdb7"}}, "__type__": "1"}, "de556126-45e4-45f9-a592-a2513214bdb7": {"__data__": {"text": "multiplication, a one-\ndimensional distribution will allow us to use up to n processes by assigning a single row of C to\neach process. On the other hand, a two-dimensional distribution will allow us to use up to n2\nprocesses by assigning a single element of C to each process.\nIn addition to allowing a higher degree of concurrency, higher dimensional distributions also\nsometimes help in reducing the amount of interactions among the different processes for many\nproblems. Figure 3.26 illustrates this in the case of dense matrix-multiplication. With a one-\ndimensional partitioning along the rows, each process needs to access the corresponding n/p\nrows of matrix A and the entire matrix B, as shown in Figure 3.26(a)  for process P5. Thus the\ntotal amount of data that needs to be accessed is n2/p + n2. However, with a two-dimensional\ndistribution, each process needs to access \n  rows of matrix A and \n  columns of matrix\nB, as shown in Figure 3.26(b)  for process P5. In the two-dimensional case, the total amount of\nshared data that each process needs to access is \n , which is significantly smaller\ncompared to O(n2) shared data in the one-dimensional case.\nFigure 3.26. Data sharing needed for matrix multiplication with (a)\none-dimensional and (b) two-dimensional partitioning of the output\nmatrix. Shaded portions of the input matrices A and B are required by\nthe process that computes the shaded portion of the output matrix C.\nCyclic and Block-Cyclic Distributions\nIf the amount of work differs for different elements of a matrix, a block distribution can\npotentially lead to load imbalances. A classic example of this phenomenon is LU factorization of\na matrix, in which the amount of computation increases from the top left to the bottom right of\nthe matrix.\nExample 3.10 Dense LU factorization\nIn its simplest form,the LU factorization algorithm factors a nonsingular square matrix\nA into the product of a lower triangular matrix L with a unit diagonal and an upper\ntriangular matrix U. Algorithm 3.3 shows the serial algorithm. Let A be an n x n matrix\nwith rows and columns numbered from 1 to n. The factorization process consists of n\nmajor steps \u2013 each consisting of an iteration of the outer loop starting at Line 3 in\nAlgorithm 3.3. In step k, first, the partial column A[k + 1 : n, k] is divided by A[k, k].\nThen, the outer product A[k + 1 : n, k] x A[k, k + 1 : n] is subtracted from the ( n - k)\nx (n - k) submatrix A[k + 1 : n, k + 1 : n]. In a practical implementation of LU\nfactorization, separate arrays are not used for L and U and A is modified to store L\nand U in its lower and upper triangular parts, respectively. The 1's on the principal\ndiagonal of L are implicit and the diagonal entries actually belong to U after\nfactorization.\nAlgorithm 3.3 A serial column-based algorithm to factor a\nnonsingular matrix A into a lower-triangular matrix L and an\nupper-triangular matrix U. Matrices L and U share space with A.\nOn Line 9, A[i, j] on the left side of the assignment is\nequivalent to L [i, j] if i > j; otherwise, it is equivalent to U [i, j].\n1.   procedure  COL_LU ( A) \n2.   begin \n3.      for k := 1 to n do \n4.          for j := k to n do \n5.              A[j, k]:= A[j, k]/A[k, k]; \n6.          endfor; \n7.          for j := k + 1 to n do \n8.              for i := k + 1 to n do \n9.   ", "doc_id": "de556126-45e4-45f9-a592-a2513214bdb7", "embedding": null, "doc_hash": "03a5e0441b9be09f321e8619273f25cf43e3c595ca83b0cae71f931881c7b7a6", "extra_info": null, "node_info": {"start": 362787, "end": 366115}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c694d345-37f9-4c9c-bab5-c3305e0cd38d", "3": "abdbe149-b32e-4dfa-8dc1-10a00d706056"}}, "__type__": "1"}, "abdbe149-b32e-4dfa-8dc1-10a00d706056": {"__data__": {"text": "on the left side of the assignment is\nequivalent to L [i, j] if i > j; otherwise, it is equivalent to U [i, j].\n1.   procedure  COL_LU ( A) \n2.   begin \n3.      for k := 1 to n do \n4.          for j := k to n do \n5.              A[j, k]:= A[j, k]/A[k, k]; \n6.          endfor; \n7.          for j := k + 1 to n do \n8.              for i := k + 1 to n do \n9.                  A[i, j] := A[i, j] - A[i, k] x A[k, j]; \n10.             endfor; \n11.         endfor; \n   /* \nAfter this iteration, column A[k + 1 : n, k] is logically the kth \ncolumn of L and row A[k, k : n] is logically the kth row of U. \n   */ \n12.     endfor; \n13.  end COL_LU \nFigure 3.27  shows a possible decomposition of LU factorization into 14 tasks using a 3\nx 3 block partitioning of the matrix and using a block version of Algorithm 3.3 . \nFigure 3.27. A decomposition of LU factorization into 14 tasks.\nFor each iteration of the outer loop k := 1 to n, the next nested loop in Algorithm 3.3  goes from\nk + 1 to n. In other words, the active part of the matrix, as shown in Figure 3.28 , shrinks\ntowards the bottom right corner of the matrix as the computation proceeds. Therefore, in a\nblock distribution, the processes assigned to the beginning rows and columns (i.e., left rows and\ntop columns) would perform far less work than those assigned to the later rows and columns.\nFor example, consider the decomposition for LU factorization shown in Figure 3.27 with a 3 x 3\ntwo-dimensional block partitioning of the matrix. If we map all tasks associated with a certain\nblock onto a process in a 9-process ensemble, then a significant amount of idle time will result.\nFirst, computing different blocks of the matrix requires different amounts of work. This is\nillustrated in Figure 3.29. For example, computing the final value of A1,1 (which is actually L1,1\nU1,1) requires only one task \u2013 Task 1. On the other hand, computing the final value of A3,3\nrequires three tasks \u2013 Task 9, Task 13, and Task 14. Secondly, the process working on a block\nmay idle even when there are unfinished tasks associated with that block. This idling can occur\nif the constraints imposed by the task-dependency graph do not allow the remaining tasks on\nthis process to proceed until one or more tasks mapped onto other processes are completed.\nFigure 3.28. A typical computation in Gaussian elimination and the\nactive part of the coefficient matrix during the kth iteration of the\nouter loop.\nFigure 3.29. A naive mapping of LU factorization tasks onto processes\nbased on a two-dimensional block distribution.\nThe block-cyclic distribution  is a variation of the block distribution scheme that can be used\nto alleviate the load-imbalance and idling problems. A detailed description of LU factorization\nwith block-cyclic mapping is covered in Chapter 8, where it is shown how a block-cyclic\nmapping leads to a substantially more balanced work distribution than in Figure 3.29 . The\ncentral idea behind a block-cyclic distribution is to partition an array into many more blocks\nthan the number of available processes. Then we assign the partitions (and the associated\ntasks) to processes in a round-robin manner so that each process gets several", "doc_id": "abdbe149-b32e-4dfa-8dc1-10a00d706056", "embedding": null, "doc_hash": "0371f79c371cb01fe3a7820eda4c6c7b01448ba5b70202ef5b64baff76d9413a", "extra_info": null, "node_info": {"start": 366566, "end": 369758}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "de556126-45e4-45f9-a592-a2513214bdb7", "3": "8f548b23-6a59-45e7-be95-2720f60d61a0"}}, "__type__": "1"}, "8f548b23-6a59-45e7-be95-2720f60d61a0": {"__data__": {"text": "kth iteration of the\nouter loop.\nFigure 3.29. A naive mapping of LU factorization tasks onto processes\nbased on a two-dimensional block distribution.\nThe block-cyclic distribution  is a variation of the block distribution scheme that can be used\nto alleviate the load-imbalance and idling problems. A detailed description of LU factorization\nwith block-cyclic mapping is covered in Chapter 8, where it is shown how a block-cyclic\nmapping leads to a substantially more balanced work distribution than in Figure 3.29 . The\ncentral idea behind a block-cyclic distribution is to partition an array into many more blocks\nthan the number of available processes. Then we assign the partitions (and the associated\ntasks) to processes in a round-robin manner so that each process gets several non-adjacent\nblocks. More precisely, in a one-dimensional block-cyclic distribution of a matrix among p\nprocesses, the rows (columns) of an n x n matrix are divided into ap groups of n/(ap)\nconsecutive rows (columns), where 1 \n a \n n/p. Now, these blocks are distributed among the\np processes in a wraparound fashion such that block bi is assigned to process Pi %p ('%' is the\nmodulo operator). This distribution assigns a blocks of the matrix to each process, but each\nsubsequent block that gets assigned to the same process is p blocks away. We can obtain a\ntwo-dimensional block-cyclic distribution of an n x n array by partitioning it into square blocks\nof size \n  and distributing them on a hypothetical \n  array of processes in a\nwraparound fashion. Similarly, the block-cyclic distribution can be extended to arrays of higher\ndimensions. Figure 3.30  illustrates one- and two-dimensional block cyclic distributions of a two-\ndimensional array.\nFigure 3.30. Examples of one- and two-dimensional block-cyclic\ndistributions among four processes. (a) The rows of the array are\ngrouped into blocks each consisting of two rows, resulting in eight\nblocks of rows. These blocks are distributed to four processes in a\nwraparound fashion. (b) The matrix is blocked into 16 blocks each of\nsize 4 x 4, and it is mapped onto a 2 x 2 grid of processes in a\nwraparound fashion.\nThe reason why a block-cyclic distribution is able to significantly reduce the amount of idling is\nthat all processes have a sampling of tasks from all parts of the matrix. As a result, even if\ndifferent parts of the matrix require different amounts of work, the overall work on each process\nbalances out. Also, since the tasks assigned to a process belong to different parts of the matrix,\nthere is a good chance that at least some of them are ready for execution at any given time.\nNote that if we increase a to its upper limit of n/p, then each block is a single row (column) of\nthe matrix in a one-dimensional block-cyclic distribution and a single element of the matrix in a\ntwo-dimensional block-cyclic distribution. Such a distribution is known as a cyclic distribution .\nA cyclic distribution is an extreme case of a block-cyclic distribution and can result in an almost\nperfect load balance due to the extreme fine-grained underlying decomposition. However, since\na process does not have any contiguous data to work on, the resulting lack of locality may result\nin serious performance penalties. Additionally, such a decomposition usually leads to a high\ndegree of interaction relative to the amount computation in each task. The lower limit of 1 for\nthe value of a results in maximum locality and interaction optimality, but the distribution\ndegenerates to a block distribution. Therefore, an appropriate value of a must be used to strike\na balance between interaction conservation and load balance.\nAs in the case of block-distributions, higher dimensional block-cyclic distributions are usually\npreferable as they tend to incur a lower volume of inter-task interaction.\nRandomized Block Distributions\nA block-cyclic distribution may not always be able to balance computations when the\ndistribution of work has some special patterns. For example, consider the", "doc_id": "8f548b23-6a59-45e7-be95-2720f60d61a0", "embedding": null, "doc_hash": "13e68fa4581009fb62d4db9dd1537acb2cae5c05a6042412587a61687d761cac", "extra_info": null, "node_info": {"start": 369308, "end": 373332}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "abdbe149-b32e-4dfa-8dc1-10a00d706056", "3": "77de79d1-9153-4bc9-a9ea-6854b733d0f3"}}, "__type__": "1"}, "77de79d1-9153-4bc9-a9ea-6854b733d0f3": {"__data__": {"text": "contiguous data to work on, the resulting lack of locality may result\nin serious performance penalties. Additionally, such a decomposition usually leads to a high\ndegree of interaction relative to the amount computation in each task. The lower limit of 1 for\nthe value of a results in maximum locality and interaction optimality, but the distribution\ndegenerates to a block distribution. Therefore, an appropriate value of a must be used to strike\na balance between interaction conservation and load balance.\nAs in the case of block-distributions, higher dimensional block-cyclic distributions are usually\npreferable as they tend to incur a lower volume of inter-task interaction.\nRandomized Block Distributions\nA block-cyclic distribution may not always be able to balance computations when the\ndistribution of work has some special patterns. For example, consider the sparse matrix shown\nin Figure 3.31(a) in which the shaded areas correspond to regions containing nonzero elements.\nIf this matrix is distributed using a two-dimensional block-cyclic distribution, as illustrated in\nFigure 3.31(b), then we will end up assigning more non-zero blocks to the diagonal processes\nP0, P5, P10, and P15 than on any other processes. In fact some processes, like P12, will not get\nany work.\nFigure 3.31. Using the block-cyclic distribution shown in (b) to\ndistribute the computations performed in array (a) will lead to load\nimbalances.\nRandomized block distribution , a more general form of the block distribution, can be used in\nsituations illustrated in Figure 3.31 . Just like a block-cyclic distribution, load balance is sought\nby partitioning the array into many more blocks than the number of available processes.\nHowever, the blocks are uniformly and randomly distributed among the processes. A one-\ndimensional randomized block distribution can be achieved as follows. A vector V of length ap\n(which is equal to the number of blocks) is used and V[j] is set to j for 0 \n j < ap. Now, V is\nrandomly permuted, and process Pi is assigned the blocks stored in V[ia...(i + 1) a - 1]. Figure\n3.32 illustrates this for p = 4 and a = 3. A two-dimensional randomized block distribution of an\nn x n array can be computed similarly by randomly permuting two vectors of length \n  each\nand using them to choose the row and column indices of the blocks to be assigned to each\nprocess. As illustrated in Figure 3.33, the random block distribution is more effective in load\nbalancing the computations performed in Figure 3.31 .\nFigure 3.32. A one-dimensional randomized block mapping of 12\nblocks onto four process (i.e., a = 3).\nFigure 3.33. Using a two-dimensional random block distribution shown\nin (b) to distribute the computations performed in array (a), as shown\nin (c).\nGraph Partitioning  The array-based distribution schemes that we described so far are quite\neffective in balancing the computations and minimizing the interactions for a wide range of\nalgorithms that use dense matrices and have structured and regular interaction patterns.\nHowever, there are many algorithms that operate on sparse data structures and for which the\npattern of interaction among data elements is data dependent and highly irregular. Numerical\nsimulations of physical phenomena provide a large source of such type of computations. In\nthese computations, the physical domain is discretized and represented by a mesh of elements.\nThe simulation of the physical phenomenon being modeled then involves computing the values\nof certain physical quantities at each mesh point. The computation at a mesh point usually\nrequires data corresponding to that mesh point and to the points that are adjacent to it in the\nmesh. For example, Figure 3.34 shows a mesh imposed on Lake Superior. The simulation of a\nphysical phenomenon such the dispersion of a water contaminant in the lake would now involve\ncomputing the level of contamination at each vertex of this mesh at various intervals of time.\nFigure 3.34. A mesh used to model Lake", "doc_id": "77de79d1-9153-4bc9-a9ea-6854b733d0f3", "embedding": null, "doc_hash": "2633cbad88daa0a00a8a55998454585c139f4f065eb991fdacc613601d35357f", "extra_info": null, "node_info": {"start": 373254, "end": 377254}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8f548b23-6a59-45e7-be95-2720f60d61a0", "3": "5f777021-d6fd-4d6e-a2de-cb69085b4c50"}}, "__type__": "1"}, "5f777021-d6fd-4d6e-a2de-cb69085b4c50": {"__data__": {"text": "Numerical\nsimulations of physical phenomena provide a large source of such type of computations. In\nthese computations, the physical domain is discretized and represented by a mesh of elements.\nThe simulation of the physical phenomenon being modeled then involves computing the values\nof certain physical quantities at each mesh point. The computation at a mesh point usually\nrequires data corresponding to that mesh point and to the points that are adjacent to it in the\nmesh. For example, Figure 3.34 shows a mesh imposed on Lake Superior. The simulation of a\nphysical phenomenon such the dispersion of a water contaminant in the lake would now involve\ncomputing the level of contamination at each vertex of this mesh at various intervals of time.\nFigure 3.34. A mesh used to model Lake Superior.\nSince, in general, the amount of computation at each point is the same, the load can be easily\nbalanced by simply assigning the same number of mesh points to each process. However, if a\ndistribution of the mesh points to processes does not strive to keep nearby mesh points\ntogether, then it may lead to high interaction overheads due to excessive data sharing. For\nexample, if each process receives a random set of points as illustrated in Figure 3.35, then each\nprocess will need to access a large set of points belonging to other processes to complete\ncomputations for its assigned portion of the mesh.\nFigure 3.35. A random distribution of the mesh elements to eight\nprocesses.\nIdeally, we would like to distribute the mesh points in a way that balances the load and at the\nsame time minimizes the amount of data that each process needs to access in order to complete\nits computations. Therefore, we need to partition the mesh into p parts such that each part\ncontains roughly the same number of mesh-points or vertices, and the number of edges that\ncross partition boundaries (i.e., those edges that connect points belonging to two different\npartitions) is minimized. Finding an exact optimal partition is an NP-complete problem.\nHowever, algorithms that employ powerful heuristics are available to compute reasonable\npartitions. After partitioning the mesh in this manner, each one of these p partitions is assigned\nto one of the p processes. As a result, each process is assigned a contiguous region of the mesh\nsuch that the total number of mesh points that needs to be accessed across partition boundaries\nis minimized. Figure 3.36 shows a good partitioning of the Lake Superior mesh \u2013 the kind that a\ntypical graph partitioning software would generate.\nFigure 3.36. A distribution of the mesh elements to eight processes, by\nusing a graph-partitioning algorithm.\nMappings Based on Task Partitioning\nA mapping based on partitioning a task-dependency graph and mapping its nodes onto\nprocesses can be used when the computation is naturally expressible in the form of a static\ntask-dependency graph with tasks of known sizes. As usual, this mapping must seek to achieve\nthe often conflicting objectives of minimizing idle time and minimizing the interaction time of\nthe parallel algorithm. Determining an optimal mapping for a general task-dependency graph is\nan NP-complete problem. However, specific situations often lend themselves to a simpler\noptimal or acceptable approximate solution.\nAs a simple example of a mapping based on task partitioning, consider a task-dependency\ngraph that is a perfect binary tree. Such a task-dependency graph can occur in practical\nproblems with recursive decomposition, such as the decomposition for finding the minimum of a\nlist of numbers ( Figure 3.9). Figure 3.37  shows a mapping of this task-dependency graph onto\neight processes. It is easy to see that this mapping minimizes the interaction overhead by\nmapping many interdependent tasks onto the same process (i.e., the tasks along a straight\nbranch of the tree) and others on processes only one communication link away from each other.\nAlthough there is some inevitable idling (e.g., when process 0 works on the root task, all", "doc_id": "5f777021-d6fd-4d6e-a2de-cb69085b4c50", "embedding": null, "doc_hash": "220cdce9274faffee647109f94852b00de98ebc78b18e0da4a34c4e88273b825", "extra_info": null, "node_info": {"start": 377336, "end": 381365}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "77de79d1-9153-4bc9-a9ea-6854b733d0f3", "3": "07b3b64a-c1f6-4438-a478-4d55877d5a7b"}}, "__type__": "1"}, "07b3b64a-c1f6-4438-a478-4d55877d5a7b": {"__data__": {"text": "simpler\noptimal or acceptable approximate solution.\nAs a simple example of a mapping based on task partitioning, consider a task-dependency\ngraph that is a perfect binary tree. Such a task-dependency graph can occur in practical\nproblems with recursive decomposition, such as the decomposition for finding the minimum of a\nlist of numbers ( Figure 3.9). Figure 3.37  shows a mapping of this task-dependency graph onto\neight processes. It is easy to see that this mapping minimizes the interaction overhead by\nmapping many interdependent tasks onto the same process (i.e., the tasks along a straight\nbranch of the tree) and others on processes only one communication link away from each other.\nAlthough there is some inevitable idling (e.g., when process 0 works on the root task, all other\nprocesses are idle), this idling is inherent in the task-dependency graph. The mapping shown in\nFigure 3.37 does not introduce any further idling and all tasks that are permitted to be\nconcurrently active by the task-dependency graph are mapped onto different processes for\nparallel execution.\nFigure 3.37. Mapping of a binary tree task-dependency graph onto a\nhypercube of processes.\nFor some problems, an approximate solution to the problem of finding a good mapping can be\nobtained by partitioning the task-interaction graph. In the problem of modeling contaminant\ndispersion in Lake Superior discussed earlier in the context of data partitioning, we can define\ntasks such that each one of them is responsible for the computations associated with a certain\nmesh point. Now the mesh used to discretize the lake also acts as a task-interaction graph.\nTherefore, for this problem, using graph-partitioning to find a good mapping can also be viewed\nas task partitioning.\nAnother similar problem where task partitioning is applicable is that of sparse matrix-vector\nmultiplication discussed in Section 3.1.2. A simple mapping of the task-interaction graph of\nFigure 3.6  is shown in Figure 3.38 . This mapping assigns tasks corresponding to four\nconsecutive entries of b to each process. Figure 3.39  shows another partitioning for the task-\ninteraction graph of the sparse matrix vector multiplication problem shown in Figure 3.6  for\nthree processes. The list C i contains the indices of b that the tasks on Process i need to access\nfrom tasks mapped onto other processes. A quick comparison of the lists C0, C1, and C2 in the\ntwo cases readily reveals that the mapping based on partitioning the task interaction graph\nentails fewer exchanges of elements of b between processes than the naive mapping.\nFigure 3.38. A mapping for sparse matrix-vector multiplication onto\nthree processes. The list C i contains the indices of b that Process i\nneeds to access from other processes.\nFigure 3.39. Reducing interaction overhead in sparse matrix-vector\nmultiplication by partitioning the task-interaction graph.\nHierarchical Mappings\nCertain algorithms are naturally expressed as task-dependency graphs; however, a mapping\nbased solely on the task-dependency graph may suffer from load-imbalance or inadequate\nconcurrency. For example, in the binary-tree task-dependency graph of Figure 3.37, only a few\ntasks are available for concurrent execution in the top part of the tree. If the tasks are large\nenough, then a better mapping can be obtained by a further decomposition of the tasks into\nsmaller subtasks. In the case where the task-dependency graph is a binary tree with four levels,\nthe root task can be partitioned among eight processes, the tasks at the next level can be\npartitioned among four processes each, followed by tasks partitioned among two processes each\nat the next level. The eight leaf tasks can have a one-to-one mapping onto the processes.\nFigure 3.40 illustrates such a hierarchical mapping. Parallel quicksort introduced in Example 3.4\nhas a task-dependency graph similar to the one shown in Figure 3.37 , and", "doc_id": "07b3b64a-c1f6-4438-a478-4d55877d5a7b", "embedding": null, "doc_hash": "a12393ecdf776257b496e31bbe4e7f6d46317777ea95c603ff24322b16cdcfa0", "extra_info": null, "node_info": {"start": 381368, "end": 385286}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5f777021-d6fd-4d6e-a2de-cb69085b4c50", "3": "af409767-88a6-4b20-a962-879e18aac71b"}}, "__type__": "1"}, "af409767-88a6-4b20-a962-879e18aac71b": {"__data__": {"text": "only a few\ntasks are available for concurrent execution in the top part of the tree. If the tasks are large\nenough, then a better mapping can be obtained by a further decomposition of the tasks into\nsmaller subtasks. In the case where the task-dependency graph is a binary tree with four levels,\nthe root task can be partitioned among eight processes, the tasks at the next level can be\npartitioned among four processes each, followed by tasks partitioned among two processes each\nat the next level. The eight leaf tasks can have a one-to-one mapping onto the processes.\nFigure 3.40 illustrates such a hierarchical mapping. Parallel quicksort introduced in Example 3.4\nhas a task-dependency graph similar to the one shown in Figure 3.37 , and hence is an ideal\ncandidate for a hierarchical mapping of the type shown in Figure 3.40 .\nFigure 3.40. An example of hierarchical mapping of a task-dependency\ngraph. Each node represented by an array is a supertask. The\npartitioning of the arrays represents subtasks, which are mapped onto\neight processes.\nAn important practical problem to which the hierarchical mapping example discussed above\napplies directly is that of sparse matrix factorization. The high-level computations in sparse\nmatrix factorization are guided by a task-dependency graph which is known as an elimination\ngraph  (elimination tree  if the matrix is symmetric). However, the tasks in the elimination graph\n(especially the ones closer to the root) usually involve substantial computations and are further\ndecomposed into subtasks using data-decomposition. A hierarchical mapping, using task\npartitioning at the top layer and array partitioning at the bottom layer, is then applied to this\nhybrid decomposition. In general, a hierarchical mapping can have many layers and different\ndecomposition and mapping techniques may be suitable for different layers.\n3.4.2 Schemes for Dynamic Mapping\nDynamic mapping is necessary in situations where a static mapping may result in a highly\nimbalanced distribution of work among processes or where the task-dependency graph itself if\ndynamic, thus precluding a static mapping. Since the primary reason for using a dynamic\nmapping is balancing the workload among processes, dynamic mapping is often referred to as\ndynamic load-balancing. Dynamic mapping techniques are usually classified as either\ncentralized  or distributed .\nCentralized Schemes\nIn a centralized dynamic load balancing scheme, all executable tasks are maintained in a\ncommon central data structure or they are maintained by a special process or a subset of\nprocesses. If a special process is designated to manage the pool of available tasks, then it is\noften referred to as the master  and the other processes that depend on the master to obtain\nwork are referred to as slaves . Whenever a process has no work, it takes a portion of available\nwork from the central data structure or the master process. Whenever a new task is generated,\nit is added to this centralized data structure or reported to the master process. Centralized load-\nbalancing schemes are usually easier to implement than distributed schemes, but may have\nlimited scalability. As more and more processes are used, the large number of accesses to the\ncommon data structure or the master process tends to become a bottleneck.\nAs an example of a scenario where centralized mapping may be applicable, consider the\nproblem of sorting the entries in each row of an n x n matrix A. Serially, this can be\naccomplished by the following simple program segment:\n1   for (i=1; i<n; i++) \n2     sort(A[i],n); \nRecall that the time to sort an array using some of the commonly used sorting algorithms, such\nas quicksort, can vary significantly depending on the initial order of the elements to be sorted.\nTherefore, each iteration of the loop in the program shown above can take different amounts of\ntime. A naive mapping of the task of sorting an equal number of rows to each process may", "doc_id": "af409767-88a6-4b20-a962-879e18aac71b", "embedding": null, "doc_hash": "6a06945c613cae67cd64f5edc1599c80905baea26659bcc1784002b3914e2e93", "extra_info": null, "node_info": {"start": 385328, "end": 389295}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "07b3b64a-c1f6-4438-a478-4d55877d5a7b", "3": "31039fce-7644-4847-a69a-02570d44570a"}}, "__type__": "1"}, "31039fce-7644-4847-a69a-02570d44570a": {"__data__": {"text": "data structure or the master process tends to become a bottleneck.\nAs an example of a scenario where centralized mapping may be applicable, consider the\nproblem of sorting the entries in each row of an n x n matrix A. Serially, this can be\naccomplished by the following simple program segment:\n1   for (i=1; i<n; i++) \n2     sort(A[i],n); \nRecall that the time to sort an array using some of the commonly used sorting algorithms, such\nas quicksort, can vary significantly depending on the initial order of the elements to be sorted.\nTherefore, each iteration of the loop in the program shown above can take different amounts of\ntime. A naive mapping of the task of sorting an equal number of rows to each process may lead\nto load-imbalance. A possible solution to the potential load-imbalance problem in this case\nwould be to maintain a central pool of indices of the rows that have yet to be sorted. Whenever\na process is idle, it picks up an available index, deletes it, and sorts the row with that index, as\nlong as the pool of indices is not empty. This method of scheduling the independent iterations of\na loop among parallel processes is known as self scheduling .\nThe assignment of a single task to a process at a time is quite effective in balancing the\ncomputation; however, it may lead to bottlenecks in accessing the shared work queue,\nespecially if each task (i.e., each loop iteration in this case) does not require a large enough\namount of computation. If the average size of each task is M, and it takes D time to assign work\nto a process, then at most M/D processes can be kept busy effectively. The bottleneck can be\neased by getting more than one task at a time. In chunk scheduling , every time a process\nruns out of work it gets a group of tasks. The potential problem with such a scheme is that it\nmay lead to load-imbalances if the number of tasks (i.e., chunk) assigned in a single step is\nlarge. The danger of load-imbalance due to large chunk sizes can be reduced by decreasing the\nchunk-size as the program progresses. That is, initially the chunk size is large, but as the\nnumber of iterations left to be executed decreases, the chunk size also decreases. A variety of\nschemes have been developed for gradually adjusting the chunk size, that decrease the chunk\nsize either linearly or non-linearly.\nDistributed Schemes\nIn a distributed dynamic load balancing scheme, the set of executable tasks are distributed\namong processes which exchange tasks at run time to balance work. Each process can send\nwork to or receive work from any other process. These methods do not suffer from the\nbottleneck associated with the centralized schemes. Some of the critical parameters of a\ndistributed load balancing scheme are as follows:\nHow are the sending and receiving processes paired together?Is the work transfer initiated by the sender or the receiver?\nHow much work is transferred in each exchange? If too little work is transferred, then the\nreceiver may not receive enough work and frequent transfers resulting in excessive\ninteraction may be required. If too much work is transferred, then the sender may be out\nof work soon, again resulting in frequent transfers.\nWhen is the work transfer performed? For example, in receiver initiated load balancing,\nwork may be requested when the process has actually run out of work or when the\nreceiver has too little work left and anticipates being out of work soon.\nA detailed study of each of these parameters is beyond the scope of this chapter. These load\nbalancing schemes will be revisited in the context of parallel algorithms to which they apply\nwhen we discuss these algorithms in the later chapters \u2013 in particular, Chapter 11 in the context\nof parallel search algorithms.\nSuitability to Parallel Architectures\nNote that, in principle, both centralized and distributed mapping schemes can be implemented\nin both message-passing and", "doc_id": "31039fce-7644-4847-a69a-02570d44570a", "embedding": null, "doc_hash": "993b37c99bc93f7a0f05b0bf26a3e5557ab05acfbe67a87c1bbfd683f11a5dbc", "extra_info": null, "node_info": {"start": 389328, "end": 393232}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "af409767-88a6-4b20-a962-879e18aac71b", "3": "87149753-e730-4e86-994b-b295e9325087"}}, "__type__": "1"}, "87149753-e730-4e86-994b-b295e9325087": {"__data__": {"text": "sender may be out\nof work soon, again resulting in frequent transfers.\nWhen is the work transfer performed? For example, in receiver initiated load balancing,\nwork may be requested when the process has actually run out of work or when the\nreceiver has too little work left and anticipates being out of work soon.\nA detailed study of each of these parameters is beyond the scope of this chapter. These load\nbalancing schemes will be revisited in the context of parallel algorithms to which they apply\nwhen we discuss these algorithms in the later chapters \u2013 in particular, Chapter 11 in the context\nof parallel search algorithms.\nSuitability to Parallel Architectures\nNote that, in principle, both centralized and distributed mapping schemes can be implemented\nin both message-passing and shared-address-space paradigms. However, by its very nature\nany dynamic load balancing scheme requires movement of tasks from one process to another.\nHence, for such schemes to be effective on message-passing computers, the size of the tasks in\nterms of computation should be much higher than the size of the data associated with the tasks.\nIn a shared-address-space paradigm, the tasks don't need to be moved explicitly, although\nthere is some implied movement of data to local caches or memory banks of processes. In\ngeneral, the computational granularity of tasks to be moved can be much smaller on shared-\naddress architecture than on message-passing architectures.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n3.5 Methods for Containing Interaction Overheads\nAs noted earlier, reducing the interaction overhead among concurrent tasks is important for an\nefficient parallel program. The overhead that a parallel program incurs due to interaction\namong its processes depends on many factors, such as the volume of data exchanged during\ninteractions, the frequency of interaction, the spatial and temporal pattern of interactions, etc.\nIn this section, we will discuss some general techniques that can be used to reduce the\ninteraction overheads incurred by parallel programs. These techniques manipulate one or more\nof the three factors above in order to reduce the interaction overhead. Some of these are\napplicable while devising the decomposition and mapping schemes for the algorithms and some\nare applicable while programming the algorithm in a given paradigm. All techniques may not be\napplicable in all parallel programming paradigms and some of them may require support from\nthe underlying hardware.\n3.5.1 Maximizing Data Locality\nIn most nontrivial parallel programs, the tasks executed by different processes require access to\nsome common data. For example, in sparse matrix-vector multiplication y = Ab, in which tasks\ncorrespond to computing individual elements of vector y (Figure 3.6), all elements of the input\nvector b need to be accessed by multiple tasks. In addition to sharing the original input data,\ninteraction may result if processes require data generated by other processes. The interaction\noverheads can be reduced by using techniques that promote the use of local data or data that\nhave been recently fetched. Data locality enhancing techniques encompass a wide range of\nschemes that try to minimize the volume of nonlocal data that are accessed, maximize the\nreuse of recently accessed data, and minimize the frequency of accesses. In many cases, these\nschemes are similar in nature to the data reuse optimizations often performed in modern cache\nbased microprocessors.\nMinimize Volume of Data-Exchange  A fundamental technique for reducing the interaction\noverhead is to minimize the overall volume of shared data that needs to be accessed by\nconcurrent processes. This is akin to maximizing the temporal data locality, i.e., making as\nmany of the consecutive references to the same data as possible. Clearly, performing as much\nof the computation as possible using locally available data obviates the need for bringing in\nmore data into local memory or cache for a process to perform its tasks. As discussed\npreviously, one way of achieving this is by using appropriate decomposition", "doc_id": "87149753-e730-4e86-994b-b295e9325087", "embedding": null, "doc_hash": "ab3eafe1c9b779383b6fe29496550697a1018fea94fee8fc19f7f565987473e7", "extra_info": null, "node_info": {"start": 393155, "end": 397247}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "31039fce-7644-4847-a69a-02570d44570a", "3": "f85a56e9-675a-40dc-92d1-5b125e7c48e3"}}, "__type__": "1"}, "f85a56e9-675a-40dc-92d1-5b125e7c48e3": {"__data__": {"text": "of recently accessed data, and minimize the frequency of accesses. In many cases, these\nschemes are similar in nature to the data reuse optimizations often performed in modern cache\nbased microprocessors.\nMinimize Volume of Data-Exchange  A fundamental technique for reducing the interaction\noverhead is to minimize the overall volume of shared data that needs to be accessed by\nconcurrent processes. This is akin to maximizing the temporal data locality, i.e., making as\nmany of the consecutive references to the same data as possible. Clearly, performing as much\nof the computation as possible using locally available data obviates the need for bringing in\nmore data into local memory or cache for a process to perform its tasks. As discussed\npreviously, one way of achieving this is by using appropriate decomposition and mapping\nschemes. For example, in the case of matrix multiplication, we saw that by using a two-\ndimensional mapping of the computations to the processes we were able to reduce the amount\nof shared data (i.e., matrices A and B) that needs to be accessed by each task to \n as\nopposed to n2/p + n2 required by a one-dimensional mapping ( Figure 3.26 ). In general, using\nhigher dimensional distribution often helps in reducing the volume of nonlocal data that needs\nto be accessed.\nAnother way of decreasing the amount of shared data that are accessed by multiple processes is\nto use local data to store intermediate results, and perform the shared data access to only place\nthe final results of the computation. For example, consider computing the dot product of two\nvectors of length n in parallel such that each of the p tasks multiplies n/p pairs of elements.\nRather than adding each individual product of a pair of numbers to the final result, each task\ncan first create a partial dot product of its assigned portion of the vectors of length n/p in its\nown local location, and only access the final shared location once to add this partial result. This\nwill reduce the number of accesses to the shared location where the result is stored to p from n.\nMinimize Frequency of Interactions  Minimizing interaction frequency is important in\nreducing the interaction overheads in parallel programs because there is a relatively high\nstartup cost associated with each interaction on many architectures. Interaction frequency can\nbe reduced by restructuring the algorithm such that shared data are accessed and used in large\npieces. Thus, by amortizing the startup cost over large accesses, we can reduce the overall\ninteraction overhead, even if such restructuring does not necessarily reduce the overall volume\nof shared data that need to be accessed. This is akin to increasing the spatial locality of data\naccess, i.e., ensuring the proximity of consecutively accessed data locations. On a shared-\naddress-space architecture, each time a word is accessed, an entire cache line containing many\nwords is fetched. If the program is structured to have spatial locality, then fewer cache lines are\naccessed. On a message-passing system, spatial locality leads to fewer message-transfers over\nthe network because each message can transfer larger amounts of useful data. The number of\nmessages can sometimes be reduced further on a message-passing system by combining\nmessages between the same source-destination pair into larger messages if the interaction\npattern permits and if the data for multiple messages are available at the same time, albeit in\nseparate data structures.\nSparse matrix-vector multiplication is a problem whose parallel formulation can use this\ntechnique to reduce interaction overhead. In typical applications, repeated sparse matrix-vector\nmultiplication is performed with matrices of the same nonzero pattern but different numerical\nnonzero values. While solving this problem in parallel, a process interacts with others to access\nelements of the input vector that it may need for its local computation. Through a one-time\nscanning of the nonzero pattern of the rows of the sparse matrix that a process is responsible\nfor, it can determine exactly which elements of the input vector it needs and from which\nprocesses. Then, before starting each multiplication, a process can first collect all the nonlocal\nentries", "doc_id": "f85a56e9-675a-40dc-92d1-5b125e7c48e3", "embedding": null, "doc_hash": "2da6cadf156b5cfe690c76ff0bede08ace9bc213defecad3650bcc78f30aa42a", "extra_info": null, "node_info": {"start": 397219, "end": 401475}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "87149753-e730-4e86-994b-b295e9325087", "3": "d6ddda2d-6a67-4eb7-9cd3-809f218b9603"}}, "__type__": "1"}, "d6ddda2d-6a67-4eb7-9cd3-809f218b9603": {"__data__": {"text": "for multiple messages are available at the same time, albeit in\nseparate data structures.\nSparse matrix-vector multiplication is a problem whose parallel formulation can use this\ntechnique to reduce interaction overhead. In typical applications, repeated sparse matrix-vector\nmultiplication is performed with matrices of the same nonzero pattern but different numerical\nnonzero values. While solving this problem in parallel, a process interacts with others to access\nelements of the input vector that it may need for its local computation. Through a one-time\nscanning of the nonzero pattern of the rows of the sparse matrix that a process is responsible\nfor, it can determine exactly which elements of the input vector it needs and from which\nprocesses. Then, before starting each multiplication, a process can first collect all the nonlocal\nentries of the input vector that it requires, and then perform an interaction-free multiplication.\nThis strategy is far superior than trying to access a nonlocal element of the input vector as and\nwhen required in the computation.\n3.5.2 Minimizing Contention and Hot Spots\nOur discussion so far has been largely focused on reducing interaction overheads by directly or\nindirectly reducing the frequency and volume of data transfers. However, the data-access and\ninter-task interaction patterns can often lead to contention that can increase the overall\ninteraction overhead. In general, contention occurs when multiple tasks try to access the same\nresources concurrently. Multiple simultaneous transmissions of data over the same\ninterconnection link, multiple simultaneous accesses to the same memory block, or multiple\nprocesses sending messages to the same process at the same time, can all lead to contention.\nThis is because only one of the multiple operations can proceed at a time and the others are\nqueued and proceed sequentially.\nConsider the problem of multiplying two matrices C = AB, using the two-dimensional\npartitioning shown in Figure 3.26(b). Let p  be the number of tasks with a one-to-one mapping\nof tasks onto processes. Let each task be responsible for computing a unique Ci,j, for\n. The straightforward way of performing this computation is for Ci,j to be\ncomputed according to the following formula (written in matrix-block notation):\nEquation 3.1\n\nLooking at the memory access patterns of the above equation, we see that at any one of the \nsteps, \n  tasks will be accessing the same block of A and B. In particular, all the tasks that\nwork on the same row of C will be accessing the same block of A. For example, all \nprocesses computing \n  will attempt to read A0,0 at once. Similarly, all the\ntasks working on the same column of C will be accessing the same block of B. The need to\nconcurrently access these blocks of matrices A and B will create contention on both NUMA\nshared-address-space and message-passing parallel architectures.\nOne way of reducing contention is to redesign the parallel algorithm to access data in\ncontention-free patterns. For the matrix multiplication algorithm, this contention can be\neliminated by modifying the order in which the block multiplications are performed in Equation\n3.1. A contention-free way of performing these block-multiplications is to compute Ci,j by using\nthe formula\nEquation 3.2\nwhere '%' denotes the modulo operation. By using this formula, all the tasks P*,j that work on\nthe same row of C will be accessing block \n , which is different for each task.\nSimilarly, all the tasks Pi,* that work on the same column of C will be accessing block\n, which is also different for each task. Thus, by simply rearranging the order in\nwhich the block-multiplications are performed, we can completely eliminate the contention. For\nexample, among the processes computing the first block row of C, the process computing C0,j\nwill access A0,j from the first block row of A instead of A0,0.\nCentralized schemes for dynamic mapping ( Section 3.4.2 ) are a frequent source of contention\nfor shared data structures or communication channels leading to the master", "doc_id": "d6ddda2d-6a67-4eb7-9cd3-809f218b9603", "embedding": null, "doc_hash": "3b0cb3326c73040ac3ba2deea0052d4d46297cbb9e4cfe2978841d0237304b7f", "extra_info": null, "node_info": {"start": 401444, "end": 405508}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f85a56e9-675a-40dc-92d1-5b125e7c48e3", "3": "8f776bac-a0dd-4fcd-bd14-526ed9fc4232"}}, "__type__": "1"}, "8f776bac-a0dd-4fcd-bd14-526ed9fc4232": {"__data__": {"text": "'%' denotes the modulo operation. By using this formula, all the tasks P*,j that work on\nthe same row of C will be accessing block \n , which is different for each task.\nSimilarly, all the tasks Pi,* that work on the same column of C will be accessing block\n, which is also different for each task. Thus, by simply rearranging the order in\nwhich the block-multiplications are performed, we can completely eliminate the contention. For\nexample, among the processes computing the first block row of C, the process computing C0,j\nwill access A0,j from the first block row of A instead of A0,0.\nCentralized schemes for dynamic mapping ( Section 3.4.2 ) are a frequent source of contention\nfor shared data structures or communication channels leading to the master process. The\ncontention may be reduced by choosing a distributed mapping scheme over a centralized one,\neven though the former may be harder to implement.\n3.5.3 Overlapping Computations with Interactions\nThe amount of time that processes spend waiting for shared data to arrive or to receive\nadditional work after an interaction has been initiated can be reduced, often substantially, by\ndoing some useful computations during this waiting time. There are a number of techniques\nthat can be used to overlap computations with interactions.\nA simple way of overlapping is to initiate an interaction early enough so that it is completed\nbefore it is needed for computation. To achieve this, we must be able to identify computations\nthat can be performed before the interaction and do not depend on it. Then the parallel\nprogram must be structured to initiate the interaction at an earlier point in the execution than it\nis needed in the original algorithm. Typically, this is possible if the interaction pattern is\nspatially and temporally static (and therefore, predictable) or if multiple tasks that are ready for\nexecution are available on the same process so that if one blocks to wait for an interaction to\ncomplete, the process can work on another task. The reader should note that by increasing the\nnumber of parallel tasks to promote computation-interaction overlap, we are reducing the\ngranularity of the tasks, which in general tends to increase overheads. Therefore, this technique\nmust be used judiciously.\nIn certain dynamic mapping schemes, as soon as a process runs out of work, it requests and\ngets additional work from another process. It then waits for the request to be serviced. If the\nprocess can anticipate that it is going to run out of work and initiate a work transfer interaction\nin advance, then it may continue towards finishing the tasks at hand while the request for more\nwork is being serviced. Depending on the problem, estimating the amount of remaining work\nmay be easy or hard.\nIn most cases, overlapping computations with interaction requires support from the\nprogramming paradigm, the operating system, and the hardware. The programming paradigm\nmust provide a mechanism to allow interactions and computations to proceed concurrently. This\nmechanism should be supported by the underlying hardware. Disjoint address-space paradigms\nand architectures usually provide this support via non-blocking message passing primitives. The\nprogramming paradigm provides functions for sending and receiving messages that return\ncontrol to the user's program before they have actually completed. Thus, the program can use\nthese primitives to initiate the interactions, and then proceed with the computations. If the\nhardware permits computation to proceed concurrently with message transfers, then the\ninteraction overhead can be reduced significantly.\nOn a shared-address-space architecture, the overlapping of computations and interaction is\noften assisted by prefetching hardware. In this case, an access to shared data is nothing more\nthan a regular load or store instruction. The prefetch hardware can anticipate the memory\naddresses that will need to be accessed in the immediate future, and can initiate the access in\nadvance of when they are needed. In the absence of prefetching hardware, the same effect can\nbe achieved", "doc_id": "8f776bac-a0dd-4fcd-bd14-526ed9fc4232", "embedding": null, "doc_hash": "1df81feb1a286fbb42d97e438ea2d5133fd78db1348e44eb612b0627022a15cd", "extra_info": null, "node_info": {"start": 405605, "end": 409711}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d6ddda2d-6a67-4eb7-9cd3-809f218b9603", "3": "8a1294c6-9dfa-4d6d-815c-fd20ad0779dd"}}, "__type__": "1"}, "8a1294c6-9dfa-4d6d-815c-fd20ad0779dd": {"__data__": {"text": "return\ncontrol to the user's program before they have actually completed. Thus, the program can use\nthese primitives to initiate the interactions, and then proceed with the computations. If the\nhardware permits computation to proceed concurrently with message transfers, then the\ninteraction overhead can be reduced significantly.\nOn a shared-address-space architecture, the overlapping of computations and interaction is\noften assisted by prefetching hardware. In this case, an access to shared data is nothing more\nthan a regular load or store instruction. The prefetch hardware can anticipate the memory\naddresses that will need to be accessed in the immediate future, and can initiate the access in\nadvance of when they are needed. In the absence of prefetching hardware, the same effect can\nbe achieved by a compiler that detects the access pattern and places pseudo-references to\ncertain key memory locations before these locations are actually utilized by the computation.\nThe degree of success of this scheme is dependent upon the available structure in the program\nthat can be inferred by the prefetch hardware and by the degree of independence with which\nthe prefetch hardware can function while computation is in progress.\n3.5.4 Replicating Data or Computations\nReplication of data or computations is another technique that may be useful in reducing\ninteraction overheads.\nIn some parallel algorithms, multiple processes may require frequent read-only access to\nshared data structure, such as a hash-table, in an irregular pattern. Unless the additional\nmemory requirements are prohibitive, it may be best in a situation like this to replicate a copy\nof the shared data structure on each process so that after the initial interaction during\nreplication, all subsequent accesses to this data structure are free of any interaction overhead.\nIn the shared-address-space paradigm, replication of frequently accessed read-only data is\noften affected by the caches without explicit programmer intervention. Explicit data replication\nis particularly suited for architectures and programming paradigms in which read-only access to\nshared data is significantly more expensive or harder to express than local data accesses.\nTherefore, the message-passing programming paradigm benefits the most from data\nreplication, which may reduce interaction overhead and also significantly simplify the writing of\nthe parallel program.\nData replication, however, does not come without its own cost. Data replication increases the\nmemory requirements of a parallel program. The aggregate amount of memory required to\nstore the replicated data increases linearly with the number of concurrent processes. This may\nlimit the size of the problem that can be solved on a given parallel computer. For this reason,\ndata replication must be used selectively to replicate relatively small amounts of data.\nIn addition to input data, the processes in a parallel program often share intermediate results.\nIn some situations, it may be more cost-effective for a process to compute these intermediate\nresults than to get them from another process that generates them. In such situations,\ninteraction overhead can be traded for replicated computation. For example, while performing\nthe Fast Fourier Transform (see Section 13.2.3 for more details), on an N-point series, N distinct\npowers of w or \"twiddle factors\" are computed and used at various points in the computation. In\na parallel implementation of FFT, different processes require overlapping subsets of these N\ntwiddle factors. In a message-passing paradigm, it is best for each process to locally compute\nall the twiddle factors it needs. Although the parallel algorithm may perform many more twiddle\nfactor computations than the serial algorithm, it may still be faster than sharing the twiddle\nfactors.\n3.5.5 Using Optimized Collective Interaction Operations\nAs discussed in Section 3.3.2, often the interaction patterns among concurrent activities are\nstatic and regular. A class of such static and regular interaction patterns are those that are\nperformed by groups of tasks, and they are used to achieve regular data accesses or to perform\ncertain type of computations on distributed data. A number of key such collective ", "doc_id": "8a1294c6-9dfa-4d6d-815c-fd20ad0779dd", "embedding": null, "doc_hash": "736da2d2e41d55e589a1e0a547eccbbf4be74382c3a79e25bb5bf37a612706ab", "extra_info": null, "node_info": {"start": 409653, "end": 413914}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8f776bac-a0dd-4fcd-bd14-526ed9fc4232", "3": "45209a62-ebaa-4ff8-a260-0601f8ae3319"}}, "__type__": "1"}, "45209a62-ebaa-4ff8-a260-0601f8ae3319": {"__data__": {"text": "parallel implementation of FFT, different processes require overlapping subsets of these N\ntwiddle factors. In a message-passing paradigm, it is best for each process to locally compute\nall the twiddle factors it needs. Although the parallel algorithm may perform many more twiddle\nfactor computations than the serial algorithm, it may still be faster than sharing the twiddle\nfactors.\n3.5.5 Using Optimized Collective Interaction Operations\nAs discussed in Section 3.3.2, often the interaction patterns among concurrent activities are\nstatic and regular. A class of such static and regular interaction patterns are those that are\nperformed by groups of tasks, and they are used to achieve regular data accesses or to perform\ncertain type of computations on distributed data. A number of key such collective  interaction\noperations have been identified that appear frequently in many parallel algorithms.\nBroadcasting some data to all the processes or adding up numbers, each belonging to a\ndifferent process, are examples of such collective operations. The collective data-sharing\noperations can be classified into three categories. The first category contains operations that are\nused by the tasks to access data, the second category of operations are used to perform some\ncommunication-intensive computations, and finally, the third category is used for\nsynchronization.\nHighly optimized implementations of these collective operations have been developed that\nminimize the overheads due to data transfer as well as contention. Chapter 4 describes\nalgorithms for implementing some of the commonly used collective interaction operations.\nOptimized implementations of these operations are available in library form from the vendors of\nmost parallel computers, e.g., MPI (message passing interface). As a result, the algorithm\ndesigner does not need to think about how these operations are implemented and needs to\nfocus only on the functionality achieved by these operations. However, as discussed in Section3.5.6 , sometimes the interaction pattern may make it worthwhile for the parallel programmer\nto implement one's own collective communication procedure.\n3.5.6 Overlapping Interactions with Other Interactions\nIf the data-transfer capacity of the underlying hardware permits, then overlapping interactions\nbetween multiple pairs of processes can reduce the effective volume of communication. As an\nexample of overlapping interactions, consider the commonly used collective communication\noperation of one-to-all broadcast in a message-passing paradigm with four processes P0, P1, P2,\nand P3. A commonly used algorithm to broadcast some data from P0 to all other processes\nworks as follows. In the first step, P0 sends the data to P2. In the second step, P0 sends the data\nto P1, and concurrently, P2 sends the same data that it had received from P0 to P3. The entire\noperation is thus complete in two steps because the two interactions of the second step require\nonly one time step. This operation is illustrated in Figure 3.41(a). On the other hand, a naive\nbroadcast algorithm would send the data from P0 to P1 to P2 to P3, thereby consuming three\nsteps as illustrated in Figure 3.41(b) .\nFigure 3.41. Illustration of overlapping interactions in broadcasting\ndata from one to four processes.\nInterestingly, however, there are situations when the naive broadcast algorithm shown in\nFigure 3.41(b)  may be adapted to actually increase the amount of overlap. Assume that a\nparallel algorithm needs to broadcast four data structures one after the other. The entire\ninteraction would require eight steps using the first two-step broadcast algorithm. However,\nusing the naive algorithm accomplishes the interaction in only six steps as shown in Figure3.41(c) . In the first step, P0 sends the first message to P1. In the second step P0 sends the\nsecond message to P1 while P1 simultaneously sends the first message to P2. In the third step,\nP0 sends the third message to P1, P1 sends the second message to P2, and P2 sends the first\nmessage to P3. Proceeding similarly", "doc_id": "45209a62-ebaa-4ff8-a260-0601f8ae3319", "embedding": null, "doc_hash": "82290278cb30ed93d9dceda1ca78175e1c2a9f4308c640eb555e05184d2cc637", "extra_info": null, "node_info": {"start": 413916, "end": 417984}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8a1294c6-9dfa-4d6d-815c-fd20ad0779dd", "3": "96560e92-36c8-400a-a092-eed4df49ee87"}}, "__type__": "1"}, "96560e92-36c8-400a-a092-eed4df49ee87": {"__data__": {"text": "situations when the naive broadcast algorithm shown in\nFigure 3.41(b)  may be adapted to actually increase the amount of overlap. Assume that a\nparallel algorithm needs to broadcast four data structures one after the other. The entire\ninteraction would require eight steps using the first two-step broadcast algorithm. However,\nusing the naive algorithm accomplishes the interaction in only six steps as shown in Figure3.41(c) . In the first step, P0 sends the first message to P1. In the second step P0 sends the\nsecond message to P1 while P1 simultaneously sends the first message to P2. In the third step,\nP0 sends the third message to P1, P1 sends the second message to P2, and P2 sends the first\nmessage to P3. Proceeding similarly in a pipelined fashion, the last of the four messages is sent\nout of P0 after four steps and reaches P 3 in six. Since this method is rather expensive for a\nsingle broadcast operation, it is unlikely to be included in a collective communication library.\nHowever, the programmer must infer from the interaction pattern of the algorithm that in this\nscenario, it is better to make an exception to the suggestion of Section 3.5.5 and write your\nown collective communication function.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n3.6 Parallel Algorithm Models\nHaving discussed the techniques for decomposition, mapping, and minimizing interaction\noverheads, we now present some of the commonly used parallel algorithm models. An\nalgorithm model is typically a way of structuring a parallel algorithm by selecting a\ndecomposition and mapping technique and applying the appropriate strategy to minimize\ninteractions.\n3.6.1 The Data-Parallel Model\nThe data-parallel model  is one of the simplest algorithm models. In this model, the tasks are\nstatically or semi-statically mapped onto processes and each task performs similar operations\non different data. This type of parallelism that is a result of identical operations being applied\nconcurrently on different data items is called data parallelism . The work may be done in\nphases and the data operated upon in different phases may be different. Typically, data-parallel\ncomputation phases are interspersed with interactions to synchronize the tasks or to get fresh\ndata to the tasks. Since all tasks perform similar computations, the decomposition of the\nproblem into tasks is usually based on data partitioning because a uniform partitioning of data\nfollowed by a static mapping is sufficient to guarantee load balance.\nData-parallel algorithms can be implemented in both shared-address-space and message-\npassing paradigms. However, the partitioned address-space in a message-passing paradigm\nmay allow better control of placement, and thus may offer a better handle on locality. On the\nother hand, shared-address space can ease the programming effort, especially if the distribution\nof data is different in different phases of the algorithm.\nInteraction overheads in the data-parallel model can be minimized by choosing a locality\npreserving decomposition and, if applicable, by overlapping computation and interaction and by\nusing optimized collective interaction routines. A key characteristic of data-parallel problems is\nthat for most problems, the degree of data parallelism increases with the size of the problem,\nmaking it possible to use more processes to effectively solve larger problems.\nAn example of a data-parallel algorithm is dense matrix multiplication described in Section3.1.1 . In the decomposition shown in Figure 3.10 , all tasks are identical; they are applied to\ndifferent data.\n3.6.2 The Task Graph Model\nAs discussed in Section 3.1 , the computations in any parallel algorithm can be viewed as a task-\ndependency graph. The task-dependency graph may be either trivial, as in the case of matrix\nmultiplication, or nontrivial (Problem 3.5). However, in certain parallel algorithms, the task-\ndependency graph is explicitly used in mapping. In the task graph", "doc_id": "96560e92-36c8-400a-a092-eed4df49ee87", "embedding": null, "doc_hash": "44b532fa239834d4f1b6287dca21877e5d2e64c8178c1793cb01c219e94323a5", "extra_info": null, "node_info": {"start": 418060, "end": 422015}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "45209a62-ebaa-4ff8-a260-0601f8ae3319", "3": "8ae87073-1048-4ed0-9d4a-aca16181b333"}}, "__type__": "1"}, "8ae87073-1048-4ed0-9d4a-aca16181b333": {"__data__": {"text": "the degree of data parallelism increases with the size of the problem,\nmaking it possible to use more processes to effectively solve larger problems.\nAn example of a data-parallel algorithm is dense matrix multiplication described in Section3.1.1 . In the decomposition shown in Figure 3.10 , all tasks are identical; they are applied to\ndifferent data.\n3.6.2 The Task Graph Model\nAs discussed in Section 3.1 , the computations in any parallel algorithm can be viewed as a task-\ndependency graph. The task-dependency graph may be either trivial, as in the case of matrix\nmultiplication, or nontrivial (Problem 3.5). However, in certain parallel algorithms, the task-\ndependency graph is explicitly used in mapping. In the task graph model , the interrelationships\namong the tasks are utilized to promote locality or to reduce interaction costs. This model is\ntypically employed to solve problems in which the amount of data associated with the tasks is\nlarge relative to the amount of computation associated with them. Usually, tasks are mapped\nstatically to help optimize the cost of data movement among tasks. Sometimes a decentralized\ndynamic mapping may be used, but even then, the mapping uses the information about the\ntask-dependency graph structure and the interaction pattern of tasks to minimize interaction\noverhead. Work is more easily shared in paradigms with globally addressable space, but\nmechanisms are available to share work in disjoint address space.\nTypical interaction-reducing techniques applicable to this model include reducing the volume\nand frequency of interaction by promoting locality while mapping the tasks based on the\ninteraction pattern of tasks, and using asynchronous interaction methods to overlap the\ninteraction with computation.\nExamples of algorithms based on the task graph model include parallel quicksort ( Section9.4.1 ), sparse matrix factorization, and many parallel algorithms derived via divide-and-\nconquer decomposition. This type of parallelism that is naturally expressed by independent\ntasks in a task-dependency graph is called task parallelism .\n3.6.3 The Work Pool Model\nThe work pool  or the task pool  model is characterized by a dynamic mapping of tasks onto\nprocesses for load balancing in which any task may potentially be performed by any process.\nThere is no desired premapping of tasks onto processes. The mapping may be centralized or\ndecentralized. Pointers to the tasks may be stored in a physically shared list, priority queue,\nhash table, or tree, or they could be stored in a physically distributed data structure. The work\nmay be statically available in the beginning, or could be dynamically generated; i.e., the\nprocesses may generate work and add it to the global (possibly distributed) work pool. If the\nwork is generated dynamically and a decentralized mapping is used, then a termination\ndetection algorithm ( Section 11.4.4) would be required so that all processes can actually detect\nthe completion of the entire program (i.e., exhaustion of all potential tasks) and stop looking for\nmore work.\nIn the message-passing paradigm, the work pool model is typically used when the amount of\ndata associated with tasks is relatively small compared to the computation associated with the\ntasks. As a result, tasks can be readily moved around without causing too much data interaction\noverhead. The granularity of the tasks can be adjusted to attain the desired level of tradeoff\nbetween load-imbalance and the overhead of accessing the work pool for adding and extracting\ntasks.\nParallelization of loops by chunk scheduling ( Section 3.4.2) or related methods is an example of\nthe use of the work pool model with centralized mapping when the tasks are statically available.\nParallel tree search where the work is represented by a centralized or distributed data structure\nis an example of the use of the work pool model where the tasks are generated dynamically.\n3.6.4 The Master-Slave Model\nIn the master-slave  or the manager-worker  model, one or more", "doc_id": "8ae87073-1048-4ed0-9d4a-aca16181b333", "embedding": null, "doc_hash": "f2f7ca5cd39b3287ce505e0ef1fa1d128387cbbe3754e21c4d35146c711aa0e5", "extra_info": null, "node_info": {"start": 422011, "end": 426038}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "96560e92-36c8-400a-a092-eed4df49ee87", "3": "3a34f740-9a5f-45a5-8c19-c5136b2d32d1"}}, "__type__": "1"}, "3a34f740-9a5f-45a5-8c19-c5136b2d32d1": {"__data__": {"text": "with the\ntasks. As a result, tasks can be readily moved around without causing too much data interaction\noverhead. The granularity of the tasks can be adjusted to attain the desired level of tradeoff\nbetween load-imbalance and the overhead of accessing the work pool for adding and extracting\ntasks.\nParallelization of loops by chunk scheduling ( Section 3.4.2) or related methods is an example of\nthe use of the work pool model with centralized mapping when the tasks are statically available.\nParallel tree search where the work is represented by a centralized or distributed data structure\nis an example of the use of the work pool model where the tasks are generated dynamically.\n3.6.4 The Master-Slave Model\nIn the master-slave  or the manager-worker  model, one or more master processes generate\nwork and allocate it to worker processes. The tasks may be allocated a priori  if the manager can\nestimate the size of the tasks or if a random mapping can do an adequate job of load balancing.\nIn another scenario, workers are assigned smaller pieces of work at different times. The latter\nscheme is preferred if it is time consuming for the master to generate work and hence it is not\ndesirable to make all workers wait until the master has generated all work pieces. In some\ncases, work may need to be performed in phases, and work in each phase must finish before\nwork in the next phases can be generated. In this case, the manager may cause all workers to\nsynchronize after each phase. Usually, there is no desired premapping of work to processes,\nand any worker can do any job assigned to it. The manager-worker model can be generalized to\nthe hierarchical or multi-level manager-worker model in which the top-level manager feeds\nlarge chunks of tasks to second-level managers, who further subdivide the tasks among their\nown workers and may perform part of the work themselves. This model is generally equally\nsuitable to shared-address-space or message-passing paradigms since the interaction is\nnaturally two-way; i.e., the manager knows that it needs to give out work and workers know\nthat they need to get work from the manager.\nWhile using the master-slave model, care should be taken to ensure that the master does not\nbecome a bottleneck, which may happen if the tasks are too small (or the workers are relatively\nfast). The granularity of tasks should be chosen such that the cost of doing work dominates the\ncost of transferring work and the cost of synchronization. Asynchronous interaction may help\noverlap interaction and the computation associated with work generation by the master. It may\nalso reduce waiting times if the nature of requests from workers is non-deterministic.\n3.6.5 The Pipeline or Producer-Consumer Model\nIn the pipeline model , a stream of data is passed on through a succession of processes, each of\nwhich perform some task on it. This simultaneous execution of different programs on a data\nstream is called stream parallelism . With the exception of the process initiating the pipeline,\nthe arrival of new data triggers the execution of a new task by a process in the pipeline. The\nprocesses could form such pipelines in the shape of linear or multidimensional arrays, trees, or\ngeneral graphs with or without cycles. A pipeline is a chain of producers and consumers. Each\nprocess in the pipeline can be viewed as a consumer of a sequence of data items for the process\npreceding it in the pipeline and as a producer of data for the process following it in the pipeline.\nThe pipeline does not need to be a linear chain; it can be a directed graph. The pipeline model\nusually involves a static mapping of tasks onto processes.\nLoad balancing is a function of task granularity. The larger the granularity, the longer it takes to\nfill up the pipeline, i.e. for the trigger produced by the first process in the chain to propagate to\nthe last process, thereby keeping some of the processes waiting. However, too fine a granularity\nmay increase interaction overheads because processes will need to interact to", "doc_id": "3a34f740-9a5f-45a5-8c19-c5136b2d32d1", "embedding": null, "doc_hash": "095b39922d13a8d5b1946cabb058614da38266f9c7fd206733f3ca52ebfabc24", "extra_info": null, "node_info": {"start": 426006, "end": 430051}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8ae87073-1048-4ed0-9d4a-aca16181b333", "3": "9a0ba7ac-2c71-4194-844d-42d7c8451ae6"}}, "__type__": "1"}, "9a0ba7ac-2c71-4194-844d-42d7c8451ae6": {"__data__": {"text": "pipeline is a chain of producers and consumers. Each\nprocess in the pipeline can be viewed as a consumer of a sequence of data items for the process\npreceding it in the pipeline and as a producer of data for the process following it in the pipeline.\nThe pipeline does not need to be a linear chain; it can be a directed graph. The pipeline model\nusually involves a static mapping of tasks onto processes.\nLoad balancing is a function of task granularity. The larger the granularity, the longer it takes to\nfill up the pipeline, i.e. for the trigger produced by the first process in the chain to propagate to\nthe last process, thereby keeping some of the processes waiting. However, too fine a granularity\nmay increase interaction overheads because processes will need to interact to receive fresh data\nafter smaller pieces of computation. The most common interaction reduction technique\napplicable to this model is overlapping interaction with computation.\nAn example of a two-dimensional pipeline is the parallel LU factorization algorithm, which is\ndiscussed in detail in Section 8.3.1.\n3.6.6 Hybrid Models\nIn some cases, more than one model may be applicable to the problem at hand, resulting in a\nhybrid algorithm model. A hybrid model may be composed either of multiple models applied\nhierarchically or multiple models applied sequentially to different phases of a parallel algorithm.\nIn some cases, an algorithm formulation may have characteristics of more than one algorithm\nmodel. For instance, data may flow in a pipelined manner in a pattern guided by a task-\ndependency graph. In another scenario, the major computation may be described by a task-\ndependency graph, but each node of the graph may represent a supertask comprising multiple\nsubtasks that may be suitable for data-parallel or pipelined parallelism. Parallel quicksort\n(Sections 3.2.5 and 9.4.1 ) is one of the applications for which a hybrid model is ideally suited.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n3.7 Bibliographic Remarks\nVarious texts, such as those by Wilson [ Wil95 ], Akl [ Akl97 ], Hwang and Xu [ HX98 ], Wilkinson\nand Allen [ WA99 ], and Culler and Singh [ CSG98 ], among others, present similar or slightly\nvarying models for parallel programs and steps in developing parallel algorithms. The book by\nGoedecker and Hoisie [ GH01 ] is among the relatively few textbooks that focus on the practical\naspects of writing high-performance parallel programs. Kwok and Ahmad [ KA99a , KA99b ]\npresent comprehensive surveys of techniques for mapping tasks onto processes.\nMost of the algorithms used in this chapter as examples are discussed in detail in other chapters\nin this book dedicated to the respective class of problems. Please refer to the bibliographic\nremarks in those chapters for further references on these algorithms.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n3.1 In Example 3.2 , each union and intersection operation can be performed in time\nproportional to the sum of the number of records in the two input tables. Based on this,\nconstruct the weighted task-dependency graphs corresponding to Figures 3.2 and 3.3,\nwhere the weight of each node is equivalent to the amount of work required by the\ncorresponding task. What is the average degree of concurrency of each graph?\n3.2 For the task graphs given in Figure 3.42, determine the following:\nMaximum degree of concurrency.1.\nCritical path length.2.\nMaximum achievable speedup over one process assuming that an arbitrarily large\nnumber of processes is available.3.\nThe minimum number of processes needed to obtain the maximum possible speedup.4.\nThe maximum achievable speedup if the number of processes is limited to (a) 2, (b)\n4, and (c) 8.5.\nFigure 3.42. Task-dependency graphs for Problem 3.2.\n3.3", "doc_id": "9a0ba7ac-2c71-4194-844d-42d7c8451ae6", "embedding": null, "doc_hash": "19bfc456135d2ab4692d6c2b843d3449dfd83aa2dd436e568dac7a49971d469f", "extra_info": null, "node_info": {"start": 430055, "end": 433803}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3a34f740-9a5f-45a5-8c19-c5136b2d32d1", "3": "30c18d8e-b3f9-4c65-ae87-4eb72f473992"}}, "__type__": "1"}, "30c18d8e-b3f9-4c65-ae87-4eb72f473992": {"__data__": {"text": "task-dependency graphs corresponding to Figures 3.2 and 3.3,\nwhere the weight of each node is equivalent to the amount of work required by the\ncorresponding task. What is the average degree of concurrency of each graph?\n3.2 For the task graphs given in Figure 3.42, determine the following:\nMaximum degree of concurrency.1.\nCritical path length.2.\nMaximum achievable speedup over one process assuming that an arbitrarily large\nnumber of processes is available.3.\nThe minimum number of processes needed to obtain the maximum possible speedup.4.\nThe maximum achievable speedup if the number of processes is limited to (a) 2, (b)\n4, and (c) 8.5.\nFigure 3.42. Task-dependency graphs for Problem 3.2.\n3.3 What are the average degrees of concurrency and critical-path lengths of task-\ndependency graphs corresponding to the decompositions for matrix multiplication shown\nin Figures 3.10 and 3.11?\n3.4 Let d be the maximum degree of concurrency in a task-dependency graph with t tasks\nand a critical-path length l. Prove that \n .\n3.5 Consider LU factorization of a dense matrix shown in Algorithm 3.3 . Figure 3.27  shows\nthe decomposition of LU factorization into 14 tasks based on a two-dimensional\npartitioning of the matrix A into nine blocks A i,j, 1 \n  i, j \n 3. The blocks of A are\nmodified into corresponding blocks of L and U as a result of factorization. The diagonal\nblocks of L are lower triangular submatrices with unit diagonals and the diagonal blocks of\nU are upper triangular submatrices. Task 1 factors the submatrix A1,1 using Algorithm 3.3 .\nTasks 2 and 3 implement the block versions of the loop on Lines 4\u20136 of Algorithm 3.3 .\nTasks 4 and 5 are the upper-triangular counterparts of tasks 2 and 3. The element version\nof LU factorization in Algorithm 3.3  does not show these steps because the diagonal\nentries of L are 1; however, a block version must compute a block-row of U as a product of\nthe inverse of the corresponding diagonal block of L with the block-row of A. Tasks 6\u20139\nimplement the block version of the loops on Lines 7\u201311 of Algorithm 3.3 . Thus, Tasks 1\u20139\ncorrespond to the block version of the first iteration of the outermost loop of Algorithm\n3.3. The remainder of the tasks complete the factorization of A. Draw a task-dependency\ngraph corresponding to the decomposition shown in Figure 3.27 .\n3.6 Enumerate the critical paths in the decomposition of LU factorization shown in Figure\n3.27.\n3.7 Show an efficient mapping of the task-dependency graph of the decomposition shown\nin Figure 3.27  onto three processes. Prove informally that your mapping is the best\npossible mapping for three processes.\n3.8 Describe and draw an efficient mapping of the task-dependency graph of the\ndecomposition shown in Figure 3.27  onto four processes and prove that your mapping is\nthe best possible mapping for four processes.\n3.9 Assuming that each task takes a unit amount of time, [1] which of the two mappings \u2013\nthe one onto three processes or the one onto four processes \u2013 solves the problem faster?\n[1] In practice, for a block size b \n 1, Tasks 1, 10, and 14 require about 2/3 b3 arithmetic operations;\nTasks 2, 3, 4, 5, 11, and 12 require about b3 operations; and Tasks 6, 7, 8, 9, and 13 require about\n2b3 operations.\n3.10 Prove that block steps 1 through 14 in Figure 3.27  with block size b (i.e., each Ai,j,\nLi,j, and Ui,j is a b x b submatrix) are mathematically equivalent to running the algorithm\nof Algorithm 3.3  on an n x n matrix A, where n = 3b.\nHint:  Using induction on b is one possible", "doc_id": "30c18d8e-b3f9-4c65-ae87-4eb72f473992", "embedding": null, "doc_hash": "599af4089b6ffc2ae8506ed32c5dec37fc85aa57f51c79b7e6bf687c52bed7a8", "extra_info": null, "node_info": {"start": 433856, "end": 437383}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9a0ba7ac-2c71-4194-844d-42d7c8451ae6", "3": "e6161d5b-cc97-4d3a-9704-fc39164f2bb1"}}, "__type__": "1"}, "e6161d5b-cc97-4d3a-9704-fc39164f2bb1": {"__data__": {"text": "one onto three processes or the one onto four processes \u2013 solves the problem faster?\n[1] In practice, for a block size b \n 1, Tasks 1, 10, and 14 require about 2/3 b3 arithmetic operations;\nTasks 2, 3, 4, 5, 11, and 12 require about b3 operations; and Tasks 6, 7, 8, 9, and 13 require about\n2b3 operations.\n3.10 Prove that block steps 1 through 14 in Figure 3.27  with block size b (i.e., each Ai,j,\nLi,j, and Ui,j is a b x b submatrix) are mathematically equivalent to running the algorithm\nof Algorithm 3.3  on an n x n matrix A, where n = 3b.\nHint:  Using induction on b is one possible approach.\n3.11 Figure 3.27  shows the decomposition into 14 tasks of LU factorization of a matrix\nsplit into blocks using a 3 x 3 two-dimensional partitioning. If an m x m partitioning is\nused, derive an expression for the number of tasks t(m) as a function of m in a\ndecomposition along similar lines.\nHint:  Show that t(m) = t(m - 1) + m2.\n3.12 In the context of Problem 3.11, derive an expression for the maximum degree of\nconcurrency d(m) as a function of m.\n3.13 In the context of Problem 3.11, derive an expression for the critical-path length l(m)\nas a function of m.\n3.14 Show efficient mappings for the decompositions for the database query problem\nshown in Figures 3.2 and 3.3. What is the maximum number of processes that you would\nuse in each case?\n3.15 In the algorithm shown in Algorithm 3.4 , assume a decomposition such that each\nexecution of Line 7 is a task. Draw a task-dependency graph and a task-interaction graph.\nAlgorithm 3.4 A sample serial program to be parallelized.\n1.   procedure  FFT_like_pattern( A, n) \n2.   begin \n3.      m := log 2 n; \n4.      for j := 0 to m - 1 do \n5.          k := 2j; \n6.          for i := 0 to n - 1 do \n7.              A[i] := A[i] + A[i XOR 2j]; \n8.      endfor \n9.   end FFT_like_pattern \n3.16 In Algorithm 3.4 , if n = 16, devise a good mapping for 16 processes.\n3.17 In Algorithm 3.4 , if n = 16, devise a good mapping for 8 processes.\n3.18 Repeat Problems 3.15, 3.16, and 3.17 if the statement of Line 3 in Algorithm 3.4  is\nchanged to m = (log 2 n) - 1.\n3.19 Consider a simplified version of bucket-sort. You are given an array A of n random\nintegers in the range [1... r] as input. The output data consist of r buckets, such that at the\nend of the algorithm, Bucket i contains indices of all the elements in A that are equal to i.\nDescribe a decomposition based on partitioning the input data (i.e., the array A) and\nan appropriate mapping onto p processes. Describe briefly how the resulting parallel\nalgorithm would work.\nDescribe a decomposition based on partitioning the output data (i.e., the set of r\nbuckets) and an appropriate mapping onto p processes. Describe briefly how the\nresulting parallel algorithm would work.\n3.20 In the context of Problem 3.19, which of the two decompositions leads to a better\nparallel algorithm? Should the relative values of n and r have a bearing on the selection of\none of the two decomposition schemes?\n3.21 Consider seven tasks with running times of 1, 2, 3, 4, 5, 5, and 10 units,\nrespectively.", "doc_id": "e6161d5b-cc97-4d3a-9704-fc39164f2bb1", "embedding": null, "doc_hash": "521433e1569aa5c7a4544907ac6d02a9b558dc19794b7515f62999dafa21d5ba", "extra_info": null, "node_info": {"start": 437508, "end": 440600}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "30c18d8e-b3f9-4c65-ae87-4eb72f473992", "3": "3fbfd5f0-9a44-4bb1-829d-938665f99e29"}}, "__type__": "1"}, "3fbfd5f0-9a44-4bb1-829d-938665f99e29": {"__data__": {"text": "i.\nDescribe a decomposition based on partitioning the input data (i.e., the array A) and\nan appropriate mapping onto p processes. Describe briefly how the resulting parallel\nalgorithm would work.\nDescribe a decomposition based on partitioning the output data (i.e., the set of r\nbuckets) and an appropriate mapping onto p processes. Describe briefly how the\nresulting parallel algorithm would work.\n3.20 In the context of Problem 3.19, which of the two decompositions leads to a better\nparallel algorithm? Should the relative values of n and r have a bearing on the selection of\none of the two decomposition schemes?\n3.21 Consider seven tasks with running times of 1, 2, 3, 4, 5, 5, and 10 units,\nrespectively. Assuming that it does not take any time to assign work to a process, compute\nthe best- and worst-case speedup for a centralized scheme for dynamic mapping with two\nprocesses.\n3.22 Suppose there are M tasks that are being mapped using a centralized dynamic load\nbalancing scheme and we have the following information about these tasks:\nAverage task size is 1.\nMinimum task size is 0.\nMaximum task size is m.\nIt takes a process \n  time to pick up a task.\nCompute the best- and worst-case speedups for self-scheduling and chunk-scheduling\nassuming that tasks are available in batches of l (l < M). What are the actual values of the\nbest- and worst-case speedups for the two scheduling methods when p = 10, \n  = 0.2, m\n= 20, M = 100, and l = 2?\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 4. Basic Communication\nOperations\nIn most parallel algorithms, processes need to exchange data with other processes. This\nexchange of data can significantly impact the efficiency of parallel programs by introducing\ninteraction delays during their execution. For instance, recall from Section 2.5 that it takes\nroughly ts + mtw time for a simple exchange of an m-word message between two processes\nrunning on different nodes of an interconnection network with cut-through routing. Here ts is\nthe latency or the startup time for the data transfer and tw is the per-word transfer time, which\nis inversely proportional to the available bandwidth between the nodes. Many interactions in\npractical parallel programs occur in well-defined patterns involving more than two processes.\nOften either all processes participate together in a single global interaction operation, or subsets\nof processes participate in interactions local to each subset. These common basic patterns of\ninterprocess interaction or communication are frequently used as building blocks in a variety of\nparallel algorithms. Proper implementation of these basic communication operations on various\nparallel architectures is a key to the efficient execution of the parallel algorithms that use them.\nIn this chapter, we present algorithms to implement some commonly used communication\npatterns on simple interconnection networks, such as the linear array, two-dimensional mesh,\nand the hypercube. The choice of these interconnection networks is motivated primarily by\npedagogical reasons. For instance, although it is unlikely that large scale parallel computers will\nbe based on the linear array or ring topology, it is important to understand various\ncommunication operations in the context of linear arrays because the rows and columns of\nmeshes are linear arrays. Parallel algorithms that perform rowwise or columnwise\ncommunication on meshes use linear array algorithms. The algorithms for a number of\ncommunication operations on a mesh are simple extensions of the corresponding linear array\nalgorithms to two dimensions. Furthermore, parallel algorithms using regular data structures\nsuch as arrays often map naturally onto one- or two-dimensional arrays of processes. This too\nmakes it important to study interprocess interaction on a linear array or mesh interconnection\nnetwork. The hypercube architecture, on the other hand, is interesting because many\nalgorithms with recursive interaction patterns map naturally onto a hypercube topology. Most of\nthese algorithms may perform equally well on interconnection", "doc_id": "3fbfd5f0-9a44-4bb1-829d-938665f99e29", "embedding": null, "doc_hash": "808d794336e9fa6b1ffecd09f064a26fbca81845354ca62f12b0da29d0e60773", "extra_info": null, "node_info": {"start": 440475, "end": 444551}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e6161d5b-cc97-4d3a-9704-fc39164f2bb1", "3": "6bbd6237-ddee-46ea-bfeb-f7795cbaf879"}}, "__type__": "1"}, "6bbd6237-ddee-46ea-bfeb-f7795cbaf879": {"__data__": {"text": "various\ncommunication operations in the context of linear arrays because the rows and columns of\nmeshes are linear arrays. Parallel algorithms that perform rowwise or columnwise\ncommunication on meshes use linear array algorithms. The algorithms for a number of\ncommunication operations on a mesh are simple extensions of the corresponding linear array\nalgorithms to two dimensions. Furthermore, parallel algorithms using regular data structures\nsuch as arrays often map naturally onto one- or two-dimensional arrays of processes. This too\nmakes it important to study interprocess interaction on a linear array or mesh interconnection\nnetwork. The hypercube architecture, on the other hand, is interesting because many\nalgorithms with recursive interaction patterns map naturally onto a hypercube topology. Most of\nthese algorithms may perform equally well on interconnection networks other than the\nhypercube, but it is simpler to visualize their communication patterns on a hypercube.\nThe algorithms presented in this chapter in the context of simple network topologies are\npractical and are highly suitable for modern parallel computers, even though most such\ncomputers are unlikely to have an interconnection network that exactly matches one of the\nnetworks considered in this chapter. The reason is that on a modern parallel computer, the time\nto transfer data of a certain size between two nodes is often independent of the relative location\nof the nodes in the interconnection network. This homogeneity is afforded by a variety of\nfirmware and hardware features such as randomized routing algorithms and cut-through\nrouting, etc. Furthermore, the end user usually does not have explicit control over mapping\nprocesses onto physical processors. Therefore, we assume that the transfer of m words of data\nbetween any pair of nodes in an interconnection network incurs a cost of ts + mtw. On most\narchitectures, this assumption is reasonably accurate as long as a free link is available between\nthe source and destination nodes for the data to traverse. However, if many pairs of nodes are\ncommunicating simultaneously, then the messages may take longer. This can happen if the\nnumber of messages passing through a cross-section of the network exceeds the cross-section\nbandwidth ( Section 2.4.4) of the network. In such situations, we need to adjust the value of tw\nto reflect the slowdown due to congestion. As discussed in Section 2.5.1 , we refer to the\nadjusted value of tw as effective tw. We will make a note in the text when we come across\ncommunication operations that may cause congestion on certain networks.\nAs discussed in Section 2.5.2 , the cost of data-sharing among processors in the shared-\naddress-space paradigm can be modeled using the same expression ts + mtw, usually with\ndifferent values of ts and tw relative to each other as well as relative to the computation speed\nof the processors of the parallel computer. Therefore, parallel algorithms requiring one or more\nof the interaction patterns discussed in this chapter can be assumed to incur costs whose\nexpression is close to one derived in the context of message-passing.\nIn the following sections we describe various communication operations and derive expressions\nfor their time complexity. We assume that the interconnection network supports cut-through\nrouting ( Section 2.5.1) and that the communication time between any pair of nodes is\npractically independent of of the number of intermediate nodes along the paths between them.\nWe also assume that the communication links are bidirectional; that is, two directly-connected\nnodes can send messages of size m to each other simultaneously in time ts + twm. We assume a\nsingle-port communication model, in which a node can send a message on only one of its links\nat a time. Similarly, it can receive a message on only one link at a time. However, a node can\nreceive a message while sending another message at the same time on the same or a different\nlink.\nMany of the operations described here have duals and other related operations that we can\nperform by using procedures very similar to those for the original operations. The dual of a\ncommunication operation is the opposite of the original operation and can be performed by\nreversing the direction and sequence of messages in the original", "doc_id": "6bbd6237-ddee-46ea-bfeb-f7795cbaf879", "embedding": null, "doc_hash": "80cbeb60936e852035464093a1071877b8645ecae992f152e79ce7448e870a75", "extra_info": null, "node_info": {"start": 444395, "end": 448724}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3fbfd5f0-9a44-4bb1-829d-938665f99e29", "3": "262fc2b6-ba2a-4445-9e91-2fbe16e3cca1"}}, "__type__": "1"}, "262fc2b6-ba2a-4445-9e91-2fbe16e3cca1": {"__data__": {"text": "along the paths between them.\nWe also assume that the communication links are bidirectional; that is, two directly-connected\nnodes can send messages of size m to each other simultaneously in time ts + twm. We assume a\nsingle-port communication model, in which a node can send a message on only one of its links\nat a time. Similarly, it can receive a message on only one link at a time. However, a node can\nreceive a message while sending another message at the same time on the same or a different\nlink.\nMany of the operations described here have duals and other related operations that we can\nperform by using procedures very similar to those for the original operations. The dual of a\ncommunication operation is the opposite of the original operation and can be performed by\nreversing the direction and sequence of messages in the original operation. We will mention\nsuch operations wherever applicable.[ Team LiB ]\n  \n\n[ Team LiB ]\n \n4.1 One-to-All Broadcast and All-to-One Reduction\nParallel algorithms often require a single process to send identical data to all other processes or\nto a subset of them. This operation is known as one-to-all broadcast  . Initially, only the\nsource process has the data of size m that needs to be broadcast. At the termination of the\nprocedure, there are p copies of the initial data \u2013 one belonging to each process. The dual of\none-to-all broadcast is all-to-one reduction  . In an all-to-one reduction operation, each of the\np participating processes starts with a buffer M containing m words. The data from all processes\nare combined through an associative operator and accumulated at a single destination process\ninto one buffer of size m . Reduction can be used to find the sum, product, maximum, or\nminimum of sets of numbers \u2013 the i th word of the accumulated M is the sum, product,\nmaximum, or minimum of the i th words of each of the original buffers. Figure 4.1  shows one-\nto-all broadcast and all-to-one reduction among p processes.\nFigure 4.1. One-to-all broadcast and all-to-one reduction.\nOne-to-all broadcast and all-to-one reduction are used in several important parallel algorithms\nincluding matrix-vector multiplication, Gaussian elimination, shortest paths, and vector inner\nproduct. In the following subsections, we consider the implementation of one-to-all broadcast in\ndetail on a variety of interconnection topologies.\n4.1.1 Ring or Linear Array\nA naive way to perform one-to-all broadcast is to sequentially send p - 1 messages from the\nsource to the other p - 1 processes. However, this is inefficient because the source process\nbecomes a bottleneck. Moreover, the communication network is underutilized because only the\nconnection between a single pair of nodes is used at a time. A better broadcast algorithm can\nbe devised using a technique commonly known as recursive doubling  . The source process\nfirst sends the message to another process. Now both these processes can simultaneously send\nthe message to two other processes that are still waiting for the message. By continuing this\nprocedure until all the processes have received the data, the message can be broadcast in log p\nsteps.\nThe steps in a one-to-all broadcast on an eight-node linear array or ring are shown in Figure\n4.2 . The nodes are labeled from 0 to 7. Each message transmission step is shown by a\nnumbered, dotted arrow from the source of the message to its destination. Arrows indicating\nmessages sent during the same time step have the same number.\nFigure 4.2. One-to-all broadcast on an eight-node ring. Node 0 is the\nsource of the broadcast. Each message transfer step is shown by a\nnumbered, dotted arrow from the source of the message to its\ndestination. The number on an arrow indicates the time step during\nwhich the message is transferred.\nNote that on a linear array, the destination node to which the message is sent in each step must\nbe carefully chosen. In Figure 4.2  , the message is first sent to the farthest node (4) from the\nsource", "doc_id": "262fc2b6-ba2a-4445-9e91-2fbe16e3cca1", "embedding": null, "doc_hash": "465b94e34b1f5e6f29c1d8f521b5ffbb391ac07f427ce8745dae0924a02e8799", "extra_info": null, "node_info": {"start": 448781, "end": 452770}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6bbd6237-ddee-46ea-bfeb-f7795cbaf879", "3": "d62faf18-e471-4ad4-8b67-402ab5a5ffa4"}}, "__type__": "1"}, "d62faf18-e471-4ad4-8b67-402ab5a5ffa4": {"__data__": {"text": "linear array or ring are shown in Figure\n4.2 . The nodes are labeled from 0 to 7. Each message transmission step is shown by a\nnumbered, dotted arrow from the source of the message to its destination. Arrows indicating\nmessages sent during the same time step have the same number.\nFigure 4.2. One-to-all broadcast on an eight-node ring. Node 0 is the\nsource of the broadcast. Each message transfer step is shown by a\nnumbered, dotted arrow from the source of the message to its\ndestination. The number on an arrow indicates the time step during\nwhich the message is transferred.\nNote that on a linear array, the destination node to which the message is sent in each step must\nbe carefully chosen. In Figure 4.2  , the message is first sent to the farthest node (4) from the\nsource (0). In the second step, the distance between the sending and receiving nodes is halved,\nand so on. The message recipients are selected in this manner at each step to avoid congestion\non the network. For example, if node 0 sent the message to node 1 in the first step and then\nnodes 0 and 1 attempted to send messages to nodes 2 and 3, respectively, in the second step,\nthe link between nodes 1 and 2 would be congested as it would be a part of the shortest route\nfor both the messages in the second step.\nReduction on a linear array can be performed by simply reversing the direction and the\nsequence of communication, as shown in Figure 4.3  . In the first step, each odd numbered node\nsends its buffer to the even numbered node just before itself, where the contents of the two\nbuffers are combined into one. After the first step, there are four buffers left to be reduced on\nnodes 0, 2, 4, and 6, respectively. In the second step, the contents of the buffers on nodes 0\nand 2 are accumulated on node 0 and those on nodes 6 and 4 are accumulated on node 4.\nFinally, node 4 sends its buffer to node 0, which computes the final result of the reduction.\nFigure 4.3. Reduction on an eight-node ring with node 0 as the\ndestination of the reduction.\n\nExample 4.1 Matrix-vector multiplication\nConsider the problem of multiplying an n x n matrix A with an n x 1 vector x on an n x\nn mesh of nodes to yield an n x 1 result vector y . Algorithm 8.1 shows a serial\nalgorithm for this problem. Figure 4.4  shows one possible mapping of the matrix and\nthe vectors in which each element of the matrix belongs to a different process, and the\nvector is distributed among the processes in the topmost row of the mesh and the\nresult vector is generated on the leftmost column of processes.\nFigure 4.4. One-to-all broadcast and all-to-one reduction in the\nmultiplication of a 4 x 4 matrix with a 4 x 1 vector.\nSince all the rows of the matrix must be multiplied with the vector, each process\nneeds the element of the vector residing in the topmost process of its column. Hence,\nbefore computing the matrix-vector product, each column of nodes performs a one-to-\nall broadcast of the vector elements with the topmost process of the column as the\nsource. This is done by treating each column of the n x n mesh as an n -node linear\narray, and simultaneously applying the linear array broadcast procedure described\npreviously to all columns.\nAfter the broadcast, each process multiplies its matrix element with the result of the\nbroadcast. Now, each row of processes needs to add its result to generate the\ncorresponding element of the product vector. This is accomplished by performing all-\nto-one reduction on each row of the process mesh with the first process of each row as\nthe destination of the reduction operation.\nFor example, P 9 will receive x [1] from P 1 as a result of the broadcast, will multiply it\nwith A [2, 1] and will participate in an all-to-one reduction with P 8 , P 10 , and P 11 to\naccumulate y [2] on P 8 . \n\n4.1.2 Mesh\nWe can regard each row and column", "doc_id": "d62faf18-e471-4ad4-8b67-402ab5a5ffa4", "embedding": null, "doc_hash": "c9e18167a2bf29b14befd535584db285189685014191ac19e42e6e446e976e34", "extra_info": null, "node_info": {"start": 452826, "end": 456666}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "262fc2b6-ba2a-4445-9e91-2fbe16e3cca1", "3": "348603bf-cdb9-4d77-b4d8-eae412c8893a"}}, "__type__": "1"}, "348603bf-cdb9-4d77-b4d8-eae412c8893a": {"__data__": {"text": "applying the linear array broadcast procedure described\npreviously to all columns.\nAfter the broadcast, each process multiplies its matrix element with the result of the\nbroadcast. Now, each row of processes needs to add its result to generate the\ncorresponding element of the product vector. This is accomplished by performing all-\nto-one reduction on each row of the process mesh with the first process of each row as\nthe destination of the reduction operation.\nFor example, P 9 will receive x [1] from P 1 as a result of the broadcast, will multiply it\nwith A [2, 1] and will participate in an all-to-one reduction with P 8 , P 10 , and P 11 to\naccumulate y [2] on P 8 . \n\n4.1.2 Mesh\nWe can regard each row and column of a square mesh of p nodes as a linear array of \nnodes. So a number of communication algorithms on the mesh are simple extensions of their\nlinear array counterparts. A linear array communication operation can be performed in two\nphases on a mesh. In the first phase, the operation is performed along one or all rows by\ntreating the rows as linear arrays. In the second phase, the columns are treated similarly.\nConsider the problem of one-to-all broadcast on a two-dimensional square mesh with \n rows\nand \n  columns. First, a one-to-all broadcast is performed from the source to the remaining (\n) nodes of the same row. Once all the nodes in a row of the mesh have acquired the\ndata, they initiate a one-to-all broadcast in their respective columns. At the end of the second\nphase, every node in the mesh has a copy of the initial message. The communication steps for\none-to-all broadcast on a mesh are illustrated in Figure 4.5  for p = 16, with node 0 at the\nbottom-left corner as the source. Steps 1 and 2 correspond to the first phase, and steps 3 and 4\ncorrespond to the second phase.\nFigure 4.5. One-to-all broadcast on a 16-node mesh.\nWe can use a similar procedure for one-to-all broadcast on a three-dimensional mesh as well.\nIn this case, rows of p 1 /3 nodes in each of the three dimensions of the mesh would be treated\nas linear arrays. As in the case of a linear array, reduction can be performed on two- and three-\ndimensional meshes by simply reversing the direction and the order of messages.\n4.1.3 Hypercube\nThe previous subsection showed that one-to-all broadcast is performed in two phases on a two-\ndimensional mesh, with the communication taking place along a different dimension in each\nphase. Similarly, the process is carried out in three phases on a three-dimensional mesh. A\nhypercube with 2d nodes can be regarded as a d -dimensional mesh with two nodes in each\ndimension. Hence, the mesh algorithm can be extended to the hypercube, except that the\nprocess is now carried out in d steps \u2013 one in each dimension.\nFigure 4.6  shows a one-to-all broadcast on an eight-node (three-dimensional) hypercube with\nnode 0 as the source. In this figure, communication starts along the highest dimension (that is,\nthe dimension specified by the most significant bit of the binary representation of a node label)\nand proceeds along successively lower dimensions in subsequent steps. Note that the source\nand the destination nodes in three communication steps of the algorithm shown in Figure 4.6\nare identical to the ones in the broadcast algorithm on a linear array shown in Figure 4.2  .\nHowever, on a hypercube, the order in which the dimensions are chosen for communication\ndoes not affect the outcome of the procedure. Figure 4.6  shows only one such order. Unlike a\nlinear array, the hypercube broadcast would not suffer from congestion if node 0 started out by\nsending the message to node 1 in the first step, followed by nodes 0 and 1 sending messages to\nnodes 2 and 3, respectively, and finally nodes 0, 1, 2, and 3 sending messages to nodes 4, 5, 6,\nand 7,", "doc_id": "348603bf-cdb9-4d77-b4d8-eae412c8893a", "embedding": null, "doc_hash": "ec293322c65b75689dd8c93c1b8207d6f9e9c0b45df71eff7b710ebde923a381", "extra_info": null, "node_info": {"start": 456718, "end": 460520}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d62faf18-e471-4ad4-8b67-402ab5a5ffa4", "3": "2808eb57-eb6d-4b12-be90-2624cc70490c"}}, "__type__": "1"}, "2808eb57-eb6d-4b12-be90-2624cc70490c": {"__data__": {"text": "proceeds along successively lower dimensions in subsequent steps. Note that the source\nand the destination nodes in three communication steps of the algorithm shown in Figure 4.6\nare identical to the ones in the broadcast algorithm on a linear array shown in Figure 4.2  .\nHowever, on a hypercube, the order in which the dimensions are chosen for communication\ndoes not affect the outcome of the procedure. Figure 4.6  shows only one such order. Unlike a\nlinear array, the hypercube broadcast would not suffer from congestion if node 0 started out by\nsending the message to node 1 in the first step, followed by nodes 0 and 1 sending messages to\nnodes 2 and 3, respectively, and finally nodes 0, 1, 2, and 3 sending messages to nodes 4, 5, 6,\nand 7, respectively.\nFigure 4.6. One-to-all broadcast on a three-dimensional hypercube.\nThe binary representations of node labels are shown in parentheses.\n4.1.4 Balanced Binary Tree\nThe hypercube algorithm for one-to-all broadcast maps naturally onto a balanced binary tree in\nwhich each leaf is a processing node and intermediate nodes serve only as switching units. This\nis illustrated in Figure 4.7  for eight nodes. In this figure, the communicating nodes have the\nsame labels as in the hypercube algorithm illustrated in Figure 4.6  . Figure 4.7  shows that there\nis no congestion on any of the communication links at any time. The difference between the\ncommunication on a hypercube and the tree shown in Figure 4.7  is that there is a different\nnumber of switching nodes along different paths on the tree.\nFigure 4.7. One-to-all broadcast on an eight-node tree.\n4.1.5 Detailed Algorithms\nA careful look at Figures 4.2  , 4.5 , 4.6 , and 4.7  would reveal that the basic communication\npattern for one-to-all broadcast is identical on all the four interconnection networks considered\nin this section. We now describe procedures to implement the broadcast and reduction\noperations. For the sake of simplicity, the algorithms are described here in the context of a\nhypercube and assume that the number of communicating processes is a power of 2. However,\nthey apply to any network topology, and can be easily extended to work for any number of\nprocesses (Problem 4.1).\nAlgorithm 4.1  shows a one-to-all broadcast procedure on a 2d -node network when node 0 is\nthe source of the broadcast. The procedure is executed at all the nodes. At any node, the value\nof my_id  is the label of that node. Let X be the message to be broadcast, which initially resides\nat the source node 0. The procedure performs d communication steps, one along each\ndimension of a hypothetical hypercube. In Algorithm 4.1  , communication proceeds from the\nhighest to the lowest dimension (although the order in which dimensions are chosen does not\nmatter). The loop counter i indicates the current dimension of the hypercube in which\ncommunication is taking place. Only the nodes with zero in the i least significant bits of their\nlabels participate in communication along dimension i . For instance, on the three-dimensional\nhypercube shown in Figure 4.6  , i is equal to 2 in the first time step. Therefore, only nodes 0\nand 4 communicate, since their two least significant bits are zero. In the next time step, when i\n= 1, all nodes (that is, 0, 2, 4, and 6) with zero in their least significant bits participate in\ncommunication. The procedure terminates after communication has taken place along all\ndimensions.\nThe variable mask  helps determine which nodes communicate in a particular iteration of the\nloop. The variable mask  has d (= log p ) bits, all of which are initially set to one (Line 3). At the\nbeginning of each iteration, the most significant nonzero bit of mask  is reset to zero (Line 5).\nLine 6 determines which nodes communicate in the current iteration of the outer loop. For\ninstance, for the hypercube of Figure 4.6  , mask  is initially", "doc_id": "2808eb57-eb6d-4b12-be90-2624cc70490c", "embedding": null, "doc_hash": "c783c4819a4ddf69d8e70145f6d9b06875c846c5b314324bc6bb81b039b83405", "extra_info": null, "node_info": {"start": 460497, "end": 464380}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "348603bf-cdb9-4d77-b4d8-eae412c8893a", "3": "809b0f70-d4d3-4e2f-96a1-4f773e32295f"}}, "__type__": "1"}, "809b0f70-d4d3-4e2f-96a1-4f773e32295f": {"__data__": {"text": "only nodes 0\nand 4 communicate, since their two least significant bits are zero. In the next time step, when i\n= 1, all nodes (that is, 0, 2, 4, and 6) with zero in their least significant bits participate in\ncommunication. The procedure terminates after communication has taken place along all\ndimensions.\nThe variable mask  helps determine which nodes communicate in a particular iteration of the\nloop. The variable mask  has d (= log p ) bits, all of which are initially set to one (Line 3). At the\nbeginning of each iteration, the most significant nonzero bit of mask  is reset to zero (Line 5).\nLine 6 determines which nodes communicate in the current iteration of the outer loop. For\ninstance, for the hypercube of Figure 4.6  , mask  is initially set to 111, and it would be 011\nduring the iteration corresponding to i = 2 (the i least significant bits of mask  are ones). The\nAND operation on Line 6 selects only those nodes that have zeros in their i least significant bits.\nAmong the nodes selected for communication along dimension i , the nodes with a zero at bit\nposition i send the data, and the nodes with a one at bit position i receive it. The test to\ndetermine the sending and receiving nodes is performed on Line 7. For example, in Figure 4.6  ,\nnode 0 (000) is the sender and node 4 (100) is the receiver in the iteration corresponding to i =\n2. Similarly, for i = 1, nodes 0 (000) and 4 (100) are senders while nodes 2 (010) and 6 (110)\nare receivers.\nAlgorithm 4.1  works only if node 0 is the source of the broadcast. For an arbitrary source, we\nmust relabel the nodes of the hypothetical hypercube by XORing the label of each node with the\nlabel of the source node before we apply this procedure. A modified one-to-all broadcast\nprocedure that works for any value of source  between 0 and p - 1 is shown in Algorithm 4.2  . By\nperforming the XOR operation at Line 3, Algorithm 4.2  relabels the source node to 0, and\nrelabels the other nodes relative to the source. After this relabeling, the algorithm of Algorithm\n4.1 can be applied to perform the broadcast.\nAlgorithm 4.3  gives a procedure to perform an all-to-one reduction on a hypothetical d -\ndimensional hypercube such that the final result is accumulated on node 0. Single node-\naccumulation is the dual of one-to-all broadcast. Therefore, we obtain the communication\npattern required to implement reduction by reversing the order and the direction of messages in\none-to-all broadcast. Procedure ALL_TO_ONE_REDUCE( d , my_id  , m , X , sum ) shown in\nAlgorithm 4.3  is very similar to procedure ONE_TO_ALL_BC( d , my_id  , X ) shown in Algorithm\n4.1 . One difference is that the communication in all-to-one reduction proceeds from the lowest\nto the highest dimension. This change is reflected in the way that variables mask  and i are\nmanipulated in Algorithm 4.3  . The criterion for determining the source and the destination\namong a pair of communicating nodes is also reversed (Line 7). Apart from these differences,\nprocedure ALL_TO_ONE_REDUCE has extra instructions (Lines 13 and 14) to add the contents\nof the messages received by a node in each iteration (any associative operation can be used in\nplace of addition).\nAlgorithm 4.1 One-to-all broadcast of a message X from node 0 of a d -\ndimensional p -node hypercube ( d = log p ). AND and XOR are bitwise\nlogical-and and exclusive-or operations, respectively.1.    procedure  ONE_TO_ALL_BC( d, my_id, X) \n2.    begin \n3.       mask := 2d - 1;               ", "doc_id": "809b0f70-d4d3-4e2f-96a1-4f773e32295f", "embedding": null, "doc_hash": "179a096afae16dfd6a1f916f44053073423a11bfd81616bbb301c42c9ffeeb01", "extra_info": null, "node_info": {"start": 464379, "end": 467880}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2808eb57-eb6d-4b12-be90-2624cc70490c", "3": "34b96a9e-4970-41ef-890d-b183e76a05ff"}}, "__type__": "1"}, "34b96a9e-4970-41ef-890d-b183e76a05ff": {"__data__": {"text": "nodes is also reversed (Line 7). Apart from these differences,\nprocedure ALL_TO_ONE_REDUCE has extra instructions (Lines 13 and 14) to add the contents\nof the messages received by a node in each iteration (any associative operation can be used in\nplace of addition).\nAlgorithm 4.1 One-to-all broadcast of a message X from node 0 of a d -\ndimensional p -node hypercube ( d = log p ). AND and XOR are bitwise\nlogical-and and exclusive-or operations, respectively.1.    procedure  ONE_TO_ALL_BC( d, my_id, X) \n2.    begin \n3.       mask := 2d - 1;                  /* Set all d bits of mask to 1 */ \n4.       for i := d - 1 downto 0 do       /* Outer loop */ \n5.           mask := mask XOR 2i;         /* Set bit i of mask to 0 */ \n6.           if (my_id AND mask) = 0 then /* If lower i bits of my_id are 0 */ \n7.               if (my_id AND 2i) = 0 then \n8.                   msg_destination  := my_id XOR 2i; \n9.                   send X to msg_destination ; \n10.              else \n11.                  msg_source  := my_id XOR 2i; \n12.                  receive X from msg_source ; \n13.              endelse; \n14.          endif; \n15.      endfor; \n16.   end ONE_TO_ALL_BC \nAlgorithm 4.2 One-to-all broadcast of a message X initiated by source\non a d -dimensional hypothetical hypercube. The AND and XOR\noperations are bitwise logical operations.\n1.   procedure  GENERAL_ONE_TO_ALL_BC( d, my_id, source, X) \n2.   begin \n3.      my_virtual id  := my_id XOR source; \n4.      mask := 2d - 1; \n5.      for i := d - 1 downto 0 do   /* Outer loop */ \n6.          mask := mask XOR 2i;    /* Set bit i of mask to 0 */ \n7.          if (my_virtual_id  AND mask) = 0 then \n8.              if (my_virtual_id  AND 2i) = 0 then \n9.                  virtual_dest  := my_virtual_id  XOR 2i; \n10.                 send X to (virtual_dest  XOR source); \n        /* Convert virtual_dest  to the label of the physical destination */ \n11.             else \n12.                 virtual_source  := my_virtual_id  XOR 2i; \n13.                 receive X from (virtual_source  XOR source); \n        /* Convert virtual_source  to the label of the physical source */ \n14.             endelse; \n15.     endfor; \n16.  end", "doc_id": "34b96a9e-4970-41ef-890d-b183e76a05ff", "embedding": null, "doc_hash": "1b57b6966229f61e016b035bfb1d5b892db8cae73f27b6a36cb2504f0abef7fa", "extra_info": null, "node_info": {"start": 468059, "end": 470250}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "809b0f70-d4d3-4e2f-96a1-4f773e32295f", "3": "41867f00-2376-4b0e-a591-7def0d7d3c09"}}, "__type__": "1"}, "41867f00-2376-4b0e-a591-7def0d7d3c09": {"__data__": {"text": "XOR 2i; \n10.                 send X to (virtual_dest  XOR source); \n        /* Convert virtual_dest  to the label of the physical destination */ \n11.             else \n12.                 virtual_source  := my_virtual_id  XOR 2i; \n13.                 receive X from (virtual_source  XOR source); \n        /* Convert virtual_source  to the label of the physical source */ \n14.             endelse; \n15.     endfor; \n16.  end GENERAL_ONE_TO_ALL_BC \nAlgorithm 4.3 Single-node accumulation on a d -dimensional\nhypercube. Each node contributes a message X containing m words,\nand node 0 is the destination of the sum. The AND and XOR operations\nare bitwise logical operations.\n1.   procedure  ALL_TO_ONE_REDUCE( d, my_id, m, X, sum) \n2.   begin \n3.      for j := 0 to m - 1 do sum[j] := X[j]; \n4.      mask := 0; \n5.      for i := 0 to d - 1 do \n            /* Select nodes whose lower i bits are 0 */ \n6.          if (my_id AND mask) = 0 then \n7.              if (my_id AND 2i) \n 0 then \n8.                  msg_destination  := my_id XOR 2i; \n9.                  send sum to msg_destination ; \n10.             else \n11.                 msg_source  := my_id XOR 2i; \n12.                 receive X from msg_source ; \n13.                 for j := 0 to m - 1 do \n14.                     sum[j] :=sum[j] + X[j]; \n15.             endelse; \n16.          mask := mask XOR 2i; /* Set bit i of mask to 1 */ \n17.     endfor; \n18.  end ALL_TO_ONE_REDUCE \n4.1.6 Cost Analysis\nAnalyzing the cost of one-to-all broadcast and all-to-one reduction is fairly straightforward.\nAssume that p processes participate in the operation and the data to be broadcast or reduced\ncontains m words. The broadcast or reduction procedure involves log p point-to-point simple\nmessage transfers, each at a time cost of ts + tw m . Therefore, the total time taken by the\nprocedure is\nEquation 4.1\n\n[ Team LiB ]\n \n\n[ Team LiB ]\n  \n4.2 All-to-All Broadcast and Reduction\nAll-to-all broadcast  is a generalization of one-to-all broadcast in which all p nodes\nsimultaneously initiate a broadcast. A process sends the same m-word message to every other\nprocess, but different processes may broadcast different messages. All-to-all broadcast is used\nin matrix operations, including matrix multiplication and matrix-vector multiplication. The dual\nof all-to-all broadcast is all-to-all reduction , in which every node is the destination of an all-\nto-one reduction (Problem 4.8). Figure 4.8 illustrates all-to-all broadcast and", "doc_id": "41867f00-2376-4b0e-a591-7def0d7d3c09", "embedding": null, "doc_hash": "fabf1362a6dadfa844aff0c8898e3171ffc13fb82bbddcd8c853535bc267d254", "extra_info": null, "node_info": {"start": 470425, "end": 472906}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "34b96a9e-4970-41ef-890d-b183e76a05ff", "3": "49efbcbf-4c37-4da7-8827-e95241536672"}}, "__type__": "1"}, "49efbcbf-4c37-4da7-8827-e95241536672": {"__data__": {"text": "taken by the\nprocedure is\nEquation 4.1\n\n[ Team LiB ]\n \n\n[ Team LiB ]\n  \n4.2 All-to-All Broadcast and Reduction\nAll-to-all broadcast  is a generalization of one-to-all broadcast in which all p nodes\nsimultaneously initiate a broadcast. A process sends the same m-word message to every other\nprocess, but different processes may broadcast different messages. All-to-all broadcast is used\nin matrix operations, including matrix multiplication and matrix-vector multiplication. The dual\nof all-to-all broadcast is all-to-all reduction , in which every node is the destination of an all-\nto-one reduction (Problem 4.8). Figure 4.8 illustrates all-to-all broadcast and all-to-all\nreduction.\nFigure 4.8. All-to-all broadcast and all-to-all reduction.\nOne way to perform an all-to-all broadcast is to perform p one-to-all broadcasts, one starting at\neach node. If performed naively, on some architectures this approach may take up to p times as\nlong as a one-to-all broadcast. It is possible to use the communication links in the\ninterconnection network more efficiently by performing all p one-to-all broadcasts\nsimultaneously so that all messages traversing the same path at the same time are\nconcatenated into a single message whose size is the sum of the sizes of individual messages.\nThe following sections describe all-to-all broadcast on linear array, mesh, and hypercube\ntopologies.\n4.2.1 Linear Array and Ring\nWhile performing all-to-all broadcast on a linear array or a ring, all communication links can be\nkept busy simultaneously until the operation is complete because each node always has some\ninformation that it can pass along to its neighbor. Each node first sends to one of its neighbors\nthe data it needs to broadcast. In subsequent steps, it forwards the data received from one of\nits neighbors to its other neighbor.\nFigure 4.9 illustrates all-to-all broadcast for an eight-node ring. The same procedure would also\nwork on a linear array with bidirectional links. As with the previous figures, the integer label of\nan arrow indicates the time step during which the message is sent. In all-to-all broadcast, p\ndifferent messages circulate in the p-node ensemble. In Figure 4.9 , each message is identified\nby its initial source, whose label appears in parentheses along with the time step. For instance,\nthe arc labeled 2 (7) between nodes 0 and 1 represents the data communicated in time step 2\nthat node 0 received from node 7 in the preceding step. As Figure 4.9 shows, if communication\nis performed circularly in a single direction, then each node receives all ( p - 1) pieces of\ninformation from all other nodes in ( p - 1) steps.\nFigure 4.9. All-to-all broadcast on an eight-node ring. The label of each\narrow shows the time step and, within parentheses, the label of the\nnode that owned the current message being transferred before the\nbeginning of the broadcast. The number(s) in parentheses next to\neach node are the labels of nodes from which data has been received\nprior to the current communication step. Only the first, second, and\nlast communication steps are shown.\nAlgorithm 4.4  gives a procedure for all-to-all broadcast on a p-node ring. The initial message to\nbe broadcast is known locally as my_msg  at each node. At the end of the procedure, each node\nstores the collection of all p messages in result . As the program shows, all-to-all broadcast on a\nmesh applies the linear array procedure twice, once along the rows and once along the\ncolumns.\nAlgorithm 4.4 All-to-all broadcast on a p-node ring.\n1.   procedure  ALL_TO_ALL_BC_RING( my_id, my_msg, p, result) \n2.   begin \n3.      left := (my_id - 1) mod p;", "doc_id": "49efbcbf-4c37-4da7-8827-e95241536672", "embedding": null, "doc_hash": "9035a095a87781b0001bd07afb22ba0e4ad331479689abde680b44b80b8f091d", "extra_info": null, "node_info": {"start": 472607, "end": 476250}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "41867f00-2376-4b0e-a591-7def0d7d3c09", "3": "21e316a5-c81b-4345-971f-5ce15d6bf70e"}}, "__type__": "1"}, "21e316a5-c81b-4345-971f-5ce15d6bf70e": {"__data__": {"text": "and\nlast communication steps are shown.\nAlgorithm 4.4  gives a procedure for all-to-all broadcast on a p-node ring. The initial message to\nbe broadcast is known locally as my_msg  at each node. At the end of the procedure, each node\nstores the collection of all p messages in result . As the program shows, all-to-all broadcast on a\nmesh applies the linear array procedure twice, once along the rows and once along the\ncolumns.\nAlgorithm 4.4 All-to-all broadcast on a p-node ring.\n1.   procedure  ALL_TO_ALL_BC_RING( my_id, my_msg, p, result) \n2.   begin \n3.      left := (my_id - 1) mod p; \n4.      right := (my_id + 1) mod p; \n5.      result := my_msg; \n6.      msg := result; \n7.      for i := 1 to p - 1 do \n8.          send msg to right; \n9.          receive msg from left; \n10.         result := result \n msg; \n11.     endfor; \n12.  end ALL_TO_ALL_BC_RING \nIn all-to-all reduction, the dual of all-to-all broadcast, each node starts with p messages, each\none destined to be accumulated at a distinct node. All-to-all reduction can be performed by\nreversing the direction and sequence of the messages. For example, the first communication\nstep for all-to-all reduction on an 8-node ring would correspond to the last step of Figure 4.9\nwith node 0 sending msg[1] to 7 instead of receiving it. The only additional step required is that\nupon receiving a message, a node must combine it with the local copy of the message that has\nthe same destination as the received message before forwarding the combined message to the\nnext neighbor. Algorithm 4.5 gives a procedure for all-to-all reduction on a p-node ring.\nAlgorithm 4.5 All-to-all reduction on a p-node ring.\n1.   procedure  ALL_TO_ALL_RED_RING( my_id, my_msg, p, result) \n2.   begin \n3.      left := (my_id - 1) mod p; \n4.      right := (my_id + 1) mod p; \n5.      recv := 0; \n6.      for i := 1 to p - 1 do \n7.          j := (my_id + i) mod p; \n8.          temp := msg[j] + recv; \n9.          send temp to left; \n10.         receive recv from right; \n11.     endfor; \n12.     result := msg[my_id] + recv; \n13.  end ALL_TO_ALL_RED_RING \n4.2.2 Mesh\nJust like one-to-all broadcast, the all-to-all broadcast algorithm for the 2-D mesh is based on\nthe linear array algorithm, treating rows and columns of the mesh as linear arrays. Once again,\ncommunication takes place in two phases. In the first phase, each row of the mesh performs an\nall-to-all broadcast using the procedure for the linear array. In this phase, all nodes collect \nmessages corresponding to the \n  nodes of their respective rows. Each node consolidates this\ninformation into a single message of size \n , and proceeds to the second communication\nphase of the algorithm. The second communication phase is a columnwise all-to-all broadcast of\nthe consolidated messages. By the end of this phase, each node obtains all p pieces of m-word\ndata that originally resided on different", "doc_id": "21e316a5-c81b-4345-971f-5ce15d6bf70e", "embedding": null, "doc_hash": "35562fcaea27aba07ba0bcfb10e796f8d0d19da9e18e9bbb2f8e473cd69b27af", "extra_info": null, "node_info": {"start": 476336, "end": 479235}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "49efbcbf-4c37-4da7-8827-e95241536672", "3": "e60d61f9-abeb-4124-bd0b-42a1097fb14e"}}, "__type__": "1"}, "e60d61f9-abeb-4124-bd0b-42a1097fb14e": {"__data__": {"text": "broadcast, the all-to-all broadcast algorithm for the 2-D mesh is based on\nthe linear array algorithm, treating rows and columns of the mesh as linear arrays. Once again,\ncommunication takes place in two phases. In the first phase, each row of the mesh performs an\nall-to-all broadcast using the procedure for the linear array. In this phase, all nodes collect \nmessages corresponding to the \n  nodes of their respective rows. Each node consolidates this\ninformation into a single message of size \n , and proceeds to the second communication\nphase of the algorithm. The second communication phase is a columnwise all-to-all broadcast of\nthe consolidated messages. By the end of this phase, each node obtains all p pieces of m-word\ndata that originally resided on different nodes. The distribution of data among the nodes of a 3\nx 3 mesh at the beginning of the first and the second phases of the algorithm is shown in Figure\n4.10.\nFigure 4.10. All-to-all broadcast on a 3 x 3 mesh. The groups of nodes\ncommunicating with each other in each phase are enclosed by dotted\nboundaries. By the end of the second phase, all nodes get\n(0,1,2,3,4,5,6,7) (that is, a message from each node).\nAlgorithm 4.6  gives a procedure for all-to-all broadcast on a \n  mesh. The mesh\nprocedure for all-to-all reduction is left as an exercise for the reader (Problem 4.4).\nAlgorithm 4.6 All-to-all broadcast on a square mesh of p nodes.\n1.   procedure  ALL_TO_ALL_BC_MESH( my_id, my_msg, p, result) \n2.   begin \n/* Communication along rows */ \n3.      left := my_id - (my_id mod \n ) + (my_id - 1)mod \n ; \n4.      right := my_id - (my_id mod \n ) + (my_id + 1) mod \n ; \n5.      result := my_msg; \n6.      msg := result; \n7.      for i := 1 to \n - 1 do \n8.          send msg to right; \n9.          receive msg from left; \n10.         result := result \n msg; \n11.     endfor; \n/* Communication along columns */ \n12.     up := (my_id - \n) mod p; \n13.     down := (my_id + \n) mod p; \n14.     msg := result; \n15.     for i := 1 to \n - 1 do \n16.         send msg to down; \n17.         receive msg from up; \n18.         result := result \n msg; \n19.     endfor; \n20.  end ALL_TO_ALL_BC_MESH \n4.2.3 Hypercube\nThe hypercube algorithm for all-to-all broadcast is an extension of the mesh algorithm to log p\ndimensions. The procedure requires log p steps. Communication takes place along a different\ndimension of the p-node hypercube in each step. In every step, pairs of nodes exchange their\ndata and double the size of the message to be transmitted in the next step by concatenating the\nreceived message with their current data. Figure 4.11  shows these steps for an eight-node\nhypercube with bidirectional communication channels.\nFigure 4.11. All-to-all broadcast on an eight-node hypercube.\nAlgorithm 4.7  gives a procedure for implementing all-to-all broadcast on a d-dimensional\nhypercube. Communication starts from the lowest dimension of the hypercube and then\nproceeds along successively higher dimensions (Line 4). In each iteration, nodes communicate\nin pairs so that the labels of the nodes communicating", "doc_id": "e60d61f9-abeb-4124-bd0b-42a1097fb14e", "embedding": null, "doc_hash": "ad3d80da9802efb95f667b0115b1a8349701eb5e5a224b6898e278699252c7f6", "extra_info": null, "node_info": {"start": 479071, "end": 482151}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "21e316a5-c81b-4345-971f-5ce15d6bf70e", "3": "bdbede43-3230-4ac5-9499-11a1f20b7d05"}}, "__type__": "1"}, "bdbede43-3230-4ac5-9499-11a1f20b7d05": {"__data__": {"text": "requires log p steps. Communication takes place along a different\ndimension of the p-node hypercube in each step. In every step, pairs of nodes exchange their\ndata and double the size of the message to be transmitted in the next step by concatenating the\nreceived message with their current data. Figure 4.11  shows these steps for an eight-node\nhypercube with bidirectional communication channels.\nFigure 4.11. All-to-all broadcast on an eight-node hypercube.\nAlgorithm 4.7  gives a procedure for implementing all-to-all broadcast on a d-dimensional\nhypercube. Communication starts from the lowest dimension of the hypercube and then\nproceeds along successively higher dimensions (Line 4). In each iteration, nodes communicate\nin pairs so that the labels of the nodes communicating with each other in the i th iteration differ\nin the i th least significant bit of their binary representations (Line 5). After an iteration's\ncommunication steps, each node concatenates the data it receives during that iteration with its\nresident data (Line 8). This concatenated message is transmitted in the following iteration.\nAlgorithm 4.7 All-to-all broadcast on a d-dimensional hypercube.\n1.   procedure  ALL_TO_ALL_BC_HCUBE( my_id, my_msg, d, result) \n2.   begin \n3.      result := my_msg; \n4.      for i := 0 to d - 1 do \n5.          partner := my id XOR 2i; \n6.          send result to partner; \n7.          receive msg from partner; \n8.          result := result \n msg; \n9.      endfor; \n10.  end ALL_TO_ALL_BC_HCUBE \nAs usual, the algorithm for all-to-all reduction can be derived by reversing the order and\ndirection of messages in all-to-all broadcast. Furthermore, instead of concatenating the\nmessages, the reduction operation needs to select the appropriate subsets of the buffer to send\nout and accumulate received messages in each iteration. Algorithm 4.8 gives a procedure for\nall-to-all reduction on a d-dimensional hypercube. It uses senloc  to index into the starting\nlocation of the outgoing message and recloc  to index into the location where the incoming\nmessage is added in each iteration.\nAlgorithm 4.8 All-to-all broadcast on a d-dimensional hypercube. AND\nand XOR are bitwise logical-and and exclusive-or operations,\nrespectively.\n1.   procedure  ALL_TO_ALL_RED_HCUBE( my_id, msg, d, result) \n2.   begin \n3.      recloc := 0; \n4.      for i := d - 1 to 0 do \n5.          partner := my_id XOR 2i; \n6.          j := my_id AND 2i; \n7.          k := (my_id XOR 2i) AND 2i; \n8.          senloc := recloc + k; \n9.          recloc := recloc + j; \n10.         send msg[senloc .. senloc  + 2i - 1] to partner; \n11.         receive temp[0 .. 2i - 1] from partner; \n12.         for j := 0 to 2i - 1 do \n13.             msg[recloc + j] := msg[recloc + j] + temp[ j]; \n14.         endfor; \n15.     endfor; \n16.     result := msg[my_id]; \n17.  end", "doc_id": "bdbede43-3230-4ac5-9499-11a1f20b7d05", "embedding": null, "doc_hash": "56377344496126f51ce4ade6844ea329ab7fbb8c92ce9aa4ea313f9962bb621e", "extra_info": null, "node_info": {"start": 482132, "end": 484979}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e60d61f9-abeb-4124-bd0b-42a1097fb14e", "3": "72b57050-2e8b-4d80-9d7e-5dbbc063e8f3"}}, "__type__": "1"}, "72b57050-2e8b-4d80-9d7e-5dbbc063e8f3": {"__data__": {"text": "      senloc := recloc + k; \n9.          recloc := recloc + j; \n10.         send msg[senloc .. senloc  + 2i - 1] to partner; \n11.         receive temp[0 .. 2i - 1] from partner; \n12.         for j := 0 to 2i - 1 do \n13.             msg[recloc + j] := msg[recloc + j] + temp[ j]; \n14.         endfor; \n15.     endfor; \n16.     result := msg[my_id]; \n17.  end ALL_TO_ALL_RED_HCUBE \n4.2.4 Cost Analysis\nOn a ring or a linear array, all-to-all broadcast involves p - 1 steps of communication between\nnearest neighbors. Each step, involving a message of size m, takes time ts + tw m. Therefore,\nthe time taken by the entire operation is\nEquation 4.2\n\nSimilarly, on a mesh, the first phase of \n  simultaneous all-to-all broadcasts (each among \nnodes) concludes in time \n . The number of nodes participating in each all-to-all\nbroadcast in the second phase is also \n , but the size of each message is now \n .\nTherefore, this phase takes time \n  to complete. The time for the entire all-\nto-all broadcast on a p-node two-dimensional square mesh is the sum of the times spent in the\nindividual phases, which is\nEquation 4.3\nOn a p-node hypercube, the size of each message exchanged in the i th of the log p steps is 2i-\n1m. It takes a pair of nodes time ts + 2i-1twm to send and receive messages from each other\nduring the i th step. Hence, the time to complete the entire procedure is\nEquation 4.4\nEquations 4.2, 4.3, and 4.4 show that the term associated with tw in the expressions for the\ncommunication time of all-to-all broadcast is twm(p - 1) for all the architectures. This term also\nserves as a lower bound for the communication time of all-to-all broadcast for parallel\ncomputers on which a node can communicate on only one of its ports at a time. This is because\neach node receives at least m(p - 1) words of data, regardless of the architecture. Thus, for\nlarge messages, a highly connected network like a hypercube is no better than a simple ring in\nperforming all-to-all broadcast or all-to-all reduction. In fact, the straightforward all-to-all\nbroadcast algorithm for a simple architecture like a ring has great practical importance. A close\nlook at the algorithm reveals that it is a sequence of p one-to-all broadcasts, each with a\ndifferent source. These broadcasts are pipelined so that all of them are complete in a total of p\nnearest-neighbor communication steps. Many parallel algorithms involve a series of one-to-all\nbroadcasts with different sources, often interspersed with some computation. If each one-to-all\nbroadcast is performed using the hypercube algorithm of Section 4.1.3, then n broadcasts\nwould require time n(ts + twm) log p. On the other hand, by pipelining the broadcasts as shown\nin Figure 4.9 , all of them can be performed spending no more than time ( ts + twm)(p - 1) in\ncommunication, provided that the sources of all broadcasts are different and n \n p. In later\nchapters, we show how such pipelined broadcast improves the performance of some parallel\nalgorithms such as Gaussian elimination ( Section 8.3.1), back substitution ( Section 8.3.3 ), and\nFloyd's algorithm for finding the shortest paths in a graph ( Section 10.4.2 ).\nAnother noteworthy property of all-to-all broadcast is that, unlike one-to-all broadcast,", "doc_id": "72b57050-2e8b-4d80-9d7e-5dbbc063e8f3", "embedding": null, "doc_hash": "0d3e0b10f5f47bdaa08a505c7aeebd006df970cbe139a0402e3e6335d97e6d0e", "extra_info": null, "node_info": {"start": 485426, "end": 488682}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bdbede43-3230-4ac5-9499-11a1f20b7d05", "3": "ba29908c-54bc-4099-97b1-ac74e21d61d3"}}, "__type__": "1"}, "ba29908c-54bc-4099-97b1-ac74e21d61d3": {"__data__": {"text": "the hypercube algorithm of Section 4.1.3, then n broadcasts\nwould require time n(ts + twm) log p. On the other hand, by pipelining the broadcasts as shown\nin Figure 4.9 , all of them can be performed spending no more than time ( ts + twm)(p - 1) in\ncommunication, provided that the sources of all broadcasts are different and n \n p. In later\nchapters, we show how such pipelined broadcast improves the performance of some parallel\nalgorithms such as Gaussian elimination ( Section 8.3.1), back substitution ( Section 8.3.3 ), and\nFloyd's algorithm for finding the shortest paths in a graph ( Section 10.4.2 ).\nAnother noteworthy property of all-to-all broadcast is that, unlike one-to-all broadcast, the\nhypercube algorithm cannot be applied unaltered to mesh and ring architectures. The reason is\nthat the hypercube procedure for all-to-all broadcast would cause congestion on the\ncommunication channels of a smaller-dimensional network with the same number of nodes. For\ninstance, Figure 4.12 shows the result of performing the third step ( Figure 4.11(c) ) of the\nhypercube all-to-all broadcast procedure on a ring. One of the links of the ring is traversed by\nall four messages and would take four times as much time to complete the communication step.\nFigure 4.12. Contention for a channel when the communication step of\nFigure 4.11(c)  for the hypercube is mapped onto a ring.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n4.3 All-Reduce and Prefix-Sum Operations\nThe communication pattern of all-to-all broadcast can be used to perform some other\noperations as well. One of these operations is a third variation of reduction, in which each node\nstarts with a buffer of size m and the final results of the operation are identical buffers of size m\non each node that are formed by combining the original p buffers using an associative operator.\nSemantically, this operation, often referred to as the all-reduce  operation, is identical to\nperforming an all-to-one reduction followed by a one-to-all broadcast of the result. This\noperation is different from all-to-all reduction, in which p simultaneous all-to-one reductions\ntake place, each with a different destination for the result.\nAn all-reduce operation with a single-word message on each node is often used to implement\nbarrier synchronization on a message-passing computer. The semantics of the reduction\noperation are such that, while executing a parallel program, no node can finish the reduction\nbefore each node has contributed a value.\nA simple method to perform all-reduce is to perform an all-to-one reduction followed by a one-\nto-all broadcast. However, there is a faster way to perform all-reduce by using the\ncommunication pattern of all-to-all broadcast. Figure 4.11 illustrates this algorithm for an eight-\nnode hypercube. Assume that each integer in parentheses in the figure, instead of denoting a\nmessage, denotes a number to be added that originally resided at the node with that integer\nlabel. To perform reduction, we follow the communication steps of the all-to-all broadcast\nprocedure, but at the end of each step, add two numbers instead of concatenating two\nmessages. At the termination of the reduction procedure, each node holds the sum (0 + 1 + 2 +\n\u00b7\u00b7\u00b7 + 7) (rather than eight messages numbered from 0 to 7, as in the case of all-to-all\nbroadcast). Unlike all-to-all broadcast, each message transferred in the reduction operation has\nonly one word. The size of the messages does not double in each step because the numbers are\nadded instead of being concatenated. Therefore, the total communication time for all log p steps\nis\nEquation 4.5\nAlgorithm 4.7  can be used to perform a sum of p numbers if my_msg , msg, and result  are\nnumbers (rather than messages), and the union operation ('\n ') on Line 8 is replaced by\naddition.\nFinding", "doc_id": "ba29908c-54bc-4099-97b1-ac74e21d61d3", "embedding": null, "doc_hash": "f5bf9ccaf106b7316aecf12e66e6d0e18e37e2f76f0cd349f076b536920d1051", "extra_info": null, "node_info": {"start": 488321, "end": 492133}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "72b57050-2e8b-4d80-9d7e-5dbbc063e8f3", "3": "0c335f71-d316-4c2f-aee1-fe5b7a6fb2ab"}}, "__type__": "1"}, "0c335f71-d316-4c2f-aee1-fe5b7a6fb2ab": {"__data__": {"text": "add two numbers instead of concatenating two\nmessages. At the termination of the reduction procedure, each node holds the sum (0 + 1 + 2 +\n\u00b7\u00b7\u00b7 + 7) (rather than eight messages numbered from 0 to 7, as in the case of all-to-all\nbroadcast). Unlike all-to-all broadcast, each message transferred in the reduction operation has\nonly one word. The size of the messages does not double in each step because the numbers are\nadded instead of being concatenated. Therefore, the total communication time for all log p steps\nis\nEquation 4.5\nAlgorithm 4.7  can be used to perform a sum of p numbers if my_msg , msg, and result  are\nnumbers (rather than messages), and the union operation ('\n ') on Line 8 is replaced by\naddition.\nFinding prefix sums  (also known as the scan operation) is another important problem that can\nbe solved by using a communication pattern similar to that used in all-to-all broadcast and all-\nreduce operations. Given p numbers n0, n1, ..., np-1 (one on each node), the problem is to\ncompute the sums \n  for all k between 0 and p - 1. For example, if the original\nsequence of numbers is <3, 1, 4, 0, 2>, then the sequence of prefix sums is <3, 4, 8, 8, 10>.\nInitially, nk resides on the node labeled k, and at the end of the procedure, the same node holds\nsk . Instead of starting with a single numbers, each node could start with a buffer or vector of\nsize m and the m-word result would be the sum of the corresponding elements of buffers.\nFigure 4.13 illustrates the prefix sums procedure for an eight-node hypercube. This figure is a\nmodification of Figure 4.11 . The modification is required to accommodate the fact that in prefix\nsums the node with label k uses information from only the k-node subset of those nodes whose\nlabels are less than or equal to k. To accumulate the correct prefix sum, every node maintains\nan additional result buffer. This buffer is denoted by square brackets in Figure 4.13 . At the end\nof a communication step, the content of an incoming message is added to the result buffer only\nif the message comes from a node with a smaller label than that of the recipient node. The\ncontents of the outgoing message (denoted by parentheses in the figure) are updated with\nevery incoming message, just as in the case of the all-reduce operation. For instance, after the\nfirst communication step, nodes 0, 2, and 4 do not add the data received from nodes 1, 3, and 5\nto their result buffers. However, the contents of the outgoing messages for the next step are\nupdated.\nFigure 4.13. Computing prefix sums on an eight-node hypercube. At\neach node, square brackets show the local prefix sum accumulated in\nthe result buffer and parentheses enclose the contents of the outgoing\nmessage buffer for the next step.\nSince not all of the messages received by a node contribute to its final result, some of the\nmessages it receives may be redundant. We have omitted these steps of the standard all-to-all\nbroadcast communication pattern from Figure 4.13, although the presence or absence of these\nmessages does not affect the results of the algorithm. Algorithm 4.9  gives a procedure to solve\nthe prefix sums problem on a d-dimensional hypercube.\nAlgorithm 4.9 Prefix sums on a d-dimensional hypercube.\n1.   procedure  PREFIX_SUMS_HCUBE( my_id, my number , d, result) \n2.   begin \n3.      result := my_number ; \n4.      msg := result; \n5.      for i := 0 to d - 1 do \n6.          partner := my_id XOR 2i; \n7.          send msg to partner; \n8.          receive number from partner; \n9.          msg := msg +", "doc_id": "0c335f71-d316-4c2f-aee1-fe5b7a6fb2ab", "embedding": null, "doc_hash": "8b3514db14c29cd89edd537fb93ffc7249780617e30e180c5b665a2bd5be654e", "extra_info": null, "node_info": {"start": 492117, "end": 495654}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ba29908c-54bc-4099-97b1-ac74e21d61d3", "3": "c201a44f-f30a-41e1-a53c-f34d554325e1"}}, "__type__": "1"}, "c201a44f-f30a-41e1-a53c-f34d554325e1": {"__data__": {"text": "a procedure to solve\nthe prefix sums problem on a d-dimensional hypercube.\nAlgorithm 4.9 Prefix sums on a d-dimensional hypercube.\n1.   procedure  PREFIX_SUMS_HCUBE( my_id, my number , d, result) \n2.   begin \n3.      result := my_number ; \n4.      msg := result; \n5.      for i := 0 to d - 1 do \n6.          partner := my_id XOR 2i; \n7.          send msg to partner; \n8.          receive number from partner; \n9.          msg := msg + number; \n10.         if (partner < my_id) then result := result + number; \n11.     endfor; \n12.  end PREFIX_SUMS_HCUBE \n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n4.4 Scatter and Gather\nIn the scatter  operation, a single node sends a unique message of size m to every other node.\nThis operation is also known as one-to-all personalized communication . One-to-all\npersonalized communication is different from one-to-all broadcast in that the source node starts\nwith p unique messages, one destined for each node. Unlike one-to-all broadcast, one-to-all\npersonalized communication does not involve any duplication of data. The dual of one-to-all\npersonalized communication or the scatter operation is the gather  operation, or\nconcatenation , in which a single node collects a unique message from each node. A gather\noperation is different from an all-to-one reduce operation in that it does not involve any\ncombination or reduction of data. Figure 4.14 illustrates the scatter and gather operations.\nFigure 4.14. Scatter and gather operations.\nAlthough the scatter operation is semantically different from one-to-all broadcast, the scatter\nalgorithm is quite similar to that of the broadcast. Figure 4.15 shows the communication steps\nfor the scatter operation on an eight-node hypercube. The communication patterns of one-to-all\nbroadcast ( Figure 4.6) and scatter ( Figure 4.15 ) are identical. Only the size and the contents of\nmessages are different. In Figure 4.15 , the source node (node 0) contains all the messages. The\nmessages are identified by the labels of their destination nodes. In the first communication\nstep, the source transfers half of the messages to one of its neighbors. In subsequent steps,\neach node that has some data transfers half of it to a neighbor that has yet to receive any data.\nThere is a total of log p communication steps corresponding to the log p dimensions of the\nhypercube.\nFigure 4.15. The scatter operation on an eight-node hypercube.\nThe gather operation is simply the reverse of scatter. Each node starts with an m word\nmessage. In the first step, every odd numbered node sends its buffer to an even numbered\nneighbor behind it, which concatenates the received message with its own buffer. Only the even\nnumbered nodes participate in the next communication step which results in nodes with\nmultiples of four labels gathering more data and doubling the sizes of their data. The process\ncontinues similarly, until node 0 has gathered the entire data.\nJust like one-to-all broadcast and all-to-one reduction, the hypercube algorithms for scatter and\ngather can be applied unaltered to linear array and mesh interconnection topologies without\nany increase in the communication time.\nCost Analysis  All links of a p-node hypercube along a certain dimension join two p/2-node\nsubcubes ( Section 2.4.3). As Figure 4.15  illustrates, in each communication step of the scatter\noperations, data flow from one subcube to another. The data that a node owns before starting\ncommunication in a certain dimension are such that half of them need to be sent to a node in\nthe other subcube. In", "doc_id": "c201a44f-f30a-41e1-a53c-f34d554325e1", "embedding": null, "doc_hash": "b19fa763fc70594814559bdc7e9adc4981115a2ba68d59330d669367c53e63d5", "extra_info": null, "node_info": {"start": 495950, "end": 499495}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0c335f71-d316-4c2f-aee1-fe5b7a6fb2ab", "3": "94f6fa57-baf6-4e75-bed9-0b8366928b53"}}, "__type__": "1"}, "94f6fa57-baf6-4e75-bed9-0b8366928b53": {"__data__": {"text": "of four labels gathering more data and doubling the sizes of their data. The process\ncontinues similarly, until node 0 has gathered the entire data.\nJust like one-to-all broadcast and all-to-one reduction, the hypercube algorithms for scatter and\ngather can be applied unaltered to linear array and mesh interconnection topologies without\nany increase in the communication time.\nCost Analysis  All links of a p-node hypercube along a certain dimension join two p/2-node\nsubcubes ( Section 2.4.3). As Figure 4.15  illustrates, in each communication step of the scatter\noperations, data flow from one subcube to another. The data that a node owns before starting\ncommunication in a certain dimension are such that half of them need to be sent to a node in\nthe other subcube. In every step, a communicating node keeps half of its data, meant for the\nnodes in its subcube, and sends the other half to its neighbor in the other subcube. The time in\nwhich all data are distributed to their respective destinations is\nEquation 4.6\nThe scatter and gather operations can also be performed on a linear array and on a 2-D square\nmesh in time ts log p + twm(p - 1) (Problem 4.7). Note that disregarding the term due to\nmessage-startup time, the cost of scatter and gather operations for large messages on any k-d\nmesh interconnection network ( Section 2.4.3 ) is similar. In the scatter operation, at least m(p -\n1) words of data must be transmitted out of the source node, and in the gather operation, at\nleast m(p - 1) words of data must be received by the destination node. Therefore, as in the case\nof all-to-all broadcast, twm(p - 1) is a lower bound on the communication time of scatter and\ngather operations. This lower bound is independent of the interconnection network.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n4.5 All-to-All Personalized Communication\nIn all-to-all personalized communication , each node sends a distinct message of size m to\nevery other node. Each node sends different messages to different nodes, unlike all-to-all\nbroadcast, in which each node sends the same message to all other nodes. Figure 4.16\nillustrates the all-to-all personalized communication operation. A careful observation of this\nfigure would reveal that this operation is equivalent to transposing a two-dimensional array of\ndata distributed among p processes using one-dimensional array partitioning ( Figure 3.24). All-\nto-all personalized communication is also known as total exchange . This operation is used in a\nvariety of parallel algorithms such as fast Fourier transform, matrix transpose, sample sort, and\nsome parallel database join operations.\nFigure 4.16. All-to-all personalized communication.\nExample 4.2 Matrix transposition\nThe transpose of an n x n matrix A is a matrix AT of the same size, such that AT [i, j] =\nA[j, i] for 0 \n  i, j < n. Consider an n x n matrix mapped onto n processors such that\neach processor contains one full row of the matrix. With this mapping, processor P i\ninitially contains the elements of the matrix with indices [ i, 0], [ i, 1], ..., [ i, n - 1].\nAfter the transposition, element [ i, 0] belongs to P 0, element [ i, 1] belongs to P 1, and\nso on. In general, element [ i, j] initially resides on P i , but moves to P j during the\ntransposition. The data-communication pattern of this procedure is shown in Figure\n4.17 for a 4 x 4 matrix mapped onto four processes using one-dimensional rowwise\npartitioning. Note that in this figure every processor sends a distinct element of the\nmatrix to every other processor. This is an example of all-to-all personalized\ncommunication.\nFigure 4.17. All-to-all personalized communication in\ntransposing a 4 x 4 matrix using four processes.\nIn general, if we use p processes such that p \n", "doc_id": "94f6fa57-baf6-4e75-bed9-0b8366928b53", "embedding": null, "doc_hash": "46459b1df322340cbf0f5ec76a6f124a6e2f5d8a48a0609a2807f7c003b711b5", "extra_info": null, "node_info": {"start": 499150, "end": 502902}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c201a44f-f30a-41e1-a53c-f34d554325e1", "3": "a2ea11ef-2755-400a-82b2-a854888619e8"}}, "__type__": "1"}, "a2ea11ef-2755-400a-82b2-a854888619e8": {"__data__": {"text": "i, 1], ..., [ i, n - 1].\nAfter the transposition, element [ i, 0] belongs to P 0, element [ i, 1] belongs to P 1, and\nso on. In general, element [ i, j] initially resides on P i , but moves to P j during the\ntransposition. The data-communication pattern of this procedure is shown in Figure\n4.17 for a 4 x 4 matrix mapped onto four processes using one-dimensional rowwise\npartitioning. Note that in this figure every processor sends a distinct element of the\nmatrix to every other processor. This is an example of all-to-all personalized\ncommunication.\nFigure 4.17. All-to-all personalized communication in\ntransposing a 4 x 4 matrix using four processes.\nIn general, if we use p processes such that p \n n, then each process initially holds\nn/p rows (that is, n2/p elements) of the matrix. Performing the transposition now\ninvolves an all-to-all personalized communication of matrix blocks of size n/p x n/p,\ninstead of individual elements. \nWe now discuss the implementation of all-to-all personalized communication on parallel\ncomputers with linear array, mesh, and hypercube interconnection networks. The\ncommunication patterns of all-to-all personalized communication are identical to those of all-to-\nall broadcast on all three architectures. Only the size and the contents of messages are\ndifferent.\n4.5.1 Ring\nFigure 4.18 shows the steps in an all-to-all personalized communication on a six-node linear\narray. To perform this operation, every node sends p - 1 pieces of data, each of size m. In the\nfigure, these pieces of data are identified by pairs of integers of the form {i , j}, where i is the\nsource of the message and j is its final destination. First, each node sends all pieces of data as\none consolidated message of size m(p - 1) to one of its neighbors (all nodes communicate in the\nsame direction). Of the m(p - 1) words of data received by a node in this step, one m-word\npacket belongs to it. Therefore, each node extracts the information meant for it from the data\nreceived, and forwards the remaining ( p - 2) pieces of size m each to the next node. This\nprocess continues for p - 1 steps. The total size of data being transferred between nodes\ndecreases by m words in each successive step. In every step, each node adds to its collection\none m-word packet originating from a different node. Hence, in p - 1 steps, every node receives\nthe information from all other nodes in the ensemble.\nFigure 4.18. All-to-all personalized communication on a six-node ring.\nThe label of each message is of the form { x, y}, where x is the label of\nthe node that originally owned the message, and y is the label of the\nnode that is the final destination of the message. The label ({ x1, y1},\n{x2, y2}, ..., { xn, yn}) indicates a message that is formed by\nconcatenating n individual messages.\nIn the above procedure, all messages are sent in the same direction. If half of the messages are\nsent in one direction and the remaining half are sent in the other direction, then the\ncommunication cost due to the tw can be reduced by a factor of two. For the sake of simplicity,\nwe ignore this constant-factor improvement.\nCost Analysis  On a ring or a bidirectional linear array, all-to-all personalized communication\ninvolves p - 1 communication steps. Since the size of the messages transferred in the i th step is\nm(p - i), the total time taken by this operation is\nEquation 4.7\nIn the all-to-all personalized communication procedure described above, each node sends m(p -\n1) words of data because it has an m-word packet for every other node. Assume that all\nmessages are sent either clockwise or counterclockwise. The average distance that an m-word\npacket travels is \n , which is equal to p/2. Since there are p nodes, each\nperforming the same type of communication, the total", "doc_id": "a2ea11ef-2755-400a-82b2-a854888619e8", "embedding": null, "doc_hash": "a43c181e51bf5a70b36ecfcb49631739e0209d8cd64cd625fafa6146d01d38f8", "extra_info": null, "node_info": {"start": 502973, "end": 506759}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "94f6fa57-baf6-4e75-bed9-0b8366928b53", "3": "17544ba6-9c95-47f6-92fd-4b6166ef83d0"}}, "__type__": "1"}, "17544ba6-9c95-47f6-92fd-4b6166ef83d0": {"__data__": {"text": "by a factor of two. For the sake of simplicity,\nwe ignore this constant-factor improvement.\nCost Analysis  On a ring or a bidirectional linear array, all-to-all personalized communication\ninvolves p - 1 communication steps. Since the size of the messages transferred in the i th step is\nm(p - i), the total time taken by this operation is\nEquation 4.7\nIn the all-to-all personalized communication procedure described above, each node sends m(p -\n1) words of data because it has an m-word packet for every other node. Assume that all\nmessages are sent either clockwise or counterclockwise. The average distance that an m-word\npacket travels is \n , which is equal to p/2. Since there are p nodes, each\nperforming the same type of communication, the total traffic (the total number of data words\ntransferred between directly-connected nodes) on the network is m(p - 1) x p/2 x p. The total\nnumber of inter-node links in the network to share this load is p. Hence, the communication\ntime for this operation is at least ( tw x m(p - 1)p2/2)/p, which is equal to twm(p - 1)p/2.\nDisregarding the message startup time ts, this is exactly the time taken by the linear array\nprocedure. Therefore, the all-to-all personalized communication algorithm described in this\nsection is optimal.\n4.5.2 Mesh\nIn all-to-all personalized communication on a \n  mesh, each node first groups its p\nmessages according to the columns of their destination nodes. Figure 4.19  shows a 3 x 3 mesh,\nin which every node initially has nine m-word messages, one meant for each node. Each node\nassembles its data into three groups of three messages each (in general, \n  groups of \nmessages each). The first group contains the messages destined for nodes labeled 0, 3, and 6;\nthe second group contains the messages for nodes labeled 1, 4, and 7; and the last group has\nmessages for nodes labeled 2, 5, and 8.\nFigure 4.19. The distribution of messages at the beginning of each\nphase of all-to-all personalized communication on a 3 x 3 mesh. At the\nend of the second phase, node i has messages ({0, i}, ..., {8, i}),\nwhere 0 \n i \n 8. The groups of nodes communicating together in each\nphase are enclosed in dotted boundaries.\nAfter the messages are grouped, all-to-all personalized communication is performed\nindependently in each row with clustered messages of size \n . One cluster contains the\ninformation for all \n  nodes of a particular column. Figure 4.19(b)  shows the distribution of\ndata among the nodes at the end of this phase of communication.\nBefore the second communication phase, the messages in each node are sorted again, this time\naccording to the rows of their destination nodes; then communication similar to the first phase\ntakes place in all the columns of the mesh. By the end of this phase, each node receives a\nmessage from every other node.\nCost Analysis  We can compute the time spent in the first phase by substituting \n for the\nnumber of nodes, and \n  for the message size in Equation 4.7 . The result of this substitution\nis \n . The time spent in the second phase is the same as that in the first\nphase. Therefore, the total time for all-to-all personalized communication of messages of size m\non a p-node two-dimensional square mesh is\nEquation 4.8\nThe expression for the communication time of all-to-all personalized communication in Equation\n4.8 does not take into account the time required for the local rearrangement of data (that is,\nsorting the messages by rows or columns). Assuming that initially the data is ready for the first\ncommunication phase, the second communication phase requires the rearrangement of mp\nwords of data. If tr is the time to perform a read and a write operation on a single word of data\nin a node's local memory, then the total time spent in data rearrangement by a node during the\nentire procedure", "doc_id": "17544ba6-9c95-47f6-92fd-4b6166ef83d0", "embedding": null, "doc_hash": "f22699cc7c50668f1a68b9eeb1ebba5478bfbc5c5d4f39602fb4e76f67856835", "extra_info": null, "node_info": {"start": 506712, "end": 510534}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a2ea11ef-2755-400a-82b2-a854888619e8", "3": "ae9a98b2-7504-400c-9157-bb1192b49b1d"}}, "__type__": "1"}, "ae9a98b2-7504-400c-9157-bb1192b49b1d": {"__data__": {"text": "\n . The time spent in the second phase is the same as that in the first\nphase. Therefore, the total time for all-to-all personalized communication of messages of size m\non a p-node two-dimensional square mesh is\nEquation 4.8\nThe expression for the communication time of all-to-all personalized communication in Equation\n4.8 does not take into account the time required for the local rearrangement of data (that is,\nsorting the messages by rows or columns). Assuming that initially the data is ready for the first\ncommunication phase, the second communication phase requires the rearrangement of mp\nwords of data. If tr is the time to perform a read and a write operation on a single word of data\nin a node's local memory, then the total time spent in data rearrangement by a node during the\nentire procedure is trmp (Problem 4.21). This time is much smaller than the time spent by each\nnode in communication.\nAn analysis along the lines of that for the linear array would show that the communication time\ngiven by Equation 4.8 for all-to-all personalized communication on a square mesh is optimal\nwithin a small constant factor (Problem 4.11).\n4.5.3 Hypercube\nOne way of performing all-to-all personalized communication on a p-node hypercube is to\nsimply extend the two-dimensional mesh algorithm to log p dimensions. Figure 4.20  shows the\ncommunication steps required to perform this operation on a three-dimensional hypercube. As\nshown in the figure, communication takes place in log p steps. Pairs of nodes exchange data in\na different dimension in each step. Recall that in a p-node hypercube, a set of p/2 links in the\nsame dimension connects two subcubes of p/2 nodes each ( Section 2.4.3 ). At any stage in all-\nto-all personalized communication, every node holds p packets of size m each. While\ncommunicating in a particular dimension, every node sends p/2 of these packets (consolidated\nas one message). The destinations of these packets are the nodes of the other subcube\nconnected by the links in current dimension.\nFigure 4.20. An all-to-all personalized communication algorithm on a\nthree-dimensional hypercube.\nIn the preceding procedure, a node must rearrange its messages locally before each of the log p\ncommunication steps. This is necessary to make sure that all p/2 messages destined for the\nsame node in a communication step occupy contiguous memory locations so that they can be\ntransmitted as a single consolidated message.\nCost Analysis  In the above hypercube algorithm for all-to-all personalized communication,\nmp/2 words of data are exchanged along the bidirectional channels in each of the log p\niterations. The resulting total communication time is\nEquation 4.9\nBefore each of the log p communication steps, a node rearranges mp words of data. Hence, a\ntotal time of trmp log p is spent by each node in local rearrangement of data during the entire\nprocedure. Here tr is the time needed to perform a read and a write operation on a single word\nof data in a node's local memory. For most practical computers, tr is much smaller than tw;\nhence, the time to perform an all-to-all personalized communication is dominated by the\ncommunication time.\nInterestingly, unlike the linear array and mesh algorithms described in this section, the\nhypercube algorithm is not optimal. Each of the p nodes sends and receives m(p - 1) words of\ndata and the average distance between any two nodes on a hypercube is (log p)/2. Therefore,\nthe total data traffic on the network is p x m(p - 1) x (log p)/2. Since there is a total of ( p log\np)/2 links in the hypercube network, the lower bound on the all-to-all personalized\ncommunication time is\nAn Optimal Algorithm\nAn all-to-all personalized communication effectively results in all pairs of nodes exchanging\nsome data. On a hypercube, the best way to perform this exchange is to have every pair of\nnodes communicate directly with each other. Thus, each node simply performs p - 1\ncommunication steps, exchanging m words of data with a", "doc_id": "ae9a98b2-7504-400c-9157-bb1192b49b1d", "embedding": null, "doc_hash": "f0c22d385a615c24ca95fac759cbd40e947849fedba57aea258ae399d8b00eed", "extra_info": null, "node_info": {"start": 510491, "end": 514489}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "17544ba6-9c95-47f6-92fd-4b6166ef83d0", "3": "0e06d3f1-d395-4699-8c19-36c378470454"}}, "__type__": "1"}, "0e06d3f1-d395-4699-8c19-36c378470454": {"__data__": {"text": "algorithm is not optimal. Each of the p nodes sends and receives m(p - 1) words of\ndata and the average distance between any two nodes on a hypercube is (log p)/2. Therefore,\nthe total data traffic on the network is p x m(p - 1) x (log p)/2. Since there is a total of ( p log\np)/2 links in the hypercube network, the lower bound on the all-to-all personalized\ncommunication time is\nAn Optimal Algorithm\nAn all-to-all personalized communication effectively results in all pairs of nodes exchanging\nsome data. On a hypercube, the best way to perform this exchange is to have every pair of\nnodes communicate directly with each other. Thus, each node simply performs p - 1\ncommunication steps, exchanging m words of data with a different node in every step. A node\nmust choose its communication partner in each step so that the hypercube links do not suffer\ncongestion. Figure 4.21 shows one such congestion-free schedule for pairwise exchange of data\nin a three-dimensional hypercube. As the figure shows, in the j th communication step, node i\nexchanges data with node ( i XOR j). For example, in part (a) of the figure (step 1), the labels of\ncommunicating partners differ in the least significant bit. In part (g) (step 7), the labels of\ncommunicating partners differ in all the bits, as the binary representation of seven is 111. In\nthis figure, all the paths in every communication step are congestion-free, and none of the\nbidirectional links carry more than one message in the same direction. This is true in general for\na hypercube of any dimension. If the messages are routed appropriately, a congestion-free\nschedule exists for the p - 1 communication steps of all-to-all personalized communication on a\np-node hypercube. Recall from Section 2.4.3 that a message traveling from node i to node j on\na hypercube must pass through at least l links, where l is the Hamming distance between i and j\n(that is, the number of nonzero bits in the binary representation of ( i XOR j)). A message\ntraveling from node i to node j traverses links in l dimensions (corresponding to the nonzero\nbits in the binary representation of ( i XOR j)). Although the message can follow one of the\nseveral paths of length l that exist between i and j (assuming l > 1), a distinct path is obtained\nby sorting the dimensions along which the message travels in ascending order. According to this\nstrategy, the first link is chosen in the dimension corresponding to the least significant nonzero\nbit of ( i XOR j), and so on. This routing scheme is known as E-cube routing .\nFigure 4.21. Seven steps in all-to-all personalized communication on\nan eight-node hypercube.\nAlgorithm 4.10 for all-to-all personalized communication on a d-dimensional hypercube is based\non this strategy.\nAlgorithm 4.10 A procedure to perform all-to-all personalized\ncommunication on a d-dimensional hypercube. The message Mi,j\ninitially resides on node i and is destined for node j.\n1.   procedure  ALL_TO_ALL_PERSONAL( d, my_id) \n2.   begin \n3.      for i := 1 to 2d - 1 do \n4.      begin \n5.         partner := my_id XOR i; \n6.         send Mmy_id, partner to partner; \n7.         receive Mpartner,my_id from partner; \n8.      endfor; \n9.   end ALL_TO_ALL_PERSONAL \nCost Analysis  E-cube routing ensures that by choosing communication pairs according to\nAlgorithm 4.10 , a communication time of ts + twm is guaranteed for a message transfer\nbetween node i and node j because there is no contention with any other message traveling in\nthe same direction along the link between nodes i and j. The total communication time for the\nentire operation is\nEquation", "doc_id": "0e06d3f1-d395-4699-8c19-36c378470454", "embedding": null, "doc_hash": "d8e0dd1e912bb3357995a7fbc7ba70042e8a708d152e6bc4f4c41922b0750a4c", "extra_info": null, "node_info": {"start": 514566, "end": 518180}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ae9a98b2-7504-400c-9157-bb1192b49b1d", "3": "dad43b8e-a4dc-4009-8798-fa9e0d4057ae"}}, "__type__": "1"}, "dad43b8e-a4dc-4009-8798-fa9e0d4057ae": {"__data__": {"text": "2d - 1 do \n4.      begin \n5.         partner := my_id XOR i; \n6.         send Mmy_id, partner to partner; \n7.         receive Mpartner,my_id from partner; \n8.      endfor; \n9.   end ALL_TO_ALL_PERSONAL \nCost Analysis  E-cube routing ensures that by choosing communication pairs according to\nAlgorithm 4.10 , a communication time of ts + twm is guaranteed for a message transfer\nbetween node i and node j because there is no contention with any other message traveling in\nthe same direction along the link between nodes i and j. The total communication time for the\nentire operation is\nEquation 4.10\nA comparison of Equations 4.9 and 4.10 shows the term associated with ts is higher for the\nsecond hypercube algorithm, while the term associated with tw is higher for the first algorithm.\nTherefore, for small messages, the startup time may dominate, and the first algorithm may still\nbe useful.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n4.6 Circular Shift\nCircular shift is a member of a broader class of global communication operations known as\npermutation . A permutation is a simultaneous, one-to-one data redistribution operation in\nwhich each node sends a packet of m words to a unique node. We define a circular  q-shift  as\nthe operation in which node i sends a data packet to node ( i + q) mod p in a p-node ensemble\n(0 < q < p). The shift operation finds application in some matrix computations and in string and\nimage pattern matching.\n4.6.1 Mesh\nThe implementation of a circular q-shift is fairly intuitive on a ring or a bidirectional linear\narray. It can be performed by min{ q , p - q} neighbor-to-neighbor communications in one\ndirection. Mesh algorithms for circular shift can be derived by using the ring algorithm.\nIf the nodes of the mesh have row-major labels, a circular q-shift can be performed on a p-node\nsquare wraparound mesh in two stages. This is illustrated in Figure 4.22  for a circular 5-shift on\na 4 x 4 mesh. First, the entire set of data is shifted simultaneously by ( q mod \n ) steps along\nthe rows. Then it is shifted by \n  steps along the columns. During the circular row shifts,\nsome of the data traverse the wraparound connection from the highest to the lowest labeled\nnodes of the rows. All such data packets must shift an additional step forward along the\ncolumns to compensate for the \n  distance that they lost while traversing the backward edge\nin their respective rows. For example, the 5-shift in Figure 4.22  requires one row shift, a\ncompensatory column shift, and finally one column shift.\nFigure 4.22. The communication steps in a circular 5-shift on a 4 x 4\nmesh.\nIn practice, we can choose the direction of the shifts in both the rows and the columns to\nminimize the number of steps in a circular shift. For instance, a 3-shift on a 4 x 4 mesh can be\nperformed by a single backward row shift. Using this strategy, the number of unit shifts in a\ndirection cannot exceed \n.\nCost Analysis  Taking into account the compensating column shift for some packets, the total\ntime for any circular q-shift on a p-node mesh using packets of size m has an upper bound of\n4.6.2 Hypercube\nIn developing a hypercube algorithm for the shift operation, we map a linear array with 2d\nnodes onto a d-dimensional hypercube. We do this by assigning node i of the linear array to\nnode j of the hypercube such that j is the d-bit binary reflected Gray code (RGC) of i. Figure\n4.23 illustrates this mapping for eight nodes. A property of this mapping is that any two nodes\nat a distance of 2i on the linear array are separated by exactly two links on the hypercube.", "doc_id": "dad43b8e-a4dc-4009-8798-fa9e0d4057ae", "embedding": null, "doc_hash": "f9aeef4124d7dbd8d13561ab7d2facc05ae166c7d6d8cbe2a11b8df34e4a8a2d", "extra_info": null, "node_info": {"start": 518316, "end": 521899}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0e06d3f1-d395-4699-8c19-36c378470454", "3": "7cb635d6-10c4-4d8d-9c5d-eeb4d7eabfd9"}}, "__type__": "1"}, "7cb635d6-10c4-4d8d-9c5d-eeb4d7eabfd9": {"__data__": {"text": "this strategy, the number of unit shifts in a\ndirection cannot exceed \n.\nCost Analysis  Taking into account the compensating column shift for some packets, the total\ntime for any circular q-shift on a p-node mesh using packets of size m has an upper bound of\n4.6.2 Hypercube\nIn developing a hypercube algorithm for the shift operation, we map a linear array with 2d\nnodes onto a d-dimensional hypercube. We do this by assigning node i of the linear array to\nnode j of the hypercube such that j is the d-bit binary reflected Gray code (RGC) of i. Figure\n4.23 illustrates this mapping for eight nodes. A property of this mapping is that any two nodes\nat a distance of 2i on the linear array are separated by exactly two links on the hypercube. An\nexception is i = 0 (that is, directly-connected nodes on the linear array) when only one\nhypercube link separates the two nodes.\nFigure 4.23. The mapping of an eight-node linear array onto a three-\ndimensional hypercube to perform a circular 5-shift as a combination\nof a 4-shift and a 1-shift.\nTo perform a q-shift, we expand q as a sum of distinct powers of 2. The number of terms in the\nsum is the same as the number of ones in the binary representation of q. For example, the\nnumber 5 can be expressed as 22 + 20. These two terms correspond to bit positions 0 and 2 in\nthe binary representation of 5, which is 101. If q is the sum of s distinct powers of 2, then the\ncircular q-shift on a hypercube is performed in s phases.\nIn each phase of communication, all data packets move closer to their respective destinations\nby short cutting the linear array (mapped onto the hypercube) in leaps of the powers of 2. For\nexample, as Figure 4.23 shows, a 5-shift is performed by a 4-shift followed by a 1-shift. The\nnumber of communication phases in a q-shift is exactly equal to the number of ones in the\nbinary representation of q. Each phase consists of two communication steps, except the 1-shift,\nwhich, if required (that is, if the least significant bit of q is 1), consists of a single step. For\nexample, in a 5-shift, the first phase of a 4-shift ( Figure 4.23(a) ) consists of two steps and the\nsecond phase of a 1-shift ( Figure 4.23(b) ) consists of one step. Thus, the total number of steps\nfor any q in a p-node hypercube is at most 2 log p - 1.\nAll communications in a given time step are congestion-free. This is ensured by the property of\nthe linear array mapping that all nodes whose mutual distance on the linear array is a power of\n2 are arranged in disjoint subarrays on the hypercube. Thus, all nodes can freely communicate\nin a circular fashion in their respective subarrays. This is shown in Figure 4.23(a), in which\nnodes labeled 0, 3, 4, and 7 form one subarray and nodes labeled 1, 2, 5, and 6 form another\nsubarray.\nThe upper bound on the total communication time for any shift of m-word packets on a p-node\nhypercube is\nEquation 4.11\nWe can reduce this upper bound to ( ts + twm) log p by performing both forward and backward\nshifts. For example, on eight nodes, a 6-shift can be performed by a single backward 2-shift\ninstead of a forward 4-shift followed by a forward 2-shift.\nWe now show that if the E-cube routing introduced in Section 4.5  is used, then the time for\ncircular shift on a hypercube can be improved by almost a factor of log p for large messages.\nThis is because with E-cube routing, each pair of nodes with a constant distance l (i \n l < p)\nhas a congestion-free path (Problem 4.22) in a p-node hypercube with bidirectional channels.\nFigure 4.24  illustrates the non-conflicting paths of all the messages in circular q -shift\noperations for 1 \n  q < 8 on an", "doc_id": "7cb635d6-10c4-4d8d-9c5d-eeb4d7eabfd9", "embedding": null, "doc_hash": "8b990a631b160b0e69021394b108a8b7ed1a5d4f4fb7372331dc26ca237523f3", "extra_info": null, "node_info": {"start": 521753, "end": 525399}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "dad43b8e-a4dc-4009-8798-fa9e0d4057ae", "3": "37742fcb-397c-4ee5-ad23-748ce7269d1b"}}, "__type__": "1"}, "37742fcb-397c-4ee5-ad23-748ce7269d1b": {"__data__": {"text": "performing both forward and backward\nshifts. For example, on eight nodes, a 6-shift can be performed by a single backward 2-shift\ninstead of a forward 4-shift followed by a forward 2-shift.\nWe now show that if the E-cube routing introduced in Section 4.5  is used, then the time for\ncircular shift on a hypercube can be improved by almost a factor of log p for large messages.\nThis is because with E-cube routing, each pair of nodes with a constant distance l (i \n l < p)\nhas a congestion-free path (Problem 4.22) in a p-node hypercube with bidirectional channels.\nFigure 4.24  illustrates the non-conflicting paths of all the messages in circular q -shift\noperations for 1 \n  q < 8 on an eight-node hypercube. In a circular q-shift on a p-node\nhypercube, the longest path contains log p - g(q) links, where g(q) is the highest integer j such\nthat q is divisible by 2j (Problem 4.23). Thus, the total communication time for messages of\nlength m is\nEquation 4.12\nFigure 4.24. Circular q-shifts on an 8-node hypercube for 1 \n  q < 8.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n4.7 Improving the Speed of Some Communication\nOperations\nSo far in this chapter, we have derived procedures for various communication operations and\ntheir communication times under the assumptions that the original messages could not be split\ninto smaller parts and that each node had a single port for sending and receiving data. In this\nsection, we briefly discuss the impact of relaxing these assumptions on some of the\ncommunication operations.\n4.7.1 Splitting and Routing Messages in Parts\nIn the procedures described in Sections 4.1\u20134.6, we assumed that an entire m-word packet of\ndata travels between the source and the destination nodes along the same path. If we split\nlarge messages into smaller parts and then route these parts through different paths, we can\nsometimes utilize the communication network better. We have already shown that, with a few\nexceptions like one-to-all broadcast, all-to-one reduction, all-reduce, etc., the communication\noperations discussed in this chapter are asymptotically optimal for large messages; that is, the\nterms associated with tw in the costs of these operations cannot be reduced asymptotically. In\nthis section, we present asymptotically optimal algorithms for three global communication\noperations.\nNote that the algorithms of this section rely on m being large enough to be split into p roughly\nequal parts. Therefore, the earlier algorithms are still useful for shorter messages. A comparison\nof the cost of the algorithms in this section with those presented earlier in this chapter for the\nsame operations would reveal that the term associated with ts increases and the term\nassociated with tw decreases when the messages are split. Therefore, depending on the actual\nvalues of ts, tw, and p, there is a cut-off value for the message size m and only the messages\nlonger than the cut-off would benefit from the algorithms in this section.\nOne-to-All Broadcast\nConsider broadcasting a single message M of size m from one source node to all the nodes in a\np-node ensemble. If m is large enough so that M can be split into p parts M0, M1, ..., M p-1 of\nsize m/p each, then a scatter operation ( Section 4.4) can place Mi on node i in time ts log p +\ntw(m/p)(p - 1). Note that the desired result of the one-to-all broadcast is to place M = M0\nM1\n\u00b7\u00b7\u00b7\nMp-1 on all nodes. This can be accomplished by an all-to-all broadcast of the messages of\nsize m/p residing on each node after the scatter operation. This all-to-all broadcast can be\ncompleted in time ts log p + tw(m/p)(p - 1) on a hypercube. Thus, on a hypercube,", "doc_id": "37742fcb-397c-4ee5-ad23-748ce7269d1b", "embedding": null, "doc_hash": "f8f811d2e75b32bd96b937714ab9ca35142109ff1e8980f8b159c718c8f56eb1", "extra_info": null, "node_info": {"start": 525444, "end": 529075}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7cb635d6-10c4-4d8d-9c5d-eeb4d7eabfd9", "3": "99ed26ed-fada-4b22-b013-4d9aabb48ffd"}}, "__type__": "1"}, "99ed26ed-fada-4b22-b013-4d9aabb48ffd": {"__data__": {"text": "from one source node to all the nodes in a\np-node ensemble. If m is large enough so that M can be split into p parts M0, M1, ..., M p-1 of\nsize m/p each, then a scatter operation ( Section 4.4) can place Mi on node i in time ts log p +\ntw(m/p)(p - 1). Note that the desired result of the one-to-all broadcast is to place M = M0\nM1\n\u00b7\u00b7\u00b7\nMp-1 on all nodes. This can be accomplished by an all-to-all broadcast of the messages of\nsize m/p residing on each node after the scatter operation. This all-to-all broadcast can be\ncompleted in time ts log p + tw(m/p)(p - 1) on a hypercube. Thus, on a hypercube, one-to-all\nbroadcast can be performed in time\nEquation 4.13\n\nCompared to Equation 4.1 , this algorithm has double the startup cost, but the cost due to the tw\nterm has been reduced by a factor of (log p)/2. Similarly, one-to-all broadcast can be improved\non linear array and mesh interconnection networks as well.\nAll-to-One Reduction\nAll-to-one reduction is a dual of one-to-all broadcast. Therefore, an algorithm for all-to-one\nreduction can be obtained by reversing the direction and the sequence of communication in\none-to-all broadcast. We showed above how an optimal one-to-all broadcast algorithm can be\nobtained by performing a scatter operation followed by an all-to-all broadcast. Therefore, using\nthe notion of duality, we should be able to perform an all-to-one reduction by performing all-to-\nall reduction (dual of all-to-all broadcast) followed by a gather operation (dual of scatter). We\nleave the details of such an algorithm as an exercise for the reader (Problem 4.17).\nAll-Reduce\nSince an all-reduce operation is semantically equivalent to an all-to-one reduction followed by a\none-to-all broadcast, the asymptotically optimal algorithms for these two operations presented\nabove can be used to construct a similar algorithm for the all-reduce operation. Breaking all-to-\none reduction and one-to-all broadcast into their component operations, it can be shown that an\nall-reduce operation can be accomplished by an all-to-all reduction followed by a gather\nfollowed by a scatter followed by an all-to-all broadcast. Since the intermediate gather and\nscatter would simply nullify each other's effect, all-reduce just requires an all-to-all reduction\nand an all-to-all broadcast. First, the m-word messages on each of the p nodes are logically\nsplit into p components of size roughly m/p words. Then, an all-to-all reduction combines all the\ni th components on pi. After this step, each node is left with a distinct m/p-word component of\nthe final result. An all-to-all broadcast can construct the concatenation of these components on\neach node.\nA p-node hypercube interconnection network allows all-to-one reduction and one-to-all\nbroadcast involving messages of size m/p in time ts log p + tw(m/p)(p - 1) each. Therefore, the\nall-reduce operation can be completed in time\nEquation 4.14\n4.7.2 All-Port Communication\nIn a parallel architecture, a single node may have multiple communication ports with links to\nother nodes in the ensemble. For example, each node in a two-dimensional wraparound mesh\nhas four ports, and each node in a d-dimensional hypercube has d ports. In this book, we\ngenerally assume what is known as the single-port communication  model. In single-port\ncommunication, a node can send data on only one of its ports at a time. Similarly, a node can\nreceive data on only one port at a time. However, a node can send and receive data\nsimultaneously, either on the same port or on separate ports. In contrast to the", "doc_id": "99ed26ed-fada-4b22-b013-4d9aabb48ffd", "embedding": null, "doc_hash": "fb20b39cebf4e07add1f3d4745f8b407b156ed93e2bafe76f2a82d78b4b0f645", "extra_info": null, "node_info": {"start": 529161, "end": 532711}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "37742fcb-397c-4ee5-ad23-748ce7269d1b", "3": "a3e8b29a-e193-40eb-9b19-8c7579e693c4"}}, "__type__": "1"}, "a3e8b29a-e193-40eb-9b19-8c7579e693c4": {"__data__": {"text": "+ tw(m/p)(p - 1) each. Therefore, the\nall-reduce operation can be completed in time\nEquation 4.14\n4.7.2 All-Port Communication\nIn a parallel architecture, a single node may have multiple communication ports with links to\nother nodes in the ensemble. For example, each node in a two-dimensional wraparound mesh\nhas four ports, and each node in a d-dimensional hypercube has d ports. In this book, we\ngenerally assume what is known as the single-port communication  model. In single-port\ncommunication, a node can send data on only one of its ports at a time. Similarly, a node can\nreceive data on only one port at a time. However, a node can send and receive data\nsimultaneously, either on the same port or on separate ports. In contrast to the single-port\nmodel, an all-port communication  model permits simultaneous communication on all the\nchannels connected to a node.\nOn a p-node hypercube with all-port communication, the coefficients of tw in the expressions for\nthe communication times of one-to-all and all-to-all broadcast and personalized communication\nare all smaller than their single-port counterparts by a factor of log p. Since the number of\nchannels per node for a linear array or a mesh is constant, all-port communication does not\nprovide any asymptotic improvement in communication time on these architectures.\nDespite the apparent speedup, the all-port communication model has certain limitations. For\ninstance, not only is it difficult to program, but it requires that the messages are large enough\nto be split efficiently among different channels. In several parallel algorithms, an increase in the\nsize of messages means a corresponding increase in the granularity of computation at the\nnodes. When the nodes are working with large data sets, the internode communication time is\ndominated by the computation time if the computational complexity of the algorithm is higher\nthan the communication complexity. For example, in the case of matrix multiplication, there are\nn3 computations for n2 words of data transferred among the nodes. If the communication time\nis a small fraction of the total parallel run time, then improving the communication by using\nsophisticated techniques is not very advantageous in terms of the overall run time of the\nparallel algorithm.\nAnother limitation of all-port communication is that it can be effective only if data can be\nfetched and stored in memory at a rate sufficient to sustain all the parallel communication. For\nexample, to utilize all-port communication effectively on a p-node hypercube, the memory\nbandwidth must be greater than the communication bandwidth of a single channel by a factor of\nat least log p; that is, the memory bandwidth must increase with the number of nodes to\nsupport simultaneous communication on all ports. Some modern parallel computers, like the\nIBM SP, have a very natural solution for this problem. Each node of the distributed-memory\nparallel computer is a NUMA shared-memory multiprocessor. Multiple ports are then served by\nseparate memory banks and full memory and communication bandwidth can be utilized if the\nbuffers for sending and receiving data are placed appropriately across different memory banks.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n4.8 Summary\nTable 4.1  summarizes the communication times for various collective communications\noperations discussed in this chapter. The time for one-to-all broadcast, all-to-one reduction,\nand the all-reduce operations is the minimum of two expressions. This is because, depending on\nthe message size m, either the algorithms described in Sections 4.1 and 4.3 or the ones\ndescribed in Section 4.7  are faster. Table 4.1  assumes that the algorithm most suitable for the\ngiven message size is chosen. The communication-time expressions in Table 4.1  have been\nderived in the earlier sections of this chapter in the context of a hypercube interconnection\nnetwork with cut-through routing. However, these expressions and the corresponding\nalgorithms are valid for any architecture with a Q(p) cross-section bandwidth ( Section 2.4.4). In\nfact, the terms associated with t w for the", "doc_id": "a3e8b29a-e193-40eb-9b19-8c7579e693c4", "embedding": null, "doc_hash": "dfdabec3c8db739df120a93163d6fc2ed687bcc275e5eb04096d8f5e450a4b3d", "extra_info": null, "node_info": {"start": 532572, "end": 536689}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "99ed26ed-fada-4b22-b013-4d9aabb48ffd", "3": "a111b34c-ecd5-4f0e-aff0-53da9d7524a2"}}, "__type__": "1"}, "a111b34c-ecd5-4f0e-aff0-53da9d7524a2": {"__data__": {"text": "chapter. The time for one-to-all broadcast, all-to-one reduction,\nand the all-reduce operations is the minimum of two expressions. This is because, depending on\nthe message size m, either the algorithms described in Sections 4.1 and 4.3 or the ones\ndescribed in Section 4.7  are faster. Table 4.1  assumes that the algorithm most suitable for the\ngiven message size is chosen. The communication-time expressions in Table 4.1  have been\nderived in the earlier sections of this chapter in the context of a hypercube interconnection\nnetwork with cut-through routing. However, these expressions and the corresponding\nalgorithms are valid for any architecture with a Q(p) cross-section bandwidth ( Section 2.4.4). In\nfact, the terms associated with t w for the expressions for all operations listed in Table 4.1 ,\nexcept all-to-all personalized communication and circular shift, would remain unchanged even\non ring and mesh networks (or any k-d mesh network) provided that the logical processes are\nmapped onto the physical nodes of the network appropriately. The last column of Table 4.1\ngives the asymptotic cross-section bandwidth required to perform an operation in the time\ngiven by the second column of the table, assuming an optimal mapping of processes to nodes.\nFor large messages, only all-to-all personalized communication and circular shift require the full\nQ(p) cross-section bandwidth. Therefore, as discussed in Section 2.5.1, when applying the\nexpressions for the time of these operations on a network with a smaller cross-section\nbandwidth, the tw term must reflect the effective bandwidth. For example, the bisection width of\na p-node square mesh is Q\n and that of a p-node ring is Q(1). Therefore, while performing\nall-to-all personalized communication on a square mesh, the effective per-word transfer time\nwould be Q\n  times the tw of individual links, and on a ring, it would be Q(p) times the tw of\nindividual links.\nTable 4.1. Summary of communication times of various operations\ndiscussed in Sections 4.1\u20134.7 on a hypercube interconnection\nnetwork. The message size for each operation is m and the number of\nnodes is p.\nOperation Hypercube Time B/W Requirement\nOne-to-all broadcast,\nAll-to-one reductionmin(( ts + twm) log p, 2(ts log p + twm)) Q(1)\nAll-to-all broadcast,\nAll-to-all reductionts log p + twm(p - 1) Q(1)\nAll-reduce min(( ts + twm) log p, 2(ts log p + twm)) Q(1)\nScatter, Gather ts log p + twm(p - 1) Q(1)\nAll-to-all personalized (ts + twm)(p - 1) Q(p)\nCircular shift ts + twm Q(p)\nTable 4.2. MPI names of the various operations discussed in this\nchapter.\nOperation MPI Name\nOne-to-all broadcast MPI_Bcast\nAll-to-one reduction MPI_Reduce\nAll-to-all broadcast MPI_Allgather\nAll-to-all reduction MPI_Reduce_scatter\nAll-reduce MPI_Allreduce\nGather MPI_Gather\nScatter MPI_Scatter\nAll-to-all personalized MPI_Alltoall\nThe collective communications operations discussed in this chapter occur frequently in many\nparallel algorithms. In order to facilitate speedy and portable design of efficient parallel\nprograms, most parallel computer vendors provide pre-packaged software for performing these\ncollective communications operations. The most commonly used standard API for these\noperations is known as the Message Passing Interface, or MPI. Table 4.2 gives the names of the\nMPI functions that correspond to the communications operations described in this chapter.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n4.9 Bibliographic Remarks\nIn this chapter, we studied a variety of data communication operations for the linear array,\nmesh, and hypercube interconnection", "doc_id": "a111b34c-ecd5-4f0e-aff0-53da9d7524a2", "embedding": null, "doc_hash": "38dfbbe617face1b60ac9518d50287be37efec199e33e641483b6a9a1e56c8ef", "extra_info": null, "node_info": {"start": 536672, "end": 540252}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a3e8b29a-e193-40eb-9b19-8c7579e693c4", "3": "ccc63e36-4354-462d-ae45-0ed197abf110"}}, "__type__": "1"}, "ccc63e36-4354-462d-ae45-0ed197abf110": {"__data__": {"text": "MPI_Scatter\nAll-to-all personalized MPI_Alltoall\nThe collective communications operations discussed in this chapter occur frequently in many\nparallel algorithms. In order to facilitate speedy and portable design of efficient parallel\nprograms, most parallel computer vendors provide pre-packaged software for performing these\ncollective communications operations. The most commonly used standard API for these\noperations is known as the Message Passing Interface, or MPI. Table 4.2 gives the names of the\nMPI functions that correspond to the communications operations described in this chapter.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n4.9 Bibliographic Remarks\nIn this chapter, we studied a variety of data communication operations for the linear array,\nmesh, and hypercube interconnection topologies. Saad and Schultz [ SS89b ] discuss\nimplementation issues for these operations on these and other architectures, such as shared-\nmemory and a switch or bus interconnect. Most parallel computer vendors provide standard\nAPIs for inter-process communications via message-passing. Two of the most common APIs are\nthe message passing interface (MPI) [ SOHL+96] and the parallel virtual machine (PVM)\n[GBD+94].\nThe hypercube algorithm for a certain communication operation is often the best algorithm for\nother less-connected architectures too, if they support cut-through routing. Due to the\nversatility of the hypercube architecture and the wide applicability of its algorithms, extensive\nwork has been done on implementing various communication operations on hypercubes\n[BOS+91, BR90 , BT97, FF86, JH89, Joh90 , MdV87 , RS90b , SS89a , SW87 ]. The properties of a\nhypercube network that are used in deriving the algorithms for various communication\noperations on it are described by Saad and Schultz [ SS88].\nThe all-to-all personalized communication problem in particular has been analyzed for the\nhypercube architecture by Boppana and Raghavendra [ BR90], Johnsson and Ho [ JH91], Seidel\n[Sei89 ], and Take [ Tak87 ]. E-cube routing that guarantees congestion-free communication in\nAlgorithm 4.10 for all-to-all personalized communication is described by Nugent [ Nug88 ].\nThe all-reduce and the prefix sums algorithms of Section 4.3  are described by Ranka and Sahni\n[RS90b ]. Our discussion of the circular shift operation is adapted from Bertsekas and Tsitsiklis\n[BT97]. A generalized form of prefix sums, often referred to as scan, has been used by some\nresearchers as a basic primitive in data-parallel programming. Blelloch [ Ble90 ] defines a scan\nvector model , and describes how a wide variety of parallel programs can be expressed in\nterms of the scan primitive and its variations.\nThe hypercube algorithm for one-to-all broadcast using spanning binomial trees is described by\nBertsekas and Tsitsiklis [ BT97] and Johnsson and Ho [ JH89]. In the spanning tree algorithm\ndescribed in Section 4.7.1 , we split the m-word message to be broadcast into log p parts of size\nm/log p for ease of presenting the algorithm. Johnsson and Ho [ JH89] show that the optimal\nsize of the parts is \n . In this case, the number of messages may be greater than\nlog p. These smaller messages are sent from the root of the spanning binomial tree to its log p\nsubtrees in a circular fashion. With this strategy, one-to-all broadcast on a p-node hypercube\ncan be performed in time \n .\nAlgorithms using the all-port communication model have been described for a variety of\ncommunication operations on the hypercube architecture by Bertsekas and Tsitsiklis [ BT97],\nJohnsson and Ho [ JH89], Ho and Johnsson [ HJ87], Saad and Schultz [ SS89a ], and Stout and\nWagar [ SW87 ]. Johnsson and Ho [ JH89] show that on a p-node hypercube with all-port\ncommunication, the", "doc_id": "ccc63e36-4354-462d-ae45-0ed197abf110", "embedding": null, "doc_hash": "1ec9fa9f10706c4a5a13a4e276093a1d99b4650de68310a0fe59384f7cd6dfe9", "extra_info": null, "node_info": {"start": 540216, "end": 543956}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a111b34c-ecd5-4f0e-aff0-53da9d7524a2", "3": "42ef1e82-640c-448d-a900-4289c3e461ed"}}, "__type__": "1"}, "42ef1e82-640c-448d-a900-4289c3e461ed": {"__data__": {"text": "show that the optimal\nsize of the parts is \n . In this case, the number of messages may be greater than\nlog p. These smaller messages are sent from the root of the spanning binomial tree to its log p\nsubtrees in a circular fashion. With this strategy, one-to-all broadcast on a p-node hypercube\ncan be performed in time \n .\nAlgorithms using the all-port communication model have been described for a variety of\ncommunication operations on the hypercube architecture by Bertsekas and Tsitsiklis [ BT97],\nJohnsson and Ho [ JH89], Ho and Johnsson [ HJ87], Saad and Schultz [ SS89a ], and Stout and\nWagar [ SW87 ]. Johnsson and Ho [ JH89] show that on a p-node hypercube with all-port\ncommunication, the coefficients of tw in the expressions for the communication times of one-to-\nall and all-to-all broadcast and personalized communication are all smaller than those of their\nsingle-port counterparts by a factor of log p. Gupta and Kumar [ GK91] show that all-port\ncommunication may not improve the scalability of an algorithm on a parallel architecture over\nsingle-port communication.\nThe elementary operations described in this chapter are not the only ones used in parallel\napplications. A variety of other useful operations for parallel computers have been described in\nliterature, including selection [ Akl89 ], pointer jumping [ HS86 , Jaj92 ], BPC permutations\n[Joh90 , RS90b ], fetch-and-op [ GGK+83], packing [ Lev87 , Sch80 ], bit reversal [ Loa92 ], and\nkeyed-scan or multi-prefix [ Ble90 , Ran89 ].\nSometimes data communication does not follow any predefined pattern, but is arbitrary,\ndepending on the application. In such cases, a simplistic approach of routing the messages\nalong the shortest data paths between their respective sources and destinations leads to\ncontention and imbalanced communication. Leighton, Maggs, and Rao [ LMR88], Valiant [ Val82 ],\nand Valiant and Brebner [ VB81 ] discuss efficient routing methods for arbitrary permutations of\nmessages.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n4.1 Modify Algorithms 4.1, 4.2, and 4.3 so that they work for any number of processes,\nnot just the powers of 2.\n4.2 Section 4.1  presents the recursive doubling algorithm for one-to-all broadcast, for all\nthree networks (ring, mesh, hypercube). Note that in the hypercube algorithm of Figure\n4.6, a message is sent along the highest dimension first, and then sent to lower\ndimensions (in Algorithm 4.1 , line 4, i goes down from d - 1 to 0). The same algorithm can\nbe used for mesh and ring and ensures that messages sent in different time steps do not\ninterfere with each other.\nLet's now change the algorithm so that the message is sent along the lowest dimension\nfirst (i.e., in Algorithm 3.1 , line 4, i goes up from 0 to d - 1). So in the first time step,\nprocessor 0 will communicate with processor 1; in the second time step, processors 0 and\n1 will communicate with 2 and 3, respectively; and so on.\nWhat is the run time of this revised algorithm on hypercube?1.\nWhat is the run time of this revised algorithm on ring?2.\nFor these derivations, if k messages have to traverse the same link at the same time, then\nassume that the effective per-word-transfer time for these messages is ktw.\n4.3 On a ring, all-to-all broadcast can be implemented in two different ways: (a) the\nstandard ring algorithm as shown in Figure 4.9 and (b) the hypercube algorithm as shown\nin Figure 4.11 .\nWhat is the run time for case (a)?1.\nWhat is the run time for case (b)?2.\nIf k messages have to traverse the same link at the same time, then assume that the\neffective per-word-transfer time for these messages is ktw. Also", "doc_id": "42ef1e82-640c-448d-a900-4289c3e461ed", "embedding": null, "doc_hash": "cd56486363d255bccf204bc2335c138c516f1acd30bf113881884380593c07fe", "extra_info": null, "node_info": {"start": 544054, "end": 547684}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ccc63e36-4354-462d-ae45-0ed197abf110", "3": "5f695748-cda2-4393-a7b4-922c9e6f6e23"}}, "__type__": "1"}, "5f695748-cda2-4393-a7b4-922c9e6f6e23": {"__data__": {"text": "the run time of this revised algorithm on hypercube?1.\nWhat is the run time of this revised algorithm on ring?2.\nFor these derivations, if k messages have to traverse the same link at the same time, then\nassume that the effective per-word-transfer time for these messages is ktw.\n4.3 On a ring, all-to-all broadcast can be implemented in two different ways: (a) the\nstandard ring algorithm as shown in Figure 4.9 and (b) the hypercube algorithm as shown\nin Figure 4.11 .\nWhat is the run time for case (a)?1.\nWhat is the run time for case (b)?2.\nIf k messages have to traverse the same link at the same time, then assume that the\neffective per-word-transfer time for these messages is ktw. Also assume that ts = 100 x tw.\nWhich of the two methods, (a) or (b), above is better if the message size m is very\nlarge?1.\nWhich method is better if m is very small (may be one word)? 2.\n4.4 Write a procedure along the lines of Algorithm 4.6  for performing all-to-all reduction\non a mesh.\n4.5 (All-to-all broadcast on a tree)  Given a balanced binary tree as shown in Figure\n4.7, describe a procedure to perform all-to-all broadcast that takes time ( ts + twmp/2) log\np for m-word messages on p nodes. Assume that only the leaves of the tree contain nodes,\nand that an exchange of two m-word messages between any two nodes connected by\nbidirectional channels takes time ts + twmk if the communication channel (or a part of it) is\nshared by k simultaneous messages.\n4.6 Consider the all-reduce operation in which each processor starts with an array of m\nwords, and needs to get the global sum of the respective words in the array at each\nprocessor. This operation can be implemented on a ring using one of the following three\nalternatives:\nAll-to-all broadcast of all the arrays followed by a local computation of the sum of the\nrespective elements of the array.i.\nSingle node accumulation of the elements of the array, followed by a one-to-all\nbroadcast of the result array.ii.\nAn algorithm that uses the pattern of the all-to-all broadcast, but simply adds\nnumbers rather than concatenating messages.iii.\nFor each of the above cases, compute the run time in terms of m, ts, and tw. 1.\nAssume that ts = 100, tw = 1, and m is very large. Which of the three alternatives\n(among (i), (ii) or (iii)) is better?2.\nAssume that ts = 100, tw = 1, and m is very small (say 1). Which of the three\nalternatives (among (i), (ii) or (iii)) is better?3.\n4.7 (One-to-all personalized communication on a linear array and a mesh)  Give the\nprocedures and their communication times for one-to-all personalized communication of\nm-word messages on p nodes for the linear array and the mesh architectures.\nHint:  For the mesh, the algorithm proceeds in two phases as usual and starts with the\nsource distributing pieces of \n words among the \n  nodes in its row such that each of\nthese nodes receives the data meant for all the \n  nodes in its column.\n4.8 (All-to-all reduction)  The dual of all-to-all broadcast is all-to-all reduction, in which\neach node is the destination of an all-to-one reduction. For example, consider the scenario\nwhere p nodes have a vector of p elements each, and the i th node (for all i such that 0 \ni < p) gets the sum of the i th elements of all the vectors. Describe an algorithm to\nperform all-to-all reduction on a hypercube with addition as the associative operator. If\neach message contains m words and tadd is the time to perform one addition, how much\ntime does your algorithm take (in terms of", "doc_id": "5f695748-cda2-4393-a7b4-922c9e6f6e23", "embedding": null, "doc_hash": "a6e2309b4f4e4da69a69357b7bb1a81adda296d2047d15a2bb11f00ffcadb5c4", "extra_info": null, "node_info": {"start": 547689, "end": 551191}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "42ef1e82-640c-448d-a900-4289c3e461ed", "3": "22a5e191-b22b-41ec-9af7-6292a9eb065c"}}, "__type__": "1"}, "22a5e191-b22b-41ec-9af7-6292a9eb065c": {"__data__": {"text": "in its row such that each of\nthese nodes receives the data meant for all the \n  nodes in its column.\n4.8 (All-to-all reduction)  The dual of all-to-all broadcast is all-to-all reduction, in which\neach node is the destination of an all-to-one reduction. For example, consider the scenario\nwhere p nodes have a vector of p elements each, and the i th node (for all i such that 0 \ni < p) gets the sum of the i th elements of all the vectors. Describe an algorithm to\nperform all-to-all reduction on a hypercube with addition as the associative operator. If\neach message contains m words and tadd is the time to perform one addition, how much\ntime does your algorithm take (in terms of m, p, tadd, ts and tw)?\nHint:  In all-to-all broadcast, each node starts with a single message and collects p such\nmessages by the end of the operation. In all-to-all reduction, each node starts with p\ndistinct messages (one meant for each node) but ends up with a single message.\n4.9 Parts (c), (e), and (f) of Figure 4.21  show that for any node in a three-dimensional\nhypercube, there are exactly three nodes whose shortest distance from the node is two\nlinks. Derive an exact expression for the number of nodes (in terms of p and l) whose\nshortest distance from any given node in a p-node hypercube is l.\n4.10 Give a hypercube algorithm to compute prefix sums of n numbers if p is the number\nof nodes and n/p is an integer greater than 1. Assuming that it takes time tadd to add two\nnumbers and time ts to send a message of unit length between two directly-connected\nnodes, give an exact expression for the total time taken by the algorithm.\n4.11 Show that if the message startup time ts is zero, then the expression \nfor the time taken by all-to-all personalized communication on a \n  mesh is\noptimal within a small (\n  4) constant factor.\n4.12 Modify the linear array and the mesh algorithms in Sections 4.1\u20134.5 to work without\nthe end-to-end wraparound connections. Compare the new communication times with\nthose of the unmodified procedures. What is the maximum factor by which the time for\nany of the operations increases on either the linear array or the mesh?\n4.13 (3-D mesh)  Give optimal (within a small constant) algorithms for one-to-all and all-\nto-all broadcasts and personalized communications on a p1/3 x p1/3 x p1/3 three-\ndimensional mesh of p nodes with store-and-forward routing. Derive expressions for the\ntotal communication times of these procedures.\n4.14 Assume that the cost of building a parallel computer with p nodes is proportional to\nthe total number of communication links within it. Let the cost effectiveness of an\narchitecture be inversely proportional to the product of the cost of a p-node ensemble of\nthis architecture and the communication time of a certain operation on it. Assuming ts to\nbe zero, which architecture is more cost effective for each of the operations discussed in\nthis chapter \u2013 a standard 3-D mesh or a sparse 3-D mesh?\n4.15 Repeat Problem 4.14 when ts is a nonzero constant but tw = 0. Under this model of\ncommunication, the message transfer time between two directly-connected nodes is fixed,\nregardless of the size of the message. Also, if two packets are combined and transmitted\nas one message, the communication latency is still ts.\n4.16 (k -to-all broadcast)  Let k-to-all broadcast be an operation in which k nodes\nsimultaneously perform a one-to-all broadcast of m-word messages. Give an algorithm for\nthis operation that has a total communication time of ts log p + twm(k log(p/k) + k - 1) on\na p-node hypercube. Assume that the m-word messages cannot be split, k is a power of", "doc_id": "22a5e191-b22b-41ec-9af7-6292a9eb065c", "embedding": null, "doc_hash": "7c8f20f7827c6eb2f432ccfab97e73db18832a18df297544bc738bf46e94efd4", "extra_info": null, "node_info": {"start": 551210, "end": 554838}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5f695748-cda2-4393-a7b4-922c9e6f6e23", "3": "bbd9cc4d-f591-4b67-b46f-1ab6dad20d14"}}, "__type__": "1"}, "bbd9cc4d-f591-4b67-b46f-1ab6dad20d14": {"__data__": {"text": "sparse 3-D mesh?\n4.15 Repeat Problem 4.14 when ts is a nonzero constant but tw = 0. Under this model of\ncommunication, the message transfer time between two directly-connected nodes is fixed,\nregardless of the size of the message. Also, if two packets are combined and transmitted\nas one message, the communication latency is still ts.\n4.16 (k -to-all broadcast)  Let k-to-all broadcast be an operation in which k nodes\nsimultaneously perform a one-to-all broadcast of m-word messages. Give an algorithm for\nthis operation that has a total communication time of ts log p + twm(k log(p/k) + k - 1) on\na p-node hypercube. Assume that the m-word messages cannot be split, k is a power of 2,\nand 1 \n k \n p.\n4.17 Give a detailed description of an algorithm for performing all-to-one reduction in\ntime 2( ts log p + twm(p - 1)/ p) on a p-node hypercube by splitting the original messages\nof size m into p nearly equal parts of size m/p each.\n4.18 If messages can be split and their parts can be routed independently, then derive an\nalgorithm for k-to-all broadcast such that its communication time is less than that of the\nalgorithm in Problem 4.16 for a p-node hypercube.\n4.19 Show that, if m \n p, then all-to-one reduction with message size m can be\nperformed on a p-node hypercube spending time 2( ts log p + twm) in communication.\nHint:  Express all-to-one reduction as a combination of all-to-all reduction and gather.\n4.20 (k -to-all personalized communication)  In k-to-all personalized communication, k\nnodes simultaneously perform a one-to-all personalized communication (1 \n  k \n p) in a\np-node ensemble with individual packets of size m. Show that, if k is a power of 2, then\nthis operation can be performed on a hypercube in time ts (log( p/k) + k - 1) + twm(p - 1).\n4.21 Assuming that it takes time tr to perform a read and a write operation on a single\nword of data in a node's local memory, show that all-to-all personalized communication on\na p-node mesh ( Section 4.5.2) spends a total of time trmp in internal data movement on\nthe nodes, where m is the size of an individual message.\nHint:  The internal data movement is equivalent to transposing a \n  array of\nmessages of size m.\n4.22 Show that in a p-node hypercube, all the p data paths in a circular q-shift are\ncongestion-free if E-cube routing ( Section 4.5 ) is used.\nHint:  (1) If q > p/2, then a q-shift is isomorphic to a ( p - q)-shift on a p-node hypercube.\n(2) Prove by induction on hypercube dimension. If all paths are congestion-free for a q-\nshift (1 \n  q < p) on a p-node hypercube, then all these paths are congestion-free on a 2\np-node hypercube also.\n4.23 Show that the length of the longest path of any message in a circular q-shift on a p-\nnode hypercube is log p - g(q), where g(q) is the highest integer j such that q is divisible by\n2j.\nHint:  (1) If q = p/2, then g(q) = log p - 1 on a p-node hypercube. (2) Prove by induction\non hypercube dimension. For a given q, g(q) increases by one each time the number of\nnodes is doubled.\n4.24 Derive an expression for the parallel run time of the hypercube algorithms for one-\nto-all broadcast, all-to-all broadcast, one-to-all personalized communication, and all-to-all\npersonalized communication adapted unaltered for a mesh with identical communication\nlinks (same channel width and channel rate). Compare", "doc_id": "bbd9cc4d-f591-4b67-b46f-1ab6dad20d14", "embedding": null, "doc_hash": "1ee31900a1b0378642ba32f8b60a0270bfbe6d8500f8fd140c601bdc8eb78184", "extra_info": null, "node_info": {"start": 554827, "end": 558166}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "22a5e191-b22b-41ec-9af7-6292a9eb065c", "3": "5ea466ed-e2b6-4f6d-9ff5-5ec9506cf7e5"}}, "__type__": "1"}, "5ea466ed-e2b6-4f6d-9ff5-5ec9506cf7e5": {"__data__": {"text": "any message in a circular q-shift on a p-\nnode hypercube is log p - g(q), where g(q) is the highest integer j such that q is divisible by\n2j.\nHint:  (1) If q = p/2, then g(q) = log p - 1 on a p-node hypercube. (2) Prove by induction\non hypercube dimension. For a given q, g(q) increases by one each time the number of\nnodes is doubled.\n4.24 Derive an expression for the parallel run time of the hypercube algorithms for one-\nto-all broadcast, all-to-all broadcast, one-to-all personalized communication, and all-to-all\npersonalized communication adapted unaltered for a mesh with identical communication\nlinks (same channel width and channel rate). Compare the performance of these\nadaptations with that of the best mesh algorithms.\n4.25 As discussed in Section 2.4.4, two common measures of the cost of a network are (1)\nthe total number of wires in a parallel computer (which is a product of number of\ncommunication links and channel width); and (2) the bisection bandwidth. Consider a\nhypercube in which the channel width of each link is one, that is tw = 1. The channel width\nof a mesh-connected computer with equal number of nodes and identical cost is higher,\nand is determined by the cost metric used. Let s and s' represent the factors by which the\nchannel width of the mesh is increased in accordance with the two cost metrics. Derive the\nvalues of s and s'. Using these, derive the communication time of the following operations\non a mesh:\nOne-to-all broadcast1.\nAll-to-all broadcast2.\nOne-to-all personalized communication3.\nAll-to-all personalized communication4.\nCompare these times with the time taken by the same operations on a hypercube with\nequal cost.\n4.26 Consider a completely-connected network of p nodes. For the four communication\noperations in Problem 4.25 derive an expression for the parallel run time of the hypercube\nalgorithms on the completely-connected network. Comment on whether the added\nconnectivity of the network yields improved performance for these operations. [ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 5. Analytical Modeling of Parallel\nPrograms\nA sequential algorithm is usually evaluated in terms of its execution time, expressed as a\nfunction of the size of its input. The execution time of a parallel algorithm depends not only on\ninput size but also on the number of processing elements used, and their relative computation\nand interprocess communication speeds. Hence, a parallel algorithm cannot be evaluated in\nisolation from a parallel architecture without some loss in accuracy. A parallel system  is the\ncombination of an algorithm and the parallel architecture on which it is implemented. In this\nchapter, we study various metrics for evaluating the performance of parallel systems.\nA number of measures of performance are intuitive. Perhaps the simplest of these is the wall-\nclock time taken to solve a given problem on a given parallel platform. However, as we shall\nsee, a single figure of merit of this nature cannot be extrapolated to other problem instances or\nlarger machine configurations. Other intuitive measures quantify the benefit of parallelism, i.e.,\nhow much faster the parallel program runs with respect to the serial program. However, this\ncharacterization suffers from other drawbacks, in addition to those mentioned above. For\ninstance, what is the impact of using a poorer serial algorithm that is more amenable to parallel\nprocessing? For these reasons, more complex measures for extrapolating performance to larger\nmachine configurations or problems are often necessary. With these objectives in mind, this\nchapter focuses on metrics for quantifying the performance of parallel programs.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.1 Sources of Overhead in Parallel Programs\nUsing twice as many hardware resources, one can reasonably expect a program to run twice as\nfast. However, in typical parallel programs, this is rarely the case, due to a variety of overheads\nassociated with", "doc_id": "5ea466ed-e2b6-4f6d-9ff5-5ec9506cf7e5", "embedding": null, "doc_hash": "7cd9f054e25d885a82e89fed79725d374ec78286af1e64af03170cb4efd2edec", "extra_info": null, "node_info": {"start": 558188, "end": 562136}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bbd9cc4d-f591-4b67-b46f-1ab6dad20d14", "3": "3c5ecec6-3dc9-42ba-b7ac-2d10ee6680a8"}}, "__type__": "1"}, "3c5ecec6-3dc9-42ba-b7ac-2d10ee6680a8": {"__data__": {"text": "much faster the parallel program runs with respect to the serial program. However, this\ncharacterization suffers from other drawbacks, in addition to those mentioned above. For\ninstance, what is the impact of using a poorer serial algorithm that is more amenable to parallel\nprocessing? For these reasons, more complex measures for extrapolating performance to larger\nmachine configurations or problems are often necessary. With these objectives in mind, this\nchapter focuses on metrics for quantifying the performance of parallel programs.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.1 Sources of Overhead in Parallel Programs\nUsing twice as many hardware resources, one can reasonably expect a program to run twice as\nfast. However, in typical parallel programs, this is rarely the case, due to a variety of overheads\nassociated with parallelism. An accurate quantification of these overheads is critical to the\nunderstanding of parallel program performance.\nA typical execution profile of a parallel program is illustrated in Figure 5.1. In addition to\nperforming essential computation (i.e., computation that would be performed by the serial\nprogram for solving the same problem instance), a parallel program may also spend time in\ninterprocess communication, idling, and excess computation (computation not performed by the\nserial formulation).\nFigure 5.1. The execution profile of a hypothetical parallel program\nexecuting on eight processing elements. Profile indicates times spent\nperforming computation (both essential and excess), communication,\nand idling.\nInterprocess Interaction  Any nontrivial parallel system requires its processing elements to\ninteract and communicate data (e.g., intermediate results). The time spent communicating data\nbetween processing elements is usually the most significant source of parallel processing\noverhead.\nIdling  Processing elements in a parallel system may become idle due to many reasons such as\nload imbalance, synchronization, and presence of serial components in a program. In many\nparallel applications (for example, when task generation is dynamic), it is impossible (or at\nleast difficult) to predict the size of the subtasks assigned to various processing elements.\nHence, the problem cannot be subdivided statically among the processing elements while\nmaintaining uniform workload. If different processing elements have different workloads, some\nprocessing elements may be idle during part of the time that others are working on the\nproblem. In some parallel programs, processing elements must synchronize at certain points\nduring parallel program execution. If all processing elements are not ready for synchronization\nat the same time, then the ones that are ready sooner will be idle until all the rest are ready.\nParts of an algorithm may be unparallelizable, allowing only a single processing element to\nwork on it. While one processing element works on the serial part, all the other processing\nelements must wait.\nExcess Computation  The fastest known sequential algorithm for a problem may be difficult or\nimpossible to parallelize, forcing us to use a parallel algorithm based on a poorer but easily\nparallelizable (that is, one with a higher degree of concurrency) sequential algorithm. The\ndifference in computation performed by the parallel program and the best serial program is the\nexcess computation overhead incurred by the parallel program.\nA parallel algorithm based on the best serial algorithm may still perform more aggregate\ncomputation than the serial algorithm. An example of such a computation is the Fast Fourier\nTransform algorithm. In its serial version, the results of certain computations can be reused.\nHowever, in the parallel version, these results cannot be reused because they are generated by\ndifferent processing elements. Therefore, some computations are performed multiple times on\ndifferent processing elements. Chapter 13 discusses these algorithms in detail.\nSince different parallel algorithms for solving the same problem incur varying overheads, it is\nimportant to quantify these overheads with a view to establishing a figure of merit for each\nalgorithm.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.2 Performance Metrics for Parallel Systems\nIt is important to study the performance of parallel", "doc_id": "3c5ecec6-3dc9-42ba-b7ac-2d10ee6680a8", "embedding": null, "doc_hash": "a6e719d38bba329e87ca37f07ce6fd066037c75048d676c649e43012974859fb", "extra_info": null, "node_info": {"start": 561986, "end": 566267}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5ea466ed-e2b6-4f6d-9ff5-5ec9506cf7e5", "3": "7ca43289-2b64-46d8-a61d-2728fdafe7d9"}}, "__type__": "1"}, "7ca43289-2b64-46d8-a61d-2728fdafe7d9": {"__data__": {"text": "may still perform more aggregate\ncomputation than the serial algorithm. An example of such a computation is the Fast Fourier\nTransform algorithm. In its serial version, the results of certain computations can be reused.\nHowever, in the parallel version, these results cannot be reused because they are generated by\ndifferent processing elements. Therefore, some computations are performed multiple times on\ndifferent processing elements. Chapter 13 discusses these algorithms in detail.\nSince different parallel algorithms for solving the same problem incur varying overheads, it is\nimportant to quantify these overheads with a view to establishing a figure of merit for each\nalgorithm.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.2 Performance Metrics for Parallel Systems\nIt is important to study the performance of parallel programs with a view to determining the\nbest algorithm, evaluating hardware platforms, and examining the benefits from parallelism. A\nnumber of metrics have been used based on the desired outcome of performance analysis.\n5.2.1 Execution Time\nThe serial runtime of a program is the time elapsed between the beginning and the end of its\nexecution on a sequential computer. The parallel runtime  is the time that elapses from the\nmoment a parallel computation starts to the moment the last processing element finishes\nexecution. We denote the serial runtime by TS and the parallel runtime by TP.\n5.2.2 Total Parallel Overhead\nThe overheads incurred by a parallel program are encapsulated into a single expression referred\nto as the overhead function . We define overhead function or total overhead  of a parallel\nsystem as the total time collectively spent by all the processing elements over and above that\nrequired by the fastest known sequential algorithm for solving the same problem on a single\nprocessing element. We denote the overhead function of a parallel system by the symbol To.\nThe total time spent in solving a problem summed over all processing elements is pTP . TS units\nof this time are spent performing useful work, and the remainder is overhead. Therefore, the\noverhead function ( To) is given by\nEquation 5.1\n5.2.3 Speedup\nWhen evaluating a parallel system, we are often interested in knowing how much performance\ngain is achieved by parallelizing a given application over a sequential implementation. Speedup\nis a measure that captures the relative benefit of solving a problem in parallel. It is defined as\nthe ratio of the time taken to solve a problem on a single processing element to the time\nrequired to solve the same problem on a parallel computer with p identical processing elements.\nWe denote speedup by the symbol S.\nExample 5.1 Adding n numbers using n processing elements\nConsider the problem of adding n numbers by using n processing elements. Initially,\neach processing element is assigned one of the numbers to be added and, at the end\nof the computation, one of the processing elements stores the sum of all the numbers.\nAssuming that n is a power of two, we can perform this operation in log n steps by\npropagating partial sums up a logical binary tree of processing elements. Figure 5.2\nillustrates the procedure for n = 16. The processing elements are labeled from 0 to 15.\nSimilarly, the 16 numbers to be added are labeled from 0 to 15. The sum of the\nnumbers with consecutive labels from i to j is denoted by \n .\nFigure 5.2. Computing the globalsum of 16 partial sums using\n16 processing elements. \n  denotes the sum of numbers with\nconsecutive labels from i to j.\nEach step shown in Figure 5.2  consists of one addition and the communication of a\nsingle word. The addition can be performed in some constant time, say t c, and the\ncommunication of a single word can be performed in time ts + tw. Therefore, the\naddition and communication operations take a constant amount of time. Thus,\nEquation 5.2\n\nSince the problem can be solved in Q(n) time on a single processing element, its\nspeedup is\nEquation 5.3\nFor a given problem, more than one sequential algorithm may be available, but all of these may\nnot", "doc_id": "7ca43289-2b64-46d8-a61d-2728fdafe7d9", "embedding": null, "doc_hash": "736e90521029e1f040412468a3e0ef91bd5efa5d4b3a8f8f3a1cf97e10b0ca69", "extra_info": null, "node_info": {"start": 566270, "end": 570333}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3c5ecec6-3dc9-42ba-b7ac-2d10ee6680a8", "3": "af776e96-2951-426f-9326-ff9d3f913c0f"}}, "__type__": "1"}, "af776e96-2951-426f-9326-ff9d3f913c0f": {"__data__": {"text": "labels from i to j is denoted by \n .\nFigure 5.2. Computing the globalsum of 16 partial sums using\n16 processing elements. \n  denotes the sum of numbers with\nconsecutive labels from i to j.\nEach step shown in Figure 5.2  consists of one addition and the communication of a\nsingle word. The addition can be performed in some constant time, say t c, and the\ncommunication of a single word can be performed in time ts + tw. Therefore, the\naddition and communication operations take a constant amount of time. Thus,\nEquation 5.2\n\nSince the problem can be solved in Q(n) time on a single processing element, its\nspeedup is\nEquation 5.3\nFor a given problem, more than one sequential algorithm may be available, but all of these may\nnot be equally suitable for parallelization. When a serial computer is used, it is natural to use\nthe sequential algorithm that solves the problem in the least amount of time. Given a parallel\nalgorithm, it is fair to judge its performance with respect to the fastest sequential algorithm for\nsolving the same problem on a single processing element. Sometimes, the asymptotically\nfastest sequential algorithm to solve a problem is not known, or its runtime has a large constant\nthat makes it impractical to implement. In such cases, we take the fastest known algorithm that\nwould be a practical choice for a serial computer to be the best sequential algorithm. We\ncompare the performance of a parallel algorithm to solve a problem with that of the best\nsequential algorithm to solve the same problem. We formally define the speedup  S as the ratio\nof the serial runtime of the best sequential algorithm for solving a problem to the time taken by\nthe parallel algorithm to solve the same problem on p processing elements. The p processing\nelements used by the parallel algorithm are assumed to be identical to the one used by the\nsequential algorithm.\nExample 5.2 Computing speedups of parallel programs\nConsider the example of parallelizing bubble sort ( Section 9.3.1). Assume that a serial\nversion of bubble sort of 105 records takes 150 seconds and a serial quicksort can sort\nthe same list in 30 seconds. If a parallel version of bubble sort, also called odd-even\nsort, takes 40 seconds on four processing elements, it would appear that the parallel\nodd-even sort algorithm results in a speedup of 150/40 or 3.75. However, this\nconclusion is misleading, as in reality the parallel algorithm results in a speedup of\n30/40 or 0.75 with respect to the best serial algorithm. \nTheoretically, speedup can never exceed the number of processing elements, p. If the best\nsequential algorithm takes TS units of time to solve a given problem on a single processing\nelement, then a speedup of p can be obtained on p processing elements if none of the\nprocessing elements spends more than time TS /p. A speedup greater than p is possible only if\neach processing element spends less than time TS /p solving the problem. In this case, a single\nprocessing element could emulate the p processing elements and solve the problem in fewer\nthan TS units of time. This is a contradiction because speedup, by definition, is computed with\nrespect to the best sequential algorithm. If TS is the serial runtime of the algorithm, then the\nproblem cannot be solved in less than time TS on a single processing element.\nIn practice, a speedup greater than p is sometimes observed (a phenomenon known as\nsuperlinear speedup). This usually happens when the work performed by a serial algorithm is\ngreater than its parallel formulation or due to hardware features that put the serial\nimplementation at a disadvantage. For example, the data for a problem might be too large to fit\ninto the cache of a single processing element, thereby degrading its performance due to the use\nof slower memory elements. But when partitioned among several processing elements, the\nindividual data-partitions would be small enough to fit into their respective processing elements'\ncaches. In the remainder of this book, we disregard superlinear speedup due to hierarchical\nmemory.\nExample 5.3 Superlinearity", "doc_id": "af776e96-2951-426f-9326-ff9d3f913c0f", "embedding": null, "doc_hash": "7fdca10a20338ed49b068d6d8b260c7be662be05013a390e3592ce59253b30bc", "extra_info": null, "node_info": {"start": 570428, "end": 574513}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7ca43289-2b64-46d8-a61d-2728fdafe7d9", "3": "023921a1-6487-43e7-92df-c16539ea6b52"}}, "__type__": "1"}, "023921a1-6487-43e7-92df-c16539ea6b52": {"__data__": {"text": "be solved in less than time TS on a single processing element.\nIn practice, a speedup greater than p is sometimes observed (a phenomenon known as\nsuperlinear speedup). This usually happens when the work performed by a serial algorithm is\ngreater than its parallel formulation or due to hardware features that put the serial\nimplementation at a disadvantage. For example, the data for a problem might be too large to fit\ninto the cache of a single processing element, thereby degrading its performance due to the use\nof slower memory elements. But when partitioned among several processing elements, the\nindividual data-partitions would be small enough to fit into their respective processing elements'\ncaches. In the remainder of this book, we disregard superlinear speedup due to hierarchical\nmemory.\nExample 5.3 Superlinearity effects from caches\nConsider the execution of a parallel program on a two-processor parallel system. The\nprogram attempts to solve a problem instance of size W. With this size and available\ncache of 64 KB on one processor, the program has a cache hit rate of 80%. Assuming\nthe latency to cache of 2 ns and latency to DRAM of 100 ns, the effective memory\naccess time is 2 x 0.8 + 100 x 0.2, or 21.6 ns. If the computation is memory bound\nand performs one FLOP/memory access, this corresponds to a processing rate of 46.3\nMFLOPS. Now consider a situation when each of the two processors is effectively\nexecuting half of the problem instance (i.e., size W/2). At this problem size, the cache\nhit ratio is expected to be higher, since the effective problem size is smaller. Let us\nassume that the cache hit ratio is 90%, 8% of the remaining data comes from local\nDRAM, and the other 2% comes from the remote DRAM (communication overhead).\nAssuming that remote data access takes 400 ns, this corresponds to an overall access\ntime of 2 x 0.9 + 100 x 0.08 + 400 x 0.02, or 17.8 ns. The corresponding execution\nrate at each processor is therefore 56.18, for a total execution rate of 112.36 MFLOPS.\nThe speedup in this case is given by the increase in speed over serial formulation, i.e.,\n112.36/46.3 or 2.43! Here, because of increased cache hit ratio resulting from lower\nproblem size per processor, we notice superlinear speedup. \nExample 5.4 Superlinearity effects due to exploratory\ndecomposition\nConsider an algorithm for exploring leaf nodes of an unstructured tree. Each leaf has a\nlabel associated with it and the objective is to find a node with a specified label, in this\ncase 'S'. Such computations are often used to solve combinatorial problems, where\nthe label 'S' could imply the solution to the problem ( Section 11.6). In Figure 5.3 , we\nillustrate such a tree. The solution node is the rightmost leaf in the tree. A serial\nformulation of this problem based on depth-first tree traversal explores the entire\ntree, i.e., all 14 nodes. If it takes time tc to visit a node, the time for this traversal is\n14tc. Now consider a parallel formulation in which the left subtree is explored by\nprocessing element 0 and the right subtree by processing element 1. If both\nprocessing elements explore the tree at the same speed, the parallel formulation\nexplores only the shaded nodes before the solution is found. Notice that the total work\ndone by the parallel algorithm is only nine node expansions, i.e., 9 tc. The\ncorresponding parallel time, assuming the root node expansion is serial, is 5 tc (one\nroot node expansion, followed by four node expansions by each processing element).\nThe speedup of this two-processor execution is therefore 14 tc/5tc , or 2.8!\nFigure 5.3. Searching an unstructured tree for a node with a\ngiven label, 'S', on two processing elements using depth-first\ntraversal. The two-processor version with processor 0\nsearching the left subtree and processor 1 searching the right\nsubtree expands only the shaded nodes before the solution is\nfound. The", "doc_id": "023921a1-6487-43e7-92df-c16539ea6b52", "embedding": null, "doc_hash": "b21f119c187515be37e0ccf5eaf1cdb8698e15ea91a9bc65b7861fa7b7720d74", "extra_info": null, "node_info": {"start": 574413, "end": 578316}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "af776e96-2951-426f-9326-ff9d3f913c0f", "3": "55a5666a-fb0b-423f-a9ac-8d5d01b57d8d"}}, "__type__": "1"}, "55a5666a-fb0b-423f-a9ac-8d5d01b57d8d": {"__data__": {"text": "same speed, the parallel formulation\nexplores only the shaded nodes before the solution is found. Notice that the total work\ndone by the parallel algorithm is only nine node expansions, i.e., 9 tc. The\ncorresponding parallel time, assuming the root node expansion is serial, is 5 tc (one\nroot node expansion, followed by four node expansions by each processing element).\nThe speedup of this two-processor execution is therefore 14 tc/5tc , or 2.8!\nFigure 5.3. Searching an unstructured tree for a node with a\ngiven label, 'S', on two processing elements using depth-first\ntraversal. The two-processor version with processor 0\nsearching the left subtree and processor 1 searching the right\nsubtree expands only the shaded nodes before the solution is\nfound. The corresponding serial formulation expands the entire\ntree. It is clear that the serial algorithm does more work than\nthe parallel algorithm.\nThe cause for this superlinearity is that the work performed by parallel and serial\nalgorithms is different. Indeed, if the two-processor algorithm was implemented as\ntwo processes on the same processing element, the algorithmic superlinearity would\ndisappear for this problem instance. Note that when exploratory decomposition is\nused, the relative amount of work performed by serial and parallel algorithms is\ndependent upon the location of the solution, and it is often not possible to find a serial\nalgorithm that is optimal for all instances. Such effects are further analyzed in greater\ndetail in Chapter 11. \n5.2.4 Efficiency\nOnly an ideal parallel system containing p processing elements can deliver a speedup equal to\np. In practice, ideal behavior is not achieved because while executing a parallel algorithm, the\nprocessing elements cannot devote 100% of their time to the computations of the algorithm. As\nwe saw in Example 5.1 , part of the time required by the processing elements to compute the\nsum of n numbers is spent idling (and communicating in real systems). Efficiency  is a measure\nof the fraction of time for which a processing element is usefully employed; it is defined as the\nratio of speedup to the number of processing elements. In an ideal parallel system, speedup is\nequal to p and efficiency is equal to one. In practice, speedup is less than p and efficiency is\nbetween zero and one, depending on the effectiveness with which the processing elements are\nutilized. We denote efficiency by the symbol E. Mathematically, it is given by\nEquation 5.4\nExample 5.5 Efficiency of adding n numbers on n processing\nelements\nFrom Equation 5.3  and the preceding definition, the efficiency of the algorithm for\nadding n numbers on n processing elements is\nWe also illustrate the process of deriving the parallel runtime, speedup, and efficiency while\npreserving various constants associated with the parallel platform.\nExample 5.6 Edge detection on images\nGiven an n x n pixel image, the problem of detecting edges corresponds to applying\na3x 3 template to each pixel. The process of applying the template corresponds to\nmultiplying pixel values with corresponding template values and summing across the\ntemplate (a convolution operation). This process is illustrated in Figure 5.4(a) along\nwith typical templates ( Figure 5.4(b) ). Since we have nine multiply-add operations for\neach pixel, if each multiply-add takes time tc, the entire operation takes time 9 tcn2 on\na serial computer.\nFigure 5.4. Example of edge detection: (a) an 8 x 8 image; (b)\ntypical templates for detecting edges; and (c) partitioning of\nthe image across four processors with shaded regions\nindicating image data that must be communicated from\nneighboring processors to processor 1.\nA simple parallel algorithm for this problem partitions the image equally across the\nprocessing elements and each processing element applies the template to its own\nsubimage. Note that for applying the template to the boundary pixels, a processing\nelement must get data that is assigned to the adjoining processing element.\nSpecifically, if a processing element is assigned a vertically", "doc_id": "55a5666a-fb0b-423f-a9ac-8d5d01b57d8d", "embedding": null, "doc_hash": "195f3b78fe2706345f4f202fa3946d08767a609dcf01271f9c05e2bc91068b7a", "extra_info": null, "node_info": {"start": 578376, "end": 582444}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "023921a1-6487-43e7-92df-c16539ea6b52", "3": "116d8cac-52de-4cc5-97e9-20787801d8ed"}}, "__type__": "1"}, "116d8cac-52de-4cc5-97e9-20787801d8ed": {"__data__": {"text": "operations for\neach pixel, if each multiply-add takes time tc, the entire operation takes time 9 tcn2 on\na serial computer.\nFigure 5.4. Example of edge detection: (a) an 8 x 8 image; (b)\ntypical templates for detecting edges; and (c) partitioning of\nthe image across four processors with shaded regions\nindicating image data that must be communicated from\nneighboring processors to processor 1.\nA simple parallel algorithm for this problem partitions the image equally across the\nprocessing elements and each processing element applies the template to its own\nsubimage. Note that for applying the template to the boundary pixels, a processing\nelement must get data that is assigned to the adjoining processing element.\nSpecifically, if a processing element is assigned a vertically sliced subimage of\ndimension n x (n/p), it must access a single layer of n pixels from the processing\nelement to the left and a single layer of n pixels from the processing element to the\nright (note that one of these accesses is redundant for the two processing elements\nassigned the subimages at the extremities). This is illustrated in Figure 5.4(c).\nOn a message passing machine, the algorithm executes in two steps: (i) exchange a\nlayer of n pixels with each of the two adjoining processing elements; and (ii) apply\ntemplate on local subimage. The first step involves two n-word messages (assuming\neach pixel takes a word to communicate RGB data). This takes time 2( ts + twn). The\nsecond step takes time 9 tcn2/p. The total time for the algorithm is therefore given by:\nThe corresponding values of speedup and efficiency are given by:\nand\n5.2.5 Cost\nWe define the cost of solving a problem on a parallel system as the product of parallel runtime\nand the number of processing elements used. Cost reflects the sum of the time that each\nprocessing element spends solving the problem. Efficiency can also be expressed as the ratio of\nthe execution time of the fastest known sequential algorithm for solving a problem to the cost of\nsolving the same problem on p processing elements.\nThe cost of solving a problem on a single processing element is the execution time of the fastest\nknown sequential algorithm. A parallel system is said to be cost-optimal  if the cost of solving a\nproblem on a parallel computer has the same asymptotic growth (in Q terms) as a function of\nthe input size as the fastest-known sequential algorithm on a single processing element. Since\nefficiency is the ratio of sequential cost to parallel cost, a cost-optimal parallel system has an\nefficiency of Q(1).\nCost is sometimes referred to as work  or processor-time product , and a cost-optimal system\nis also known as a pTP -optimal system.\nExample 5.7 Cost of adding n numbers on n processing\nelements\nThe algorithm given in Example 5.1  for adding n numbers on n processing elements\nhas a processor-time product of Q(n log n). Since the serial runtime of this operation\nis Q(n), the algorithm is not cost optimal. \nCost optimality is a very important practical concept although it is defined in terms of\nasymptotics. We illustrate this using the following example.\nExample 5.8 Performance of non-cost optimal algorithms\nConsider a sorting algorithm that uses n processing elements to sort the list in time\n(log n)2. Since the serial runtime of a (comparison-based) sort is n log n, the speedup\nand efficiency of this algorithm are given by n/log n and 1/log n, respectively. The pTP\nproduct of this algorithm is n(log n)2. Therefore, this algorithm is not cost optimal but\nonly by a factor of log n. Let us consider a realistic scenario in which the number of\nprocessing elements p is much less than n. An assignment of these n tasks to p < n\nprocessing elements gives us a parallel time less than n(log n)2/p. This follows from\nthe fact that if n processing elements take time (log n)2, then one processing element\nwould take time n(log n)2; and p processing elements would take", "doc_id": "116d8cac-52de-4cc5-97e9-20787801d8ed", "embedding": null, "doc_hash": "fa78c99519ae466b5cc9d2c0d1eaf3ad7eaaaad53f3ca5264f60094933c1e92c", "extra_info": null, "node_info": {"start": 582424, "end": 586367}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "55a5666a-fb0b-423f-a9ac-8d5d01b57d8d", "3": "1de960fd-008d-4088-b15a-f7deec3530f3"}}, "__type__": "1"}, "1de960fd-008d-4088-b15a-f7deec3530f3": {"__data__": {"text": "n processing elements to sort the list in time\n(log n)2. Since the serial runtime of a (comparison-based) sort is n log n, the speedup\nand efficiency of this algorithm are given by n/log n and 1/log n, respectively. The pTP\nproduct of this algorithm is n(log n)2. Therefore, this algorithm is not cost optimal but\nonly by a factor of log n. Let us consider a realistic scenario in which the number of\nprocessing elements p is much less than n. An assignment of these n tasks to p < n\nprocessing elements gives us a parallel time less than n(log n)2/p. This follows from\nthe fact that if n processing elements take time (log n)2, then one processing element\nwould take time n(log n)2; and p processing elements would take time n(log n)2/p.\nThe corresponding speedup of this formulation is p/log n. Consider the problem of\nsorting 1024 numbers ( n = 1024, log n = 10) on 32 processing elements. The\nspeedup expected is only p/log n or 3.2. This number gets worse as n increases. For n\n= 106, log n = 20 and the speedup is only 1.6. Clearly, there is a significant cost\nassociated with not being cost-optimal even by a very small factor (note that a factor\nof log p is smaller than even \n ). This emphasizes the practical importance of cost-\noptimality. \n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.3 The Effect of Granularity on Performance\nExample 5.7  illustrated an instance of an algorithm that is not cost-optimal. The algorithm\ndiscussed in this example uses as many processing elements as the number of inputs, which is\nexcessive in terms of the number of processing elements. In practice, we assign larger pieces of\ninput data to processing elements. This corresponds to increasing the granularity of\ncomputation on the processing elements. Using fewer than the maximum possible number of\nprocessing elements to execute a parallel algorithm is called scaling down  a parallel system in\nterms of the number of processing elements. A naive way to scale down a parallel system is to\ndesign a parallel algorithm for one input element per processing element, and then use fewer\nprocessing elements to simulate a large number of processing elements. If there are n inputs\nand only p processing elements ( p < n), we can use the parallel algorithm designed for n\nprocessing elements by assuming n virtual processing elements and having each of the p\nphysical processing elements simulate n/p virtual processing elements.\nAs the number of processing elements decreases by a factor of n/p, the computation at each\nprocessing element increases by a factor of n/p because each processing element now performs\nthe work of n/p processing elements. If virtual processing elements are mapped appropriately\nonto physical processing elements, the overall communication time does not grow by more than\na factor of n/p. The total parallel runtime increases, at most, by a factor of n/p, and the\nprocessor-time product does not increase. Therefore, if a parallel system with n processing\nelements is cost-optimal, using p processing elements (where p < n)to simulate n processing\nelements preserves cost-optimality.\nA drawback of this naive method of increasing computational granularity is that if a parallel\nsystem is not cost-optimal to begin with, it may still not be cost-optimal after the granularity of\ncomputation increases. This is illustrated by the following example for the problem of adding n\nnumbers.\nExample 5.9 Adding n numbers on p processing elements\nConsider the problem of adding n numbers on p processing elements such that p < n\nand both n and p are powers of 2. We use the same algorithm as in Example 5.1 and\nsimulate n processing elements on p processing elements. The steps leading to the\nsolution are shown in Figure 5.5  for n = 16 and p = 4. Virtual processing element i is\nsimulated by the physical processing element labeled i mod p; the numbers to be\nadded are distributed similarly. The first log p of the log n steps of the original\nalgorithm are simulated in ( n/p) log", "doc_id": "1de960fd-008d-4088-b15a-f7deec3530f3", "embedding": null, "doc_hash": "e9eaab059bea04bfbc5241525140535675f6dc338e4a62466b52338e37b217c7", "extra_info": null, "node_info": {"start": 586442, "end": 590423}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "116d8cac-52de-4cc5-97e9-20787801d8ed", "3": "ed686fd1-3a09-45dd-8c93-3c57462283c5"}}, "__type__": "1"}, "ed686fd1-3a09-45dd-8c93-3c57462283c5": {"__data__": {"text": "to begin with, it may still not be cost-optimal after the granularity of\ncomputation increases. This is illustrated by the following example for the problem of adding n\nnumbers.\nExample 5.9 Adding n numbers on p processing elements\nConsider the problem of adding n numbers on p processing elements such that p < n\nand both n and p are powers of 2. We use the same algorithm as in Example 5.1 and\nsimulate n processing elements on p processing elements. The steps leading to the\nsolution are shown in Figure 5.5  for n = 16 and p = 4. Virtual processing element i is\nsimulated by the physical processing element labeled i mod p; the numbers to be\nadded are distributed similarly. The first log p of the log n steps of the original\nalgorithm are simulated in ( n/p) log p steps on p processing elements. In the\nremaining steps, no communication is required because the processing elements that\ncommunicate in the original algorithm are simulated by the same processing element;\nhence, the remaining numbers are added locally. The algorithm takes Q((n/p) log p)\ntime in the steps that require communication, after which a single processing element\nis left with n/p numbers to add, taking time Q(n/p). Thus, the overall parallel\nexecution time of this parallel system is Q((n/p) log p). Consequently, its cost is Q(n\nlog p), which is asymptotically higher than the Q(n) cost of adding n numbers\nsequentially. Therefore, the parallel system is not cost-optimal. \nFigure 5.5. Four processing elements simulating 16 processing\nelements to compute the sum of 16 numbers (first two steps).\n denotes the sum of numbers with consecutive labels from i\nto j . Four processing elements simulating 16 processing\nelements to compute the sum of 16 numbers (last three steps).\n\nExample 5.1  showed that n numbers can be added on an n-processor machine in time Q(log n).\nWhen using p processing elements to simulate n virtual processing elements ( p < n), the\nexpected parallel runtime is Q((n/p) log n). However, in Example 5.9  this task was performed in\ntime Q((n/p) log p) instead. The reason is that every communication step of the original\nalgorithm does not have to be simulated; at times, communication takes place between virtual\nprocessing elements that are simulated by the same physical processing element. For these\noperations, there is no associated overhead. For example, the simulation of the third and fourth\nsteps ( Figure 5.5(c) and (d)) did not require any communication. However, this reduction in\ncommunication was not enough to make the algorithm cost-optimal. Example 5.10  illustrates\nthat the same problem (adding n numbers on p processing elements) can be performed cost-\noptimally with a smarter assignment of data to processing elements.\nExample 5.10 Adding n numbers cost-optimally\nAn alternate method for adding n numbers using p processing elements is illustrated\nin Figure 5.6  for n = 16 and p = 4. In the first step of this algorithm, each processing\nelement locally adds its n/p numbers in time Q(n/p). Now the problem is reduced to\nadding the p partial sums on p processing elements, which can be done in time Q(log\np) by the method described in Example 5.1 . The parallel runtime of this algorithm is\nEquation 5.5\nand its cost is Q(n + p log p). As long as n = W(p log p), the cost is Q(n), which is the\nsame as the serial runtime. Hence, this parallel system is cost-optimal. \nFigure 5.6. A cost-optimal way of computing the sum of 16 numbers\nusing four processing elements.\nThese simple examples demonstrate that the manner in which the computation is mapped onto\nprocessing elements may determine whether a parallel system is cost-optimal. Note, however,\nthat we cannot make all non-cost-optimal systems cost-optimal by scaling down the number of\nprocessing elements.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.4 Scalability of Parallel", "doc_id": "ed686fd1-3a09-45dd-8c93-3c57462283c5", "embedding": null, "doc_hash": "83bd3e3d93bc1a842687b5e33c496b42ef8e0a494ad91fee396b165755bdd4ee", "extra_info": null, "node_info": {"start": 590386, "end": 594236}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1de960fd-008d-4088-b15a-f7deec3530f3", "3": "5d6a0469-beae-4d32-9eb5-d2cc571e1d05"}}, "__type__": "1"}, "5d6a0469-beae-4d32-9eb5-d2cc571e1d05": {"__data__": {"text": "The parallel runtime of this algorithm is\nEquation 5.5\nand its cost is Q(n + p log p). As long as n = W(p log p), the cost is Q(n), which is the\nsame as the serial runtime. Hence, this parallel system is cost-optimal. \nFigure 5.6. A cost-optimal way of computing the sum of 16 numbers\nusing four processing elements.\nThese simple examples demonstrate that the manner in which the computation is mapped onto\nprocessing elements may determine whether a parallel system is cost-optimal. Note, however,\nthat we cannot make all non-cost-optimal systems cost-optimal by scaling down the number of\nprocessing elements.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.4 Scalability of Parallel Systems\nVery often, programs are designed and tested for smaller problems on fewer processing\nelements. However, the real problems these programs are intended to solve are much larger,\nand the machines contain larger number of processing elements. Whereas code development is\nsimplified by using scaled-down versions of the machine and the problem, their performance\nand correctness (of programs) is much more difficult to establish based on scaled-down\nsystems. In this section, we will investigate techniques for evaluating the scalability of parallel\nprograms using analytical tools.\nExample 5.11 Why is performance extrapolation so difficult?\nConsider three parallel algorithms for computing an n-point Fast Fourier Transform\n(FFT) on 64 processing elements. Figure 5.7 illustrates speedup as the value of n is\nincreased to 18 K. Keeping the number of processing elements constant, at smaller\nvalues of n, one would infer from observed speedups that binary exchange and 3-D\ntranspose algorithms are the best. However, as the problem is scaled up to 18 K\npoints or more, it is evident from Figure 5.7 that the 2-D transpose algorithm yields\nbest speedup. (These algorithms are discussed in greater detail in Chapter 13 .) \nFigure 5.7. A comparison of the speedups obtained by the\nbinary-exchange, 2-D transpose and 3-D transpose algorithms\non 64 processing elements with tc = 2, tw = 4, ts = 25, and th =\n2 (see Chapter 13  for details).\n\nSimilar results can be shown relating to the variation in number of processing elements as the\nproblem size is held constant. Unfortunately, such parallel performance traces are the norm as\nopposed to the exception, making performance prediction based on limited observed data very\ndifficult.\n5.4.1 Scaling Characteristics of Parallel Programs\nThe efficiency of a parallel program can be written as:\nUsing the expression for parallel overhead ( Equation 5.1 ), we can rewrite this expression as\nEquation 5.6\nThe total overhead function To is an increasing function of p. This is because every program\nmust contain some serial component. If this serial component of the program takes time tserial,\nthen during this time all the other processing elements must be idle. This corresponds to a total\noverhead function of ( p - 1) x tserial. Therefore, the total overhead function To grows at least\nlinearly with p. In addition, due to communication, idling, and excess computation, this function\nmay grow superlinearly in the number of processing elements. Equation 5.6  gives us several\ninteresting insights into the scaling of parallel programs. First, for a given problem size (i.e. the\nvalue of TS remains constant), as we increase the number of processing elements, To increases.\nIn such a scenario, it is clear from Equation 5.6 that the overall efficiency of the parallel\nprogram goes down. This characteristic of decreasing efficiency with increasing number of\nprocessing elements for a given problem size is common to all parallel programs.\nExample 5.12 Speedup and efficiency as functions of the\nnumber of processing elements\nConsider the problem of adding n numbers on p processing elements. We use the\nsame algorithm as in Example 5.10. However, to illustrate actual speedups, we work\nwith constants here instead of", "doc_id": "5d6a0469-beae-4d32-9eb5-d2cc571e1d05", "embedding": null, "doc_hash": "88f31ebe6ae8ee239acac6f95efbf5ca0907a4b06c7d3f56d6cd152c56ad19a5", "extra_info": null, "node_info": {"start": 594306, "end": 598243}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ed686fd1-3a09-45dd-8c93-3c57462283c5", "3": "091ea130-063e-4de2-9288-9b2e25921200"}}, "__type__": "1"}, "091ea130-063e-4de2-9288-9b2e25921200": {"__data__": {"text": "in the number of processing elements. Equation 5.6  gives us several\ninteresting insights into the scaling of parallel programs. First, for a given problem size (i.e. the\nvalue of TS remains constant), as we increase the number of processing elements, To increases.\nIn such a scenario, it is clear from Equation 5.6 that the overall efficiency of the parallel\nprogram goes down. This characteristic of decreasing efficiency with increasing number of\nprocessing elements for a given problem size is common to all parallel programs.\nExample 5.12 Speedup and efficiency as functions of the\nnumber of processing elements\nConsider the problem of adding n numbers on p processing elements. We use the\nsame algorithm as in Example 5.10. However, to illustrate actual speedups, we work\nwith constants here instead of asymptotics. Assuming unit time for adding two\nnumbers, the first phase (local summations) of the algorithm takes roughly n/p time.\nThe second phase involves log p  steps with a communication and an addition at each\nstep. If a single communication takes unit time as well, the time for this phase is 2 log\np. Therefore, we can derive parallel time, speedup, and efficiency as:\nEquation 5.7\n\nEquation 5.8\nEquation 5.9\nThese expressions can be used to calculate the speedup and efficiency for any pair of n\nand p. Figure 5.8  shows the S versus p curves for a few different values of n and p.\nTable 5.1  shows the corresponding efficiencies.\nFigure 5.8. Speedup versus the number of processing elements\nfor adding a list of numbers.\nFigure 5.8  and Table 5.1  illustrate that the speedup tends to saturate and efficiency\ndrops as a consequence of Amdahl's law  (Problem 5.1). Furthermore, a larger\ninstance of the same problem yields higher speedup and efficiency for the same\nnumber of processing elements, although both speedup and efficiency continue to\ndrop with increasing p. \nLet us investigate the effect of increasing the problem size keeping the number of processing\nelements constant. We know that the total overhead function To is a function of both problem\nsize TS and the number of processing elements p. In many cases, To grows sublinearly with\nrespect to TS . In such cases, we can see that efficiency increases if the problem size is\nincreased keeping the number of processing elements constant. For such algorithms, it should\nbe possible to keep the efficiency fixed by increasing both the size of the problem and the\nnumber of processing elements simultaneously. For instance, in Table 5.1, the efficiency of\nadding 64 numbers using four processing elements is 0.80. If the number of processing\nelements is increased to 8 and the size of the problem is scaled up to add 192 numbers, the\nefficiency remains 0.80. Increasing p to 16 and n  to 512 results in the same efficiency. This\nability to maintain efficiency at a fixed value by simultaneously increasing the number of\nprocessing elements and the size of the problem is exhibited by many parallel systems. We call\nsuch systems scalable  parallel systems. The scalability  of a parallel system is a measure of its\ncapacity to increase speedup in proportion to the number of processing elements. It reflects a\nparallel system's ability to utilize increasing processing resources effectively.\nTable 5.1. Efficiency as a function of n and p for adding n numbers on\np processing elements.\nn p = 1 p = 4 p = 8 p = 16 p = 32\n64 1.0 0.80 0.57 0.33 0.17\n192 1.0 0.92 0.80 0.60 0.38\n320 1.0 0.95 0.87 0.71 0.50\n512 1.0 0.97 0.91 0.80 0.62\nRecall from Section 5.2.5  that a cost-optimal parallel system has an efficiency of Q(1).\nTherefore, scalability and cost-optimality of parallel systems are related. A scalable parallel\nsystem can always be made cost-optimal if the number of processing elements and the size of\nthe computation are chosen appropriately. For instance, Example 5.10 shows that the parallel\nsystem for adding", "doc_id": "091ea130-063e-4de2-9288-9b2e25921200", "embedding": null, "doc_hash": "861806648e275d0c513789327ffa9f621754a1772aacaf6c3953f84c43d4f3ce", "extra_info": null, "node_info": {"start": 598121, "end": 602017}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5d6a0469-beae-4d32-9eb5-d2cc571e1d05", "3": "3b88bf01-d417-4c41-96bd-49045ffe9251"}}, "__type__": "1"}, "3b88bf01-d417-4c41-96bd-49045ffe9251": {"__data__": {"text": "n and p for adding n numbers on\np processing elements.\nn p = 1 p = 4 p = 8 p = 16 p = 32\n64 1.0 0.80 0.57 0.33 0.17\n192 1.0 0.92 0.80 0.60 0.38\n320 1.0 0.95 0.87 0.71 0.50\n512 1.0 0.97 0.91 0.80 0.62\nRecall from Section 5.2.5  that a cost-optimal parallel system has an efficiency of Q(1).\nTherefore, scalability and cost-optimality of parallel systems are related. A scalable parallel\nsystem can always be made cost-optimal if the number of processing elements and the size of\nthe computation are chosen appropriately. For instance, Example 5.10 shows that the parallel\nsystem for adding n numbers on p processing elements is cost-optimal when n = W(p log p).\nExample 5.13  shows that the same parallel system is scalable if n is increased in proportion to\nQ(p log p) as p is increased.\nExample 5.13 Scalability of adding n numbers\nFor the cost-optimal addition of n numbers on p processing elements n = W(p log p).\nAs shown in Table 5.1 , the efficiency is 0.80 for n = 64 and p = 4. At this point, the\nrelation between n and p is n = 8 p log p. If the number of processing elements is\nincreased to eight, then 8 p log p = 192. Table 5.1  shows that the efficiency is indeed\n0.80 with n = 192 for eight processing elements. Similarly, for p = 16, the efficiency is\n0.80 for n = 8 p log p = 512. Thus, this parallel system remains cost-optimal at an\nefficiency of 0.80 if n is increased as 8 p log p. \n5.4.2 The Isoefficiency Metric of Scalability\nWe summarize the discussion in the section above with the following two observations:\n1.\nFor a given problem size, as we increase the number of processing elements, the overall\nefficiency of the parallel system goes down. This phenomenon is common to all parallel\nsystems.1.\nIn many cases, the efficiency of a parallel system increases if the problem size is increased\nwhile keeping the number of processing elements constant.2.\nThese two phenomena are illustrated in Figure 5.9(a) and (b), respectively. Following from\nthese two observations, we define a scalable parallel system as one in which the efficiency can\nbe kept constant as the number of processing elements is increased, provided that the problem\nsize is also increased. It is useful to determine the rate at which the problem size must increase\nwith respect to the number of processing elements to keep the efficiency fixed. For different\nparallel systems, the problem size must increase at different rates in order to maintain a fixed\nefficiency as the number of processing elements is increased. This rate determines the degree of\nscalability of the parallel system. As we shall show, a lower rate is more desirable than a higher\ngrowth rate in problem size. Let us now investigate metrics for quantitatively determining the\ndegree of scalability of a parallel system. However, before we do that, we must define the\nnotion of problem size  precisely.\nFigure 5.9. Variation of efficiency: (a) as the number of processing\nelements is increased for a given problem size; and (b) as the problem\nsize is increased for a given number of processing elements. The\nphenomenon illustrated in graph (b) is not common to all parallel\nsystems.\nProblem Size  When analyzing parallel systems, we frequently encounter the notion of the size\nof the problem being solved. Thus far, we have used the term problem size  informally, without\ngiving a precise definition. A naive way to express problem size is as a parameter of the input\nsize; for instance, n in case of a matrix operation involving n x n matrices. A drawback of this\ndefinition is that the interpretation of problem size changes from one problem to another. For\nexample, doubling the input size results in an eight-fold increase in the execution time for\nmatrix", "doc_id": "3b88bf01-d417-4c41-96bd-49045ffe9251", "embedding": null, "doc_hash": "f5fddcefa263cd66fc6922cff09f5383720b58fbad63e6da72922c3ec3487723", "extra_info": null, "node_info": {"start": 602217, "end": 605939}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "091ea130-063e-4de2-9288-9b2e25921200", "3": "6d695841-de87-4cec-b8c6-65e0cd5e96c6"}}, "__type__": "1"}, "6d695841-de87-4cec-b8c6-65e0cd5e96c6": {"__data__": {"text": "processing\nelements is increased for a given problem size; and (b) as the problem\nsize is increased for a given number of processing elements. The\nphenomenon illustrated in graph (b) is not common to all parallel\nsystems.\nProblem Size  When analyzing parallel systems, we frequently encounter the notion of the size\nof the problem being solved. Thus far, we have used the term problem size  informally, without\ngiving a precise definition. A naive way to express problem size is as a parameter of the input\nsize; for instance, n in case of a matrix operation involving n x n matrices. A drawback of this\ndefinition is that the interpretation of problem size changes from one problem to another. For\nexample, doubling the input size results in an eight-fold increase in the execution time for\nmatrix multiplication and a four-fold increase for matrix addition (assuming that the\nconventional Q(n3) algorithm is the best matrix multiplication algorithm, and disregarding more\ncomplicated algorithms with better asymptotic complexities).\nA consistent definition of the size or the magnitude of the problem should be such that,\nregardless of the problem, doubling the problem size always means performing twice the\namount of computation. Therefore, we choose to express problem size in terms of the total\nnumber of basic operations required to solve the problem. By this definition, the problem size is\nQ(n3) for n x n matrix multiplication (assuming the conventional algorithm) and Q(n2) for n x n\nmatrix addition. In order to keep it unique for a given problem, we define problem size  as the\nnumber of basic computation steps in the best sequential algorithm to solve the problem on a\nsingle processing element, where the best sequential algorithm is defined as in Section 5.2.3 .\nBecause it is defined in terms of sequential time complexity, the problem size is a function of\nthe size of the input. The symbol we use to denote problem size is W.\nIn the remainder of this chapter, we assume that it takes unit time to perform one basic\ncomputation step of an algorithm. This assumption does not impact the analysis of any parallel\nsystem because the other hardware-related constants, such as message startup time, per-word\ntransfer time, and per-hop time, can be normalized with respect to the time taken by a basic\ncomputation step. With this assumption, the problem size W is equal to the serial runtime TS of\nthe fastest known algorithm to solve the problem on a sequential computer.\nThe Isoefficiency Function\nParallel execution time can be expressed as a function of problem size, overhead function, and\nthe number of processing elements. We can write parallel runtime as:\nEquation 5.10\nThe resulting expression for speedup is\nEquation 5.11\nFinally, we write the expression for efficiency as\nEquation 5.12\nIn Equation 5.12 , if the problem size is kept constant and p  is increased, the efficiency\ndecreases because the total overhead To increases with p. If W is increased keeping the number\nof processing elements fixed, then for scalable parallel systems, the efficiency increases. This is\nbecause To grows slower than Q(W) for a fixed p. For these parallel systems, efficiency can be\nmaintained at a desired value (between 0 and 1) for increasing p, provided W is also increased.\nFor different parallel systems, W must be increased at different rates with respect to p in order\nto maintain a fixed efficiency. For instance, in some cases, W might need to grow as an\nexponential function of p  to keep the efficiency from dropping as p increases. Such parallel\nsystems are poorly scalable. The reason is that on these parallel systems it is difficult to obtain\ngood speedups for a large number of processing elements unless the problem size is enormous.\nOn the other hand, if W needs to grow only linearly with respect to p, then the parallel system is\nhighly scalable. That is because it can easily deliver speedups proportional to the number of\nprocessing elements for reasonable problem sizes.\nFor scalable parallel systems, efficiency can be maintained at a fixed value (between 0 and 1) if\nthe", "doc_id": "6d695841-de87-4cec-b8c6-65e0cd5e96c6", "embedding": null, "doc_hash": "fffea5fca19104ee00d7bd09a8fbf9a7a00d94d0f41efd06b960287c131b0024", "extra_info": null, "node_info": {"start": 605757, "end": 609862}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3b88bf01-d417-4c41-96bd-49045ffe9251", "3": "d314d3c1-39f1-43de-b6f3-ca383750ac29"}}, "__type__": "1"}, "d314d3c1-39f1-43de-b6f3-ca383750ac29": {"__data__": {"text": "different parallel systems, W must be increased at different rates with respect to p in order\nto maintain a fixed efficiency. For instance, in some cases, W might need to grow as an\nexponential function of p  to keep the efficiency from dropping as p increases. Such parallel\nsystems are poorly scalable. The reason is that on these parallel systems it is difficult to obtain\ngood speedups for a large number of processing elements unless the problem size is enormous.\nOn the other hand, if W needs to grow only linearly with respect to p, then the parallel system is\nhighly scalable. That is because it can easily deliver speedups proportional to the number of\nprocessing elements for reasonable problem sizes.\nFor scalable parallel systems, efficiency can be maintained at a fixed value (between 0 and 1) if\nthe ratio To/W in Equation 5.12 is maintained at a constant value. For a desired value E of\nefficiency,\nEquation 5.13\nLet K = E/(1 - E) be a constant depending on the efficiency to be maintained. Since To is a\nfunction of W and p, Equation 5.13  can be rewritten as\nEquation 5.14\nFrom Equation 5.14 , the problem size W can usually be obtained as a function of p  by algebraic\nmanipulations. This function dictates the growth rate of W required to keep the efficiency fixed\nas p increases. We call this function the isoefficiency function  of the parallel system. The\nisoefficiency function determines the ease with which a parallel system can maintain a constant\nefficiency and hence achieve speedups increasing in proportion to the number of processing\nelements. A small isoefficiency function means that small increments in the problem size are\nsufficient for the efficient utilization of an increasing number of processing elements, indicating\nthat the parallel system is highly scalable. However, a large isoefficiency function indicates a\npoorly scalable parallel system. The isoefficiency function does not exist for unscalable parallel\nsystems, because in such systems the efficiency cannot be kept at any constant value as p\nincreases, no matter how fast the problem size is increased.\nExample 5.14 Isoefficiency function of adding numbers\nThe overhead function for the problem of adding n numbers on p processing elements\nis approximately 2 p log p, as given by Equations 5.9 and 5.1. Substituting To by 2 p\nlog p in Equation 5.14 , we get\nEquation 5.15\nThus, the asymptotic isoefficiency function for this parallel system is Q(p log p). This\nmeans that, if the number of processing elements is increased from p to p', the\nproblem size (in this case, n) must be increased by a factor of ( p' log p')/(p log p) to\nget the same efficiency as on p processing elements. In other words, increasing the\nnumber of processing elements by a factor of p'/p requires that n be increased by a\nfactor of ( p' log p')/(p log p) to increase the speedup by a factor of p'/p. \nIn the simple example of adding n numbers, the overhead due to communication (hereafter\nreferred to as the communication overhead ) is a function of p only. In general,\ncommunication overhead can depend on both the problem size and the number of processing\nelements. A typical overhead function can have several distinct terms of different orders of\nmagnitude with respect to p and W. In such a case, it can be cumbersome (or even impossible)\nto obtain the isoefficiency function as a closed function of p. For example, consider a\nhypothetical parallel system for which To = p3/2 + p3/4 W 3/4. For this overhead function,\nEquation 5.14 can be rewritten as W = Kp3/2 + Kp3/4 W 3/4. It is hard to solve this equation for\nW in terms of p.\nRecall that the condition for constant efficiency is that the ratio To/W remains fixed. As p and W\nincrease, the efficiency is nondecreasing as long as none of the terms of To grow faster than W.\nIf To has multiple terms, we balance W against each term of To and compute the respective\nisoefficiency", "doc_id": "d314d3c1-39f1-43de-b6f3-ca383750ac29", "embedding": null, "doc_hash": "dfeaef6f347715568a2790a58b77cab986a3cf062b0f638ddcf0a6a37e6273ce", "extra_info": null, "node_info": {"start": 609853, "end": 613768}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6d695841-de87-4cec-b8c6-65e0cd5e96c6", "3": "d4411b26-079d-4bf9-abc3-c56395a383a9"}}, "__type__": "1"}, "d4411b26-079d-4bf9-abc3-c56395a383a9": {"__data__": {"text": "with respect to p and W. In such a case, it can be cumbersome (or even impossible)\nto obtain the isoefficiency function as a closed function of p. For example, consider a\nhypothetical parallel system for which To = p3/2 + p3/4 W 3/4. For this overhead function,\nEquation 5.14 can be rewritten as W = Kp3/2 + Kp3/4 W 3/4. It is hard to solve this equation for\nW in terms of p.\nRecall that the condition for constant efficiency is that the ratio To/W remains fixed. As p and W\nincrease, the efficiency is nondecreasing as long as none of the terms of To grow faster than W.\nIf To has multiple terms, we balance W against each term of To and compute the respective\nisoefficiency functions for individual terms. The component of To that requires the problem size\nto grow at the highest rate with respect to p determines the overall asymptotic isoefficiency\nfunction of the parallel system. Example 5.15  further illustrates this technique of isoefficiency\nanalysis.\nExample 5.15 Isoefficiency function of a parallel system with a\ncomplex overhead function\nConsider a parallel system for which To = p3/2 + p3/4 W 3/4. Using only the first term of\nTo in Equation 5.14 , we get\nEquation 5.16\nUsing only the second term, Equation 5.14  yields the following relation between W and\np:\nEquation 5.17\n\nTo ensure that the efficiency does not decrease as the number of processing elements\nincreases, the first and second terms of the overhead function require the problem\nsize to grow as Q(p3/2) and Q(p3), respectively. The asymptotically higher of the two\nrates, Q(p3), gives the overall asymptotic isoefficiency function of this parallel system,\nsince it subsumes the rate dictated by the other term. The reader may indeed verify\nthat if the problem size is increased at this rate, the efficiency is Q(1) and that any\nrate lower than this causes the efficiency to fall with increasing p. \nIn a single expression, the isoefficiency function captures the characteristics of a parallel\nalgorithm as well as the parallel architecture on which it is implemented. After performing\nisoefficiency analysis, we can test the performance of a parallel program on a few processing\nelements and then predict its performance on a larger number of processing elements.\nHowever, the utility of isoefficiency analysis is not limited to predicting the impact on\nperformance of an increasing number of processing elements. Section 5.4.5 shows how the\nisoefficiency function characterizes the amount of parallelism inherent in a parallel algorithm.\nWe will see in Chapter 13 that isoefficiency analysis can also be used to study the behavior of a\nparallel system with respect to changes in hardware parameters such as the speed of processing\nelements and communication channels. Chapter 11 illustrates how isoefficiency analysis can be\nused even for parallel algorithms for which we cannot derive a value of parallel runtime.\n5.4.3 Cost-Optimality and the Isoefficiency Function\nIn Section 5.2.5 , we stated that a parallel system is cost-optimal if the product of the number of\nprocessing elements and the parallel execution time is proportional to the execution time of the\nfastest known sequential algorithm on a single processing element. In other words, a parallel\nsystem is cost-optimal if and only if\nEquation 5.18\nSubstituting the expression for TP from the right-hand side of Equation 5.10 , we get the\nfollowing:\nEquation 5.19\nEquation 5.20\nEquations 5.19 and 5.20 suggest that a parallel system is cost-optimal if and only if its\noverhead function does not asymptotically exceed the problem size. This is very similar to the\ncondition given by Equation 5.14  for maintaining a fixed efficiency while increasing the number\nof processing elements in a parallel system. If Equation 5.14  yields an isoefficiency function\nf(p), then it follows from Equation 5.20  that the relation W = W(f(p)) must be satisfied to", "doc_id": "d4411b26-079d-4bf9-abc3-c56395a383a9", "embedding": null, "doc_hash": "898b29474884891f121783cf85fe4759f1e014549cef47430525c4ac3eb5d88c", "extra_info": null, "node_info": {"start": 613896, "end": 617786}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d314d3c1-39f1-43de-b6f3-ca383750ac29", "3": "02c131b6-5c9b-4c6b-a959-1362ff2609c4"}}, "__type__": "1"}, "02c131b6-5c9b-4c6b-a959-1362ff2609c4": {"__data__": {"text": "on a single processing element. In other words, a parallel\nsystem is cost-optimal if and only if\nEquation 5.18\nSubstituting the expression for TP from the right-hand side of Equation 5.10 , we get the\nfollowing:\nEquation 5.19\nEquation 5.20\nEquations 5.19 and 5.20 suggest that a parallel system is cost-optimal if and only if its\noverhead function does not asymptotically exceed the problem size. This is very similar to the\ncondition given by Equation 5.14  for maintaining a fixed efficiency while increasing the number\nof processing elements in a parallel system. If Equation 5.14  yields an isoefficiency function\nf(p), then it follows from Equation 5.20  that the relation W = W(f(p)) must be satisfied to ensure\nthe cost-optimality of a parallel system as it is scaled up. The following example further\nillustrates the relationship between cost-optimality and the isoefficiency function.\nExample 5.16 Relationship between cost-optimality and\nisoefficiency\nConsider the cost-optimal solution to the problem of adding n numbers on p\nprocessing elements, presented in Example 5.10. For this parallel system, W \n n, and\nTo = Q(p log p). From Equation 5.14 , its isoefficiency function is Q(p log p); that is,\nthe problem size must increase as Q(p log p) to maintain a constant efficiency. In\nExample 5.10  we also derived the condition for cost-optimality as W = W(p log p). \n5.4.4 A Lower Bound on the Isoefficiency Function\nWe discussed earlier that a smaller isoefficiency function indicates higher scalability.\nAccordingly, an ideally-scalable parallel system must have the lowest possible isoefficiency\nfunction. For a problem consisting of W units of work, no more than W processing elements can\nbe used cost-optimally; additional processing elements will be idle. If the problem size grows at\na rate slower than Q(p) as the number of processing elements increases, then the number of\nprocessing elements will eventually exceed W. Even for an ideal parallel system with no\ncommunication, or other overhead, the efficiency will drop because processing elements added\nbeyond p = W will be idle. Thus, asymptotically, the problem size must increase at least as fast\nas Q(p) to maintain fixed efficiency; hence, W(p) is the asymptotic lower bound on the\nisoefficiency function. It follows that the isoefficiency function of an ideally scalable parallel\nsystem is Q(p).\n5.4.5 The Degree of Concurrency and the Isoefficiency Function\nA lower bound of W(p) is imposed on the isoefficiency function of a parallel system by the\nnumber of operations that can be performed concurrently. The maximum number of tasks that\ncan be executed simultaneously at any time in a parallel algorithm is called its degree of\nconcurrency . The degree of concurrency is a measure of the number of operations that an\nalgorithm can perform in parallel for a problem of size W; it is independent of the parallel\narchitecture. If C(W) is the degree of concurrency of a parallel algorithm, then for a problem of\nsize W, no more than C(W) processing elements can be employed effectively.\nExample 5.17 Effect of concurrency on isoefficiency function\nConsider solving a system of n equations in n variables by using Gaussian elimination\n(Section 8.3.1). The total amount of computation is Q(n3). But then variables must be\neliminated one after the other, and eliminating each variable requires Q(n2)\ncomputations. Thus, at most Q(n2) processing elements can be kept busy at any time.\nSince W = Q(n3) for this problem, the degree of concurrency C( W) is Q(W2/3) and at\nmost Q(W2/3) processing elements can be used efficiently. On the other hand, given p\nprocessing elements, the problem size should be at least W(p3/2) to use them all. Thus,\nthe isoefficiency function of this", "doc_id": "02c131b6-5c9b-4c6b-a959-1362ff2609c4", "embedding": null, "doc_hash": "fffe4e957102dfed9cd89eea97f543199ea03b2c302afecd86c25fb5d4ec79c1", "extra_info": null, "node_info": {"start": 617740, "end": 621488}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d4411b26-079d-4bf9-abc3-c56395a383a9", "3": "bd0bc731-7071-4602-b03e-34201e2f0966"}}, "__type__": "1"}, "bd0bc731-7071-4602-b03e-34201e2f0966": {"__data__": {"text": "5.17 Effect of concurrency on isoefficiency function\nConsider solving a system of n equations in n variables by using Gaussian elimination\n(Section 8.3.1). The total amount of computation is Q(n3). But then variables must be\neliminated one after the other, and eliminating each variable requires Q(n2)\ncomputations. Thus, at most Q(n2) processing elements can be kept busy at any time.\nSince W = Q(n3) for this problem, the degree of concurrency C( W) is Q(W2/3) and at\nmost Q(W2/3) processing elements can be used efficiently. On the other hand, given p\nprocessing elements, the problem size should be at least W(p3/2) to use them all. Thus,\nthe isoefficiency function of this computation due to concurrency is Q(p3/2). \nThe isoefficiency function due to concurrency is optimal (that is, Q(p)) only if the degree of\nconcurrency of the parallel algorithm is Q(W). If the degree of concurrency of an algorithm is\nless than Q(W), then the isoefficiency function due to concurrency is worse (that is, greater)\nthan Q(p). In such cases, the overall isoefficiency function of a parallel system is given by the\nmaximum of the isoefficiency functions due to concurrency, communication, and other\noverheads.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.5 Minimum Execution Time and Minimum Cost-\nOptimal Execution Time\nWe are often interested in knowing how fast a problem can be solved, or what the minimum\npossible execution time of a parallel algorithm is, provided that the number of processing\nelements is not a constraint. As we increase the number of processing elements for a given\nproblem size, either the parallel runtime continues to decrease and asymptotically approaches a\nminimum value, or it starts rising after attaining a minimum value (Problem 5.12). We can\ndetermine the minimum parallel runtime \n for a given W by differentiating the expression\nfor TP with respect to p and equating the derivative to zero (assuming that the function TP (W,\np) is differentiable with respect to p). The number of processing elements for which TP is\nminimum is determined by the following equation:\nEquation 5.21\nLet p0 be the value of the number of processing elements that satisfies Equation 5.21 . The value\nof \n  can be determined by substituting p0 for p in the expression for TP . In the following\nexample, we derive the expression for \n  for the problem of adding n numbers.\nExample 5.18 Minimum execution time for adding n numbers\nUnder the assumptions of Example 5.12 , the parallel run time for the problem of\nadding n numbers on p processing elements can be approximated by\nEquation 5.22\nEquating the derivative with respect to p of the right-hand side of Equation 5.22  to\nzero we get the solutions for p as follows:\nEquation 5.23\n\nSubstituting p = n/2 in Equation 5.22 , we get\nEquation 5.24\nIn Example 5.18 , the processor-time product for p = p0 is Q(n log n), which is higher than the\nQ(n) serial complexity of the problem. Hence, the parallel system is not cost-optimal for the\nvalue of p that yields minimum parallel runtime. We now derive an important result that gives a\nlower bound on parallel runtime if the problem is solved cost-optimally.\nLet \n  be the minimum time in which a problem can be solved by a cost-optimal parallel\nsystem. From the discussion regarding the equivalence of cost-optimality and the isoefficiency\nfunction in Section 5.4.3 , we conclude that if the isoefficiency function of a parallel system is\nQ(f(p)), then a problem of size W can be solved cost-optimally if and only if W = W(f(p)). In\nother words, given a problem of size W, a cost-optimal solution requires that p = O(f-1(W)).\nSince the parallel runtime is Q(W/p) for a cost-optimal", "doc_id": "bd0bc731-7071-4602-b03e-34201e2f0966", "embedding": null, "doc_hash": "5ef19c7469375cf4557f5cef20b7f22d4e57e97e0e5d617dd400e0a2ed6410e5", "extra_info": null, "node_info": {"start": 621517, "end": 625188}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "02c131b6-5c9b-4c6b-a959-1362ff2609c4", "3": "9321c92a-90bb-4025-99f9-c0d6fe3bf5e6"}}, "__type__": "1"}, "9321c92a-90bb-4025-99f9-c0d6fe3bf5e6": {"__data__": {"text": "for the\nvalue of p that yields minimum parallel runtime. We now derive an important result that gives a\nlower bound on parallel runtime if the problem is solved cost-optimally.\nLet \n  be the minimum time in which a problem can be solved by a cost-optimal parallel\nsystem. From the discussion regarding the equivalence of cost-optimality and the isoefficiency\nfunction in Section 5.4.3 , we conclude that if the isoefficiency function of a parallel system is\nQ(f(p)), then a problem of size W can be solved cost-optimally if and only if W = W(f(p)). In\nother words, given a problem of size W, a cost-optimal solution requires that p = O(f-1(W)).\nSince the parallel runtime is Q(W/p) for a cost-optimal parallel system ( Equation 5.18 ), the\nlower bound on the parallel runtime for solving a problem of size W cost-optimally is\nEquation 5.25\nExample 5.19 Minimum cost-optimal execution time for adding\nn numbers\nAs derived in Example 5.14 , the isoefficiency function f(p) of this parallel system is\nQ(p log p). If W = n = f(p) = p log p, then log n = log p + log log p. Ignoring the\ndouble logarithmic term, log n \n log p. If n = f (p) = p log p, then p = f-1(n) = n/log\np \n n/log n. Hence, f-1(W) = Q(n/log n). As a consequence of the relation between\ncost-optimality and the isoefficiency function, the maximum number of processing\nelements that can be used to solve this problem cost-optimally is Q(n/log n). Using p\n= n/log n in Equation 5.2 , we get\nEquation 5.26\n\nIt is interesting to observe that both \n  and \n  for adding n numbers are Q(log n)\n(Equations 5.24 and 5.26). Thus, for this problem, a cost-optimal solution is also the\nasymptotically fastest solution. The parallel execution time cannot be reduced asymptotically by\nusing a value of p greater than that suggested by the isoefficiency function for a given problem\nsize (due to the equivalence between cost-optimality and the isoefficiency function). This is not\ntrue for parallel systems in general, however, and it is quite possible that \n .\nThe following example illustrates such a parallel system.\nExample 5.20 A parallel system with\nConsider the hypothetical parallel system of Example 5.15 , for which\nEquation 5.27\nFrom Equation 5.10 , the parallel runtime for this system is\nEquation 5.28\nUsing the methodology of Example 5.18 ,\nFrom the preceding analysis, p0 = Q(W). Substituting p by the value of p0 in Equation\n5.28, we get\nEquation 5.29\nAccording to Example 5.15 , the overall isoefficiency function for this parallel system is\nQ(p3), which implies that the maximum number of processing elements that can be\nused cost-optimally is Q(W1/3). Substituting p = Q(W1/3) in Equation 5.28 , we get\nEquation 5.30\nA comparison of Equations 5.29 and 5.30 shows that \n  is asymptotically\ngreater than \n . \nIn this section, we have seen examples of both types of parallel systems: those for which\n is asymptotically equal to \n , and those for which \n  is asymptotically greater\nthan \n . Most parallel systems presented in this book are of the first type. Parallel systems for\nwhich the runtime can be reduced by an order of magnitude by using an asymptotically higher\nnumber of processing elements than indicated by the isoefficiency function are rare.\nWhile deriving the minimum execution time for any parallel system, it is important to be aware\nthat the maximum number of processing elements that can be utilized is bounded by the degree\nof concurrency C(W) of the parallel algorithm. It is quite possible that p0 is greater than C", "doc_id": "9321c92a-90bb-4025-99f9-c0d6fe3bf5e6", "embedding": null, "doc_hash": "b2ac3c08a354c17fac75485e0b4a8eb053ae9b96fa895090c82f525470ffdb43", "extra_info": null, "node_info": {"start": 625176, "end": 628679}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bd0bc731-7071-4602-b03e-34201e2f0966", "3": "292cea4b-7e08-496a-a2d1-ec32285b2624"}}, "__type__": "1"}, "292cea4b-7e08-496a-a2d1-ec32285b2624": {"__data__": {"text": "shows that \n  is asymptotically\ngreater than \n . \nIn this section, we have seen examples of both types of parallel systems: those for which\n is asymptotically equal to \n , and those for which \n  is asymptotically greater\nthan \n . Most parallel systems presented in this book are of the first type. Parallel systems for\nwhich the runtime can be reduced by an order of magnitude by using an asymptotically higher\nnumber of processing elements than indicated by the isoefficiency function are rare.\nWhile deriving the minimum execution time for any parallel system, it is important to be aware\nthat the maximum number of processing elements that can be utilized is bounded by the degree\nof concurrency C(W) of the parallel algorithm. It is quite possible that p0 is greater than C (W)\nfor a parallel system (Problems 5.13 and 5.14). In such cases, the value of p0 is meaningless,\nand \n is given by\nEquation 5.31\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.6 Asymptotic Analysis of Parallel Programs\nAt this point, we have accumulated an arsenal of powerful tools for quantifying the performance\nand scalability of an algorithm. Let us illustrate the use of these tools for evaluating a set of\nparallel programs for solving a given problem. Often, we ignore constants and concern\nourselves with the asymptotic behavior of quantities. In many cases, this can yield a clearer\npicture of relative merits and demerits of various parallel programs.\nTable 5.2. Comparison of four different algorithms for sorting a given\nlist of numbers. The table shows number of processing elements,\nparallel runtime, speedup, efficiency and the pTP product.\nAlgorithm A1 A2 A3 A4\np n2log n n\nTP 1 n\nS n log n log n\nE\n 1\n 1\npTP n2n log n n1.5n log n\nConsider the problem of sorting a list of n numbers. The fastest serial programs for this problem\nrun in time O (n log n). Let us look at four different parallel algorithms A1, A2, A3, and A4, for\nsorting a given list. The parallel runtime of the four algorithms along with the number of\nprocessing elements they can use is given in Table 5.2 . The objective of this exercise is to\ndetermine which of these four algorithms is the best. Perhaps the simplest metric is one of\nspeed; the algorithm with the lowest TP is the best. By this metric, algorithm A1 is the best,\nfollowed by A3, A4, and A2. This is also reflected in the fact that the speedups of the set of\nalgorithms are also in this order.\nHowever, in practical situations, we will rarely have n2 processing elements as are required by\nalgorithm A1. Furthermore, resource utilization is an important aspect of practical program\ndesign. So let us look at how efficient each of these algorithms are. This metric of evaluating the\nalgorithm presents a starkly different image. Algorithms A2 and A4 are the best, followed by A3\nand A1. The last row of Table 5.2 presents the cost of the four algorithms. From this row, it is\nevident that the costs of algorithms A1 and A3 are higher than the serial runtime of n log n and\ntherefore neither of these algorithms is cost optimal. However, algorithms A2 and A4 are cost\noptimal.\nThis set of algorithms illustrate that it is important to first understand the objectives of parallel\nalgorithm analysis and to use appropriate metrics. This is because use of different metrics may\noften result in contradictory outcomes.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.7 Other Scalability Metrics\nA number of other metrics of scalability of parallel systems have been proposed. These metrics\nare specifically suited to different system requirements. For example, in real time applications,\nthe objective is to scale up a system to accomplish a task in a specified time bound. One such\napplication is", "doc_id": "292cea4b-7e08-496a-a2d1-ec32285b2624", "embedding": null, "doc_hash": "ee5a8294a1f5393f495b8579a18282139c66fa89498fcd823e682fa6221db906", "extra_info": null, "node_info": {"start": 628619, "end": 632318}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9321c92a-90bb-4025-99f9-c0d6fe3bf5e6", "3": "32730335-a5f7-4ddf-b6f6-3b699482765e"}}, "__type__": "1"}, "32730335-a5f7-4ddf-b6f6-3b699482765e": {"__data__": {"text": "and A3 are higher than the serial runtime of n log n and\ntherefore neither of these algorithms is cost optimal. However, algorithms A2 and A4 are cost\noptimal.\nThis set of algorithms illustrate that it is important to first understand the objectives of parallel\nalgorithm analysis and to use appropriate metrics. This is because use of different metrics may\noften result in contradictory outcomes.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.7 Other Scalability Metrics\nA number of other metrics of scalability of parallel systems have been proposed. These metrics\nare specifically suited to different system requirements. For example, in real time applications,\nthe objective is to scale up a system to accomplish a task in a specified time bound. One such\napplication is multimedia decompression, where MPEG streams must be decompressed at the\nrate of 25 frames/second. Consequently, a parallel system must decode a single frame in 40 ms\n(or with buffering, at an average of 1 frame in 40 ms over the buffered frames). Other such\napplications arise in real-time control, where a control vector must be generated in real-time.\nSeveral scalability metrics consider constraints on physical architectures. In many applications,\nthe maximum size of a problem is constrained not by time, efficiency, or underlying models, but\nby the memory available on the machine. In such cases, metrics make assumptions on the\ngrowth function of available memory (with number of processing elements) and estimate how\nthe performance of the parallel system changes with such scaling. In this section, we examine\nsome of the related metrics and how they can be used in various parallel applications.\nScaled Speedup  This metric is defined as the speedup obtained when the problem size is\nincreased linearly with the number of processing elements. If the scaled-speedup curve is close\nto linear with respect to the number of processing elements, then the parallel system is\nconsidered scalable. This metric is related to isoefficiency if the parallel algorithm under\nconsideration has linear or near-linear isoefficiency function. In this case the scaled-speedup\nmetric provides results very close to those of isoefficiency analysis, and the scaled-speedup is\nlinear or near-linear with respect to the number of processing elements. For parallel systems\nwith much worse isoefficiencies, the results provided by the two metrics may be quite different.\nIn this case, the scaled-speedup versus number of processing elements curve is sublinear.\nTwo generalized notions of scaled speedup have been examined. They differ in the methods by\nwhich the problem size is scaled up with the number of processing elements. In one method,\nthe size of the problem is increased to fill the available memory on the parallel computer. The\nassumption here is that aggregate memory of the system increases with the number of\nprocessing elements. In the other method, the size of the problem grows with p subject to an\nupper-bound on execution time.\nExample 5.21 Memory and time-constrained scaled speedup for\nmatrix-vector products\nThe serial runtime of multiplying a matrix of dimension n x n with a vector is tcn2,\nwhere tc is the time for a single multiply-add operation. The corresponding parallel\nruntime using a simple parallel algorithm is given by:\nand the speedup S is given by:\nEquation 5.32\nThe total memory requirement of the algorithm is Q(n2). Let us consider the two cases\nof problem scaling. In the case of memory constrained scaling, we assume that the\nmemory of the parallel system grows linearly with the number of processing elements,\ni.e., m = Q(p). This is a reasonable assumption for most current parallel platforms.\nSince m = Q(n2), we have n2 = c x p, for some constant c . Therefore, the scaled\nspeedup S' is given by:\nor\nIn the limiting case, \n .\nIn the case of time constrained scaling, we have TP = O(n2/p). Since this is\nconstrained to be constant, n2 = O(p). We notice that this case is identical to", "doc_id": "32730335-a5f7-4ddf-b6f6-3b699482765e", "embedding": null, "doc_hash": "0053e099c4d6724313b54306d5f121a38fbbc88d1bb7bd48b7eac3d71767de42", "extra_info": null, "node_info": {"start": 632322, "end": 636301}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "292cea4b-7e08-496a-a2d1-ec32285b2624", "3": "250210c9-e39b-4aac-a165-a8ce358a344a"}}, "__type__": "1"}, "250210c9-e39b-4aac-a165-a8ce358a344a": {"__data__": {"text": "S is given by:\nEquation 5.32\nThe total memory requirement of the algorithm is Q(n2). Let us consider the two cases\nof problem scaling. In the case of memory constrained scaling, we assume that the\nmemory of the parallel system grows linearly with the number of processing elements,\ni.e., m = Q(p). This is a reasonable assumption for most current parallel platforms.\nSince m = Q(n2), we have n2 = c x p, for some constant c . Therefore, the scaled\nspeedup S' is given by:\nor\nIn the limiting case, \n .\nIn the case of time constrained scaling, we have TP = O(n2/p). Since this is\nconstrained to be constant, n2 = O(p). We notice that this case is identical to the\nmemory constrained case. This happened because the memory and runtime of the\nalgorithm are asymptotically identical. \nExample 5.22 Memory and time-constrained scaled speedup for\nmatrix-matrix products\nThe serial runtime of multiplying two matrices of dimension n x n is tcn3, where tc, as\nbefore, is the time for a single multiply-add operation. The corresponding parallel\nruntime using a simple parallel algorithm is given by:\nand the speedup S is given by:\nEquation 5.33\n\nThe total memory requirement of the algorithm is Q(n2). Let us consider the two cases\nof problem scaling. In the case of memory constrained scaling, as before, we assume\nthat the memory of the parallel system grows linearly with the number of processing\nelements, i.e., m = Q(p). Since m = Q(n2), we have n2 = c x p, for some constant c.\nTherefore, the scaled speedup S' is given by:\nIn the case of time constrained scaling, we have TP = O(n3/p). Since this is\nconstrained to be constant, n3 = O(p), or n3 = c x p (for some constant c).\nTherefore, the time-constrained speedup S\" is given by:\nThis example illustrates that memory-constrained scaling yields linear speedup,\nwhereas time-constrained speedup yields sublinear speedup in the case of matrix\nmultiplication. \nSerial Fraction  f The experimentally determined serial fraction f can be used to quantify the\nperformance of a parallel system on a fixed-size problem. Consider a case when the serial\nruntime of a computation can be divided into a totally parallel and a totally serial component,\ni.e.,\nHere, Tser and Tpar correspond to totally serial and totally parallel components. From this, we\ncan write:\nHere, we have assumed that all of the other parallel overheads such as excess computation and\ncommunication are captured in the serial component Tser. From these equations, it follows that:\nEquation 5.34\n\nThe serial fraction f of a parallel program is defined as:\nTherefore, from Equation 5.34 , we have:\nSince S = W/TP , we have\nSolving for f , we get:\nEquation 5.35\nIt is easy to see that smaller values of f are better since they result in higher efficiencies. If f\nincreases with the number of processing elements, then it is considered as an indicator of rising\ncommunication overhead, and thus an indicator of poor scalability.\nExample 5.23 Serial component of the matrix-vector product\nFrom Equations 5.35 and 5.32, we have\nEquation 5.36\nSimplifying the above expression, we get\nIt is useful to note that the denominator of this equation is the serial runtime of the\nalgorithm and the numerator corresponds to the overhead in parallel execution. \nIn addition to these metrics, a number of other metrics of performance have been proposed in\nthe literature. We refer interested readers to the bibliography for references to these.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.8 Bibliographic Remarks\nTo use today's massively parallel computers effectively, larger problems must be solved as\nmore processing elements are added.", "doc_id": "250210c9-e39b-4aac-a165-a8ce358a344a", "embedding": null, "doc_hash": "f523a367a6ae2911ef00a6d983122c1fb0009a9bccbcc5466e81ab11801dc505", "extra_info": null, "node_info": {"start": 636402, "end": 640023}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "32730335-a5f7-4ddf-b6f6-3b699482765e", "3": "5e22dadf-7d0d-48d9-83c7-78f282b305b1"}}, "__type__": "1"}, "5e22dadf-7d0d-48d9-83c7-78f282b305b1": {"__data__": {"text": "and thus an indicator of poor scalability.\nExample 5.23 Serial component of the matrix-vector product\nFrom Equations 5.35 and 5.32, we have\nEquation 5.36\nSimplifying the above expression, we get\nIt is useful to note that the denominator of this equation is the serial runtime of the\nalgorithm and the numerator corresponds to the overhead in parallel execution. \nIn addition to these metrics, a number of other metrics of performance have been proposed in\nthe literature. We refer interested readers to the bibliography for references to these.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n5.8 Bibliographic Remarks\nTo use today's massively parallel computers effectively, larger problems must be solved as\nmore processing elements are added. However, when the problem size is fixed, the objective is\nto attain the best compromise between efficiency and parallel runtime. Performance issues for\nfixed-size problems have been addressed by several researchers [ FK89, GK93a , KF90, NW88 ,\nTL90, Wor90 ]. In most situations, additional computing power derived from increasing the\nnumber of processing elements can be used to solve bigger problems. In some situations,\nhowever, different ways of increasing the problem size may apply, and a variety of constraints\nmay guide the scaling up of the workload with respect to the number of processing elements\n[SHG93]. Time-constrained scaling and memory-constrained scaling have been explored by\nGustafson et al. [GMB88 , Gus88 , Gus92 ], Sun and Ni [ SN90 , SN93 ], and Worley [ Wor90 ,\nWor88 , Wor91 ] (Problem 5.9).\nAn important scenario is one in which we want to make the most efficient use of the parallel\nsystem; in other words, we want the overall performance of the parallel system to increase\nlinearly with p. This is possible only for scalable parallel systems, which are exactly those for\nwhich a fixed efficiency can be maintained for arbitrarily large p by simply increasing the\nproblem size. For such systems, it is natural to use the isoefficiency function or related metrics\n[GGK93, CD87 , KR87b , KRS88 ]. Isoefficiency analysis has been found to be very useful in\ncharacterizing the scalability of a variety of parallel algorithms [ GK91 , GK93b , GKS92 , HX98 ,\nKN91 , KR87b , KR89 , KS91b , RS90b , SKAT91b , TL90, WS89 , WS91 ]. Gupta and Kumar [ GK93a ,\nKG94 ] have demonstrated the relevance of the isoefficiency function in the fixed time case as\nwell. They have shown that if the isoefficiency function is greater than Q(p), then the problem\nsize cannot be increased indefinitely while maintaining a fixed execution time, no matter how\nmany processing elements are used. A number of other researchers have analyzed the\nperformance of parallel systems with concern for overall efficiency [ EZL89, FK89, MS88 , NW88 ,\nTL90, Zho89 , ZRV89 ].\nKruskal, Rudolph, and Snir [ KRS88 ] define the concept of parallel efficient (PE)  problems.\nTheir definition is related to the concept of isoefficiency function. Problems in the class PE have\nalgorithms with a polynomial isoefficiency function at some efficiency. The class PE makes an\nimportant distinction between algorithms with polynomial isoefficiency functions and those with\nworse isoefficiency functions. Kruskal et al. proved the invariance of the class PE over a variety\nof parallel computational models and interconnection schemes. An important consequence of\nthis result is that an algorithm with a polynomial isoefficiency on one architecture will have a\npolynomial isoefficiency on many other architectures as well. There can be exceptions, however;\nfor instance, Gupta and Kumar [ GK93b] show that the fast Fourier transform algorithm has a\npolynomial isoefficiency on a hypercube but an exponential isoefficiency", "doc_id": "5e22dadf-7d0d-48d9-83c7-78f282b305b1", "embedding": null, "doc_hash": "6b0a3428f6fa7e768c5274fc6752d9972836cf503b794f20c7dd7bc1c02c1ce2", "extra_info": null, "node_info": {"start": 639947, "end": 643672}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "250210c9-e39b-4aac-a165-a8ce358a344a", "3": "632c0c58-7140-44ca-a752-4d03377793c2"}}, "__type__": "1"}, "632c0c58-7140-44ca-a752-4d03377793c2": {"__data__": {"text": "concept of isoefficiency function. Problems in the class PE have\nalgorithms with a polynomial isoefficiency function at some efficiency. The class PE makes an\nimportant distinction between algorithms with polynomial isoefficiency functions and those with\nworse isoefficiency functions. Kruskal et al. proved the invariance of the class PE over a variety\nof parallel computational models and interconnection schemes. An important consequence of\nthis result is that an algorithm with a polynomial isoefficiency on one architecture will have a\npolynomial isoefficiency on many other architectures as well. There can be exceptions, however;\nfor instance, Gupta and Kumar [ GK93b] show that the fast Fourier transform algorithm has a\npolynomial isoefficiency on a hypercube but an exponential isoefficiency on a mesh.\nVitter and Simons [ VS86 ] define a class of problems called PC*. PC* includes problems with\nefficient parallel algorithms on a PRAM. A problem in class P (the polynomial-time class) is in\nPC* if it has a parallel algorithm on a PRAM that can use a polynomial (in terms of input size)\nnumber of processing elements and achieve a minimal efficiency \n . Any problem in PC* has at\nleast one parallel algorithm such that, for an efficiency \n , its isoefficiency function exists and is a\npolynomial.\nA discussion of various scalability and performance measures can be found in the survey by\nKumar and Gupta [ KG94 ]. Besides the ones cited so far, a number of other metrics of\nperformance and scalability of parallel systems have been proposed [ BW89 , CR89 , CR91 , Fla90 ,\nHil90, Kun86 , Mol87 , MR, NA91 , SG91 , SR91 , SZ96 , VC89 ].\nFlatt and Kennedy [ FK89, Fla90 ] show that if the overhead function satisfies certain\nmathematical properties, then there exists a unique value p0 of the number of processing\nelements for which TP is minimum for a given W. A property of To on which their analysis\ndepends heavily is that To > Q(p). Gupta and Kumar [ GK93a ] show that there exist parallel\nsystems that do not obey this condition, and in such cases the point of peak performance is\ndetermined by the degree of concurrency of the algorithm being used.\nMarinescu and Rice [ MR] develop a model to describe and analyze a parallel computation on an\nMIMD computer in terms of the number of threads of control p into which the computation is\ndivided and the number of events g(p) as a function of p. They consider the case where each\nevent is of a fixed duration q and hence To = qg(p). Under these assumptions on To, they\nconclude that with increasing number of processing elements, the speedup saturates at some\nvalue if To = Q(p), and it asymptotically approaches zero if To = Q(pm),where m \n 2. Gupta\nand Kumar [ GK93a ] generalize these results for a wider class of overhead functions. They show\nthat the speedup saturates at some maximum value if To \n Q(p), and the speedup attains a\nmaximum value and then drops monotonically with p if To > Q(p).\nEager et al. [EZL89 ] and Tang and Li [ TL90] have proposed a criterion of optimality of a parallel\nsystem so that a balance is struck between efficiency and speedup. They propose that a good\nchoice of operating point on the execution time versus efficiency curve is that where the\nincremental benefit of adding processing elements is roughly \n  per processing element or, in\nother words, efficiency is 0.5. They conclude that for To = Q(p), this is also equivalent to\noperating at a point where the ES product is maximum or p(TP)2 is minimum. This conclusion is\na special case of the more general case presented by Gupta and Kumar [ GK93a ].\nBelkhale and Banerjee [ BB90 ], Leuze et al. [LDP89 ], Ma and Shea [ MS88 ], and Park and\nDowdy [ PD89 ] address the important problem of optimal", "doc_id": "632c0c58-7140-44ca-a752-4d03377793c2", "embedding": null, "doc_hash": "7a02ec039d6c68411cbf60258fc74488d2a278ce225220de427dd04a5ff0aee1", "extra_info": null, "node_info": {"start": 643602, "end": 647351}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5e22dadf-7d0d-48d9-83c7-78f282b305b1", "3": "6538ca1c-6389-475d-95e3-0dda8f2aae70"}}, "__type__": "1"}, "6538ca1c-6389-475d-95e3-0dda8f2aae70": {"__data__": {"text": "have proposed a criterion of optimality of a parallel\nsystem so that a balance is struck between efficiency and speedup. They propose that a good\nchoice of operating point on the execution time versus efficiency curve is that where the\nincremental benefit of adding processing elements is roughly \n  per processing element or, in\nother words, efficiency is 0.5. They conclude that for To = Q(p), this is also equivalent to\noperating at a point where the ES product is maximum or p(TP)2 is minimum. This conclusion is\na special case of the more general case presented by Gupta and Kumar [ GK93a ].\nBelkhale and Banerjee [ BB90 ], Leuze et al. [LDP89 ], Ma and Shea [ MS88 ], and Park and\nDowdy [ PD89 ] address the important problem of optimal partitioning of the processing\nelements of a parallel computer among several applications of different scalabilities executing\nsimultaneously.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n5.1 (Amdahl's law [ Amd67 ]) If a problem of size W has a serial component WS, prove\nthat W/WS is an upper bound on its speedup, no matter how many processing elements\nare used.\n5.2 (Superlinear speedup)  Consider the search tree shown in Figure 5.10(a) , in which\nthe dark node represents the solution.\nFigure 5.10. Superlinear(?) speedup in parallel depth first search.\nIf a sequential search of the tree is performed using the standard depth-first search\n(DFS) algorithm ( Section 11.2.1 ), how much time does it take to find the solution if\ntraversing each arc of the tree takes one unit of time?a.\nAssume that the tree is partitioned between two processing elements that are\nassigned to do the search job, as shown in Figure 5.10(b). If both processing\nelements perform a DFS on their respective halves of the tree, how much time does it\ntake for the solution to be found? What is the speedup? Is there a speedup anomaly?\nIf so, can you explain the anomaly?b.\n5.3 (The DAG model of parallel computation)  Parallel algorithms can often be\nrepresented by dependency graphs. Four such dependency graphs are shown in Figure5.11. If a program can be broken into several tasks, then each node of the graph\nrepresents one task. The directed edges of the graph represent the dependencies between\nthe tasks or the order in which they must be performed to yield correct results. A node of\nthe dependency graph can be scheduled for execution as soon as the tasks at all the nodes\nthat have incoming edges to that node have finished execution. For example, in Figure 5.11(b) , the nodes on the second level from the root can begin execution only after the\ntask at the root is finished. Any deadlock-free dependency graph must be a directed\nacyclic graph  (DAG); that is, it is devoid of any cycles. All the nodes that are scheduled\nfor execution can be worked on in parallel provided enough processing elements are\navailable. If N is the number of nodes in a graph, and n is an integer, then N = 2n - 1 for\ngraphs (a) and (b), N = n2 for graph (c), and N = n(n + 1)/2 for graph (d) (graphs (a)\nand (b) are drawn for n = 4 and graphs (c) and (d) are drawn for n = 8). Assuming that\neach task takes one unit of time and that interprocessor communication time is zero, for\nthe algorithms represented by each of these graphs:\n1.\n2.\nCompute the degree of concurrency.1.\nCompute the maximum possible speedup if an unlimited number of processing\nelements is available.2.\nCompute the values of speedup, efficiency, and the overhead function if the number\nof processing elements is (i) the same as the degree of concurrency and (ii) equal to\nhalf of the degree of concurrency.3.\nFigure 5.11. Dependency graphs for Problem 5.3.\n5.4 Consider a parallel system containing p processing elements solving a", "doc_id": "6538ca1c-6389-475d-95e3-0dda8f2aae70", "embedding": null, "doc_hash": "d963ed92e60425d03aee4da17f304150bfdde2e1e772bcfaa9624a6eff021bfb", "extra_info": null, "node_info": {"start": 647430, "end": 651142}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "632c0c58-7140-44ca-a752-4d03377793c2", "3": "5d7b809c-3664-4da3-8e01-1570fafc43d0"}}, "__type__": "1"}, "5d7b809c-3664-4da3-8e01-1570fafc43d0": {"__data__": {"text": "graph (d) (graphs (a)\nand (b) are drawn for n = 4 and graphs (c) and (d) are drawn for n = 8). Assuming that\neach task takes one unit of time and that interprocessor communication time is zero, for\nthe algorithms represented by each of these graphs:\n1.\n2.\nCompute the degree of concurrency.1.\nCompute the maximum possible speedup if an unlimited number of processing\nelements is available.2.\nCompute the values of speedup, efficiency, and the overhead function if the number\nof processing elements is (i) the same as the degree of concurrency and (ii) equal to\nhalf of the degree of concurrency.3.\nFigure 5.11. Dependency graphs for Problem 5.3.\n5.4 Consider a parallel system containing p processing elements solving a problem\nconsisting of W units of work. Prove that if the isoefficiency function of the system is worse\n(greater) than Q(p), then the problem cannot be solved cost-optimally with p = (W). Also\nprove the converse that if the problem can be solved cost-optimally only for p < Q(W),\nthen the isoefficiency function of the parallel system is worse than linear.\n5.5 (Scaled speedup)  Scaled speedup  is defined as the speedup obtained when the\nproblem size is increased linearly with the number of processing elements; that is, if W is\nchosen as a base problem size for a single processing element, then\nEquation 5.37\nFor the problem of adding n numbers on p processing elements ( Example 5.1 ), plot the\nspeedup curves, assuming that the base problem for p = 1 is that of adding 256 numbers.\nUse p = 1, 4, 16, 64, and 256. Assume that it takes 10 time units to communicate a\nnumber between two processing elements, and that it takes one unit of time to add two\nnumbers. Now plot the standard speedup curve for the base problem size and compare it\nwith the scaled speedup curve.\nHint:  The parallel runtime is ( n/p - 1) + 11 log p.\n5.6 Plot a third speedup curve for Problem 5.5, in which the problem size is scaled up\naccording to the isoefficiency function, which is Q(p log p). Use the same expression for TP\n.\nHint:  The scaled speedup under this method of scaling is given by the following equation:\n5.7 Plot the efficiency curves for the problem of adding n numbers on p processing\nelements corresponding to the standard speedup curve (Problem 5.5), the scaled speedup\ncurve (Problem 5.5), and the speedup curve when the problem size is increased according\nto the isoefficiency function (Problem 5.6).\n5.8 A drawback of increasing the number of processing elements without increasing the\ntotal workload is that the speedup does not increase linearly with the number of\nprocessing elements, and the efficiency drops monotonically. Based on your experience\nwith Problems 5.5 and 5.7, discuss whether or not scaled speedup increases linearly with\nthe number of processing elements in general. What can you say about the isoefficiency\nfunction of a parallel system whose scaled speedup curve matches the speedup curve\ndetermined by increasing the problem size according to the isoefficiency function?\n5.9 (Time-constrained scaling)  Using the expression for TP from Problem 5.5 for p = 1,\n4, 16, 64, 256, 1024, and 4096, what is the largest problem that can be solved if the total\nexecution time is not to exceed 512 time units? In general, is it possible to solve an\narbitrarily large problem in a fixed amount of time, provided that an unlimited number of\nprocessing elements is available? Why?\n5.10 (Prefix sums)  Consider the problem of computing the prefix sums ( Example 5.1) of\nn numbers on n processing elements. What is the parallel runtime, speedup, and efficiency\nof this algorithm? Assume that adding two numbers takes one unit of time and that\ncommunicating one number between two processing elements takes 10 units of time. Is\nthe algorithm cost-optimal?\n5.11 Design a cost-optimal", "doc_id": "5d7b809c-3664-4da3-8e01-1570fafc43d0", "embedding": null, "doc_hash": "163ff58d77803b0aa18f757a83b0f9a6de2d6b18746464f8e3f00d1ecd203a10", "extra_info": null, "node_info": {"start": 651148, "end": 654960}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6538ca1c-6389-475d-95e3-0dda8f2aae70", "3": "8d8c0b13-0579-4305-985e-35d71852e463"}}, "__type__": "1"}, "8d8c0b13-0579-4305-985e-35d71852e463": {"__data__": {"text": "TP from Problem 5.5 for p = 1,\n4, 16, 64, 256, 1024, and 4096, what is the largest problem that can be solved if the total\nexecution time is not to exceed 512 time units? In general, is it possible to solve an\narbitrarily large problem in a fixed amount of time, provided that an unlimited number of\nprocessing elements is available? Why?\n5.10 (Prefix sums)  Consider the problem of computing the prefix sums ( Example 5.1) of\nn numbers on n processing elements. What is the parallel runtime, speedup, and efficiency\nof this algorithm? Assume that adding two numbers takes one unit of time and that\ncommunicating one number between two processing elements takes 10 units of time. Is\nthe algorithm cost-optimal?\n5.11 Design a cost-optimal version of the prefix sums algorithm (Problem 5.10) for\ncomputing all prefix-sums of n numbers on p processing elements where p < n. Assuming\nthat adding two numbers takes one unit of time and that communicating one number\nbetween two processing elements takes 10 units of time, derive expressions for TP , S, E ,\ncost, and the isoefficiency function.\n5.12 [GK93a]  Prove that if To \n (p) for a given problem size, then the parallel execution\ntime will continue to decrease as p is increased and will asymptotically approach a\nconstant value. Also prove that if To > Q(p), then TP first decreases and then increases\nwith p; hence, it has a distinct minimum.\n5.13 The parallel runtime of a parallel implementation of the FFT algorithm with p\nprocessing elements is given by TP = (n/p) log n + tw(n/p) log p for an input sequence of\nlength n (Equation 13.4  with ts = 0). The maximum number of processing elements that\nthe algorithm can use for an n-point FFT is n. What are the values of p0 (the value of p that\nsatisfies Equation 5.21 ) and \n  for tw = 10?\n5.14 [ GK93a ] Consider two parallel systems with the same overhead function, but with\ndifferent degrees of concurrency. Let the overhead function of both parallel systems be W\n1/3 p3/2 + 0.1 W 2/3 p. Plot the TP versus p curve for W = 106, and 1 \n  p \n 2048. If the\ndegree of concurrency is W1/3 for the first algorithm and W2/3 for the second algorithm,\ncompute the values of \n  for both parallel systems. Also compute the cost and efficiency\nfor both the parallel systems at the point on the TP versus p curve where their respective\nminimum runtimes are achieved.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 6. Programming Using the\nMessage-Passing Paradigm\nNumerous programming languages and libraries have been developed for explicit parallel\nprogramming. These differ in their view of the address space that they make available to the\nprogrammer, the degree of synchronization imposed on concurrent activities, and the\nmultiplicity of programs. The message-passing programming paradigm  is one of the oldest\nand most widely used approaches for programming parallel computers. Its roots can be traced\nback in the early days of parallel processing and its wide-spread adoption can be attributed to\nthe fact that it imposes minimal requirements on the underlying hardware.\nIn this chapter, we first describe some of the basic concepts of the message-passing\nprogramming paradigm and then explore various message-passing programming techniques\nusing the standard and widely-used Message Passing Interface.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n6.1 Principles of Message-Passing Programming\nThere are two key attributes that characterize the message-passing programming paradigm.\nThe first is that it assumes a partitioned address space and the second is that it supports only\nexplicit parallelization.\nThe logical view of a machine supporting the message-passing paradigm consists of p\nprocesses, each with its own exclusive address space. Instances of such a view", "doc_id": "8d8c0b13-0579-4305-985e-35d71852e463", "embedding": null, "doc_hash": "a2b55e8da24fbc2f348651dab359e5545b863a0d1776d7e66089477db2cca220", "extra_info": null, "node_info": {"start": 654953, "end": 658705}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5d7b809c-3664-4da3-8e01-1570fafc43d0", "3": "66a4aac8-c161-4900-a806-732aabe3645c"}}, "__type__": "1"}, "66a4aac8-c161-4900-a806-732aabe3645c": {"__data__": {"text": "adoption can be attributed to\nthe fact that it imposes minimal requirements on the underlying hardware.\nIn this chapter, we first describe some of the basic concepts of the message-passing\nprogramming paradigm and then explore various message-passing programming techniques\nusing the standard and widely-used Message Passing Interface.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n6.1 Principles of Message-Passing Programming\nThere are two key attributes that characterize the message-passing programming paradigm.\nThe first is that it assumes a partitioned address space and the second is that it supports only\nexplicit parallelization.\nThe logical view of a machine supporting the message-passing paradigm consists of p\nprocesses, each with its own exclusive address space. Instances of such a view come naturally\nfrom clustered workstations and non-shared address space multicomputers. There are two\nimmediate implications of a partitioned address space. First, each data element must belong to\none of the partitions of the space; hence, data must be explicitly partitioned and placed. This\nadds complexity to programming, but encourages locality of access that is critical for achieving\nhigh performance on non-UMA architecture, since a processor can access its local data much\nfaster than non-local data on such architectures. The second implication is that all interactions\n(read-only or read/write) require cooperation of two processes \u2013 the process that has the data\nand the process that wants to access the data. This requirement for cooperation adds a great\ndeal of complexity for a number of reasons. The process that has the data must participate in\nthe interaction even if it has no logical connection to the events at the requesting process. In\ncertain circumstances, this requirement leads to unnatural programs. In particular, for dynamic\nand/or unstructured interactions the complexity of the code written for this type of paradigm\ncan be very high for this reason. However, a primary advantage of explicit two-way interactions\nis that the programmer is fully aware of all the costs of non-local interactions, and is more likely\nto think about algorithms (and mappings) that minimize interactions. Another major advantage\nof this type of programming paradigm is that it can be efficiently implemented on a wide variety\nof architectures.\nThe message-passing programming paradigm requires that the parallelism is coded explicitly by\nthe programmer. That is, the programmer is responsible for analyzing the underlying serial\nalgorithm/application and identifying ways by which he or she can decompose the computations\nand extract concurrency. As a result, programming using the message-passing paradigm tends\nto be hard and intellectually demanding. However, on the other hand, properly written\nmessage-passing programs can often achieve very high performance and scale to a very large\nnumber of processes.\nStructure of Message-Passing Programs  Message-passing programs are often written using\nthe asynchronous  or loosely synchronous  paradigms. In the asynchronous paradigm, all\nconcurrent tasks execute asynchronously. This makes it possible to implement any parallel\nalgorithm. However, such programs can be harder to reason about, and can have non-\ndeterministic behavior due to race conditions. Loosely synchronous programs are a good\ncompromise between these two extremes. In such programs, tasks or subsets of tasks\nsynchronize to perform interactions. However, between these interactions, tasks execute\ncompletely asynchronously. Since the interaction happens synchronously, it is still quite easy to\nreason about the program. Many of the known parallel algorithms can be naturally implemented\nusing loosely synchronous programs.\nIn its most general form, the message-passing paradigm supports execution of a different\nprogram on each of the p processes. This provides the ultimate flexibility in parallel\nprogramming, but makes the job of writing parallel programs effectively unscalable. For this\nreason, most message-passing programs are written using the single program multiple data\n(SPMD) approach. In SPMD programs", "doc_id": "66a4aac8-c161-4900-a806-732aabe3645c", "embedding": null, "doc_hash": "65cea2848bb1db8b1591f2700925062c7ca8b05968a40d754d4314b8743a1183", "extra_info": null, "node_info": {"start": 658645, "end": 662778}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8d8c0b13-0579-4305-985e-35d71852e463", "3": "a8958aca-2dc3-4d83-943b-21a3ff0ef2c2"}}, "__type__": "1"}, "a8958aca-2dc3-4d83-943b-21a3ff0ef2c2": {"__data__": {"text": "are a good\ncompromise between these two extremes. In such programs, tasks or subsets of tasks\nsynchronize to perform interactions. However, between these interactions, tasks execute\ncompletely asynchronously. Since the interaction happens synchronously, it is still quite easy to\nreason about the program. Many of the known parallel algorithms can be naturally implemented\nusing loosely synchronous programs.\nIn its most general form, the message-passing paradigm supports execution of a different\nprogram on each of the p processes. This provides the ultimate flexibility in parallel\nprogramming, but makes the job of writing parallel programs effectively unscalable. For this\nreason, most message-passing programs are written using the single program multiple data\n(SPMD) approach. In SPMD programs the code executed by different processes is identical\nexcept for a small number of processes (e.g., the \"root\" process). This does not mean that the\nprocesses work in lock-step. In an extreme case, even in an SPMD program, each process could\nexecute a different code (the program contains a large case statement with code for each\nprocess). But except for this degenerate case, most processes execute the same code. SPMD\nprograms can be loosely synchronous or completely asynchronous.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n \n6.2 The Building Blocks: Send and Receive Operations\nSince interactions are accomplished by sending and receiving messages, the basic operations in\nthe message-passing programming paradigm are send and receive  . In their simplest form,\nthe prototypes of these operations are defined as follows:\nsend(void *sendbuf, int nelems, int dest) \nreceive(void *recvbuf, int nelems, int source) \nThe sendbuf  points to a buffer that stores the data to be sent, recvbuf  points to a buffer that\nstores the data to be received, nelems is the number of data units to be sent and received, dest\nis the identifier of the process that receives the data, and source is the identifier of the process\nthat sends the data.\nHowever, to stop at this point would be grossly simplifying the programming and performance\nramifications of how these functions are implemented. To motivate the need for further\ninvestigation, let us start with a simple example of a process sending a piece of data to another\nprocess as illustrated in the following code-fragment:\n1         P0                               P1 \n2 \n3         a = 100;                         receive(&a, 1, 0) \n4         send(&a, 1, 1);                  printf(\"%d\\n\", a); \n5         a=0; \nIn this simple example, process P0 sends a message to process P1 which receives and prints the\nmessage. The important thing to note is that process P0 changes the value of a to 0\nimmediately following the send . The semantics of the send operation require that the value\nreceived by process P1 must be 100 as opposed to 0. That is, the value of a at the time of the\nsend operation must be the value that is received by process P1 .\nIt may seem that it is quite straightforward to ensure the semantics of the send and receive\noperations. However, based on how the send and receive operations are implemented this may\nnot be the case. Most message passing platforms have additional hardware support for sending\nand receiving messages. They may support DMA (direct memory access) and asynchronous\nmessage transfer using network interface hardware. Network interfaces allow the transfer of\nmessages from buffer memory to desired location without CPU intervention. Similarly, DMA\nallows copying of data from one memory location to another (e.g., communication buffers)\nwithout CPU support (once they have been programmed). As a result, if the send operation\nprograms the communication", "doc_id": "a8958aca-2dc3-4d83-943b-21a3ff0ef2c2", "embedding": null, "doc_hash": "7cea8f1c7e7d9b92519ebb3a85d171983b81fb01f9b12a73b72e05ae4efd2d3a", "extra_info": null, "node_info": {"start": 662762, "end": 666486}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "66a4aac8-c161-4900-a806-732aabe3645c", "3": "fcee70a3-c249-4d25-b081-a7d124db4bb9"}}, "__type__": "1"}, "fcee70a3-c249-4d25-b081-a7d124db4bb9": {"__data__": {"text": "opposed to 0. That is, the value of a at the time of the\nsend operation must be the value that is received by process P1 .\nIt may seem that it is quite straightforward to ensure the semantics of the send and receive\noperations. However, based on how the send and receive operations are implemented this may\nnot be the case. Most message passing platforms have additional hardware support for sending\nand receiving messages. They may support DMA (direct memory access) and asynchronous\nmessage transfer using network interface hardware. Network interfaces allow the transfer of\nmessages from buffer memory to desired location without CPU intervention. Similarly, DMA\nallows copying of data from one memory location to another (e.g., communication buffers)\nwithout CPU support (once they have been programmed). As a result, if the send operation\nprograms the communication hardware and returns before the communication operation has\nbeen accomplished, process P1 might receive the value 0 in a instead of 100!\nWhile this is undesirable, there are in fact reasons for supporting such send operations for\nperformance reasons. In the rest of this section, we will discuss send and receive operations in\nthe context of such a hardware environment, and motivate various implementation details and\nmessage-passing protocols that help in ensuring the semantics of the send and receive\noperations.\n6.2.1 Blocking Message Passing Operations\nA simple solution to the dilemma presented in the code fragment above is for the send\noperation to return only when it is semantically safe to do so. Note that this is not the same as\nsaying that the send operation returns only after the receiver has received the data. It simply\nmeans that the sending operation blocks until it can guarantee that the semantics will not be\nviolated on return irrespective of what happens in the program subsequently. There are two\nmechanisms by which this can be achieved.\nBlocking Non-Buffered Send/Receive\nIn the first case, the send operation does not return until the matching receive has been\nencountered at the receiving process. When this happens, the message is sent and the send\noperation returns upon completion of the communication operation. Typically, this process\ninvolves a handshake between the sending and receiving processes. The sending process sends\na request to communicate to the receiving process. When the receiving process encounters the\ntarget receive, it responds to the request. The sending process upon receiving this response\ninitiates a transfer operation. The operation is illustrated in Figure 6.1  . Since there are no\nbuffers used at either sending or receiving ends, this is also referred to as a non-buffered\nblocking operation  .\nFigure 6.1. Handshake for a blocking non-buffered send/receive\noperation. It is easy to see that in cases where sender and receiver do\nnot reach communication point at similar times, there can be\nconsiderable idling overheads.\nIdling Overheads in Blocking Non-Buffered Operations  In Figure 6.1  , we illustrate three\nscenarios in which the send is reached before the receive is posted, the send and receive are\nposted around the same time, and the receive is posted before the send is reached. In cases (a)\nand (c), we notice that there is considerable idling at the sending and receiving process. It is\nalso clear from the figures that a blocking non-buffered protocol is suitable when the send and\nreceive are posted at roughly the same time. However, in an asynchronous environment, this\nmay be impossible to predict. This idling overhead is one of the major drawbacks of this\nprotocol.\nDeadlocks in Blocking Non-Buffered Operations  Consider the following simple exchange of\nmessages that can lead to a deadlock:\n1          P0                               P1 \n2 \n3          send(&a, 1, 1);                  send(&a, 1, 0); \n4          receive(&b, 1,", "doc_id": "fcee70a3-c249-4d25-b081-a7d124db4bb9", "embedding": null, "doc_hash": "6520c7dd57d2f26698aba15466d8cc107b1e943a9d87b277625450465619ccff", "extra_info": null, "node_info": {"start": 666442, "end": 670329}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a8958aca-2dc3-4d83-943b-21a3ff0ef2c2", "3": "dae754a0-1a1a-4e6f-9641-86af3214d9d6"}}, "__type__": "1"}, "dae754a0-1a1a-4e6f-9641-86af3214d9d6": {"__data__": {"text": "protocol is suitable when the send and\nreceive are posted at roughly the same time. However, in an asynchronous environment, this\nmay be impossible to predict. This idling overhead is one of the major drawbacks of this\nprotocol.\nDeadlocks in Blocking Non-Buffered Operations  Consider the following simple exchange of\nmessages that can lead to a deadlock:\n1          P0                               P1 \n2 \n3          send(&a, 1, 1);                  send(&a, 1, 0); \n4          receive(&b, 1, 1);               receive(&b, 1, 0); \nThe code fragment makes the values of a available to both processes P0 and P1. However, if the\nsend and receive operations are implemented using a blocking non-buffered protocol, the send\nat P0 waits for the matching receive at P1 whereas the send at process P1 waits for the\ncorresponding receive at P0, resulting in an infinite wait.\nAs can be inferred, deadlocks are very easy in blocking protocols and care must be taken to\nbreak cyclic waits of the nature outlined. In the above example, this can be corrected by\nreplacing the operation sequence of one of the processes by a receive  and a send as opposed\nto the other way around. This often makes the code more cumbersome and buggy.\nBlocking Buffered Send/Receive\nA simple solution to the idling and deadlocking problem outlined above is to rely on buffers at\nthe sending and receiving ends. We start with a simple case in which the sender has a buffer\npre-allocated for communicating messages. On encountering a send operation, the sender\nsimply copies the data into the designated buffer and returns after the copy operation has been\ncompleted. The sender process can now continue with the program knowing that any changes to\nthe data will not impact program semantics. The actual communication can be accomplished in\nmany ways depending on the available hardware resources. If the hardware supports\nasynchronous communication (independent of the CPU), then a network transfer can be\ninitiated after the message has been copied into the buffer. Note that at the receiving end, the\ndata cannot be stored directly at the target location since this would violate program semantics.\nInstead, the data is copied into a buffer at the receiver as well. When the receiving process\nencounters a receive operation, it checks to see if the message is available in its receive buffer.\nIf so, the data is copied into the target location. This operation is illustrated in Figure 6.2(a)  .\nFigure 6.2. Blocking buffered transfer protocols: (a) in the presence of\ncommunication hardware with buffers at send and receive ends; and\n(b) in the absence of communication hardware, sender interrupts\nreceiver and deposits data in buffer at receiver end.\nIn the protocol illustrated above, buffers are used at both sender and receiver and\ncommunication is handled by dedicated hardware. Sometimes machines do not have such\ncommunication hardware. In this case, some of the overhead can be saved by buffering only on\none side. For example, on encountering a send operation, the sender interrupts the receiver,\nboth processes participate in a communication operation and the message is deposited in a\nbuffer at the receiver end. When the receiver eventually encounters a receive operation, the\nmessage is copied from the buffer into the target location. This protocol is illustrated in Figure\n6.2(b)  . It is not difficult to conceive a protocol in which the buffering is done only at the sender\nand the receiver initiates a transfer by interrupting the sender.\nIt is easy to see that buffered protocols alleviate idling overheads at the cost of adding buffer\nmanagement overheads. In general, if the parallel program is highly synchronous (i.e.,", "doc_id": "dae754a0-1a1a-4e6f-9641-86af3214d9d6", "embedding": null, "doc_hash": "4c1efc717e06e6c4d9e6a9b26bd325d4d3668ea77b43694b914474093510b55f", "extra_info": null, "node_info": {"start": 670717, "end": 674428}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "fcee70a3-c249-4d25-b081-a7d124db4bb9", "3": "861c3b80-aa6f-43d5-8b49-66f5744a746d"}}, "__type__": "1"}, "861c3b80-aa6f-43d5-8b49-66f5744a746d": {"__data__": {"text": "some of the overhead can be saved by buffering only on\none side. For example, on encountering a send operation, the sender interrupts the receiver,\nboth processes participate in a communication operation and the message is deposited in a\nbuffer at the receiver end. When the receiver eventually encounters a receive operation, the\nmessage is copied from the buffer into the target location. This protocol is illustrated in Figure\n6.2(b)  . It is not difficult to conceive a protocol in which the buffering is done only at the sender\nand the receiver initiates a transfer by interrupting the sender.\nIt is easy to see that buffered protocols alleviate idling overheads at the cost of adding buffer\nmanagement overheads. In general, if the parallel program is highly synchronous (i.e., sends\nand receives are posted around the same time), non-buffered sends may perform better than\nbuffered sends. However, in general applications, this is not the case and buffered sends are\ndesirable unless buffer capacity becomes an issue.\nExample 6.1 Impact of finite buffers in message passing\nConsider the following code fragment:\n1         P0                                   P1 \n2 \n3         for (i = 0; i < 1000; i++) {         for (i = 0; i < 1000; i++) { \n4           produce_data(&a);                    receive(&a, 1, 0); \n5           send(&a, 1, 1);                      consume_data(&a); \n6         }                                    } \nIn this code fragment, process P0 produces 1000 data items and process P1 consumes\nthem. However, if process P1 was slow getting to this loop, process P0 might have sent all of\nits data. If there is enough buffer space, then both processes can proceed; however, if the\nbuffer is not sufficient (i.e., buffer overflow), the sender would have to be blocked until\nsome of the corresponding receive operations had been posted, thus freeing up buffer space.\nThis can often lead to unforeseen overheads and performance degradation. In general, it is\na good idea to write programs that have bounded buffer requirements. \nDeadlocks in Buffered Send and Receive Operations  While buffering alleviates many of the\ndeadlock situations, it is still possible to write code that deadlocks. This is due to the fact that as\nin the non-buffered case, receive calls are always blocking (to ensure semantic consistency).\nThus, a simple code fragment such as the following deadlocks since both processes wait to\nreceive data but nobody sends it.\n1         P0                                  P1 \n2 \n3         receive(&a, 1, 1);                  receive(&a, 1, 0); \n4         send(&b, 1, 1);                     send(&b, 1, 0); \nOnce again, such circular waits have to be broken. However, deadlocks are caused only by waits\non receive operations in this case.\n6.2.2 Non-Blocking Message Passing Operations\nIn blocking protocols, the overhead of guaranteeing semantic correctness was paid in the form\nof idling (non-buffered) or buffer management (buffered). Often, it is possible to require", "doc_id": "861c3b80-aa6f-43d5-8b49-66f5744a746d", "embedding": null, "doc_hash": "c87b5612d72efb8f8914af6b204d535865bba120a522d7672b025005c021e8d0", "extra_info": null, "node_info": {"start": 674121, "end": 677127}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "dae754a0-1a1a-4e6f-9641-86af3214d9d6", "3": "425ec74a-64f8-4a7c-b713-487cf73c3b9b"}}, "__type__": "1"}, "425ec74a-64f8-4a7c-b713-487cf73c3b9b": {"__data__": {"text": "      P1 \n2 \n3         receive(&a, 1, 1);                  receive(&a, 1, 0); \n4         send(&b, 1, 1);                     send(&b, 1, 0); \nOnce again, such circular waits have to be broken. However, deadlocks are caused only by waits\non receive operations in this case.\n6.2.2 Non-Blocking Message Passing Operations\nIn blocking protocols, the overhead of guaranteeing semantic correctness was paid in the form\nof idling (non-buffered) or buffer management (buffered). Often, it is possible to require the\nprogrammer to ensure semantic correctness and provide a fast send/receive operation that\nincurs little overhead. This class of non-blocking protocols returns from the send or receive\noperation before it is semantically safe to do so. Consequently, the user must be careful not to\nalter data that may be potentially participating in a communication operation. Non-blocking\noperations are generally accompanied by a check-status  operation, which indicates whether\nthe semantics of a previously initiated transfer may be violated or not. Upon return from a non-\nblocking send or receive operation, the process is free to perform any computation that does not\ndepend upon the completion of the operation. Later in the program, the process can check\nwhether or not the non-blocking operation has completed, and, if necessary, wait for its\ncompletion.\nAs illustrated in Figure 6.3  , non-blocking operations can themselves be buffered or non-\nbuffered. In the non-buffered case, a process wishing to send data to another simply posts a\npending message and returns to the user program. The program can then do other useful work.\nAt some point in the future, when the corresponding receive is posted, the communication\noperation is initiated. When this operation is completed, the check-status operation indicates\nthat it is safe for the programmer to touch this data. This transfer is indicated in Figure 6.4(a)  .\nFigure 6.3. Space of possible protocols for send and receive\noperations.\nFigure 6.4. Non-blocking non-buffered send and receive operations (a)\nin absence of communication hardware; (b) in presence of\ncommunication hardware.\nComparing Figures 6.4(a)  and 6.1(a)  , it is easy to see that the idling time when the process is\nwaiting for the corresponding receive in a blocking operation can now be utilized for\ncomputation, provided it does not update the data being sent. This alleviates the major\nbottleneck associated with the former at the expense of some program restructuring. The\nbenefits of non-blocking operations are further enhanced by the presence of dedicated\ncommunication hardware. This is illustrated in Figure 6.4(b)  . In this case, the communication\noverhead can be almost entirely masked by non-blocking operations. In this case, however, the\ndata being received is unsafe for the duration of the receive operation.\nNon-blocking operations can also be used with a buffered protocol. In this case, the sender\ninitiates a DMA operation and returns immediately. The data becomes safe the moment the\nDMA operation has been completed. At the receiving end, the receive operation initiates a\ntransfer from the sender's buffer to the receiver's target location. Using buffers with non-\nblocking operation has the effect of reducing the time during which the data is unsafe.\nTypical message-passing libraries such as Message Passing Interface (MPI) and Parallel Virtual\nMachine (PVM) implement both blocking and non-blocking operations. Blocking operations\nfacilitate safe and easier programming and non-blocking operations are useful for performance\noptimization by masking communication overhead. One must, however, be careful using non-\nblocking protocols since errors can result from unsafe access to data that is in the process of\nbeing communicated.\n[ Team LiB ]\n", "doc_id": "425ec74a-64f8-4a7c-b713-487cf73c3b9b", "embedding": null, "doc_hash": "c87a93432b1a49563ed2c4dd88fa27c27f5c4606755eb304928b02b8ef236625", "extra_info": null, "node_info": {"start": 677412, "end": 681208}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "861c3b80-aa6f-43d5-8b49-66f5744a746d", "3": "04079a44-a0c5-40c1-a670-090ab6eb22bc"}}, "__type__": "1"}, "04079a44-a0c5-40c1-a670-090ab6eb22bc": {"__data__": {"text": "and returns immediately. The data becomes safe the moment the\nDMA operation has been completed. At the receiving end, the receive operation initiates a\ntransfer from the sender's buffer to the receiver's target location. Using buffers with non-\nblocking operation has the effect of reducing the time during which the data is unsafe.\nTypical message-passing libraries such as Message Passing Interface (MPI) and Parallel Virtual\nMachine (PVM) implement both blocking and non-blocking operations. Blocking operations\nfacilitate safe and easier programming and non-blocking operations are useful for performance\noptimization by masking communication overhead. One must, however, be careful using non-\nblocking protocols since errors can result from unsafe access to data that is in the process of\nbeing communicated.\n[ Team LiB ]\n \n\n[ Team LiB ]\n \n6.3 MPI: the Message Passing Interface\nMany early generation commercial parallel computers were based on the message-passing\narchitecture due to its lower cost relative to shared-address-space architectures. Since\nmessage-passing is the natural programming paradigm for these machines, this resulted in the\ndevelopment of many different message-passing libraries. In fact, message-passing became the\nmodern-age form of assembly language, in which every hardware vendor provided its own\nlibrary, that performed very well on its own hardware, but was incompatible with the parallel\ncomputers offered by other vendors. Many of the differences between the various vendor-\nspecific message-passing libraries were only syntactic; however, often enough there were some\nserious semantic differences that required significant re-engineering to port a message-passing\nprogram from one library to another.\nThe message-passing interface, or MPI as it is commonly known, was created to essentially\nsolve this problem. MPI defines a standard library for message-passing that can be used to\ndevelop portable message-passing programs using either C or Fortran. The MPI standard\ndefines both the syntax as well as the semantics of a core set of library routines that are very\nuseful in writing message-passing programs. MPI was developed by a group of researchers from\nacademia and industry, and has enjoyed wide support by almost all the hardware vendors.\nVendor implementations of MPI are available on almost all commercial parallel computers.\nThe MPI library contains over 125 routines, but the number of key concepts is much smaller. In\nfact, it is possible to write fully-functional message-passing programs by using only the six\nroutines shown in Table 6.1 . These routines are used to initialize and terminate the MPI library,\nto get information about the parallel computing environment, and to send and receive\nmessages.\nIn this section we describe these routines as well as some basic concepts that are essential in\nwriting correct and efficient message-passing programs using MPI.\nMPI_Init\nInitializes MPI.\nMPI_Finalize\nTerminates MPI.\nMPI_Comm_size\nDetermines the number of processes.\nMPI_Comm_rank\nDetermines the label of the calling process.\nMPI_Send\nSends a message.\nMPI_Recv\nReceives a message.\nTable 6.1. The minimal set of MPI routines.\n6.3.1 Starting and Terminating the MPI Library\nMPI_Init  is called prior to any calls to other MPI routines. Its purpose is to initialize the MPI\nenvironment. Calling MPI_Init  more than once during the execution of a program will lead to\nan error. MPI_Finalize  is called at the end of the computation, and it performs various clean-\nup tasks to terminate the MPI environment. No MPI calls may be performed after MPI_Finalize\nhas been called, not even MPI_Init  . Both MPI_Init  and MPI_Finalize  must be called by all\nthe processes, otherwise MPI's behavior will be undefined. The exact calling sequences of these\ntwo routines for C are as follows:\nint MPI_Init(int *argc, char ***argv) \nint", "doc_id": "04079a44-a0c5-40c1-a670-090ab6eb22bc", "embedding": null, "doc_hash": "7cc0231168e7d03507c3e91fb9158b9fce0990d70f80236171dbad3fe07aa68d", "extra_info": null, "node_info": {"start": 680872, "end": 684747}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "425ec74a-64f8-4a7c-b713-487cf73c3b9b", "3": "c185ca05-fbcb-4496-94c8-1d49eac6315a"}}, "__type__": "1"}, "c185ca05-fbcb-4496-94c8-1d49eac6315a": {"__data__": {"text": "the MPI Library\nMPI_Init  is called prior to any calls to other MPI routines. Its purpose is to initialize the MPI\nenvironment. Calling MPI_Init  more than once during the execution of a program will lead to\nan error. MPI_Finalize  is called at the end of the computation, and it performs various clean-\nup tasks to terminate the MPI environment. No MPI calls may be performed after MPI_Finalize\nhas been called, not even MPI_Init  . Both MPI_Init  and MPI_Finalize  must be called by all\nthe processes, otherwise MPI's behavior will be undefined. The exact calling sequences of these\ntwo routines for C are as follows:\nint MPI_Init(int *argc, char ***argv) \nint MPI_Finalize() \nThe arguments argc and argv of MPI_Init  are the command-line arguments of the C program.\nAn MPI implementation is expected to remove from the argv array any command-line\narguments that should be processed by the implementation before returning back to the\nprogram, and to decrement argc accordingly. Thus, command-line processing should be\nperformed only after MPI_Init  has been called. Upon successful execution, MPI_Init  and\nMPI_Finalize  return MPI_SUCCESS ; otherwise they return an implementation-defined error\ncode.\nThe bindings and calling sequences of these two functions are illustrative of the naming\npractices and argument conventions followed by MPI. All MPI routines, data-types, and\nconstants are prefixed by \" MPI_ \". The return code for successful completion is MPI_SUCCESS .\nThis and other MPI constants and data-structures are defined for C in the file \"mpi.h\"  . This\nheader file must be included in each MPI program.\n6.3.2 Communicators\nA key concept used throughout MPI is that of the communication domain  . A communication\ndomain is a set of processes that are allowed to communicate with each other. Information\nabout communication domains is stored in variables of type MPI_Comm  ,that are called\ncommunicators  . These communicators are used as arguments to all message transfer MPI\nroutines and they uniquely identify the processes participating in the message transfer\noperation. Note that each process can belong to many different (possibly overlapping)\ncommunication domains.\nThe communicator is used to define a set of processes that can communicate with each other.\nThis set of processes form a communication domain  . In general, all the processes may need\nto communicate with each other. For this reason, MPI defines a default communicator called\nMPI_COMM_WORLD  which includes all the processes involved in the parallel execution. However, in\nmany cases we want to perform communication only within (possibly overlapping) groups of\nprocesses. By using a different communicator for each such group, we can ensure that no\nmessages will ever interfere with messages destined to any other group. How to create and use\nsuch communicators is described at a later point in this chapter. For now, it suffices to use\nMPI_COMM_WORLD  as the communicator argument to all the MPI functions that require a\ncommunicator.\n6.3.3 Getting Information\nThe MPI_Comm_size  and MPI_Comm_rank  functions are used to determine the number of\nprocesses and the label of the calling process, respectively. The calling sequences of these\nroutines are as follows:\nint MPI_Comm_size(MPI_Comm comm, int *size) \nint MPI_Comm_rank(MPI_Comm comm, int *rank) \nThe function MPI_Comm_size  returns in the variable size the number of processes that belong\nto the communicator comm . So, when there is a single process per processor, the call\nMPI_Comm_size(MPI_COMM_WORLD, &size)  will return in size the number of processors used\nby the program. Every process that belongs to a communicator is uniquely identified by its rank\n. The rank of a process is an integer that ranges from zero up to the size of the communicator\nminus one. A process can determine", "doc_id": "c185ca05-fbcb-4496-94c8-1d49eac6315a", "embedding": null, "doc_hash": "14dbf345bcb49e32cc08d50322cabe5ac55291c7cca760ea57d1e55bd8a88e6d", "extra_info": null, "node_info": {"start": 684909, "end": 688743}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "04079a44-a0c5-40c1-a670-090ab6eb22bc", "3": "7ed6bd9b-9036-48fa-956a-3be468c826e7"}}, "__type__": "1"}, "7ed6bd9b-9036-48fa-956a-3be468c826e7": {"__data__": {"text": "the number of\nprocesses and the label of the calling process, respectively. The calling sequences of these\nroutines are as follows:\nint MPI_Comm_size(MPI_Comm comm, int *size) \nint MPI_Comm_rank(MPI_Comm comm, int *rank) \nThe function MPI_Comm_size  returns in the variable size the number of processes that belong\nto the communicator comm . So, when there is a single process per processor, the call\nMPI_Comm_size(MPI_COMM_WORLD, &size)  will return in size the number of processors used\nby the program. Every process that belongs to a communicator is uniquely identified by its rank\n. The rank of a process is an integer that ranges from zero up to the size of the communicator\nminus one. A process can determine its rank in a communicator by using the MPI_Comm_rank\nfunction that takes two arguments: the communicator and an integer variable rank . Up on\nreturn, the variable rank stores the rank of the process. Note that each process that calls either\none of these functions must belong in the supplied communicator, otherwise an error will occur.\nExample 6.2 Hello World\nWe can use the four MPI functions just described to write a program that prints out a\n\"Hello World\" message from each processor.\n1   #include <mpi.h> \n2 \n3   main(int argc, char *argv[]) \n4   { \n5     int npes, myrank; \n6 \n7     MPI_Init(&argc, &argv); \n8     MPI_Comm_size(MPI_COMM_WORLD, &npes); \n9     MPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n10    printf(\"From process %d out of %d, Hello World!\\n\", \n11            myrank, npes); \n12    MPI_Finalize(); \n13  } \n6.3.4 Sending and Receiving Messages\nThe basic functions for sending and receiving messages in MPI are the MPI_Send  and MPI_Recv\n, respectively. The calling sequences of these routines are as follows:\nint MPI_Send(void *buf, int count, MPI_Datatype datatype, \n        int dest, int tag, MPI_Comm comm) \nint MPI_Recv(void *buf, int count, MPI_Datatype datatype, \n        int source, int tag, MPI_Comm comm, MPI_Status *status) \nMPI_Send  sends the data stored in the buffer pointed by buf . This buffer consists of consecutive\nentries of the type specified by the parameter datatype  . The number of entries in the buffer is\ngiven by the parameter count  . The correspondence between MPI datatypes and those provided\nby C is shown in Table 6.2  . Note that for all C datatypes, an equivalent MPI datatype is\nprovided. However, MPI allows two additional datatypes that are not part of the C language.\nThese are MPI_BYTE  and MPI_PACKED  .\nMPI_BYTE  corresponds to a byte (8 bits) and MPI_PACKED  corresponds to a collection of data\nitems that has been created by packing non-contiguous data. Note that the length of the\nmessage in MPI_Send  , as well as in other MPI routines, is specified in terms of the number of\nentries being sent and not in terms of the number of bytes. Specifying the length in terms of the\nnumber of entries has the advantage of making the MPI code portable, since the number of\nbytes used to store various datatypes can be different for different architectures.\nThe destination of the message sent by MPI_Send  is uniquely specified by the dest and comm\narguments. The dest argument is the rank of the destination process in the communication\ndomain specified by the communicator comm . Each message has an integer-valued tag\nassociated with it. This is used to distinguish different types of messages. The message- tag", "doc_id": "7ed6bd9b-9036-48fa-956a-3be468c826e7", "embedding": null, "doc_hash": "694f7b8c5ca464dbac4d86249f99f262c0ec512d86950859d48ecccdfcda5f38", "extra_info": null, "node_info": {"start": 688692, "end": 692077}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c185ca05-fbcb-4496-94c8-1d49eac6315a", "3": "9e416e6c-3310-4159-800e-87f59f678829"}}, "__type__": "1"}, "9e416e6c-3310-4159-800e-87f59f678829": {"__data__": {"text": "corresponds to a collection of data\nitems that has been created by packing non-contiguous data. Note that the length of the\nmessage in MPI_Send  , as well as in other MPI routines, is specified in terms of the number of\nentries being sent and not in terms of the number of bytes. Specifying the length in terms of the\nnumber of entries has the advantage of making the MPI code portable, since the number of\nbytes used to store various datatypes can be different for different architectures.\nThe destination of the message sent by MPI_Send  is uniquely specified by the dest and comm\narguments. The dest argument is the rank of the destination process in the communication\ndomain specified by the communicator comm . Each message has an integer-valued tag\nassociated with it. This is used to distinguish different types of messages. The message- tag can\ntake values ranging from zero up to the MPI defined constant MPI_TAG_UB  . Even though the\nvalue of MPI_TAG_UB  is implementation specific, it is at least 32,767.\nMPI_Recv  receives a message sent by a process whose rank is given by the source in the\ncommunication domain specified by the comm argument. The tag of the sent message must be\nthat specified by the tag argument. If there are many messages with identical tag from the\nsame process, then any one of these messages is received. MPI allows specification of wildcard\narguments for both source and tag . If source is set to MPI_ANY_SOURCE  , then any process of\nthe communication domain can be the source of the message. Similarly, if tag is set to\nMPI_ANY_TAG , then messages with any tag are accepted. The received message is stored in\ncontinuous locations in the buffer pointed to by buf . The count  and datatype  arguments of\nMPI_Recv  are used to specify the length of the supplied buffer. The received message should be\nof length equal to or less than this length. This allows the receiving process to not know the\nexact size of the message being sent. If the received message is larger than the supplied buffer,\nthen an overflow error will occur, and the routine will return the error MPI_ERR_TRUNCATE .\nMPI_CHAR\nsigned char\nMPI_SHORT\nsigned short int\nMPI_INT\nsigned int\nMPI_LONG\nsigned long int\nMPI_UNSIGNED_CHAR\nunsigned char\nMPI_UNSIGNED_SHORT\nunsigned short int\nMPI_UNSIGNED\nunsigned int\nMPI_UNSIGNED_LONG\nunsigned long int\nMPI_FLOAT\nfloat\nMPI_DOUBLE\ndouble\nMPI_LONG_DOUBLE\nlong double\nMPI_BYTE\n\u00a0\nMPI_PACKED\n\u00a0\nTable 6.2. Correspondence between the datatypes supported by MPI\nand those supported by C.MPI Datatype C Datatype\nAfter a message has been received, the status variable can be used to get information about\nthe MPI_Recv  operation. In C, status is stored using the MPI_Status  data-structure. This is\nimplemented as a structure with three fields, as follows:\ntypedef struct MPI_Status { \n  int MPI_SOURCE; \n  int MPI_TAG; \n  int MPI_ERROR; \n}; \nMPI_SOURCE  and MPI_TAG  store the source and the tag of the received message. They are\nparticularly useful when MPI_ANY_SOURCE  and MPI_ANY_TAG are used for the source and tag\narguments. MPI_ERROR  stores the error-code of the received message.\nThe status argument also returns information about the length of the received message. This\ninformation is not directly accessible from the status variable, but it can be retrieved by calling\nthe MPI_Get_count  function. The calling sequence of this function is as follows:\nint MPI_Get_count(MPI_Status *status, MPI_Datatype datatype, \n        int *count) \nMPI_Get_count  takes as arguments the status returned by MPI_Recv  and the type of the\nreceived data in datatype  , and returns the number of entries that were actually received in the\ncount  variable.\nThe", "doc_id": "9e416e6c-3310-4159-800e-87f59f678829", "embedding": null, "doc_hash": "1d32bdb1260ec86f768b7b83731814c826f974bec8f416578d78bf632d6267b1", "extra_info": null, "node_info": {"start": 691974, "end": 695659}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7ed6bd9b-9036-48fa-956a-3be468c826e7", "3": "d9876c2f-258c-4025-956e-ef15b2291d87"}}, "__type__": "1"}, "d9876c2f-258c-4025-956e-ef15b2291d87": {"__data__": {"text": "useful when MPI_ANY_SOURCE  and MPI_ANY_TAG are used for the source and tag\narguments. MPI_ERROR  stores the error-code of the received message.\nThe status argument also returns information about the length of the received message. This\ninformation is not directly accessible from the status variable, but it can be retrieved by calling\nthe MPI_Get_count  function. The calling sequence of this function is as follows:\nint MPI_Get_count(MPI_Status *status, MPI_Datatype datatype, \n        int *count) \nMPI_Get_count  takes as arguments the status returned by MPI_Recv  and the type of the\nreceived data in datatype  , and returns the number of entries that were actually received in the\ncount  variable.\nThe MPI_Recv  returns only after the requested message has been received and copied into the\nbuffer. That is, MPI_Recv  is a blocking receive operation. However, MPI allows two different\nimplementations for MPI_Send  . In the first implementation, MPI_Send  returns only after the\ncorresponding MPI_Recv  have been issued and the message has been sent to the receiver. In\nthe second implementation, MPI_Send  first copies the message into a buffer and then returns,\nwithout waiting for the corresponding MPI_Recv  to be executed. In either implementation, the\nbuffer that is pointed by the buf argument of MPI_Send  can be safely reused and overwritten.\nMPI programs must be able to run correctly regardless of which of the two methods is used for\nimplementing MPI_Send  . Such programs are called safe . In writing safe MPI programs,\nsometimes it is helpful to forget about the alternate implementation of MPI_Send  and just think\nof it as being a blocking send operation.\nAvoiding Deadlocks  The semantics of MPI_Send  and MPI_Recv  place some restrictions on how\nwe can mix and match send and receive operations. For example, consider the following piece\nof code in which process 0 sends two messages with different tags to process 1, and process 1\nreceives them in the reverse order.\n1   int a[10], b[10], myrank; \n2   MPI_Status status; \n3   ... \n4   MPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n5   if (myrank == 0) { \n6     MPI_Send(a, 10, MPI_INT, 1, 1, MPI_COMM_WORLD); \n7     MPI_Send(b, 10, MPI_INT, 1, 2, MPI_COMM_WORLD); \n8   } \n9   else if (myrank == 1) { \n10    MPI_Recv(b, 10, MPI_INT, 0, 2, MPI_COMM_WORLD); \n11    MPI_Recv(a, 10, MPI_INT, 0, 1, MPI_COMM_WORLD); \n12  } \n13  ... \nIf MPI_Send  is implemented using buffering, then this code will run correctly provided that\nsufficient buffer space is available. However, if MPI_Send  is implemented by blocking until the\nmatching receive has been issued, then neither of the two processes will be able to proceed.\nThis is because process zero (i.e., myrank == 0  ) will wait until process one issues the matching\nMPI_Recv  (i.e., the one with tag equal to 1), and at the same time process one will wait until\nprocess zero performs the matching MPI_Send  (i.e., the one with tag equal to 2). This code\nfragment is not safe, as its behavior is implementation dependent. It is up to the programmer\nto ensure that his or her program will run correctly on any MPI implementation. The problem in\nthis program can be corrected by matching the order in which the send and receive operations\nare issued  . Similar deadlock situations can also occur when a process sends a message to itself.\nEven though this is legal, its behavior is implementation", "doc_id": "d9876c2f-258c-4025-956e-ef15b2291d87", "embedding": null, "doc_hash": "057023e3eb03eb6771dd54208fd3bad211a930d1d211bea43ce5f9ccfe589487", "extra_info": null, "node_info": {"start": 695774, "end": 699179}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9e416e6c-3310-4159-800e-87f59f678829", "3": "be7782c5-393d-43f2-9642-c4dffa73c03f"}}, "__type__": "1"}, "be7782c5-393d-43f2-9642-c4dffa73c03f": {"__data__": {"text": "of the two processes will be able to proceed.\nThis is because process zero (i.e., myrank == 0  ) will wait until process one issues the matching\nMPI_Recv  (i.e., the one with tag equal to 1), and at the same time process one will wait until\nprocess zero performs the matching MPI_Send  (i.e., the one with tag equal to 2). This code\nfragment is not safe, as its behavior is implementation dependent. It is up to the programmer\nto ensure that his or her program will run correctly on any MPI implementation. The problem in\nthis program can be corrected by matching the order in which the send and receive operations\nare issued  . Similar deadlock situations can also occur when a process sends a message to itself.\nEven though this is legal, its behavior is implementation dependent and must be avoided.\nImproper use of MPI_Send  and MPI_Recv  can also lead to deadlocks in situations when each\nprocessor needs to send and receive a message in a circular fashion. Consider the following\npiece of code, in which process i sends a message to process i + 1 (modulo the number of\nprocesses) and receives a message from process i - 1 (module the number of processes).\n1   int a[10], b[10], npes, myrank; \n2   MPI_Status status; \n3   ... \n4   MPI_Comm_size(MPI_COMM_WORLD, &npes); \n5   MPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n6   MPI_Send(a, 10, MPI_INT, (myrank+1)%npes, 1, MPI_COMM_WORLD); \n7   MPI_Recv(b, 10, MPI_INT, (myrank-1+npes)%npes, 1, MPI_COMM_WORLD); \n8   ... \nWhen MPI_Send  is implemented using buffering, the program will work correctly, since every\ncall to MPI_Send  will get buffered, allowing the call of the MPI_Recv  to be performed, which will\ntransfer the required data. However, if MPI_Send  blocks until the matching receive has been\nissued, all processes will enter an infinite wait state, waiting for the neighboring process to\nissue a MPI_Recv  operation. Note that the deadlock still remains even when we have only two\nprocesses. Thus, when pairs of processes need to exchange data, the above method leads to an\nunsafe program. The above example can be made safe, by rewriting it as follows:\n1   int a[10], b[10], npes, myrank; \n2   MPI_Status status; \n3   ... \n4   MPI_Comm_size(MPI_COMM_WORLD, &npes); \n5   MPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n6   if (myrank%2 == 1) { \n7     MPI_Send(a, 10, MPI_INT, (myrank+1)%npes, 1, MPI_COMM_WORLD); \n8     MPI_Recv(b, 10, MPI_INT, (myrank-1+npes)%npes, 1, MPI_COMM_WORLD); \n9   } \n10  else { \n11    MPI_Recv(b, 10, MPI_INT, (myrank-1+npes)%npes, 1, MPI_COMM_WORLD); \n12    MPI_Send(a, 10, MPI_INT, (myrank+1)%npes, 1, MPI_COMM_WORLD); \n13  } \n14  ... \nThis new implementation partitions the processes into two groups. One consists of the odd-\nnumbered processes and the other of the even-numbered processes. The odd-numbered\nprocesses perform a send followed by a receive, and the even-numbered processes perform a\nreceive followed by a send. Thus, when an odd-numbered process calls MPI_Send  ,the target\nprocess", "doc_id": "be7782c5-393d-43f2-9642-c4dffa73c03f", "embedding": null, "doc_hash": "08982a305139379da277d374187b4aa8514cf159b883435c3cacb5e065a704cf", "extra_info": null, "node_info": {"start": 699138, "end": 702119}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d9876c2f-258c-4025-956e-ef15b2291d87", "3": "6278ed83-2ce8-4bd7-a340-2a749944d25c"}}, "__type__": "1"}, "6278ed83-2ce8-4bd7-a340-2a749944d25c": {"__data__": {"text": "\n9   } \n10  else { \n11    MPI_Recv(b, 10, MPI_INT, (myrank-1+npes)%npes, 1, MPI_COMM_WORLD); \n12    MPI_Send(a, 10, MPI_INT, (myrank+1)%npes, 1, MPI_COMM_WORLD); \n13  } \n14  ... \nThis new implementation partitions the processes into two groups. One consists of the odd-\nnumbered processes and the other of the even-numbered processes. The odd-numbered\nprocesses perform a send followed by a receive, and the even-numbered processes perform a\nreceive followed by a send. Thus, when an odd-numbered process calls MPI_Send  ,the target\nprocess (which has an even number) will call MPI_Recv  to receive that message, before\nattempting to send its own message.\nSending and Receiving Messages Simultaneously  The above communication pattern\nappears frequently in many message-passing programs, and for this reason MPI provides the\nMPI_Sendrecv  function that both sends and receives a message.\nMPI_Sendrecv  does not suffer from the circular deadlock problems of MPI_Send  and MPI_Recv  .\nYou can think of MPI_Sendrecv  as allowing data to travel for both send and receive\nsimultaneously. The calling sequence of MPI_Sendrecv  is the following:\nint MPI_Sendrecv(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, int dest, int sendtag, \n        void *recvbuf, int recvcount, MPI_Datatype recvdatatype, \n        int source, int recvtag, MPI_Comm comm, \n        MPI_Status *status) \nThe arguments of MPI_Sendrecv  are essentially the combination of the arguments of MPI_Send\nand MPI_Recv  . The send and receive buffers must be disjoint, and the source and destination of\nthe messages can be the same or different. The safe version of our earlier example using\nMPI_Sendrecv  is as follows.\n1   int a[10], b[10], npes, myrank; \n2   MPI_Status status; \n3   ... \n4   MPI_Comm_size(MPI_COMM_WORLD, &npes); \n5   MPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n6   MPI_SendRecv(a, 10, MPI_INT, (myrank+1)%npes, 1, \n7                b, 10, MPI_INT, (myrank-1+npes)%npes, 1, \n8                MPI_COMM_WORLD, &status); \n9   ... \nIn many programs, the requirement for the send and receive buffers of MPI_Sendrecv  be\ndisjoint may force us to use a temporary buffer. This increases the amount of memory required\nby the program and also increases the overall run time due to the extra copy. This problem can\nbe solved by using that MPI_Sendrecv_replace  MPI function. This function performs a blocking\nsend and receive, but it uses a single buffer for both the send and receive operation. That is, the\nreceived data replaces the data that was sent out of the buffer. The calling sequence of this\nfunction is the following:\nint MPI_Sendrecv_replace(void *buf, int count, \n        MPI_Datatype datatype, int dest, int sendtag, \n        int source, int recvtag, MPI_Comm comm, \n        MPI_Status *status) \nNote that both the send and receive operations must transfer data of the same datatype.\n6.3.5 Example: Odd-Even Sort\nWe will now use the", "doc_id": "6278ed83-2ce8-4bd7-a340-2a749944d25c", "embedding": null, "doc_hash": "7754b8141c5638d29d8bbd02a1ffd5d463b8b63fcc475f6f68e18154ab95c6ba", "extra_info": null, "node_info": {"start": 702303, "end": 705232}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "be7782c5-393d-43f2-9642-c4dffa73c03f", "3": "5d57a5e6-a217-46ac-b645-b9e986fe01e9"}}, "__type__": "1"}, "5d57a5e6-a217-46ac-b645-b9e986fe01e9": {"__data__": {"text": "problem can\nbe solved by using that MPI_Sendrecv_replace  MPI function. This function performs a blocking\nsend and receive, but it uses a single buffer for both the send and receive operation. That is, the\nreceived data replaces the data that was sent out of the buffer. The calling sequence of this\nfunction is the following:\nint MPI_Sendrecv_replace(void *buf, int count, \n        MPI_Datatype datatype, int dest, int sendtag, \n        int source, int recvtag, MPI_Comm comm, \n        MPI_Status *status) \nNote that both the send and receive operations must transfer data of the same datatype.\n6.3.5 Example: Odd-Even Sort\nWe will now use the MPI functions described in the previous sections to write a complete\nmessage-passing program that will sort a list of numbers using the odd-even sorting algorithm.\nRecall from Section 9.3.1  that the odd-even sorting algorithm sorts a sequence of n elements\nusing p processes in a total of p phases. During each of these phases, the odd-or even-\nnumbered processes perform a compare-split step with their right neighbors. The MPI program\nfor performing the odd-even sort in parallel is shown in Program 6.1  . To simplify the\npresentation, this program assumes that n is divisible by p .\nProgram 6.1 Odd-Even Sorting\n[View full width]\n  1  #include <stdlib.h> \n  2  #include <mpi.h> /* Include MPI's header file */  \n  3 \n  4  main(int argc, char *argv[]) \n  5  { \n  6    int n;         /* The total number of elements to be sorted */  \n  7    int npes;      /* The total number of processes */  \n  8    int myrank;    /* The rank of the calling process */  \n  9    int nlocal;    /* The local number of elements, and the array that stores them */  \n 10    int *elmnts;   /* The array that stores the local elements */  \n 11    int *relmnts;  /* The array that stores the received elements */  \n 12    int oddrank;   /* The rank of the process during odd-phase communication */  \n 13    int evenrank;  /* The rank of the process during even-phase communication */  \n 14    int *wspace;   /* Working space during the compare-split operation */  \n 15    int i; \n 16    MPI_Status status; \n 17 \n 18    /* Initialize MPI and get system information */  \n 19    MPI_Init(&argc, &argv); \n 20    MPI_Comm_size(MPI_COMM_WORLD, &npes); \n 21    MPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n 22 \n 23    n = atoi(argv[1]); \n 24    nlocal = n/npes; /* Compute the number of elements to be stored locally. */  \n 25 \n 26    /* Allocate memory for the various arrays */  \n 27    elmnts  = (int *)malloc(nlocal*sizeof(int)); \n 28    relmnts = (int *)malloc(nlocal*sizeof(int)); \n 29    wspace  = (int *)malloc(nlocal*sizeof(int)); \n 30 \n 31    /* Fill-in the elmnts array with random elements */  \n 32    srandom(myrank); \n 33    for (i=0; i<nlocal; i++) \n 34      elmnts[i] = random(); \n 35 \n 36    /* Sort the local elements using the built-in quicksort routine */  \n 37    qsort(elmnts, nlocal, sizeof(int), IncOrder); \n 38 \n 39    /*", "doc_id": "5d57a5e6-a217-46ac-b645-b9e986fe01e9", "embedding": null, "doc_hash": "e11cb811583f14dae59af05e0ca49414ded449936682776c6efd82e31535b8fa", "extra_info": null, "node_info": {"start": 705160, "end": 708120}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6278ed83-2ce8-4bd7-a340-2a749944d25c", "3": "cecf4042-47c1-4f30-bf01-f576f615225b"}}, "__type__": "1"}, "cecf4042-47c1-4f30-bf01-f576f615225b": {"__data__": {"text": "*/  \n 27    elmnts  = (int *)malloc(nlocal*sizeof(int)); \n 28    relmnts = (int *)malloc(nlocal*sizeof(int)); \n 29    wspace  = (int *)malloc(nlocal*sizeof(int)); \n 30 \n 31    /* Fill-in the elmnts array with random elements */  \n 32    srandom(myrank); \n 33    for (i=0; i<nlocal; i++) \n 34      elmnts[i] = random(); \n 35 \n 36    /* Sort the local elements using the built-in quicksort routine */  \n 37    qsort(elmnts, nlocal, sizeof(int), IncOrder); \n 38 \n 39    /* Determine the rank of the processors that myrank needs to communicate during \nthe */ \n 40    /* odd and even phases of the algorithm */  \n 41    if (myrank%2 == 0) { \n 42      oddrank  = myrank-1; \n 43      evenrank = myrank+1; \n 44    } \n 45    else { \n 46      oddrank  = myrank+1; \n 47      evenrank = myrank-1; \n 48    } \n 49 \n 50    /* Set the ranks of the processors at the end of the linear */  \n 51    if (oddrank == -1 || oddrank == npes) \n 52      oddrank = MPI_PROC_NULL; \n 53    if (evenrank == -1 || evenrank == npes) \n 54      evenrank = MPI_PROC_NULL; \n 55 \n 56    /* Get into the main loop of the odd-even sorting algorithm */  \n 57    for (i=0; i<npes-1; i++) { \n 58      if (i%2 == 1) /* Odd phase */  \n 59        MPI_Sendrecv(elmnts, nlocal, MPI_INT, oddrank, 1, relmnts, \n 60            nlocal, MPI_INT, oddrank, 1, MPI_COMM_WORLD, &status); \n 61      else /* Even phase */  \n 62        MPI_Sendrecv(elmnts, nlocal, MPI_INT, evenrank, 1, relmnts, \n 63            nlocal, MPI_INT, evenrank, 1, MPI_COMM_WORLD, &status); \n 64 \n 65      CompareSplit(nlocal, elmnts, relmnts, wspace, \n\n 66                   myrank < status.MPI_SOURCE); \n 67    } \n 68 \n 69    free(elmnts); free(relmnts); free(wspace); \n 70    MPI_Finalize(); \n 71  } \n 72 \n 73  /* This is the CompareSplit function */  \n 74  CompareSplit(int nlocal, int *elmnts, int *relmnts, int *wspace, \n 75               int keepsmall) \n 76  { \n 77    int i, j, k; \n 78 \n 79    for (i=0; i<nlocal; i++) \n 80      wspace[i] = elmnts[i]; /* Copy the elmnts array into the wspace array */  \n 81 \n 82    if (keepsmall) { /* Keep the nlocal smaller elements */  \n 83      for (i=j=k=0; k<nlocal; k++) { \n 84        if (j == nlocal || (i < nlocal &&", "doc_id": "cecf4042-47c1-4f30-bf01-f576f615225b", "embedding": null, "doc_hash": "5c71a311ba857ea04eb0a484854d3a53976ef0d44e563c6864a4c56d714cf0d8", "extra_info": null, "node_info": {"start": 708289, "end": 710474}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5d57a5e6-a217-46ac-b645-b9e986fe01e9", "3": "2903b270-59d1-4707-9c33-c8ee7b3729d4"}}, "__type__": "1"}, "2903b270-59d1-4707-9c33-c8ee7b3729d4": {"__data__": {"text": "    int keepsmall) \n 76  { \n 77    int i, j, k; \n 78 \n 79    for (i=0; i<nlocal; i++) \n 80      wspace[i] = elmnts[i]; /* Copy the elmnts array into the wspace array */  \n 81 \n 82    if (keepsmall) { /* Keep the nlocal smaller elements */  \n 83      for (i=j=k=0; k<nlocal; k++) { \n 84        if (j == nlocal || (i < nlocal && wspace[i] < relmnts[j])) \n 85          elmnts[k] = wspace[i++]; \n 86        else \n 87          elmnts[k] = relmnts[j++]; \n 88      } \n 89    } \n 90    else { /* Keep the nlocal larger elements */  \n 91      for (i=k=nlocal-1, j=nlocal-1; k>=0; k--) { \n 92        if (j == 0 || (i >= 0 && wspace[i] >= relmnts[j])) \n 93          elmnts[k] = wspace[i--]; \n 94        else \n 95          elmnts[k] = relmnts[j--]; \n 96      } \n 97    } \n 98  } \n 99 \n100  /* The IncOrder function that is called by qsort is defined as follows */  \n101  int IncOrder(const void *e1, const void *e2) \n102  { \n103    return (*((int *)e1) - *((int *)e2)); \n104  } \n[ Team LiB ]\n \n\n[ Team LiB ]\n \n6.4 Topologies and Embedding\nMPI views the processes as being arranged in a one-dimensional topology and uses a linear\nordering to number the processes. However, in many parallel programs, processes are naturally\narranged in higher-dimensional topologies (e.g., two- or three-dimensional). In such programs,\nboth the computation and the set of interacting processes are naturally identified by their\ncoordinates in that topology. For example, in a parallel program in which the processes are\narranged in a two-dimensional topology, process ( i , j ) may need to send message to (or\nreceive message from) process ( k , l ). To implement these programs in MPI, we need to map\neach MPI process to a process in that higher-dimensional topology.\nMany such mappings are possible. Figure 6.5  illustrates some possible mappings of eight MPI\nprocesses onto a 4 x 4 two-dimensional topology. For example, for the mapping shown in\nFigure 6.5(a)  , an MPI process with rank rank corresponds to process ( row , col ) in the grid\nsuch that row = rank/4  and col = rank%4  (where '%' is C's module operator). As an\nillustration, the process with rank 7 is mapped to process (1, 3) in the grid.\nFigure 6.5. Different ways to map a set of processes to a two-\ndimensional grid. (a) and (b) show a row- and column-wise mapping\nof these processes, (c) shows a mapping that follows a space-filling\ncurve (dotted line), and (d) shows a mapping in which neighboring\nprocesses are directly connected in a hypercube.\nIn general, the goodness of a mapping is determined by the pattern of interaction among the\nprocesses in the higher-dimensional topology, the connectivity of physical processors, and the\nmapping of MPI processes to physical processors. For example, consider a program that uses", "doc_id": "2903b270-59d1-4707-9c33-c8ee7b3729d4", "embedding": null, "doc_hash": "933cf25bac233ef75d1364b56fca43601cfb59299c9b76f022cea5dfd5991512", "extra_info": null, "node_info": {"start": 710608, "end": 713376}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "cecf4042-47c1-4f30-bf01-f576f615225b", "3": "061194be-e1a6-4d02-826e-5a9867278024"}}, "__type__": "1"}, "061194be-e1a6-4d02-826e-5a9867278024": {"__data__": {"text": "map a set of processes to a two-\ndimensional grid. (a) and (b) show a row- and column-wise mapping\nof these processes, (c) shows a mapping that follows a space-filling\ncurve (dotted line), and (d) shows a mapping in which neighboring\nprocesses are directly connected in a hypercube.\nIn general, the goodness of a mapping is determined by the pattern of interaction among the\nprocesses in the higher-dimensional topology, the connectivity of physical processors, and the\nmapping of MPI processes to physical processors. For example, consider a program that uses a\ntwo-dimensional topology and each process needs to communicate with its neighboring\nprocesses along the x and y directions of this topology. Now, if the processors of the underlying\nparallel system are connected using a hypercube interconnection network, then the mapping\nshown in Figure 6.5(d)  is better, since neighboring processes in the grid are also neighboring\nprocessors in the hypercube topology.\nHowever, the mechanism used by MPI to assign ranks to the processes in a communication\ndomain does not use any information about the interconnection network, making it impossible\nto perform topology embeddings in an intelligent manner. Furthermore, even if we had that\ninformation, we will need to specify different mappings for different interconnection networks,\ndiminishing the architecture independent advantages of MPI. A better approach is to let the\nlibrary itself compute the most appropriate embedding of a given topology to the processors of\nthe underlying parallel computer. This is exactly the approach facilitated by MPI. MPI provides a\nset of routines that allows the programmer to arrange the processes in different topologies\nwithout having to explicitly specify how these processes are mapped onto the processors. It is\nup to the MPI library to find the most appropriate mapping that reduces the cost of sending and\nreceiving messages.\n6.4.1 Creating and Using Cartesian Topologies\nMPI provides routines that allow the specification of virtual process topologies of arbitrary\nconnectivity in terms of a graph. Each node in the graph corresponds to a process and two\nnodes are connected if they communicate with each other. Graphs of processes can be used to\nspecify any desired topology. However, most commonly used topologies in message-passing\nprograms are one-, two-, or higher-dimensional grids, that are also referred to as Cartesian\ntopologies  . For this reason, MPI provides a set of specialized routines for specifying and\nmanipulating this type of multi-dimensional grid topologies.\nMPI's function for describing Cartesian topologies is called MPI_Cart_create  . Its calling\nsequence is as follows.\nint MPI_Cart_create(MPI_Comm comm_old, int ndims, int *dims, \n        int *periods, int reorder, MPI_Comm *comm_cart) \nThis function takes the group of processes that belong to the communicator comm_old  and\ncreates a virtual process topology. The topology information is attached to a new communicator\ncomm_cart  that is created by MPI_Cart_create  . Any subsequent MPI routines that want to\ntake advantage of this new Cartesian topology must use comm_cart  as the communicator\nargument. Note that all the processes that belong to the comm_old  communicator must call this\nfunction. The shape and properties of the topology are specified by the arguments ndims  , dims\n, and periods  . The argument ndims  specifies the number of dimensions of the topology. The\narray dims specify the size along each dimension of the topology. The i th element of this array\nstores the size of the i th dimension of the topology. The array periods  is used to specify\nwhether or not the topology has wraparound connections. In particular, if periods[i]  is true\n(non-zero in C), then the", "doc_id": "061194be-e1a6-4d02-826e-5a9867278024", "embedding": null, "doc_hash": "c96a41749e2eb453774f2e5306657c9111bb6d6805705a1f700c98e6af6e5beb", "extra_info": null, "node_info": {"start": 713127, "end": 716894}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2903b270-59d1-4707-9c33-c8ee7b3729d4", "3": "35ee9614-a94b-43b1-b64d-65aea654a9f8"}}, "__type__": "1"}, "35ee9614-a94b-43b1-b64d-65aea654a9f8": {"__data__": {"text": "all the processes that belong to the comm_old  communicator must call this\nfunction. The shape and properties of the topology are specified by the arguments ndims  , dims\n, and periods  . The argument ndims  specifies the number of dimensions of the topology. The\narray dims specify the size along each dimension of the topology. The i th element of this array\nstores the size of the i th dimension of the topology. The array periods  is used to specify\nwhether or not the topology has wraparound connections. In particular, if periods[i]  is true\n(non-zero in C), then the topology has wraparound connections along dimension i , otherwise it\ndoes not. Finally, the argument reorder  is used to determine if the processes in the new group\n(i.e., communicator) are to be reordered or not. If reorder  is false, then the rank of each\nprocess in the new group is identical to its rank in the old group. Otherwise, MPI_Cart_create\nmay reorder the processes if that leads to a better embedding of the virtual topology onto the\nparallel computer. If the total number of processes specified in the dims array is smaller than\nthe number of processes in the communicator specified by comm_old  , then some processes will\nnot be part of the Cartesian topology. For this set of processes, the value of comm_cart  will be\nset to MPI_COMM_NULL  (an MPI defined constant). Note that it will result in an error if the total\nnumber of processes specified by dims is greater than the number of processes in the comm_old\ncommunicator.\nProcess Naming  When a Cartesian topology is used, each process is better identified by its\ncoordinates in this topology. However, all MPI functions that we described for sending and\nreceiving messages require that the source and the destination of each message be specified\nusing the rank of the process. For this reason, MPI provides two functions, MPI_Cart_rank  and\nMPI_Cart_coord  , for performing coordinate-to-rank and rank-to-coordinate translations,\nrespectively. The calling sequences of these routines are the following:\nint MPI_Cart_rank(MPI_Comm comm_cart, int *coords, int *rank) \nint MPI_Cart_coord(MPI_Comm comm_cart, int rank, int maxdims, \n        int *coords) \nThe MPI_Cart_rank  takes the coordinates of the process as argument in the coords array and\nreturns its rank in rank . The MPI_Cart_coords  takes the rank of the process rank and returns\nits Cartesian coordinates in the array coords , of length maxdims  . Note that maxdims  should be\nat least as large as the number of dimensions in the Cartesian topology specified by the\ncommunicator comm_cart  .\nFrequently, the communication performed among processes in a Cartesian topology is that of\nshifting data along a dimension of the topology. MPI provides the function MPI_Cart_shift  ,\nthat can be used to compute the rank of the source and destination processes for such\noperation. The calling sequence of this function is the following:\nint MPI_Cart_shift(MPI_Comm comm_cart, int dir, int s_step, \n        int *rank_source, int *rank_dest) \nThe direction of the shift is specified in the dir argument, and is one of the dimensions of the\ntopology. The size of the shift step is specified in the s_step argument. The computed ranks are\nreturned in rank_source and rank_dest  . If the Cartesian topology was created with\nwraparound connections (i.e., the periods[dir]  entry was set to true), then the shift wraps\naround. Otherwise, a MPI_PROC_NULL  value is returned for rank_source and/or rank_dest  for\nthose processes that are outside the topology.\n6.4.2", "doc_id": "35ee9614-a94b-43b1-b64d-65aea654a9f8", "embedding": null, "doc_hash": "920110184864e758708f722888a04de7f4717b2b69a9434e1a694c870710a51e", "extra_info": null, "node_info": {"start": 716898, "end": 720451}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "061194be-e1a6-4d02-826e-5a9867278024", "3": "414b5133-b882-4d85-9428-2e4a97caedfb"}}, "__type__": "1"}, "414b5133-b882-4d85-9428-2e4a97caedfb": {"__data__": {"text": "*rank_dest) \nThe direction of the shift is specified in the dir argument, and is one of the dimensions of the\ntopology. The size of the shift step is specified in the s_step argument. The computed ranks are\nreturned in rank_source and rank_dest  . If the Cartesian topology was created with\nwraparound connections (i.e., the periods[dir]  entry was set to true), then the shift wraps\naround. Otherwise, a MPI_PROC_NULL  value is returned for rank_source and/or rank_dest  for\nthose processes that are outside the topology.\n6.4.2 Example: Cannon's Matrix-Matrix Multiplication\nTo illustrate how the various topology functions are used we will implement Cannon's algorithm\nfor multiplying two matrices A and B , described in Section 8.2.2  . Cannon's algorithm views the\nprocesses as being arranged in a virtual two-dimensional square array. It uses this array to\ndistribute the matrices A , B , and the result matrix C  in a block fashion. That is, if n x n is the\nsize of each matrix and p is the total number of process, then each matrix is divided into square\nblocks of size \n (assuming that p is a perfect square). Now, process Pi , j in the grid\nis assigned the Ai , j , Bi , j , and Ci , j blocks of each matrix. After an initial data alignment\nphase, the algorithm proceeds in \n  steps. In each step, every process multiplies the locally\navailable blocks of matrices A and B , and then sends the block of A to the leftward process, and\nthe block of B to the upward process.\nProgram 6.2  shows the MPI function that implements Cannon's algorithm. The dimension of the\nmatrices is supplied in the parameter n . The parameters a , b , and c point to the locally stored\nportions of the matrices A , B , and C , respectively. The size of these arrays is \n ,\nwhere p is the number of processes. This routine assumes that p is a perfect square and that n\nis a multiple of \n . The parameter comm stores the communicator describing the processes that\ncall the MatrixMatrixMultiply  function. Note that the remaining programs in this chapter will\nbe provided in the form of a function, as opposed to complete stand-alone programs.\nProgram 6.2 Cannon's Matrix-Matrix Multiplication with MPI's\nTopologies\n 1  MatrixMatrixMultiply(int n, double *a, double *b, double *c, \n 2                       MPI_Comm comm) \n 3  { \n 4    int i; \n 5    int nlocal; \n 6    int npes, dims[2], periods[2]; \n 7    int myrank, my2drank, mycoords[2]; \n 8    int uprank, downrank, leftrank, rightrank, coords[2]; \n 9    int shiftsource, shiftdest; \n10    MPI_Status status; \n11    MPI_Comm comm_2d; \n12 \n13    /* Get the communicator related information */  \n14    MPI_Comm_size(comm, &npes); \n15    MPI_Comm_rank(comm, &myrank); \n16 \n17    /* Set up the Cartesian topology */  \n18    dims[0] = dims[1] = sqrt(npes); \n19 \n20    /* Set the periods for wraparound connections */  \n21    periods[0] = periods[1] = 1; \n22 \n23    /* Create the Cartesian topology, with rank reordering */  \n24    MPI_Cart_create(comm, 2, dims, periods, 1, &comm_2d); \n25 \n26    /* Get the rank and coordinates with respect to the new topology */ ", "doc_id": "414b5133-b882-4d85-9428-2e4a97caedfb", "embedding": null, "doc_hash": "a07c01d712a924aea0b677ba66bcbd66200245bd54c1ce115634df06702226e3", "extra_info": null, "node_info": {"start": 720480, "end": 723577}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "35ee9614-a94b-43b1-b64d-65aea654a9f8", "3": "060507c6-6e1a-45ee-b716-7b42b3befdbe"}}, "__type__": "1"}, "060507c6-6e1a-45ee-b716-7b42b3befdbe": {"__data__": {"text": "   /* Set up the Cartesian topology */  \n18    dims[0] = dims[1] = sqrt(npes); \n19 \n20    /* Set the periods for wraparound connections */  \n21    periods[0] = periods[1] = 1; \n22 \n23    /* Create the Cartesian topology, with rank reordering */  \n24    MPI_Cart_create(comm, 2, dims, periods, 1, &comm_2d); \n25 \n26    /* Get the rank and coordinates with respect to the new topology */  \n27    MPI_Comm_rank(comm_2d, &my2drank); \n28    MPI_Cart_coords(comm_2d, my2drank, 2, mycoords); \n29 \n30    /* Compute ranks of the up and left shifts */  \n31    MPI_Cart_shift(comm_2d, 0, -1, &rightrank, &leftrank); \n32    MPI_Cart_shift(comm_2d, 1, -1, &downrank, &uprank); \n33 \n34    /* Determine the dimension of the local matrix block */  \n35    nlocal = n/dims[0]; \n36 \n37    /* Perform the initial matrix alignment. First for A and then for B */  \n38    MPI_Cart_shift(comm_2d, 0, -mycoords[0], &shiftsource, &shiftdest); \n39    MPI_Sendrecv_replace(a, nlocal*nlocal, MPI_DOUBLE, shiftdest, \n40        1, shiftsource, 1, comm_2d, &status); \n41 \n42    MPI_Cart_shift(comm_2d, 1, -mycoords[1], &shiftsource, &shiftdest); \n43    MPI_Sendrecv_replace(b, nlocal*nlocal, MPI_DOUBLE, \n44        shiftdest, 1, shiftsource, 1, comm_2d, &status); \n45 \n46    /* Get into the main computation loop */  \n47    for (i=0; i<dims[0]; i++) { \n48      MatrixMultiply(nlocal, a, b, c); /*c=c+a*b*/  \n49 \n50      /* Shift matrix a left by one */  \n51      MPI_Sendrecv_replace(a, nlocal*nlocal, MPI_DOUBLE, \n52          leftrank, 1, rightrank, 1, comm_2d, &status); \n53 \n54      /* Shift matrix b up by one */  \n55      MPI_Sendrecv_replace(b, nlocal*nlocal, MPI_DOUBLE, \n56          uprank, 1, downrank, 1, comm_2d, &status); \n57    } \n58 \n59    /* Restore the original distribution of a and b */  \n60    MPI_Cart_shift(comm_2d, 0, +mycoords[0], &shiftsource, &shiftdest); \n61    MPI_Sendrecv_replace(a, nlocal*nlocal, MPI_DOUBLE, \n62        shiftdest, 1, shiftsource, 1, comm_2d, &status); \n63 \n64    MPI_Cart_shift(comm_2d, 1, +mycoords[1], &shiftsource, &shiftdest); \n65    MPI_Sendrecv_replace(b, nlocal*nlocal, MPI_DOUBLE, \n66        shiftdest, 1, shiftsource, 1, comm_2d, &status); \n67 \n68", "doc_id": "060507c6-6e1a-45ee-b716-7b42b3befdbe", "embedding": null, "doc_hash": "e6caa6c8fe3f4d5c5dbfd86e13bab202521ce751f0c9952bd09be6408b151990", "extra_info": null, "node_info": {"start": 723725, "end": 725896}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "414b5133-b882-4d85-9428-2e4a97caedfb", "3": "e4d778e6-e49e-4369-a886-5b12797a5575"}}, "__type__": "1"}, "e4d778e6-e49e-4369-a886-5b12797a5575": {"__data__": {"text": "MPI_Sendrecv_replace(a, nlocal*nlocal, MPI_DOUBLE, \n62        shiftdest, 1, shiftsource, 1, comm_2d, &status); \n63 \n64    MPI_Cart_shift(comm_2d, 1, +mycoords[1], &shiftsource, &shiftdest); \n65    MPI_Sendrecv_replace(b, nlocal*nlocal, MPI_DOUBLE, \n66        shiftdest, 1, shiftsource, 1, comm_2d, &status); \n67 \n68    MPI_Comm_free(&comm_2d); /* Free up communicator */  \n69  } \n70 \n71  /* This function performs a serial matrix-matrix multiplication c = a*b */  \n72  MatrixMultiply(int n, double *a, double *b, double *c) \n73  { \n74    int i, j, k; \n75 \n76    for (i=0; i<n; i++) \n77      for (j=0; j<n; j++) \n78        for (k=0; k<n; k++) \n79          c[i*n+j] += a[i*n+k]*b[k*n+j]; \n80  } \n[ Team LiB ]\n \n\n[ Team LiB ]\n  \n6.5 Overlapping Communication with Computation\nThe MPI programs we developed so far used blocking send and receive operations whenever\nthey needed to perform point-to-point communication. Recall that a blocking send operation\nremains blocked until the message has been copied out of the send buffer (either into a system\nbuffer at the source process or sent to the destination process). Similarly, a blocking receive\noperation returns only after the message has been received and copied into the receive buffer.\nFor example, consider Cannon's matrix-matrix multiplication program described in Program6.2. During each iteration of its main computational loop (lines 47\u2013 57), it first computes the\nmatrix multiplication of the sub-matrices stored in a and b, and then shifts the blocks of a and\nb, using MPI_Sendrecv_replace  which blocks until the specified matrix block has been sent and\nreceived by the corresponding processes. In each iteration, each process spends O (n3/ p1.5)\ntime for performing the matrix-matrix multiplication and O(n2/p) time for shifting the blocks of\nmatrices A and B. Now, since the blocks of matrices A and B do not change as they are shifted\namong the processors, it will be preferable if we can overlap the transmission of these blocks\nwith the computation for the matrix-matrix multiplication, as many recent distributed-memory\nparallel computers have dedicated communication controllers that can perform the transmission\nof messages without interrupting the CPUs.\n6.5.1 Non-Blocking Communication Operations\nIn order to overlap communication with computation, MPI provides a pair of functions for\nperforming non-blocking send and receive operations. These functions are MPI_Isend  and\nMPI_Irecv . MPI_Isend  starts a send operation but does not complete, that is, it returns before\nthe data is copied out of the buffer. Similarly, MPI_Irecv  starts a receive operation but returns\nbefore the data has been received and copied into the buffer. With the support of appropriate\nhardware, the transmission and reception of messages can proceed concurrently with the\ncomputations performed by the program upon the return of the above functions.\nHowever, at a later point in the program, a process that has started a non-blocking send or\nreceive operation must make sure that this operation has completed before it proceeds with its\ncomputations. This is because a process that has started a non-blocking send operation may\nwant to overwrite the buffer that stores", "doc_id": "e4d778e6-e49e-4369-a886-5b12797a5575", "embedding": null, "doc_hash": "593a38ca846145ea4559afb208343ff88ab85d210095e9ff327c86260e74448c", "extra_info": null, "node_info": {"start": 725931, "end": 729148}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "060507c6-6e1a-45ee-b716-7b42b3befdbe", "3": "09ce386f-c4bb-4427-b762-a5eef372d82b"}}, "__type__": "1"}, "09ce386f-c4bb-4427-b762-a5eef372d82b": {"__data__": {"text": "of the buffer. Similarly, MPI_Irecv  starts a receive operation but returns\nbefore the data has been received and copied into the buffer. With the support of appropriate\nhardware, the transmission and reception of messages can proceed concurrently with the\ncomputations performed by the program upon the return of the above functions.\nHowever, at a later point in the program, a process that has started a non-blocking send or\nreceive operation must make sure that this operation has completed before it proceeds with its\ncomputations. This is because a process that has started a non-blocking send operation may\nwant to overwrite the buffer that stores the data that are being sent, or a process that has\nstarted a non-blocking receive operation may want to use the data it requested. To check the\ncompletion of non-blocking send and receive operations, MPI provides a pair of functions\nMPI_Test  and MPI_Wait . The first tests whether or not a non-blocking operation has finished\nand the second waits (i.e., gets blocked) until a non-blocking operation actually finishes.\nThe calling sequences of MPI_Isend  and MPI_Irecv  are the following: int MPI_Isend(void *buf, int count, MPI_Datatype datatype, \n        int dest, int tag, MPI_Comm comm, MPI_Request *request) \nint MPI_Irecv(void *buf, int count, MPI_Datatype datatype, \n        int source, int tag, MPI_Comm comm, MPI_Request *request) \nNote that these functions have similar arguments as the corresponding blocking send and\nreceive functions. The main difference is that they take an additional argument request .\nMPI_Isend  and MPI_Irecv  functions allocate a request object  and return a pointer to it in the\nrequest  variable. This request object is used as an argument in the MPI_Test  and MPI_Wait\nfunctions to identify the operation whose status we want to query or to wait for its completion.\nNote that the MPI_Irecv  function does not take a status argument similar to the blocking\nreceive function, but the status information associated with the receive operation is returned by\nthe MPI_Test  and MPI_Wait  functions.\nint MPI_Test(MPI_Request *request, int *flag, MPI_Status *status) \nint MPI_Wait(MPI_Request *request, MPI_Status *status) \nMPI_Test  tests whether or not the non-blocking send or receive operation identified by its\nrequest  has finished. It returns flag = {true}  (non-zero value in C) if it completed, otherwise\nit returns {false}  (a zero value in C). In the case that the non-blocking operation has finished,\nthe request object pointed to by request  is deallocated and request  is set to\nMPI_REQUEST_NULL. Also the status object is set to contain information about the operation. If\nthe operation has not finished, request  is not modified and the value of the status object is\nundefined. The MPI_Wait  function blocks until the non-blocking operation identified by request\ncompletes. In that case it deal-locates the request  object, sets it to MPI_REQUEST_NULL, and\nreturns information about the completed operation in the status object.\nFor the cases that the programmer wants to explicitly deallocate a request object, MPI provides\nthe following function.\nint MPI_Request_free(MPI_Request *request) \nNote that the deallocation of the request object does not have any effect on the associated non-\nblocking send or receive operation. That is, if it has not yet completed it will proceed until its\ncompletion. Hence, one must be careful before explicitly deallocating a request object, since\nwithout it, we cannot check whether or not the non-blocking operation has completed.\nA non-blocking communication operation can be matched with a corresponding blocking\noperation. For example, a process can", "doc_id": "09ce386f-c4bb-4427-b762-a5eef372d82b", "embedding": null, "doc_hash": "d5144254065f2666e4dad317bfab3869b7813dc40bef914519f65878fd30ec60", "extra_info": null, "node_info": {"start": 728859, "end": 732550}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e4d778e6-e49e-4369-a886-5b12797a5575", "3": "84969d9f-7bb2-47b6-b789-514eb4aa6506"}}, "__type__": "1"}, "84969d9f-7bb2-47b6-b789-514eb4aa6506": {"__data__": {"text": "explicitly deallocate a request object, MPI provides\nthe following function.\nint MPI_Request_free(MPI_Request *request) \nNote that the deallocation of the request object does not have any effect on the associated non-\nblocking send or receive operation. That is, if it has not yet completed it will proceed until its\ncompletion. Hence, one must be careful before explicitly deallocating a request object, since\nwithout it, we cannot check whether or not the non-blocking operation has completed.\nA non-blocking communication operation can be matched with a corresponding blocking\noperation. For example, a process can send a message using a non-blocking send operation and\nthis message can be received by the other process using a blocking receive operation.\nAvoiding Deadlocks  By using non-blocking communication operations we can remove most of\nthe deadlocks associated with their blocking counterparts. For example, as we discussed in\nSection 6.3 the following piece of code is not safe.\n 1   int a[10], b[10], myrank; \n 2   MPI_Status status; \n 3   ... \n 4   MPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n 5   if (myrank == 0) { \n 6     MPI_Send(a, 10, MPI_INT, 1, 1, MPI_COMM_WORLD); \n 7     MPI_Send(b, 10, MPI_INT, 1, 2, MPI_COMM_WORLD); \n 8   } \n 9   else if (myrank == 1) { \n10     MPI_Recv(b, 10, MPI_INT, 0, 2, &status, MPI_COMM_WORLD); \n11     MPI_Recv(a, 10, MPI_INT, 0, 1, &status, MPI_COMM_WORLD); \n12   } \n13   ... \nHowever, if we replace either the send or receive operations with their non-blocking\ncounterparts, then the code will be safe, and will correctly run on any MPI implementation.\n 1   int a[10], b[10], myrank; \n 2   MPI_Status status; \n 3   MPI_Request requests[2]; \n 4   ... \n 5   MPI_Comm_rank(MPI_COMM_WORLD, &myrank); \n 6   if (myrank == 0) { \n 7     MPI_Send(a, 10, MPI_INT, 1, 1, MPI_COMM_WORLD); \n 8     MPI_Send(b, 10, MPI_INT, 1, 2, MPI_COMM_WORLD); \n 9   } \n10   else if (myrank == 1) { \n11     MPI_Irecv(b, 10, MPI_INT, 0, 2, &requests[0], MPI_COMM_WORLD); \n12     MPI_Irecv(a, 10, MPI_INT, 0, 1, &requests[1], MPI_COMM_WORLD); \n13   } \n14   ... \nThis example also illustrates that the non-blocking operations started by any process can finish\nin any order depending on the transmission or reception of the corresponding messages. For\nexample, the second receive operation will finish before the first does.\nExample: Cannon's Matrix-Matrix Multiplication (Using Non-Blocking Operations)\nProgram 6.3 shows the MPI program that implements Cannon's algorithm using non-blocking\nsend and receive operations. The various parameters are identical to those of Program 6.2 .\nProgram 6.3 Non-Blocking Cannon's Matrix-Matrix Multiplication\n 1   MatrixMatrixMultiply_NonBlocking(int n, double *a, double *b, \n 2                                    double *c,", "doc_id": "84969d9f-7bb2-47b6-b789-514eb4aa6506", "embedding": null, "doc_hash": "be5daee588b0713a322293a7b321f73d68f6c5434b435b571606300c7b634321", "extra_info": null, "node_info": {"start": 732573, "end": 735356}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "09ce386f-c4bb-4427-b762-a5eef372d82b", "3": "c7620063-db41-4e45-b45f-56075a76b263"}}, "__type__": "1"}, "c7620063-db41-4e45-b45f-56075a76b263": {"__data__": {"text": "Cannon's Matrix-Matrix Multiplication (Using Non-Blocking Operations)\nProgram 6.3 shows the MPI program that implements Cannon's algorithm using non-blocking\nsend and receive operations. The various parameters are identical to those of Program 6.2 .\nProgram 6.3 Non-Blocking Cannon's Matrix-Matrix Multiplication\n 1   MatrixMatrixMultiply_NonBlocking(int n, double *a, double *b, \n 2                                    double *c, MPI_Comm comm) \n 3   { \n 4     int i, j, nlocal; \n 5     double *a_buffers[2], *b_buffers[2]; \n 6     int npes, dims[2], periods[2]; \n 7     int myrank, my2drank, mycoords[2]; \n 8     int uprank, downrank, leftrank, rightrank, coords[2]; \n 9     int shiftsource, shiftdest; \n10     MPI_Status status; \n11     MPI_Comm comm_2d; \n12     MPI_Request reqs[4]; \n13 \n14     /* Get the communicator related information */  \n15     MPI_Comm_size(comm, &npes); \n16     MPI_Comm_rank(comm, &myrank); \n17 \n18     /* Set up the Cartesian topology */  \n19     dims[0] = dims[1] = sqrt(npes); \n20 \n21     /* Set the periods for wraparound connections */  \n22     periods[0] = periods[1] = 1; \n23 \n24     /* Create the Cartesian topology, with rank reordering */  \n25     MPI_Cart_create(comm, 2, dims, periods, 1, &comm_2d); \n26 \n27     /* Get the rank and coordinates with respect to the new topology */  \n28     MPI_Comm_rank(comm_2d, &my2drank); \n29     MPI_Cart_coords(comm_2d, my2drank, 2, mycoords); \n30 \n31     /* Compute ranks of the up and left shifts */  \n32     MPI_Cart_shift(comm_2d, 0, -1, &rightrank, &leftrank); \n33     MPI_Cart_shift(comm_2d, 1, -1, &downrank, &uprank); \n34 \n35     /* Determine the dimension of the local matrix block */  \n36     nlocal = n/dims[0]; \n37 \n38     /* Setup the a_buffers and b_buffers arrays */  \n39     a_buffers[0] = a; \n40     a_buffers[1] = (double *)malloc(nlocal*nlocal*sizeof(double)); \n41     b_buffers[0] = b; \n42     b_buffers[1] = (double *)malloc(nlocal*nlocal*sizeof(double)); \n43 \n44     /* Perform the initial matrix alignment. First for A and then for B */  \n45     MPI_Cart_shift(comm_2d, 0, -mycoords[0], &shiftsource, &shiftdest); \n46     MPI_Sendrecv_replace(a_buffers[0], nlocal*nlocal, MPI_DOUBLE, \n47         shiftdest, 1, shiftsource, 1, comm_2d, &status); \n48 \n49     MPI_Cart_shift(comm_2d,", "doc_id": "c7620063-db41-4e45-b45f-56075a76b263", "embedding": null, "doc_hash": "02a8e8d1f3cf94fc47e4b92ddac604a82b06a564fb66c5772a740539bd72de62", "extra_info": null, "node_info": {"start": 735545, "end": 737826}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "84969d9f-7bb2-47b6-b789-514eb4aa6506", "3": "09fc725d-4ec7-49f9-b5d0-64869840f38b"}}, "__type__": "1"}, "09fc725d-4ec7-49f9-b5d0-64869840f38b": {"__data__": {"text": "\n43 \n44     /* Perform the initial matrix alignment. First for A and then for B */  \n45     MPI_Cart_shift(comm_2d, 0, -mycoords[0], &shiftsource, &shiftdest); \n46     MPI_Sendrecv_replace(a_buffers[0], nlocal*nlocal, MPI_DOUBLE, \n47         shiftdest, 1, shiftsource, 1, comm_2d, &status); \n48 \n49     MPI_Cart_shift(comm_2d, 1, -mycoords[1], &shiftsource, &shiftdest); \n50     MPI_Sendrecv_replace(b_buffers[0], nlocal*nlocal, MPI_DOUBLE, \n51         shiftdest, 1, shiftsource, 1, comm_2d, &status); \n52 \n53     /* Get into the main computation loop */  \n54     for (i=0; i<dims[0]; i++) { \n55       MPI_Isend(a_buffers[i%2], nlocal*nlocal, MPI_DOUBLE, \n56           leftrank, 1, comm_2d, &reqs[0]); \n57       MPI_Isend(b_buffers[i%2], nlocal*nlocal, MPI_DOUBLE, \n58           uprank, 1, comm_2d, &reqs[1]); \n59       MPI_Irecv(a_buffers[(i+1)%2], nlocal*nlocal, MPI_DOUBLE, \n60           rightrank, 1, comm_2d, &reqs[2]); \n61       MPI_Irecv(b_buffers[(i+1)%2], nlocal*nlocal, MPI_DOUBLE, \n62           downrank, 1, comm_2d, &reqs[3]); \n63 \n64       /* c = c + a*b */  \n65       MatrixMultiply(nlocal, a_buffers[i%2], b_buffers[i%2], c); \n66 \n67       for (j=0; j<4; j++) \n68         MPI_Wait(&reqs[j], &status); \n69     } \n70 \n71     /* Restore the original distribution of a and b */  \n72     MPI_Cart_shift(comm_2d, 0, +mycoords[0], &shiftsource, &shiftdest); \n73     MPI_Sendrecv_replace(a_buffers[i%2], nlocal*nlocal, MPI_DOUBLE, \n74         shiftdest, 1, shiftsource, 1, comm_2d, &status); \n75 \n76     MPI_Cart_shift(comm_2d, 1, +mycoords[1], &shiftsource, &shiftdest); \n77     MPI_Sendrecv_replace(b_buffers[i%2], nlocal*nlocal, MPI_DOUBLE, \n78         shiftdest, 1, shiftsource, 1, comm_2d, &status); \n79 \n80     MPI_Comm_free(&comm_2d); /* Free up communicator */  \n81 \n82     free(a_buffers[1]); \n83     free(b_buffers[1]); \n84  } \nThere are two main differences between the blocking program ( Program 6.2 ) and this non-\nblocking one. The first difference is that the non-blocking program requires the use of", "doc_id": "09fc725d-4ec7-49f9-b5d0-64869840f38b", "embedding": null, "doc_hash": "f205f910bd26eaaed1325616ff730cc41e36d6aaec98d5a719e7ff27f0ec76c8", "extra_info": null, "node_info": {"start": 737907, "end": 739929}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c7620063-db41-4e45-b45f-56075a76b263", "3": "a334b79d-bc58-47b6-a005-720688223122"}}, "__type__": "1"}, "a334b79d-bc58-47b6-a005-720688223122": {"__data__": {"text": "nlocal*nlocal, MPI_DOUBLE, \n78         shiftdest, 1, shiftsource, 1, comm_2d, &status); \n79 \n80     MPI_Comm_free(&comm_2d); /* Free up communicator */  \n81 \n82     free(a_buffers[1]); \n83     free(b_buffers[1]); \n84  } \nThere are two main differences between the blocking program ( Program 6.2 ) and this non-\nblocking one. The first difference is that the non-blocking program requires the use of the\nadditional arrays a_buffers  and b_buffers , that are used as the buffer of the blocks of A and B\nthat are being received while the computation involving the previous blocks is performed. The\nsecond difference is that in the main computational loop, it first starts the non-blocking send\noperations to send the locally stored blocks of A and B to the processes left and up the grid, and\nthen starts the non-blocking receive operations to receive the blocks for the next iteration from\nthe processes right and down the grid. Having initiated these four non-blocking operations, it\nproceeds to perform the matrix-matrix multiplication of the blocks it currently stores. Finally,\nbefore it proceeds to the next iteration, it uses MPI_Wait  to wait for the send and receive\noperations to complete.\nNote that in order to overlap communication with computation we have to use two auxiliary\narrays \u2013 one for A and one for B. This is to ensure that incoming messages never overwrite the\nblocks of A and B that are used in the computation, which proceeds concurrently with the data\ntransfer. Thus, increased performance (by overlapping communication with computation) comes\nat the expense of increased memory requirements. This is a trade-off that is often made in\nmessage-passing programs, since communication overheads can be quite high for loosely\ncoupled distributed memory parallel computers.[ Team LiB ]\n  \n\n[ Team LiB ]\n \n6.6 Collective Communication and Computation\nOperations\nMPI provides an extensive set of functions for performing many commonly used collective\ncommunication operations. In particular, the majority of the basic communication operations\ndescribed in Chapter 4  are supported by MPI. All of the collective communication functions\nprovided by MPI take as an argument a communicator that defines the group of processes that\nparticipate in the collective operation. All the processes that belong to this communicator\nparticipate in the operation, and all of them must call the collective communication function.\nEven though collective communication operations do not act like barriers (i.e., it is possible for\na processor to go past its call for the collective communication operation even before other\nprocesses have reached it), it acts like a virtual  synchronization step in the following sense: the\nparallel program should be written such that it behaves correctly even if a global\nsynchronization is performed before and after the collective call. Since the operations are\nvirtually synchronous, they do not require tags. In some of the collective functions data is\nrequired to be sent from a single process (source-process) or to be received by a single process\n(target-process). In these functions, the source- or target-process is one of the arguments\nsupplied to the routines. All the processes in the group (i.e., communicator) must specify the\nsame source- or target-process. For most collective communication operations, MPI provides\ntwo different variants. The first transfers equal-size data to or from each process, and the\nsecond transfers data that can be of different sizes.\n6.6.1 Barrier\nThe barrier synchronization operation is performed in MPI using the MPI_Barrier function.int MPI_Barrier(MPI_Comm comm) \nThe only argument of MPI_Barrier is the communicator that defines the group of processes\nthat are synchronized. The call", "doc_id": "a334b79d-bc58-47b6-a005-720688223122", "embedding": null, "doc_hash": "8806dd197fb855793da8fbbaf0298346e50038d4a3d09aa22d49e5a5ff8aab64", "extra_info": null, "node_info": {"start": 739872, "end": 743640}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "09fc725d-4ec7-49f9-b5d0-64869840f38b", "3": "8b7ceb80-aa2e-4e48-8e2f-ba3a7d47c1f5"}}, "__type__": "1"}, "8b7ceb80-aa2e-4e48-8e2f-ba3a7d47c1f5": {"__data__": {"text": "processes in the group (i.e., communicator) must specify the\nsame source- or target-process. For most collective communication operations, MPI provides\ntwo different variants. The first transfers equal-size data to or from each process, and the\nsecond transfers data that can be of different sizes.\n6.6.1 Barrier\nThe barrier synchronization operation is performed in MPI using the MPI_Barrier function.int MPI_Barrier(MPI_Comm comm) \nThe only argument of MPI_Barrier is the communicator that defines the group of processes\nthat are synchronized. The call to MPI_Barrier returns only after all the processes in the group\nhave called this function.\n6.6.2 Broadcast\nThe one-to-all broadcast operation described in Section 4.1  is performed in MPI using the\nMPI_Bcast  function.\nint MPI_Bcast(void *buf, int count, MPI_Datatype datatype, \n        int source, MPI_Comm comm) \nMPI_Bcast  sends the data stored in the buffer buf of process source to all the other processes\nin the group. The data received by each process is stored in the buffer buf . The data that is\nbroadcast consist of count  entries of type datatype  . The amount of data sent by the source\nprocess must be equal to the amount of data that is being received by each process; i.e., the\ncount  and datatype  fields must match on all processes.\n6.6.3 Reduction\nThe all-to-one reduction operation described in Section 4.1  is performed in MPI using the\nMPI_Reduce  function.\nint MPI_Reduce(void *sendbuf, void *recvbuf, int count, \n        MPI_Datatype datatype, MPI_Op op, int target, \n        MPI_Comm comm) \nMPI_Reduce  combines the elements stored in the buffer sendbuf  of each process in the group,\nusing the operation specified in op , and returns the combined values in the buffer recvbuf  of\nthe process with rank target . Both the sendbuf  and recvbuf  must have the same number of\ncount  items of type datatype  . Note that all processes must provide a recvbuf  array, even if\nthey are not the target  of the reduction operation. When count  is more than one, then the\ncombine operation is applied element-wise on each entry of the sequence. All the processes\nmust call MPI_Reduce  with the same value for count  , datatype  , op , target , and comm .\nMPI provides a list of predefined operations that can be used to combine the elements stored in\nsendbuf  . MPI also allows programmers to define their own operations, which is not covered in\nthis book. The predefined operations are shown in Table 6.3  . For example, in order to compute\nthe maximum of the elements stored in sendbuf  , the MPI_MAX  value must be used for the op\nargument. Not all of these operations can be applied to all possible data-types supported by\nMPI. For example, a bit-wise OR operation (i.e., op = MPI_BOR  ) is not defined for real-valued\ndata-types such as MPI_FLOAT  and MPI_REAL  . The last column of Table 6.3  shows the various\ndata-types that can be used with each operation.\nMPI_MAX\nMaximum\nC integers and floating point\nMPI_MIN\nMinimum\nC integers and floating point\nMPI_SUM\nSum\nC integers and floating point\nMPI_PROD\nProduct\nC integers and floating point\nMPI_LAND\nLogical AND\nC integers\nMPI_BAND\nBit-wise AND\nC integers and byte\nMPI_LOR\nLogical OR\nC integers\nMPI_BOR\nBit-wise OR\nC integers and", "doc_id": "8b7ceb80-aa2e-4e48-8e2f-ba3a7d47c1f5", "embedding": null, "doc_hash": "822688acb0ad3080ebc42cf6f22cc4dba9895a3c262c6033031e53bfc8af6432", "extra_info": null, "node_info": {"start": 743483, "end": 746737}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a334b79d-bc58-47b6-a005-720688223122", "3": "f6d21bf0-a8da-42b5-bd12-ae224c4b9f51"}}, "__type__": "1"}, "f6d21bf0-a8da-42b5-bd12-ae224c4b9f51": {"__data__": {"text": " . The last column of Table 6.3  shows the various\ndata-types that can be used with each operation.\nMPI_MAX\nMaximum\nC integers and floating point\nMPI_MIN\nMinimum\nC integers and floating point\nMPI_SUM\nSum\nC integers and floating point\nMPI_PROD\nProduct\nC integers and floating point\nMPI_LAND\nLogical AND\nC integers\nMPI_BAND\nBit-wise AND\nC integers and byte\nMPI_LOR\nLogical OR\nC integers\nMPI_BOR\nBit-wise OR\nC integers and byte\nMPI_LXOR\nLogical XOR\nC integers\nMPI_BXOR\nBit-wise XOR\nC integers and byte\nMPI_MAXLOC\nmax-min value-location\nData-pairs\nMPI_MINLOC\nmin-min value-location\nData-pairs\nTable 6.3. Predefined reduction operations.\nOperation Meaning Datatypes\nThe operation MPI_MAXLOC  combines pairs of values ( vi , li ) and returns the pair ( v , l ) such\nthat v is the maximum among all vi 's and l is the smallest among all li 's such that v = vi .\nSimilarly, MPI_MINLOC  combines pairs of values and returns the pair ( v , l ) such that v is the\nminimum among all vi 's and l is the smallest among all li 's such that v = vi . One possible\napplication of MPI_MAXLOC  or MPI_MINLOC  is to compute the maximum or minimum of a list of\nnumbers each residing on a different process and also the rank of the first process that stores\nthis maximum or minimum, as illustrated in Figure 6.6  . Since both MPI_MAXLOC  and\nMPI_MINLOC  require datatypes that correspond to pairs of values, a new set of MPI datatypes\nhave been defined as shown in Table 6.4  . In C, these datatypes are implemented as structures\ncontaining the corresponding types.\nFigure 6.6. An example use of the MPI_MINLOC  and MPI_MAXLOC  operators.\nWhen the result of the reduction operation is needed by all the processes, MPI provides the\nMPI_Allreduce  operation that returns the result to all the processes. This function provides the\nfunctionality of the all-reduce operation described in Section 4.3  .\nMPI_2INT\npair of int s\nMPI_SHORT_INT\nshort  and int\nMPI_LONG_INT\nlong and int\nMPI_LONG_DOUBLE_INT\nlong double  and int\nMPI_FLOAT_INT\nfloat  and int\nMPI_DOUBLE_INT\ndouble and int\nTable 6.4. MPI datatypes for data-pairs used with the MPI_MAXLOC  and\nMPI_MINLOC  reduction operations.MPI Datatype C Datatype\nint MPI_Allreduce(void *sendbuf, void *recvbuf, int count, \n        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) \nNote that there is no target argument since all processes receive the result of the operation.\n6.6.4 Prefix\nThe prefix-sum operation described in Section 4.3 is performed in MPI using the MPI_Scan\nfunction.\nint MPI_Scan(void *sendbuf, void *recvbuf, int count, \n        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) \nMPI_Scan  performs a prefix reduction of the data stored in the buffer sendbuf  at each process\nand returns the result in the buffer recvbuf  . The receive buffer of the process with rank i will\nstore, at the end of the operation, the reduction of the send buffers of the processes whose\nranks range from 0 up to and including i . The type of supported operations (i.e., op ) as well as\nthe", "doc_id": "f6d21bf0-a8da-42b5-bd12-ae224c4b9f51", "embedding": null, "doc_hash": "59d058492e3bcf600fa147e93d75714e55b3eb160ce3c0a9cc9b8b1af80fc5e4", "extra_info": null, "node_info": {"start": 746846, "end": 749855}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8b7ceb80-aa2e-4e48-8e2f-ba3a7d47c1f5", "3": "44d4d1f9-7517-492b-8911-c8bd2739d6a0"}}, "__type__": "1"}, "44d4d1f9-7517-492b-8911-c8bd2739d6a0": {"__data__": {"text": "*sendbuf, void *recvbuf, int count, \n        MPI_Datatype datatype, MPI_Op op, MPI_Comm comm) \nMPI_Scan  performs a prefix reduction of the data stored in the buffer sendbuf  at each process\nand returns the result in the buffer recvbuf  . The receive buffer of the process with rank i will\nstore, at the end of the operation, the reduction of the send buffers of the processes whose\nranks range from 0 up to and including i . The type of supported operations (i.e., op ) as well as\nthe restrictions on the various arguments of MPI_Scan  are the same as those for the reduction\noperation MPI_Reduce  .\n6.6.5 Gather\nThe gather operation described in Section 4.4 is performed in MPI using the MPI_Gather\nfunction.\nint MPI_Gather(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, void *recvbuf, int recvcount, \n        MPI_Datatype recvdatatype, int target, MPI_Comm comm) \nEach process, including the target process, sends the data stored in the array sendbuf  to the\ntarget process. As a result, if p is the number of processors in the communication comm , the\ntarget process receives a total of p buffers. The data is stored in the array recvbuf  of the target\nprocess, in a rank order. That is, the data from process with rank i are stored in the recvbuf\nstarting at location i * sendcount  (assuming that the array recvbuf  is of the same type as\nrecvdatatype  ).\nThe data sent by each process must be of the same size and type. That is, MPI_Gather  must be\ncalled with the sendcount  and senddatatype  arguments having the same values at each\nprocess. The information about the receive buffer, its length and type applies only for the target\nprocess and is ignored for all the other processes. The argument recvcount  specifies the\nnumber of elements received by each process and not the total number of elements it receives.\nSo, recvcount  must be the same as sendcount  and their datatypes must be matching.\nMPI also provides the MPI_Allgather  function in which the data are gathered to all the\nprocesses and not only at the target process.\nint MPI_Allgather(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, void *recvbuf, int recvcount, \n        MPI_Datatype recvdatatype, MPI_Comm comm) \nThe meanings of the various parameters are similar to those for MPI_Gather  ; however, each\nprocess must now supply a recvbuf  array that will store the gathered data.\nIn addition to the above versions of the gather operation, in which the sizes of the arrays sent\nby each process are the same, MPI also provides versions in which the size of the arrays can be\ndifferent. MPI refers to these operations as the vector  variants. The vector variants of the\nMPI_Gather  and MPI_Allgather  operations are provided by the functions MPI_Gatherv and\nMPI_Allgatherv  , respectively.\nint MPI_Gatherv(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, void *recvbuf, \n        int *recvcounts, int *displs, \n        MPI_Datatype recvdatatype, int target, MPI_Comm comm) \nint MPI_Allgatherv(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, void *recvbuf, \n     ", "doc_id": "44d4d1f9-7517-492b-8911-c8bd2739d6a0", "embedding": null, "doc_hash": "fca801ea2bc11782df9b82d9d8c5939b75e5a522c2600a758e75330c169aa528", "extra_info": null, "node_info": {"start": 749836, "end": 752945}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f6d21bf0-a8da-42b5-bd12-ae224c4b9f51", "3": "cc20e891-73fb-4374-b9d0-f6a6a7ed1baa"}}, "__type__": "1"}, "cc20e891-73fb-4374-b9d0-f6a6a7ed1baa": {"__data__": {"text": " , respectively.\nint MPI_Gatherv(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, void *recvbuf, \n        int *recvcounts, int *displs, \n        MPI_Datatype recvdatatype, int target, MPI_Comm comm) \nint MPI_Allgatherv(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, void *recvbuf, \n        int *recvcounts, int *displs, MPI_Datatype recvdatatype, \n        MPI_Comm comm) \nThese functions allow a different number of data elements to be sent by each process by\nreplacing the recvcount  parameter with the array recvcounts  . The amount of data sent by\nprocess i is equal to recvcounts[i]  . Note that the size of recvcounts  is equal to the size of\nthe communicator comm . The array parameter displs , which is also of the same size, is used\nto determine where in recvbuf  the data sent by each process will be stored. In particular, the\ndata sent by process i are stored in recvbuf  starting at location displs[i]  . Note that, as\nopposed to the non-vector variants, the sendcount  parameter can be different for different\nprocesses.\n6.6.6 Scatter\nThe scatter operation described in Section 4.4  is performed in MPI using the MPI_Scatter\nfunction.\nint MPI_Scatter(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, void *recvbuf, int recvcount, \n        MPI_Datatype recvdatatype, int source, MPI_Comm comm) \nThe source process sends a different part of the send buffer sendbuf  to each processes,\nincluding itself. The data that are received are stored in recvbuf  . Process i receives sendcount\ncontiguous elements of type senddatatype  starting from the i * sendcount  location of the\nsendbuf  of the source process (assuming that sendbuf  is of the same type as senddatatype  ).\nMPI_Scatter must be called by all the processes with the same values for the sendcount  ,\nsenddatatype  , recvcount  , recvdatatype  , source , and comm arguments. Note again that\nsendcount  is the number of elements sent to each individual process.\nSimilarly to the gather operation, MPI provides a vector variant of the scatter operation, called\nMPI_Scatterv  , that allows different amounts of data to be sent to different processes.\nint MPI_Scatterv(void *sendbuf, int *sendcounts, int *displs, \n        MPI_Datatype senddatatype, void *recvbuf, int recvcount, \n        MPI_Datatype recvdatatype, int source, MPI_Comm comm) \nAs we can see, the parameter sendcount  has been replaced by the array sendcounts  that\ndetermines the number of elements to be sent to each process. In particular, the target\nprocess sends sendcounts[i]  elements to process i . Also, the array displs is used to\ndetermine where in sendbuf  these elements will be sent from. In particular, if sendbuf  is of the\nsame type is senddatatype  , the data sent to process i start at location displs[i]  of array\nsendbuf  . Both the sendcounts  and displs arrays are of size equal to the number of processes\nin the communicator. Note that by appropriately setting the displs array we can use\nMPI_Scatterv  to send overlapping regions of sendbuf ", "doc_id": "cc20e891-73fb-4374-b9d0-f6a6a7ed1baa", "embedding": null, "doc_hash": "8c43ac020f8f4b8f1213e64188bcc9c58ba6499f9929417d2f698fbe8ae6831e", "extra_info": null, "node_info": {"start": 753084, "end": 756135}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "44d4d1f9-7517-492b-8911-c8bd2739d6a0", "3": "47176e8c-572a-40c6-968e-4bc1161f6818"}}, "__type__": "1"}, "47176e8c-572a-40c6-968e-4bc1161f6818": {"__data__": {"text": "the target\nprocess sends sendcounts[i]  elements to process i . Also, the array displs is used to\ndetermine where in sendbuf  these elements will be sent from. In particular, if sendbuf  is of the\nsame type is senddatatype  , the data sent to process i start at location displs[i]  of array\nsendbuf  . Both the sendcounts  and displs arrays are of size equal to the number of processes\nin the communicator. Note that by appropriately setting the displs array we can use\nMPI_Scatterv  to send overlapping regions of sendbuf  .\n6.6.7 All-to-All\nThe all-to-all personalized communication operation described in Section 4.5  is performed in\nMPI by using the MPI_Alltoall  function.\nint MPI_Alltoall(void *sendbuf, int sendcount, \n        MPI_Datatype senddatatype, void *recvbuf, int recvcount, \n        MPI_Datatype recvdatatype, MPI_Comm comm) \nEach process sends a different portion of the sendbuf  array to each other process, including\nitself. Each process sends to process i sendcount  contiguous elements of type senddatatype\nstarting from the i * sendcount  location of its sendbuf  array. The data that are received are\nstored in the recvbuf  array. Each process receives from process i recvcount  elements of type\nrecvdatatype  and stores them in its recvbuf  array starting at location i * recvcount  .\nMPI_Alltoall  must be called by all the processes with the same values for the sendcount  ,\nsenddatatype  , recvcount  , recvdatatype  , and comm arguments. Note that sendcount  and\nrecvcount  are the number of elements sent to, and received from, each individual process.\nMPI also provides a vector variant of the all-to-all personalized communication operation called\nMPI_Alltoallv  that allows different amounts of data to be sent to and received from each\nprocess.\nint MPI_Alltoallv(void *sendbuf, int *sendcounts, int *sdispls \n        MPI_Datatype senddatatype, void *recvbuf, int *recvcounts, \n        int *rdispls, MPI_Datatype recvdatatype, MPI_Comm comm) \nThe parameter sendcounts  is used to specify the number of elements sent to each process, and\nthe parameter sdispls  is used to specify the location in sendbuf  in which these elements are\nstored. In particular, each process sends to process i , starting at location sdispls[i]  of the\narray sendbuf  , sendcounts[i]  contiguous elements. The parameter recvcounts  is used to\nspecify the number of elements received by each process, and the parameter rdispls  is used to\nspecify the location in recvbuf  in which these elements are stored. In particular, each process\nreceives from process i recvcounts[i]  elements that are stored in contiguous locations of\nrecvbuf  starting at location rdispls[i]  . MPI_Alltoallv  must be called by all the processes\nwith the same values for the senddatatype  , recvdatatype  , and comm arguments.\n6.6.8 Example: One-Dimensional Matrix-Vector Multiplication\nOur first message-passing program using collective communications will be to multiply a dense\nn x n matrix A with a vector b , i.e., x = Ab . As discussed in Section 8.1  , one way of\nperforming this multiplication in parallel is to have each process compute different portions of\nthe product-vector x . In particular, each one of the p processes is responsible for computing n\n/p", "doc_id": "47176e8c-572a-40c6-968e-4bc1161f6818", "embedding": null, "doc_hash": "b76ccbabd41594c1c534f605d6b84b2143531fec3c6174a0b5ac75c7057e069e", "extra_info": null, "node_info": {"start": 755960, "end": 759211}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "cc20e891-73fb-4374-b9d0-f6a6a7ed1baa", "3": "6d28232c-9ab6-46eb-8be9-6e5237a94df8"}}, "__type__": "1"}, "6d28232c-9ab6-46eb-8be9-6e5237a94df8": {"__data__": {"text": "be called by all the processes\nwith the same values for the senddatatype  , recvdatatype  , and comm arguments.\n6.6.8 Example: One-Dimensional Matrix-Vector Multiplication\nOur first message-passing program using collective communications will be to multiply a dense\nn x n matrix A with a vector b , i.e., x = Ab . As discussed in Section 8.1  , one way of\nperforming this multiplication in parallel is to have each process compute different portions of\nthe product-vector x . In particular, each one of the p processes is responsible for computing n\n/p consecutive elements of x . This algorithm can be implemented in MPI by distributing the\nmatrix A in a row-wise fashion, such that each process receives the n /p rows that correspond\nto the portion of the product-vector x it computes. Vector b is distributed in a fashion similar to\nx .\nProgram 6.4  shows the MPI program that uses a row-wise distribution of matrix A . The\ndimension of the matrices is supplied in the parameter n , the parameters a and b point to the\nlocally stored portions of matrix A and vector b , respectively, and the parameter x points to the\nlocal portion of the output matrix-vector product. This program assumes that n is a multiple of\nthe number of processors.\nProgram 6.4 Row-wise Matrix-Vector Multiplication\n 1   RowMatrixVectorMultiply(int n, double *a, double *b, double *x, \n 2                           MPI_Comm comm) \n 3   { \n 4     int i, j; \n 5     int nlocal;        /* Number of locally stored rows of A */  \n 6     double *fb;        /* Will point to a buffer that stores the entire vector b */  \n 7     int npes, myrank; \n 8     MPI_Status status; \n 9 \n10     /* Get information about the communicator */  \n11     MPI_Comm_size(comm, &npes); \n12     MPI_Comm_rank(comm, &myrank); \n13 \n14     /* Allocate the memory that will store the entire vector b */  \n15     fb = (double *)malloc(n*sizeof(double)); \n16 \n17     nlocal = n/npes; \n18 \n19     /* Gather the entire vector b on each processor using MPI's ALLGATHER operation */  \n20     MPI_Allgather(b, nlocal, MPI_DOUBLE, fb, nlocal, MPI_DOUBLE, \n21         comm); \n22 \n23     /* Perform the matrix-vector multiplication involving the locally stored submatrix */  \n24     for (i=0; i<nlocal; i++) { \n25       x[i] = 0.0; \n26       for (j=0; j<n; j++) \n27         x[i] += a[i*n+j]*fb[j]; \n28     } \n29 \n30     free(fb); \n31   } \nAn alternate way of computing x is to parallelize the task of performing the dot-product for each\nelement of x . That is, for each element xi , of vector x , all the processes will compute a part of\nit, and the result will be obtained by adding up these partial dot-products. This algorithm can be\nimplemented in MPI by distributing matrix A in a column-wise fashion. Each process gets n /p\nconsecutive columns of A , and the elements of vector b that correspond to these columns.\nFurthermore, at the end of the", "doc_id": "6d28232c-9ab6-46eb-8be9-6e5237a94df8", "embedding": null, "doc_hash": "d47078ecbaa7072c17769d58210824e7edddf59856e9a6518a9cc6a80e453120", "extra_info": null, "node_info": {"start": 759180, "end": 762067}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "47176e8c-572a-40c6-968e-4bc1161f6818", "3": "83b70094-efd0-46b8-999f-3da3c52e6e09"}}, "__type__": "1"}, "83b70094-efd0-46b8-999f-3da3c52e6e09": {"__data__": {"text": "\n30     free(fb); \n31   } \nAn alternate way of computing x is to parallelize the task of performing the dot-product for each\nelement of x . That is, for each element xi , of vector x , all the processes will compute a part of\nit, and the result will be obtained by adding up these partial dot-products. This algorithm can be\nimplemented in MPI by distributing matrix A in a column-wise fashion. Each process gets n /p\nconsecutive columns of A , and the elements of vector b that correspond to these columns.\nFurthermore, at the end of the computation we want the product-vector x to be distributed in a\nfashion similar to vector b . Program 6.5  shows the MPI program that implements this column-\nwise distribution of the matrix.\nProgram 6.5 Column-wise Matrix-Vector Multiplication\n 1   ColMatrixVectorMultiply(int n, double *a, double *b, double *x, \n 2                           MPI_Comm comm) \n 3   { \n 4     int i, j; \n 5     int nlocal; \n 6     double *px; \n 7     double *fx; \n 8     int npes, myrank; \n 9     MPI_Status status; \n10 \n11     /* Get identity and size information from the communicator */  \n12     MPI_Comm_size(comm, &npes); \n13     MPI_Comm_rank(comm, &myrank); \n14 \n15     nlocal = n/npes; \n16 \n17     /* Allocate memory for arrays storing intermediate results. */  \n18     px = (double *)malloc(n*sizeof(double)); \n19     fx = (double *)malloc(n*sizeof(double)); \n20 \n21     /* Compute the partial-dot products that correspond to the local columns of A.*/  \n22     for (i=0; i<n; i++) { \n23       px[i] = 0.0; \n24       for (j=0; j<nlocal; j++) \n25         px[i] += a[i*nlocal+j]*b[j]; \n26     } \n27 \n28     /* Sum-up the results by performing an element-wise reduction operation */  \n29     MPI_Reduce(px, fx, n, MPI_DOUBLE, MPI_SUM, 0, comm); \n30 \n31     /* Redistribute fx in a fashion similar to that of vector b */  \n32     MPI_Scatter(fx, nlocal, MPI_DOUBLE, x, nlocal, MPI_DOUBLE, 0, \n33         comm); \n34 \n35     free(px); free(fx); \n36   } \nComparing these two programs for performing matrix-vector multiplication we see that the row-\nwise version needs to perform only a MPI_Allgather  operation whereas the column-wise\nprogram needs to perform a MPI_Reduce  and a MPI_Scatter operation. In general, a row-wise\ndistribution is preferable as it leads to small communication overhead (see Problem 6.6  ).\nHowever, many times, an application needs to compute not only Ax but also AT x . In that case,\nthe row-wise distribution can be used to compute Ax , but the computation of AT x requires the\ncolumn-wise distribution (a row-wise distribution of A is a column-wise distribution of its\ntranspose AT ). It is much cheaper to use the program for the column-wise distribution than to\ntranspose the matrix and then use the row-wise program. We must also note that using", "doc_id": "83b70094-efd0-46b8-999f-3da3c52e6e09", "embedding": null, "doc_hash": "41ee9b482f9eb02bdd2ce4e7b0deec861c4570fe669e278c4b19fc43e7c26217", "extra_info": null, "node_info": {"start": 762093, "end": 764893}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6d28232c-9ab6-46eb-8be9-6e5237a94df8", "3": "51a95c28-2547-4734-9da8-eae059c51383"}}, "__type__": "1"}, "51a95c28-2547-4734-9da8-eae059c51383": {"__data__": {"text": "operation. In general, a row-wise\ndistribution is preferable as it leads to small communication overhead (see Problem 6.6  ).\nHowever, many times, an application needs to compute not only Ax but also AT x . In that case,\nthe row-wise distribution can be used to compute Ax , but the computation of AT x requires the\ncolumn-wise distribution (a row-wise distribution of A is a column-wise distribution of its\ntranspose AT ). It is much cheaper to use the program for the column-wise distribution than to\ntranspose the matrix and then use the row-wise program. We must also note that using a dual\nof the all-gather operation, it is possible to develop a parallel formulation for column-wise\ndistribution that is as fast as the program using row-wise distribution (see Problem 6.7  ).\nHowever, this dual operation is not available in MPI.\n6.6.9 Example: Single-Source Shortest-Path\nOur second message-passing program that uses collective communication operations computes\nthe shortest paths from a source-vertex s to all the other vertices in a graph using Dijkstra's\nsingle-source shortest-path algorithm described in Section 10.3  . This program is shown in\nProgram 6.6.\nThe parameter n stores the total number of vertices in the graph, and the parameter source\nstores the vertex from which we want to compute the single-source shortest path. The\nparameter wgt points to the locally stored portion of the weighted adjacency matrix of the\ngraph. The parameter lengths  points to a vector that will store the length of the shortest paths\nfrom source to the locally stored vertices. Finally, the parameter comm is the communicator to\nbe used by the MPI routines. Note that this routine assumes that the number of vertices is a\nmultiple of the number of processors.\nProgram 6.6 Dijkstra's Single-Source Shortest-Path\n[View full width]\n 1   SingleSource(int n, int source, int *wgt, int *lengths, MPI_Comm comm) \n 2   { \n 3     int i, j; \n 4     int nlocal;  /* The number of vertices stored locally */  \n 5     int *marker;  /* Used to mark the vertices belonging to V o */ \n 6     int firstvtx;  /* The index number of the first vertex that is stored locally */  \n 7     int lastvtx;  /* The index number of the last vertex that is stored locally */  \n 8     int u, udist; \n 9     int lminpair[2], gminpair[2]; \n10     int npes, myrank; \n11     MPI_Status status; \n12 \n13     MPI_Comm_size(comm, &npes); \n14     MPI_Comm_rank(comm, &myrank); \n15 \n16     nlocal   = n/npes; \n17     firstvtx = myrank*nlocal; \n18     lastvtx  = firstvtx+nlocal-1; \n19 \n20     /* Set the initial distances from source to all the other vertices */  \n21     for (j=0; j<nlocal; j++) \n22       lengths[j] = wgt[source*nlocal + j]; \n23 \n24     /* This array is used to indicate if the shortest part to a vertex has been found \nor not. */  \n25     /* if marker  [v] is one, then the shortest path to v has been found, otherwise it \nhas not. */  \n26     marker = (int *)malloc(nlocal*sizeof(int)); \n27     for (j=0; j<nlocal; j++) \n28       marker[j] = 1; \n29", "doc_id": "51a95c28-2547-4734-9da8-eae059c51383", "embedding": null, "doc_hash": "be2db629a58ed7a55cf3c9a777854c1f4257f25f0c307da3b750fbcd7fdf787d", "extra_info": null, "node_info": {"start": 764841, "end": 767870}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "83b70094-efd0-46b8-999f-3da3c52e6e09", "3": "666cc462-52c3-4e94-b3e7-7984029798c8"}}, "__type__": "1"}, "666cc462-52c3-4e94-b3e7-7984029798c8": {"__data__": {"text": "   lengths[j] = wgt[source*nlocal + j]; \n23 \n24     /* This array is used to indicate if the shortest part to a vertex has been found \nor not. */  \n25     /* if marker  [v] is one, then the shortest path to v has been found, otherwise it \nhas not. */  \n26     marker = (int *)malloc(nlocal*sizeof(int)); \n27     for (j=0; j<nlocal; j++) \n28       marker[j] = 1; \n29 \n30     /* The process that stores the source vertex, marks it as being seen */  \n31     if (source >= firstvtx && source <= lastvtx) \n32       marker[source-firstvtx] = 0; \n33 \n34     /* The main loop of Dijkstra's algorithm */  \n35     for (i=1; i<n; i++) { \n36       /* Step 1: Find the local vertex that is at the smallest distance from source */  \n37       lminpair[0] = MAXINT; /* set it to an architecture dependent large number */  \n38       lminpair[1] = -1; \n39       for (j=0; j<nlocal; j++) { \n40         if (marker[j] && lengths[j] < lminpair[0]) { \n41           lminpair[0] = lengths[j]; \n42           lminpair[1] = firstvtx+j; \n43         } \n44       } \n45 \n46       /* Step 2: Compute the global minimum vertex, and insert it into V c */ \n47       MPI_Allreduce(lminpair, gminpair, 1, MPI_2INT, MPI_MINLOC, \n48           comm); \n49       udist = gminpair[0]; \n50       u = gminpair[1]; \n51 \n52       /* The process that stores the minimum vertex, marks it as being seen */  \n\n53       if (u == lminpair[1]) \n54         marker[u-firstvtx] = 0; \n55 \n56       /* Step 3: Update the distances given that u got inserted */  \n57       for (j=0; j<nlocal; j++) { \n58         if (marker[j] && udist + wgt[u*nlocal+j] < lengths[j]) \n59           lengths[j] = udist + wgt[u*nlocal+j]; \n60       } \n61     } \n62 \n63     free(marker); \n64   } \nThe main computational loop of Dijkstra's parallel single-source shortest path algorithm\nperforms three steps. First, each process finds the locally stored vertex in Vo that has the\nsmallest distance from the source. Second, the vertex that has the smallest distance over all\nprocesses is determined, and it is included in Vc . Third, all processes update their distance\narrays to reflect the inclusion of the new vertex in Vc .\nThe first step is performed by scanning the locally stored vertices in Vo and determining the one\nvertex v with the smaller lengths  [v ] value. The result of this computation is stored in the array\nlminpair  . In particular, lminpair  [0] stores the distance of the vertex, and", "doc_id": "666cc462-52c3-4e94-b3e7-7984029798c8", "embedding": null, "doc_hash": "748662114950ca70150447cbf310f110008a74003e92cd7a0a8a7e9ff6bef9ba", "extra_info": null, "node_info": {"start": 768088, "end": 770509}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "51a95c28-2547-4734-9da8-eae059c51383", "3": "4c72a3e2-88fc-4705-a399-ef2052cb34e8"}}, "__type__": "1"}, "4c72a3e2-88fc-4705-a399-ef2052cb34e8": {"__data__": {"text": "vertex in Vo that has the\nsmallest distance from the source. Second, the vertex that has the smallest distance over all\nprocesses is determined, and it is included in Vc . Third, all processes update their distance\narrays to reflect the inclusion of the new vertex in Vc .\nThe first step is performed by scanning the locally stored vertices in Vo and determining the one\nvertex v with the smaller lengths  [v ] value. The result of this computation is stored in the array\nlminpair  . In particular, lminpair  [0] stores the distance of the vertex, and lminpair  [1] stores\nthe vertex itself. The reason for using this storage scheme will become clear when we consider\nthe next step, in which we must compute the vertex that has the smallest overall distance from\nthe source. We can find the overall shortest distance by performing a min-reduction on the\ndistance values stored in lminpair  [0]. However, in addition to the shortest distance, we also\nneed to know the vertex that is at that shortest distance. For this reason, the appropriate\nreduction operation is the MPI_MINLOC  which returns both the minimum as well as an index\nvalue associated with that minimum. Because of MPI_MINLOC  we use the two-element array\nlminpair  to store the distance as well as the vertex that achieves this distance. Also, because the\nresult of the reduction operation is needed by all the processes to perform the third step, we use\nthe MPI_Allreduce  operation to perform the reduction. The result of the reduction operation is\nreturned in the gminpair  array. The third and final step during each iteration is performed by\nscanning the local vertices that belong in Vo and updating their shortest distances from the\nsource vertex.\nAvoiding Load Imbalances  Program 6.6  assigns n /p consecutive columns of W to each\nprocessor and in each iteration it uses the MPI_MINLOC  reduction operation to select the vertex v\nto be included in Vc . Recall that the MPI_MINLOC  operation for the pairs ( a , i ) and ( a , j ) will\nreturn the one that has the smaller index (since both of them have the same value).\nConsequently, among the vertices that are equally close to the source vertex, it favors the\nsmaller numbered vertices. This may lead to load imbalances, because vertices stored in lower-\nranked processes will tend to be included in Vc faster than vertices in higher-ranked processes\n(especially when many vertices in Vo are at the same minimum distance from the source).\nConsequently, the size of the set Vo will be larger in higher-ranked processes, dominating the\noverall runtime.\nOne way of correcting this problem is to distribute the columns of W using a cyclic distribution.\nIn this distribution process i gets every p th vertex starting from vertex i . This scheme also\nassigns n /p vertices to each process but these vertices have indices that span almost the entire\ngraph. Consequently, the preference given to lower-numbered vertices by MPI_MINLOC  does not\nlead to load-imbalance problems.\n6.6.10 Example: Sample Sort\nThe last problem requiring collective communications that we will consider is that of sorting a\nsequence A of n elements using the sample sort algorithm described in Section 9.5  . The\nprogram is shown in Program 6.7  .\nThe SampleSort  function takes as input the sequence of elements stored at each process and\nreturns a pointer to an array that stores the sorted sequence as well as the number of elements\nin this sequence. The elements of this SampleSort  function are integers and they are sorted in\nincreasing order. The total number of elements to be sorted is specified by the parameter n and\na pointer to the array that stores the local portion of these elements is specified by elmnts . On\nreturn, the parameter nsorted  will store the number of elements in the", "doc_id": "4c72a3e2-88fc-4705-a399-ef2052cb34e8", "embedding": null, "doc_hash": "a59542d22805d3cbf826bf7fb1108fbf5d2770c261ba022e9e661e7294db4b5b", "extra_info": null, "node_info": {"start": 770328, "end": 774119}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "666cc462-52c3-4e94-b3e7-7984029798c8", "3": "48aec26e-3e79-4b06-a7ea-7ed7d5c3939d"}}, "__type__": "1"}, "48aec26e-3e79-4b06-a7ea-7ed7d5c3939d": {"__data__": {"text": "described in Section 9.5  . The\nprogram is shown in Program 6.7  .\nThe SampleSort  function takes as input the sequence of elements stored at each process and\nreturns a pointer to an array that stores the sorted sequence as well as the number of elements\nin this sequence. The elements of this SampleSort  function are integers and they are sorted in\nincreasing order. The total number of elements to be sorted is specified by the parameter n and\na pointer to the array that stores the local portion of these elements is specified by elmnts . On\nreturn, the parameter nsorted  will store the number of elements in the returned sorted array.\nThis routine assumes that n is a multiple of the number of processes.\nProgram 6.7 Samplesort\n[View full width]\n 1   int *SampleSort(int n, int *elmnts, int *nsorted, MPI_Comm comm) \n 2   { \n 3     int i, j, nlocal, npes, myrank; \n 4     int *sorted_elmnts, *splitters, *allpicks; \n 5     int *scounts, *sdispls, *rcounts, *rdispls; \n 6 \n 7     /* Get communicator-related information */  \n 8     MPI_Comm_size(comm, &npes); \n 9     MPI_Comm_rank(comm, &myrank); \n10 \n11     nlocal = n/npes; \n12 \n13     /* Allocate memory for the arrays that will store the splitters */  \n14     splitters = (int *)malloc(npes*sizeof(int)); \n15     allpicks = (int *)malloc(npes*(npes-1)*sizeof(int)); \n16 \n17     /* Sort local array */  \n18     qsort(elmnts, nlocal, sizeof(int), IncOrder); \n19 \n20     /* Select local npes-1 equally spaced elements */  \n21     for (i=1; i<npes; i++) \n22       splitters[i-1] = elmnts[i*nlocal/npes]; \n23 \n24     /* Gather the samples in the processors */  \n25     MPI_Allgather(splitters, npes-1, MPI_INT, allpicks, npes-1, \n26         MPI_INT, comm); \n27 \n28     /* sort these samples */  \n29     qsort(allpicks, npes*(npes-1), sizeof(int), IncOrder); \n30 \n31     /* Select splitters */  \n32     for (i=1; i<npes; i++) \n33       splitters[i-1] = allpicks[i*npes]; \n34     splitters[npes-1] = MAXINT; \n35 \n36     /* Compute the number of elements that belong to each bucket */  \n37     scounts = (int *)malloc(npes*sizeof(int)); \n38     for (i=0; i<npes; i++) \n39       scounts[i] = 0; \n40 \n41     for (j=i=0; i<nlocal; i++) { \n42       if (elmnts[i] < splitters[j]) \n43         scounts[j]++; \n44       else \n45         scounts[++j]++; \n46     } \n47 \n48     /* Determine the starting location of each bucket's elements in the elmnts array */  \n49     sdispls =", "doc_id": "48aec26e-3e79-4b06-a7ea-7ed7d5c3939d", "embedding": null, "doc_hash": "b678fb6c9f87cf8cc0c1c4995de7f4b1ac4c09b47df197f11b2e5110d76f42d9", "extra_info": null, "node_info": {"start": 774067, "end": 776487}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4c72a3e2-88fc-4705-a399-ef2052cb34e8", "3": "ac580ae6-71f3-4a58-82af-fa9e9e8b0914"}}, "__type__": "1"}, "ac580ae6-71f3-4a58-82af-fa9e9e8b0914": {"__data__": {"text": " scounts[i] = 0; \n40 \n41     for (j=i=0; i<nlocal; i++) { \n42       if (elmnts[i] < splitters[j]) \n43         scounts[j]++; \n44       else \n45         scounts[++j]++; \n46     } \n47 \n48     /* Determine the starting location of each bucket's elements in the elmnts array */  \n49     sdispls = (int *)malloc(npes*sizeof(int)); \n50     sdispls[0] = 0; \n51     for (i=1; i<npes; i++) \n52       sdispls[i] = sdispls[i-1]+scounts[i-1]; \n53 \n54     /* Perform an all-to-all to inform the corresponding processes of the number of \nelements */  \n55     /* they are going to receive. This information is stored in rcounts array */  \n56     rcounts = (int *)malloc(npes*sizeof(int)); \n57     MPI_Alltoall(scounts, 1, MPI_INT, rcounts, 1, MPI_INT, comm); \n58 \n59     /* Based on rcounts determine where in the local array the data from each \nprocessor */  \n60     /* will be stored. This array will store the received elements as well as the \nfinal */  \n61     /* sorted sequence */  \n62     rdispls = (int *)malloc(npes*sizeof(int)); \n63     rdispls[0] = 0; \n64     for (i=1; i<npes; i++) \n65       rdispls[i] = rdispls[i-1]+rcounts[i-1]; \n66 \n67     *nsorted = rdispls[npes-1]+rcounts[i-1]; \n68     sorted_elmnts = (int *)malloc((*nsorted)*sizeof(int)); \n69 \n70     /* Each process sends and receives the corresponding elements, using the \nMPI_Alltoallv */  \n71     /* operation. The arrays scounts and sdispls are used to specify the number of \nelements */  \n72     /* to be sent and where these elements are stored, respectively. The arrays \nrcounts */  \n73     /* and rdispls are used to specify the number of elements to be received, and \nwhere these */  \n74     /* elements will be stored, respectively. */  \n75     MPI_Alltoallv(elmnts, scounts, sdispls, MPI_INT, sorted_elmnts, \n76         rcounts, rdispls, MPI_INT, comm); \n77 \n78     /* Perform the final local sort */  \n79     qsort(sorted_elmnts, *nsorted, sizeof(int), IncOrder); \n80 \n81     free(splitters); free(allpicks); free(scounts); free(sdispls); \n82     free(rcounts); free(rdispls); \n83 \n84     return sorted_elmnts; \n85   } \n[ Team LiB ]\n\n \n\n[ Team LiB ]\n \n6.7 Groups and Communicators\nIn many parallel algorithms, communication operations need to be restricted to certain subsets\nof processes. MPI provides several mechanisms for", "doc_id": "ac580ae6-71f3-4a58-82af-fa9e9e8b0914", "embedding": null, "doc_hash": "dcedc8c2d0d16975f4efa41e18ceac4e248293d9df93bbbabe598624a6aa2472", "extra_info": null, "node_info": {"start": 776794, "end": 779087}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "48aec26e-3e79-4b06-a7ea-7ed7d5c3939d", "3": "991893b7-43c4-478d-b71c-ee8de6a1e887"}}, "__type__": "1"}, "991893b7-43c4-478d-b71c-ee8de6a1e887": {"__data__": {"text": "*nsorted, sizeof(int), IncOrder); \n80 \n81     free(splitters); free(allpicks); free(scounts); free(sdispls); \n82     free(rcounts); free(rdispls); \n83 \n84     return sorted_elmnts; \n85   } \n[ Team LiB ]\n\n \n\n[ Team LiB ]\n \n6.7 Groups and Communicators\nIn many parallel algorithms, communication operations need to be restricted to certain subsets\nof processes. MPI provides several mechanisms for partitioning the group of processes that\nbelong to a communicator into subgroups each corresponding to a different communicator. A\ngeneral method for partitioning a graph of processes is to use MPI_Comm_split  that is defined\nas follows:\nint MPI_Comm_split(MPI_Comm comm, int color, int key, \n        MPI_Comm *newcomm) \nThis function is a collective operation, and thus needs to be called by all the processes in the\ncommunicator comm . The function takes color  and key as input parameters in addition to the\ncommunicator, and partitions the group of processes in the communicator comm into disjoint\nsubgroups. Each subgroup contains all processes that have supplied the same value for the\ncolor  parameter. Within each subgroup, the processes are ranked in the order defined by the\nvalue of the key parameter, with ties broken according to their rank in the old communicator\n(i.e., comm ). A new communicator for each subgroup is returned in the newcomm  parameter.\nFigure 6.7  shows an example of splitting a communicator using the MPI_Comm_split  function. If\neach process called MPI_Comm_split  using the values of parameters color  and key as shown in\nFigure 6.7  , then three communicators will be created, containing processes {0, 1, 2}, {3, 4, 5,\n6}, and {7}, respectively.\nFigure 6.7. Using MPI_Comm_split  to split a group of processes in a\ncommunicator into subgroups.\nSplitting Cartesian Topologies  In many parallel algorithms, processes are arranged in a\nvirtual grid, and in different steps of the algorithm, communication needs to be restricted to a\ndifferent subset of the grid. MPI provides a convenient way to partition a Cartesian topology to\nform lower-dimensional grids.\nMPI provides the MPI_Cart_sub  function that allows us to partition a Cartesian topology into\nsub-topologies that form lower-dimensional grids. For example, we can partition a two-\ndimensional topology into groups, each consisting of the processes along the row or column of\nthe topology. The calling sequence of MPI_Cart_sub  is the following:\nint MPI_Cart_sub(MPI_Comm comm_cart, int *keep_dims, \n        MPI_Comm *comm_subcart) \nThe array keep_dims  is used to specify how the Cartesian topology is partitioned. In particular,\nif keep_dims[i]  is true (non-zero value in C) then the i th dimension is retained in the new\nsub-topology. For example, consider a three-dimensional topology of size 2 x 4 x 7. If\nkeep_dims  is {true, false, true}, then the original topology is split into four two-dimensional\nsub-topologies of size 2 x 7, as illustrated in Figure 6.8(a)  . If keep_dims  is {false, false, true},\nthen the original topology is split into eight one-dimensional topologies of size seven, illustrated\nin Figure 6.8(b)  . Note that the number of sub-topologies created is equal to the product of the\nnumber of processes along the dimensions that are not being retained. The original topology is\nspecified by the communicator comm_cart  , and the returned communicator comm_subcart\nstores information about the created", "doc_id": "991893b7-43c4-478d-b71c-ee8de6a1e887", "embedding": null, "doc_hash": "b4d9cfc6c4aef0add54a275377ea21d875d216bfa62a8704d48f6d9a845d89c0", "extra_info": null, "node_info": {"start": 778958, "end": 782377}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ac580ae6-71f3-4a58-82af-fa9e9e8b0914", "3": "e75e3fbc-d8c0-4bc6-98af-08e3bf6ae3bb"}}, "__type__": "1"}, "e75e3fbc-d8c0-4bc6-98af-08e3bf6ae3bb": {"__data__": {"text": "then the original topology is split into four two-dimensional\nsub-topologies of size 2 x 7, as illustrated in Figure 6.8(a)  . If keep_dims  is {false, false, true},\nthen the original topology is split into eight one-dimensional topologies of size seven, illustrated\nin Figure 6.8(b)  . Note that the number of sub-topologies created is equal to the product of the\nnumber of processes along the dimensions that are not being retained. The original topology is\nspecified by the communicator comm_cart  , and the returned communicator comm_subcart\nstores information about the created sub-topology. Only a single communicator is returned to\neach process, and for processes that do not belong to the same sub-topology, the group\nspecified by the returned communicator is different.\nFigure 6.8. Splitting a Cartesian topology of size 2 x 4 x 7 into (a) four\nsubgroups of size 2 x 1 x 7, and (b) eight subgroups of size 1 x 1 x 7.\nThe processes belonging to a given sub-topology can be determined as follows. Consider a\nthree-dimensional topology of size d 1 x d 2 x d 3 , and assume that keep_dims  is set to {true,\nfalse, true}. The group of processes that belong to the same sub-topology as the process with\ncoordinates ( x , y , z ) is given by (*, y , *), where a '*' in a coordinate denotes all the possible\nvalues for this coordinate. Note also that since the second coordinate can take d 2 values, a total\nof d 2 sub-topologies are created.\nAlso, the coordinate of a process in a sub-topology created by MPI_Cart_sub  can be obtained\nfrom its coordinate in the original topology by disregarding the coordinates that correspond to\nthe dimensions that were not retained. For example, the coordinate of a process in the column-\nbased sub-topology is equal to its row-coordinate in the two-dimensional topology. For\ninstance, the process with coordinates (2, 3) has a coordinate of (2) in the sub-topology that\ncorresponds to the third column of the grid.\n6.7.1 Example: Two-Dimensional Matrix-Vector Multiplication\nIn Section 6.6.8  , we presented two programs for performing the matrix-vector multiplication x\n= Ab using a row- and column-wise distribution of the matrix. As discussed in Section 8.1.2  , an\nalternative way of distributing matrix A is to use a two-dimensional distribution, giving rise to\nthe two-dimensional parallel formulations of the matrix-vector multiplication algorithm.\nProgram 6.8  shows how these topologies and their partitioning are used to implement the two-\ndimensional matrix-vector multiplication. The dimension of the matrix is supplied in the\nparameter n , the parameters a and b point to the locally stored portions of matrix A and vector\nb , respectively, and the parameter x points to the local portion of the output matrix-vector\nproduct. Note that only the processes along the first column of the process grid will store b\ninitially, and that upon return, the same set of processes will store the result x . For simplicity,\nthe program assumes that the number of processes p is a perfect square and that n is a\nmultiple of \n .\nProgram 6.8 Two-Dimensional Matrix-Vector Multiplication\n[View full width]\n 1   MatrixVectorMultiply_2D(int n, double *a, double *b, double *x, \n 2                           MPI_Comm comm) \n 3   { \n 4     int ROW=0, COL=1; /* Improve readability */  \n 5     int i, j, nlocal; \n 6     double *px; /* Will store partial dot products */  \n 7     int npes, dims[2],", "doc_id": "e75e3fbc-d8c0-4bc6-98af-08e3bf6ae3bb", "embedding": null, "doc_hash": "6d3ad3aa2db8d646ffd7d6184d23ade0bd4bc30f4c909fbbb064c47359faf480", "extra_info": null, "node_info": {"start": 782217, "end": 785646}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "991893b7-43c4-478d-b71c-ee8de6a1e887", "3": "4271e01f-18dc-4d56-8c12-719847a7811f"}}, "__type__": "1"}, "4271e01f-18dc-4d56-8c12-719847a7811f": {"__data__": {"text": "Matrix-Vector Multiplication\n[View full width]\n 1   MatrixVectorMultiply_2D(int n, double *a, double *b, double *x, \n 2                           MPI_Comm comm) \n 3   { \n 4     int ROW=0, COL=1; /* Improve readability */  \n 5     int i, j, nlocal; \n 6     double *px; /* Will store partial dot products */  \n 7     int npes, dims[2], periods[2], keep_dims[2]; \n 8     int myrank, my2drank, mycoords[2]; \n 9     int other_rank, coords[2]; \n10     MPI_Status status; \n11     MPI_Comm comm_2d, comm_row, comm_col; \n12 \n13     /* Get information about the communicator */  \n14     MPI_Comm_size(comm, &npes); \n15     MPI_Comm_rank(comm, &myrank); \n16 \n17     /* Compute the size of the square grid */  \n18     dims[ROW] = dims[COL] = sqrt(npes); \n19 \n20     nlocal = n/dims[ROW]; \n21 \n22     /* Allocate memory for the array that will hold the partial dot-products */  \n23     px = malloc(nlocal*sizeof(double)); \n24 \n25     /* Set up the Cartesian topology and get the rank & coordinates of the process in \nthis topology */  \n26     periods[ROW] = periods[COL] = 1; /* Set the periods for wrap-around connections */  \n27 \n28     MPI_Cart_create(MPI_COMM_WORLD, 2, dims, periods, 1, &comm_2d); \n29 \n30     MPI_Comm_rank(comm_2d, &my2drank); /* Get my rank in the new topology */  \n31     MPI_Cart_coords(comm_2d, my2drank, 2, mycoords); /* Get my coordinates */  \n32 \n33     /* Create the row-based sub-topology */  \n\n34     keep_dims[ROW] = 0; \n35     keep_dims[COL] = 1; \n36     MPI_Cart_sub(comm_2d, keep_dims, &comm_row); \n37 \n38     /* Create the column-based sub-topology */  \n39     keep_dims[ROW] = 1; \n40     keep_dims[COL] = 0; \n41     MPI_Cart_sub(comm_2d, keep_dims, &comm_col); \n42 \n43     /* Redistribute the b vector. */  \n44     /* Step 1. The processors along the 0th column send their data to the diagonal \nprocessors */  \n45     if (mycoords[COL] == 0 && mycoords[ROW] != 0) { /* I'm in the first column */  \n46       coords[ROW] = mycoords[ROW]; \n47       coords[COL] = mycoords[ROW]; \n48       MPI_Cart_rank(comm_2d, coords, &other_rank); \n49       MPI_Send(b, nlocal, MPI_DOUBLE, other_rank, 1, comm_2d); \n50     } \n51     if (mycoords[ROW] == mycoords[COL] && mycoords[ROW] != 0) {", "doc_id": "4271e01f-18dc-4d56-8c12-719847a7811f", "embedding": null, "doc_hash": "3061754fece2c1b209ac0bf16ddeff073c42553601bab02b86af1cf7432d2c5f", "extra_info": null, "node_info": {"start": 785907, "end": 788107}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e75e3fbc-d8c0-4bc6-98af-08e3bf6ae3bb", "3": "6201432e-157d-4ebb-bdc9-b87c4b23e19f"}}, "__type__": "1"}, "6201432e-157d-4ebb-bdc9-b87c4b23e19f": {"__data__": {"text": " \n46       coords[ROW] = mycoords[ROW]; \n47       coords[COL] = mycoords[ROW]; \n48       MPI_Cart_rank(comm_2d, coords, &other_rank); \n49       MPI_Send(b, nlocal, MPI_DOUBLE, other_rank, 1, comm_2d); \n50     } \n51     if (mycoords[ROW] == mycoords[COL] && mycoords[ROW] != 0) { \n52       coords[ROW] = mycoords[ROW]; \n53       coords[COL] = 0; \n54       MPI_Cart_rank(comm_2d, coords, &other_rank); \n55       MPI_Recv(b, nlocal, MPI_DOUBLE, other_rank, 1, comm_2d, \n56           &status); \n57     } \n58 \n59     /* Step 2. The diagonal processors perform a column-wise broadcast */  \n60     coords[0] = mycoords[COL]; \n61     MPI_Cart_rank(comm_col, coords, &other_rank); \n62     MPI_Bcast(b, nlocal, MPI_DOUBLE, other_rank, comm_col); \n63 \n64     /* Get into the main computational loop */  \n65     for (i=0; i<nlocal; i++) { \n66       px[i] = 0.0; \n67       for (j=0; j<nlocal; j++) \n68         px[i] += a[i*nlocal+j]*b[j]; \n69     } \n70 \n71     /* Perform the sum-reduction along the rows to add up the partial dot-products */  \n72     coords[0] = 0; \n73     MPI_Cart_rank(comm_row, coords, &other_rank); \n74     MPI_Reduce(px, x, nlocal, MPI_DOUBLE, MPI_SUM, other_rank, \n75         comm_row); \n76 \n77     MPI_Comm_free(&comm_2d); /* Free up communicator */  \n78     MPI_Comm_free(&comm_row); /* Free up communicator */  \n79     MPI_Comm_free(&comm_col); /* Free up communicator */  \n80 \n81     free(px); \n82   } \n[ Team LiB ]\n \n\n[ Team LiB ]\n  \n6.8 Bibliographic Remarks\nThe best source for information about MPI is the actual reference of the library itself [ Mes94 ]. At\nthe time of writing of this book, there have been two major releases of the MPI standard. The\nfirst release, version 1.0, was released in 1994 and its most recent revision, version 1.2, has\nbeen implemented by the majority of hardware vendors. The second release of the MPI\nstandard, version 2.0 [ Mes97], contains numerous significant enhancements over version 1.x,\nsuch as one-sided communication, dynamic process creation, and extended collective\noperations. However, despite the fact that the standard was voted in 1997, there are no widely\navailable MPI-2 implementations that support the entire set of features specified in that\nstandard. In addition to the above reference manuals, a number of books have been written\nthat focus on parallel programming using MPI [ Pac98, GSNL98 , GLS99 ].\nIn addition to MPI implementations provided by various hardware vendors, there are a", "doc_id": "6201432e-157d-4ebb-bdc9-b87c4b23e19f", "embedding": null, "doc_hash": "834242f3692aac825ba5f372a7b82fb8fbc5e7caf51abfecb42bba0059fdb8e3", "extra_info": null, "node_info": {"start": 788128, "end": 790587}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4271e01f-18dc-4d56-8c12-719847a7811f", "3": "84da2bac-e9f7-4709-9729-def7a50e786a"}}, "__type__": "1"}, "84da2bac-e9f7-4709-9729-def7a50e786a": {"__data__": {"text": "2.0 [ Mes97], contains numerous significant enhancements over version 1.x,\nsuch as one-sided communication, dynamic process creation, and extended collective\noperations. However, despite the fact that the standard was voted in 1997, there are no widely\navailable MPI-2 implementations that support the entire set of features specified in that\nstandard. In addition to the above reference manuals, a number of books have been written\nthat focus on parallel programming using MPI [ Pac98, GSNL98 , GLS99 ].\nIn addition to MPI implementations provided by various hardware vendors, there are a number\nof publicly available MPI implementations that were developed by various government research\nlaboratories and universities. Among them, the MPICH [ GLDS96, GL96b ] (available at\nhttp://www-unix.mcs.anl.gov/mpi/mpich ) distributed by Argonne National Laboratories and the\nLAM-MPI (available at http://www.lam-mpi.org ) distributed by Indiana University are widely\nused and are portable to a number of different architectures. In fact, these implementations of\nMPI have been used as the starting point for a number of specialized MPI implementations that\nare suitable for off-the-shelf high-speed interconnection networks such as those based on\ngigabit Ethernet and Myrinet networks.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n6.1 Describe a message-transfer protocol for buffered sends and receives in which the\nbuffering is performed only by the sending process. What kind of additional hardware\nsupport is needed to make these types of protocols practical?\n6.2 One of the advantages of non-blocking communication operations is that they allow\nthe transmission of the data to be done concurrently with computations. Discuss the type\nof restructuring that needs to be performed on a program to allow for the maximal overlap\nof computation with communication. Is the sending process in a better position to benefit\nfrom this overlap than the receiving process?\n6.3 As discussed in Section 6.3.4 the MPI standard allows for two different\nimplementations of the MPI_Send  operation \u2013 one using buffered-sends and the other\nusing blocked-sends. Discuss some of the potential reasons why MPI allows these two\ndifferent implementations. In particular, consider the cases of different message-sizes\nand/or different architectural characteristics.\n6.4 Consider the various mappings of 16 processors on a 4 x 4 two-dimensional grid\nshown in Figure 6.5. Show how \n  processors will be mapped using each one\nof these four schemes.\n6.5 Consider Cannon's matrix-matrix multiplication algorithm. Our discussion of Cannon's\nalgorithm has been limited to cases in which A and B are square matrices, mapped onto a\nsquare grid of processes. However, Cannon's algorithm can be extended for cases in which\nA, B, and the process grid are not square. In particular, let matrix A be of size n x k and\nmatrix B be of size k x m. The matrix C obtained by multiplying A and B is of size n x m.\nAlso, let q x r be the number of processes in the grid arranged in q rows and r columns.\nDevelop an MPI program for multiplying two such matrices on a q x r process grid using\nCannon's algorithm.\n6.6 Show how the row-wise matrix-vector multiplication program ( Program 6.4) needs to\nbe changed so that it will work correctly in cases in which the dimension of the matrix does\nnot have to be a multiple of the number of processes.\n6.7 Consider the column-wise implementation of matrix-vector product ( Program 6.5 ). An\nalternate implementation will be to use MPI_Allreduce  to perform the required reduction\noperation and then have each process copy the locally stored elements of vector x from the\nvector fx.", "doc_id": "84da2bac-e9f7-4709-9729-def7a50e786a", "embedding": null, "doc_hash": "ec42c38392ffec6647d35376fd523ae275570235a1c2d9f9198e3fa7bda448ba", "extra_info": null, "node_info": {"start": 790297, "end": 793969}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6201432e-157d-4ebb-bdc9-b87c4b23e19f", "3": "fe13f16d-6976-49bf-81db-f726614a2a62"}}, "__type__": "1"}, "fe13f16d-6976-49bf-81db-f726614a2a62": {"__data__": {"text": "two such matrices on a q x r process grid using\nCannon's algorithm.\n6.6 Show how the row-wise matrix-vector multiplication program ( Program 6.4) needs to\nbe changed so that it will work correctly in cases in which the dimension of the matrix does\nnot have to be a multiple of the number of processes.\n6.7 Consider the column-wise implementation of matrix-vector product ( Program 6.5 ). An\nalternate implementation will be to use MPI_Allreduce  to perform the required reduction\noperation and then have each process copy the locally stored elements of vector x from the\nvector fx. What will be the cost of this implementation? Another implementation can be to\nperform p single-node reduction operations using a different process as the root. What will\nbe the cost of this implementation?\n6.8 Consider Dijkstra's single-source shortest-path algorithm described in Section 6.6.9.\nDescribe why a column-wise distribution is preferable to a row-wise distribution of the\nweighted adjacency matrix.\n6.9 Show how the two-dimensional matrix-vector multiplication program ( Program 6.8 )\nneeds to be changed so that it will work correctly for a matrix of size n x m on a q x r\nprocess grid.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 7. Programming Shared Address\nSpace Platforms\nExplicit parallel programming requires specification of parallel tasks along with their\ninteractions. These interactions may be in the form of synchronization between concurrent tasks\nor communication of intermediate results. In shared address space architectures,\ncommunication is implicitly specified since some (or all) of the memory is accessible to all the\nprocessors. Consequently, programming paradigms for shared address space machines focus on\nconstructs for expressing concurrency and synchronization along with techniques for minimizing\nassociated overheads. In this chapter, we discuss shared-address-space programming\nparadigms along with their performance issues and related extensions to directive-based\nparadigms.\nShared address space programming paradigms can vary on mechanisms for data sharing,\nconcurrency models, and support for synchronization. Process based models assume that all\ndata associated with a process is private, by default, unless otherwise specified (using UNIX\nsystem calls such as shmget and shmat ). While this is important for ensuring protection in\nmultiuser systems, it is not necessary when multiple concurrent aggregates are cooperating to\nsolve the same problem. The overheads associated with enforcing protection domains make\nprocesses less suitable for parallel programming. In contrast, lightweight processes and threads\nassume that all memory is global. By relaxing the protection domain, lightweight processes and\nthreads support much faster manipulation. As a result, this is the preferred model for parallel\nprogramming and forms the focus of this chapter. Directive based programming models extend\nthe threaded model by facilitating creation and synchronization of threads. In this chapter, we\nstudy various aspects of programming using threads and parallel directives.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.1 Thread Basics\nA thread  is a single stream of control in the flow of a program. We initiate threads with a\nsimple example:\nExample 7.1 What are threads?\nConsider the following code segment that computes the product of two dense matrices\nof size n x n.\n1   for (row = 0; row < n; row++) \n2       for (column = 0; column < n; column++) \n3           c[row][column] = \n4               dot_product(get_row(a, row), \n5                           get_col(b, col)); \nThe for loop in this code fragment has n2", "doc_id": "fe13f16d-6976-49bf-81db-f726614a2a62", "embedding": null, "doc_hash": "d45641609754118e2c65c4c9112251decd8ca4341864153f73d759f63919438d", "extra_info": null, "node_info": {"start": 793985, "end": 797613}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "84da2bac-e9f7-4709-9729-def7a50e786a", "3": "da4a8ed3-10c5-405a-aba5-060ee9c4c2e0"}}, "__type__": "1"}, "da4a8ed3-10c5-405a-aba5-060ee9c4c2e0": {"__data__": {"text": "that computes the product of two dense matrices\nof size n x n.\n1   for (row = 0; row < n; row++) \n2       for (column = 0; column < n; column++) \n3           c[row][column] = \n4               dot_product(get_row(a, row), \n5                           get_col(b, col)); \nThe for loop in this code fragment has n2 iterations, each of which can be executed\nindependently. Such an independent sequence of instructions is referred to as a\nthread. In the example presented above, there are n2 threads, one for each iteration\nof the for-loop. Since each of these threads can be executed independently of the\nothers, they can be scheduled concurrently on multiple processors. We can transform\nthe above code segment as follows:\n1   for (row = 0; row < n; row++) \n2       for (column = 0; column < n; column++) \n3           c[row][column] = \n4               create_thread(dot_product(get_row(a, row), \n5                                         get_col(b, col))); \nHere, we use a function, create_thread , to provide a mechanism for specifying a C\nfunction as a thread. The underlying system can then schedule these threads on\nmultiple processors. \nLogical Memory Model of a Thread  To execute the code fragment in Example 7.1  on multiple\nprocessors, each processor must have access to matrices a, b, and c. This is accomplished via a\nshared address space (described in Chapter 2 ). All memory in the logical machine model of a\nthread is globally accessible to every thread as illustrated in Figure 7.1(a) . However, since\nthreads are invoked as function calls, the stack corresponding to the function call is generally\ntreated as being local to the thread. This is due to the liveness considerations of the stack. Since\nthreads are scheduled at runtime (and no a priori  schedule of their execution can be safely\nassumed), it is not possible to determine which stacks are live. Therefore, it is considered poor\nprogramming practice to treat stacks (thread-local variables) as global data. This implies a\nlogical machine model illustrated in Figure 7.1(b), where memory modules M hold thread-local\n(stack allocated) data.\nFigure 7.1. The logical machine model of a thread-based programming\nparadigm.\nWhile this logical machine model gives the view of an equally accessible address space, physical\nrealizations of this model deviate from this assumption. In distributed shared address space\nmachines such as the Origin 2000, the cost to access a physically local memory may be an\norder of magnitude less than that of accessing remote memory. Even in architectures where the\nmemory is truly equally accessible to all processors (such as shared bus architectures with\nglobal shared memory), the presence of caches with processors skews memory access time.\nIssues of locality of memory reference become important for extracting performance from such\narchitectures.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.2 Why Threads?\nThreaded programming models offer significant advantages over message-passing\nprogramming models along with some disadvantages as well. Before we discuss threading APIs,\nlet us briefly look at some of these.\nSoftware Portability  Threaded applications can be developed on serial machines and run on\nparallel machines without any changes. This ability to migrate programs between", "doc_id": "da4a8ed3-10c5-405a-aba5-060ee9c4c2e0", "embedding": null, "doc_hash": "2b6e532aa8b64ab40d3762af83cd367f3759ae784b5d4735beea6526d8ce7948", "extra_info": null, "node_info": {"start": 797899, "end": 801178}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "fe13f16d-6976-49bf-81db-f726614a2a62", "3": "f8e1bf56-6206-4ff4-a24a-0ead616c7779"}}, "__type__": "1"}, "f8e1bf56-6206-4ff4-a24a-0ead616c7779": {"__data__": {"text": "of caches with processors skews memory access time.\nIssues of locality of memory reference become important for extracting performance from such\narchitectures.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.2 Why Threads?\nThreaded programming models offer significant advantages over message-passing\nprogramming models along with some disadvantages as well. Before we discuss threading APIs,\nlet us briefly look at some of these.\nSoftware Portability  Threaded applications can be developed on serial machines and run on\nparallel machines without any changes. This ability to migrate programs between diverse\narchitectural platforms is a very significant advantage of threaded APIs. It has implications not\njust for software utilization but also for application development since supercomputer time is\noften scarce and expensive.\nLatency Hiding  One of the major overheads in programs (both serial and parallel) is the\naccess latency for memory access, I/O, and communication. By allowing multiple threads to\nexecute on the same processor, threaded APIs enable this latency to be hidden (as seen in\nChapter 2). In effect, while one thread is waiting for a communication operation, other threads\ncan utilize the CPU, thus masking associated overhead.\nScheduling and Load Balancing  While writing shared address space parallel programs, a\nprogrammer must express concurrency in a way that minimizes overheads of remote interaction\nand idling. While in many structured applications the task of allocating equal work to processors\nis easily accomplished, in unstructured and dynamic applications (such as game playing and\ndiscrete optimization) this task is more difficult. Threaded APIs allow the programmer to specify\na large number of concurrent tasks and support system-level dynamic mapping of tasks to\nprocessors with a view to minimizing idling overheads. By providing this support at the system\nlevel, threaded APIs rid the programmer of the burden of explicit scheduling and load\nbalancing.\nEase of Programming, Widespread Use  Due to the aforementioned advantages, threaded\nprograms are significantly easier to write than corresponding programs using message passing\nAPIs. Achieving identical levels of performance for the two programs may require additional\neffort, however. With widespread acceptance of the POSIX thread API, development tools for\nPOSIX threads are more widely available and stable. These issues are important from the\nprogram development and software engineering aspects.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.3 The POSIX Thread API\nA number of vendors provide vendor-specific thread APIs. The IEEE specifies a standard\n1003.1c-1995, POSIX API. Also referred to as Pthreads, POSIX has emerged as the standard\nthreads API, supported by most vendors. We will use the Pthreads API for introducing\nmultithreading concepts. The concepts themselves are largely independent of the API and can\nbe used for programming with other thread APIs (NT threads, Solaris threads, Java threads,\netc.) as well. All the illustrative programs presented in this chapter can be executed on\nworkstations as well as parallel computers that support Pthreads.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.4 Thread Basics: Creation and Termination\nLet us start our discussion with a simple threaded program for computing the value of p.\nExample 7.2 Threaded program for computing p\nThe method we use here is based on generating random numbers in a unit length\nsquare and counting the number of points that fall within the largest circle inscribed in\nthe square. Since the area of the circle ( p r2) is equal to p/4, and the area of the\nsquare is 1 x 1, the fraction of random points that fall in the circle should approach\np/4.\nA simple threaded strategy for generating the value of p assigns a fixed number of\npoints to each thread. Each thread generates these random points and keeps track", "doc_id": "f8e1bf56-6206-4ff4-a24a-0ead616c7779", "embedding": null, "doc_hash": "763571df3a8ed3a81119043ebb94cbf771ac1c40036a87a90122ad929ca2da58", "extra_info": null, "node_info": {"start": 800877, "end": 804740}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "da4a8ed3-10c5-405a-aba5-060ee9c4c2e0", "3": "81fae6ac-d851-4449-b510-dbfdca52c67e"}}, "__type__": "1"}, "81fae6ac-d851-4449-b510-dbfdca52c67e": {"__data__": {"text": "threaded program for computing the value of p.\nExample 7.2 Threaded program for computing p\nThe method we use here is based on generating random numbers in a unit length\nsquare and counting the number of points that fall within the largest circle inscribed in\nthe square. Since the area of the circle ( p r2) is equal to p/4, and the area of the\nsquare is 1 x 1, the fraction of random points that fall in the circle should approach\np/4.\nA simple threaded strategy for generating the value of p assigns a fixed number of\npoints to each thread. Each thread generates these random points and keeps track of\nthe number of points that land in the circle locally. After all threads finish execution,\ntheir counts are combined to compute the value of p (by calculating the fraction over\nall threads and multiplying by 4).\nTo implement this threaded program, we need a function for creating threads and\nwaiting for all threads to finish execution (so we can accrue count). Threads can be\ncreated in the Pthreads API using the function pthread_create . The prototype of this\nfunction is:\n1   #include <pthread.h> \n2   int \n3   pthread_create ( \n4       pthread_t   *thread_handle, \n5       const pthread_attr_t   *attribute, \n6       void *   (*thread_function)(void *), \n7       void   *arg); \nThe pthread_create  function creates a single thread that corresponds to the\ninvocation of the function thread_function  (and any other functions called by\nthread_function ). On successful creation of a thread, a unique identifier is\nassociated with the thread and assigned to the location pointed to by thread_handle .\nThe thread has the attributes described by the attribute  argument. When this\nargument is NULL, a thread with default attributes is created. We will discuss the\nattribute  parameter in detail in Section 7.6. The arg field specifies a pointer to the\nargument to function thread_function . This argument is typically used to pass the\nworkspace and other thread-specific data to a thread. In the compute_pi  example, it\nis used to pass an integer id that is used as a seed for randomization. The\nthread_handle  variable is written before the the function pthread_create  returns;\nand the new thread is ready for execution as soon as it is created. If the thread is\nscheduled on the same processor, the new thread may, in fact, preempt its creator.\nThis is important to note because all thread initialization procedures must be\ncompleted before creating the thread. Otherwise, errors may result based on thread\nscheduling. This is a very common class of errors caused by race conditions for data\naccess that shows itself in some execution instances, but not in others. On successful\ncreation of a thread, pthread_create  returns 0; else it returns an error code. The\nreader is referred to the Pthreads specification for a detailed description of the error-\ncodes.\nIn our program for computing the value of p, we first read in the desired number of\nthreads, num threads , and the desired number of sample points, sample_points .\nThese points are divided equally among the threads. The program uses an array,\nhits, for assigning an integer id to each thread (this id is used as a seed for\nrandomizing the random number generator). The same array is used to keep track of\nthe number of hits (points inside the circle) encountered by each thread upon return.\nThe program creates num_threads threads, each invoking the function compute_pi ,\nusing the pthread_create  function.\nOnce the respective compute_pi  threads have generated assigned number of random\npoints and computed their hit ratios, the results must be combined to determine p.\nThe main program must wait for the threads to run to completion. This is done using\nthe function pthread_join  which suspends execution", "doc_id": "81fae6ac-d851-4449-b510-dbfdca52c67e", "embedding": null, "doc_hash": "2e25715d88fbb66c6df1e99d2dbc62faff37ca87c890b2201240ec7f21779bf9", "extra_info": null, "node_info": {"start": 804750, "end": 808524}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f8e1bf56-6206-4ff4-a24a-0ead616c7779", "3": "55e028f3-f449-4969-b7ca-a01e8a419b8e"}}, "__type__": "1"}, "55e028f3-f449-4969-b7ca-a01e8a419b8e": {"__data__": {"text": "is used as a seed for\nrandomizing the random number generator). The same array is used to keep track of\nthe number of hits (points inside the circle) encountered by each thread upon return.\nThe program creates num_threads threads, each invoking the function compute_pi ,\nusing the pthread_create  function.\nOnce the respective compute_pi  threads have generated assigned number of random\npoints and computed their hit ratios, the results must be combined to determine p.\nThe main program must wait for the threads to run to completion. This is done using\nthe function pthread_join  which suspends execution of the calling thread until the\nspecified thread terminates. The prototype of the pthread_join  function is as follows:\n1   int \n2   pthread_join ( \n3       pthread_t thread, \n4       void **ptr); \nA call to this function waits for the termination of the thread whose id is given by\nthread. On a successful call to pthread_join , the value passed to pthread_exit  is\nreturned in the location pointed to by ptr. On successful completion, pthread_join\nreturns 0, else it returns an error-code.\nOnce all threads have joined, the value of p is computed by multiplying the combined\nhit ratio by 4.0. The complete program is as follows:\n1   #include <pthread.h> \n2   #include <stdlib.h> \n3 \n4   #define MAX_THREADS 512 \n5   void *compute_pi (void *); \n6 \n7   int total_hits, total_misses, hits[MAX_THREADS], \n8       sample_points, sample_points_per_thread, num_threads; \n9 \n10  main() { \n11      int i; \n12      pthread_t p_threads[MAX_THREADS]; \n13      pthread_attr_t attr; \n14      double computed_pi; \n15      double time_start, time_end; \n16      struct timeval tv; \n17      struct timezone tz; \n18 \n19      pthread_attr_init (&attr); \n20      pthread_attr_setscope (&attr,PTHREAD_SCOPE_SYSTEM); \n21      printf(\"Enter number of sample points: \"); \n22      scanf(\"%d\", &sample_points); \n23      printf(\"Enter number of threads: \"); \n24      scanf(\"%d\", &num_threads); \n25 \n26      gettimeofday(&tv, &tz); \n27      time_start = (double)tv.tv_sec + \n28                   (double)tv.tv_usec / 1000000.0; \n29 \n30      total_hits = 0; \n31      sample_points_per_thread = sample_points / num_threads; \n32      for (i=0; i< num_threads; i++) { \n33          hits[i] = i; \n34          pthread_create(&p_threads[i], &attr, compute_pi, \n35              (void *) &hits[i]); \n36      } \n37      for (i=0; i< num_threads; i++) { \n38          pthread_join(p_threads[i], NULL); \n39          total_hits += hits[i]; \n40      } \n41      computed_pi =", "doc_id": "55e028f3-f449-4969-b7ca-a01e8a419b8e", "embedding": null, "doc_hash": "f794b508e81fbc47e830a6453f8278d357932db78eb95e48bc0f70740c6fd50d", "extra_info": null, "node_info": {"start": 808508, "end": 811046}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "81fae6ac-d851-4449-b510-dbfdca52c67e", "3": "b76d07f2-f954-41ba-ab13-8f2c14be6282"}}, "__type__": "1"}, "b76d07f2-f954-41ba-ab13-8f2c14be6282": {"__data__": {"text": "         pthread_create(&p_threads[i], &attr, compute_pi, \n35              (void *) &hits[i]); \n36      } \n37      for (i=0; i< num_threads; i++) { \n38          pthread_join(p_threads[i], NULL); \n39          total_hits += hits[i]; \n40      } \n41      computed_pi = 4.0*(double) total_hits / \n42          ((double)(sample_points)); \n43      gettimeofday(&tv, &tz); \n44      time_end = (double)tv.tv_sec + \n45                 (double)tv.tv_usec / 1000000.0; \n46 \n47      printf(\"Computed PI = %lf\\n\", computed_pi); \n48      printf(\" %lf\\n\", time_end - time_start); \n49  } \n50 \n51  void *compute_pi (void *s) { \n52      int seed, i, *hit_pointer; \n53      double rand_no_x, rand_no_y; \n54      int local_hits; \n55 \n56      hit_pointer = (int *) s; \n57      seed = *hit_pointer; \n58      local_hits = 0; \n59      for (i = 0; i < sample_points_per_thread; i++) { \n60          rand_no_x =(double)(rand_r(&seed))/(double)((2<<14)-1); \n61          rand_no_y =(double)(rand_r(&seed))/(double)((2<<14)-1); \n62          if (((rand_no_x - 0.5) * (rand_no_x - 0.5) + \n63              (rand_no_y - 0.5) * (rand_no_y - 0.5)) < 0.25) \n64              local_hits ++; \n65          seed *= i; \n66      } \n67      *hit_pointer = local_hits; \n68      pthread_exit(0); \n69  } \nProgramming Notes  The reader must note, in the above example, the use of the\nfunction rand_r (instead of superior random number generators such as drand48 ).\nThe reason for this is that many functions (including rand and drand48 ) are not\nreentrant . Reentrant functions are those that can be safely called when another\ninstance has been suspended in the middle of its invocation. It is easy to see why all\nthread functions must be reentrant because a thread can be preempted in the middle\nof its execution. If another thread starts executing the same function at this point, a\nnon-reentrant function might not work as desired.\nPerformance Notes  We execute this program on a four-processor SGI Origin 2000.\nThe logarithm of the number of threads and execution time are illustrated in Figure\n7.2 (the curve labeled \"local\"). We can see that at 32 threads, the runtime of the\nprogram is roughly 3.91 times less than the corresponding time for one thread. On a\nfour-processor machine, this corresponds to a parallel efficiency of 0.98.\nFigure 7.2. Execution time of the compute_pi  program as a\nfunction of number of threads.\nThe other curves in Figure 7.2  illustrate an", "doc_id": "b76d07f2-f954-41ba-ab13-8f2c14be6282", "embedding": null, "doc_hash": "d7f789221a316a76be43444d6bf5380bc73c6827694ceae5cbe79b25018042b9", "extra_info": null, "node_info": {"start": 811386, "end": 813811}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "55e028f3-f449-4969-b7ca-a01e8a419b8e", "3": "cf5f7ca1-8d66-4d74-b631-804a2f74506c"}}, "__type__": "1"}, "cf5f7ca1-8d66-4d74-b631-804a2f74506c": {"__data__": {"text": "Notes  We execute this program on a four-processor SGI Origin 2000.\nThe logarithm of the number of threads and execution time are illustrated in Figure\n7.2 (the curve labeled \"local\"). We can see that at 32 threads, the runtime of the\nprogram is roughly 3.91 times less than the corresponding time for one thread. On a\nfour-processor machine, this corresponds to a parallel efficiency of 0.98.\nFigure 7.2. Execution time of the compute_pi  program as a\nfunction of number of threads.\nThe other curves in Figure 7.2  illustrate an important performance overhead called\nfalse sharing . Consider the following change to the program: instead of incrementing\na local variable, local_hits , and assigning it to the array entry outside the loop, we\nnow directly increment the corresponding entry in the hits array. This can be done by\nchanging line 64 to *(hit_pointer) ++; , and deleting line 67. It is easy to verify\nthat the program is semantically identical to the one before. However, on executing\nthis modified program the observed performance is illustrated in the curve labeled\n\"spaced_1\" in Figure 7.2. This represents a significant slowdown instead of a speedup!\nThe drastic impact of this seemingly innocuous change is explained by a phenomenon\ncalled false sharing . In this example, two adjoining data items (which likely reside on\nthe same cache line) are being continually written to by threads that might be\nscheduled on different processors. From our discussion in Chapter 2, we know that a\nwrite to a shared cache line results in an invalidate and a subsequent read must fetch\nthe cache line from the most recent write location. With this in mind, we can see that\nthe cache lines corresponding to the hits array generate a large number of\ninvalidates and reads because of repeated increment operations. This situation, in\nwhich two threads 'falsely' share data because it happens to be on the same cache\nline, is called false sharing.\nIt is in fact possible to use this simple example to estimate the cache line size of the\nsystem. We change hits to a two-dimensional array and use only the first column of\nthe array to store counts. By changing the size of the second dimension, we can force\nentries in the first column of the hits array to lie on different cache lines (since arrays\nin C are stored row-major). The results of this experiment are illustrated in Figure 7.2\nby curves labeled \"spaced_16\" and \"spaced_32\", in which the second dimension of the\nhits array is 16 and 32 integers, respectively. It is evident from the figure that as the\nentries are spaced apart, the performance improves. This is consistent with our\nunderstanding that spacing the entries out pushes them into different cache lines,\nthereby reducing the false sharing overhead. \nHaving understood how to create and join threads, let us now explore mechanisms in Pthreads\nfor synchronizing threads.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.5 Synchronization Primitives in Pthreads\nWhile communication is implicit in shared-address-space programming, much of the effort\nassociated with writing correct threaded programs is spent on synchronizing concurrent threads\nwith respect to their data accesses or scheduling.\n7.5.1 Mutual Exclusion for Shared Variables\nUsing pthread_create  and pthread_join  calls, we can create concurrent tasks. These tasks\nwork together to manipulate data and accomplish a given task. When multiple threads attempt\nto manipulate the same data item, the results can often be incoherent if proper care is not taken\nto synchronize them. Consider the following code fragment being executed by multiple threads.\nThe variable my_cost  is thread-local and best_cost  is a global variable shared by all threads.\n1   /* each thread tries to update variable best_cost as follows", "doc_id": "cf5f7ca1-8d66-4d74-b631-804a2f74506c", "embedding": null, "doc_hash": "e705a2299006a49b3c53235a9bb795cf13b9aa7e0f8d7ddf7ce956805d076494", "extra_info": null, "node_info": {"start": 813544, "end": 817321}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b76d07f2-f954-41ba-ab13-8f2c14be6282", "3": "de8e8d6e-bf9a-4337-9827-a1061c84fc24"}}, "__type__": "1"}, "de8e8d6e-bf9a-4337-9827-a1061c84fc24": {"__data__": {"text": "Exclusion for Shared Variables\nUsing pthread_create  and pthread_join  calls, we can create concurrent tasks. These tasks\nwork together to manipulate data and accomplish a given task. When multiple threads attempt\nto manipulate the same data item, the results can often be incoherent if proper care is not taken\nto synchronize them. Consider the following code fragment being executed by multiple threads.\nThe variable my_cost  is thread-local and best_cost  is a global variable shared by all threads.\n1   /* each thread tries to update variable best_cost as follows */ \n2   if (my_cost < best_cost) \n3       best_cost = my_cost; \nTo understand the problem with shared data access, let us examine one execution instance of\nthe above code fragment. Assume that there are two threads, the initial value of best_cost  is\n100, and the values of my_cost  are 50 and 75 at threads t1 and t2, respectively. If both threads\nexecute the condition inside the if statement concurrently, then both threads enter the then\npart of the statement. Depending on which thread executes first, the value of best_cost  at the\nend could be either 50 or 75. There are two problems here: the first is the non-deterministic\nnature of the result; second, and more importantly, the value 75 of best_cost  is inconsistent in\nthe sense that no serialization of the two threads can possibly yield this result. This is an\nundesirable situation, sometimes also referred to as a race condition (so called because the\nresult of the computation depends on the race between competing threads).\nThe aforementioned situation occurred because the test-and-update operation illustrated above\nis an atomic operation; i.e., the operation should not be broken into sub-operations.\nFurthermore, the code corresponds to a critical segment; i.e., a segment that must be executed\nby only one thread at any time. Many statements that seem atomic in higher level languages\nsuch as C may in fact be non-atomic; for example, a statement of the form global_count += 5\nmay comprise several assembler instructions and therefore must be handled carefully.\nThreaded APIs provide support for implementing critical sections and atomic operations using\nmutex-locks  (mutual exclusion locks). Mutex-locks have two states: locked and unlocked. At\nany point of time, only one thread can lock a mutex lock. A lock is an atomic operation\ngenerally associated with a piece of code that manipulates shared data. To access the shared\ndata, a thread must first try to acquire a mutex-lock. If the mutex-lock is already locked, the\nprocess trying to acquire the lock is blocked. This is because a locked mutex-lock implies that\nthere is another thread currently in the critical section and that no other thread must be allowed\nin. When a thread leaves a critical section, it must unlock the mutex-lock so that other threads\ncan enter the critical section. All mutex-locks must be initialized to the unlocked state at the\nbeginning of the program.\nThe Pthreads API provides a number of functions for handling mutex-locks. The function\npthread_mutex_lock  can be used to attempt a lock on a mutex-lock. The prototype of the\nfunction is:\n1   int \n2   pthread_mutex_lock ( \n3       pthread_mutex_t *mutex_lock); \nA call to this function attempts a lock on the mutex-lock mutex_lock . (The data type of a\nmutex_lock  is predefined to be pthread_mutex_t .) If the mutex-lock is already locked, the\ncalling thread blocks; otherwise the mutex-lock is locked and the calling thread returns. A\nsuccessful return from the function returns a value 0. Other values indicate error conditions\nsuch as deadlocks.\nOn leaving a critical section, a thread must unlock the", "doc_id": "de8e8d6e-bf9a-4337-9827-a1061c84fc24", "embedding": null, "doc_hash": "f740f049203aa4414a1998102a70a12bc7e05cb22ce3cb15358859351ceebf0b", "extra_info": null, "node_info": {"start": 817286, "end": 820970}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "cf5f7ca1-8d66-4d74-b631-804a2f74506c", "3": "51640bf9-6692-4269-82d2-0e8aff4fdb3d"}}, "__type__": "1"}, "51640bf9-6692-4269-82d2-0e8aff4fdb3d": {"__data__": {"text": " pthread_mutex_lock ( \n3       pthread_mutex_t *mutex_lock); \nA call to this function attempts a lock on the mutex-lock mutex_lock . (The data type of a\nmutex_lock  is predefined to be pthread_mutex_t .) If the mutex-lock is already locked, the\ncalling thread blocks; otherwise the mutex-lock is locked and the calling thread returns. A\nsuccessful return from the function returns a value 0. Other values indicate error conditions\nsuch as deadlocks.\nOn leaving a critical section, a thread must unlock the mutex-lock associated with the section. If\nit does not do so, no other thread will be able to enter this section, typically resulting in a\ndeadlock. The Pthreads function pthread_mutex_unlock  is used to unlock a mutex-lock. The\nprototype of this function is:\n1   int \n2   pthread_mutex_unlock ( \n3       pthread_mutex_t *mutex_lock); \nOn calling this function, in the case of a normal mutex-lock, the lock is relinquished and one of\nthe blocked threads is scheduled to enter the critical section. The specific thread is determined\nby the scheduling policy. There are other types of locks (other than normal locks), which are\ndiscussed in Section 7.6 along with the associated semantics of the function\npthread_mutex_unlock . If a programmer attempts a pthread_mutex_unlock  on a previously\nunlocked mutex or one that is locked by another thread, the effect is undefined.\nWe need one more function before we can start using mutex-locks, namely, a function to\ninitialize a mutex-lock to its unlocked state. The Pthreads function for this is\npthread_mutex_init . The prototype of this function is as follows:\n1   int \n2   pthread_mutex_init ( \n3       pthread_mutex_t *mutex_lock, \n4       const pthread_mutexattr_t *lock_attr); \nThis function initializes the mutex-lock mutex_lock  to an unlocked state. The attributes of the\nmutex-lock are specified by lock_attr . If this argument is set to NULL, the default mutex-lock\nattributes are used (normal mutex-lock). Attributes objects for threads are discussed in greater\ndetail in Section 7.6 .\nExample 7.3 Computing the minimum entry in a list of integers\nArmed with basic mutex-lock functions, let us write a simple threaded program to\ncompute the minimum of a list of integers. The list is partitioned equally among the\nthreads. The size of each thread's partition is stored in the variable\npartial_list_size  and the pointer to the start of each thread's partial list is passed\nto it as the pointer list_ptr . The threaded program for accomplishing this is as\nfollows:\n1   #include <pthread.h> \n2   void *find_min(void *list_ptr); \n3   pthread_mutex_t minimum_value_lock; \n4   int minimum_value, partial_list_size; \n5 \n6   main() { \n7       /* declare and initialize data structures and list */ \n8       minimum_value = MIN_INT; \n9       pthread_init(); \n10      pthread_mutex_init(&minimum_value_lock, NULL); \n11 \n12      /* initialize lists, list_ptr, and partial_list_size */ \n13      /* create and join threads here */ \n14  } \n15 \n16  void *find_min(void *list_ptr) { \n17      int *partial_list_pointer, my_min, i; \n18      my_min = MIN_INT; \n19     ", "doc_id": "51640bf9-6692-4269-82d2-0e8aff4fdb3d", "embedding": null, "doc_hash": "ebd1b14304d019b1edd30a972bad71d86e3d77d47968acf9a3264023ed6c6185", "extra_info": null, "node_info": {"start": 821027, "end": 824138}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "de8e8d6e-bf9a-4337-9827-a1061c84fc24", "3": "0ddfac00-ff5e-4702-b4fa-332efd549f8e"}}, "__type__": "1"}, "0ddfac00-ff5e-4702-b4fa-332efd549f8e": {"__data__": {"text": " minimum_value = MIN_INT; \n9       pthread_init(); \n10      pthread_mutex_init(&minimum_value_lock, NULL); \n11 \n12      /* initialize lists, list_ptr, and partial_list_size */ \n13      /* create and join threads here */ \n14  } \n15 \n16  void *find_min(void *list_ptr) { \n17      int *partial_list_pointer, my_min, i; \n18      my_min = MIN_INT; \n19      partial_list_pointer = (int *) list_ptr; \n20      for (i = 0; i < partial_list_size; i++) \n21          if (partial_list_pointer[i] < my_min) \n22              my_min = partial_list_pointer[i]; \n23      /* lock the mutex associated with minimum_value and \n24      update the variable as required */ \n25      pthread_mutex_lock(&minimum_value_lock); \n26      if (my_min < minimum_value) \n27          minimum_value = my_min; \n28      /* and unlock the mutex */ \n29      pthread_mutex_unlock(&minimum_value_lock); \n30      pthread_exit(0); \n31  } \nProgramming Notes  In this example, the test-update operation for minimum_value\nis protected by the mutex-lock minimum value lock . Threads execute\npthread_mutex_lock  to gain exclusive access to the variable minimum_value . Once\nthis access is gained, the value is updated as required, and the lock subsequently\nreleased. Since at any point of time, only one thread can hold a lock, only one thread\ncan test-update the variable. \nExample 7.4 Producer-consumer work queues\nA common use of mutex-locks is in establishing a producer-consumer relationship\nbetween threads. The producer creates tasks and inserts them into a work-queue. The\nconsumer threads pick up tasks from the task queue and execute them. Let us\nconsider a simple instance of this paradigm in which the task queue can hold only one\ntask (in a general case, the task queue may be longer but is typically of bounded\nsize). Producer-consumer relations are ubiquitous. See Exercise 7.4 for an example\napplication in multimedia processing. A simple (and incorrect) threaded program\nwould associate a producer thread with creating a task and placing it in a shared data\nstructure and the consumer threads with picking up tasks from this shared data\nstructure and executing them. However, this simple version does not account for the\nfollowing possibilities:\nThe producer thread must not overwrite the shared buffer when the previous\ntask has not been picked up by a consumer thread.\nThe consumer threads must not pick up tasks until there is something present in\nthe shared data structure.\nIndividual consumer threads should pick up tasks one at a time.\nTo implement this, we can use a variable called task_available . If this variable is 0,\nconsumer threads must wait, but the producer thread can insert tasks into the shared\ndata structure task_queue . If task_available  is equal to 1, the producer thread\nmust wait to insert the task into the shared data structure but one of the consumer\nthreads can pick up the task available. All of these operations on the variable\ntask_available  should be protected by mutex-locks to ensure that only one thread is\nexecuting test-update on it. The threaded version of this program is as follows:\n1   pthread_mutex_t task_queue_lock; \n2   int task_available; \n3 \n4   /* other shared data structures here */ \n5 \n6   main() { \n7       /* declarations and initializations */ \n8", "doc_id": "0ddfac00-ff5e-4702-b4fa-332efd549f8e", "embedding": null, "doc_hash": "57cbfe3f21e51fbf3d50c227cf4ffa660bc8504633ef0c6381138c158c5ea55e", "extra_info": null, "node_info": {"start": 824294, "end": 827566}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "51640bf9-6692-4269-82d2-0e8aff4fdb3d", "3": "b3e1d7d6-9d2f-4a62-a632-67e4f4c594e2"}}, "__type__": "1"}, "b3e1d7d6-9d2f-4a62-a632-67e4f4c594e2": {"__data__": {"text": "1, the producer thread\nmust wait to insert the task into the shared data structure but one of the consumer\nthreads can pick up the task available. All of these operations on the variable\ntask_available  should be protected by mutex-locks to ensure that only one thread is\nexecuting test-update on it. The threaded version of this program is as follows:\n1   pthread_mutex_t task_queue_lock; \n2   int task_available; \n3 \n4   /* other shared data structures here */ \n5 \n6   main() { \n7       /* declarations and initializations */ \n8       task_available = 0; \n9       pthread_init(); \n10      pthread_mutex_init(&task_queue_lock, NULL); \n11     /* create and join producer and consumer threads */ \n12  } \n13 \n14  void *producer(void *producer_thread_data) { \n15      int inserted; \n16      struct task my_task; \n17      while (!done()) { \n18          inserted = 0; \n19          create_task(&my_task); \n20          while (inserted == 0) { \n21              pthread_mutex_lock(&task_queue_lock); \n22              if (task_available == 0) { \n23                  insert_into_queue(my_task); \n24                  task_available = 1; \n25                  inserted = 1; \n26              } \n27              pthread_mutex_unlock(&task_queue_lock); \n28          } \n29      } \n30  } \n31 \n32  void *consumer(void *consumer_thread_data) { \n33      int extracted; \n34      struct task my_task; \n35      /* local data structure declarations */ \n36      while (!done()) { \n37          extracted = 0; \n38          while (extracted == 0) { \n39              pthread_mutex_lock(&task_queue_lock); \n40              if (task_available == 1) { \n41                  extract_from_queue(&my_task); \n42                  task_available = 0; \n43                  extracted = 1; \n44              } \n45              pthread_mutex_unlock(&task_queue_lock); \n46          } \n47          process_task(my_task); \n48      } \n49  } \nProgramming Notes  In this example, the producer thread creates a task and waits\nfor space on the queue. This is indicated by the variable task_available  being 0. The\ntest and update of this variable as well as insertion and extraction from the shared\nqueue are protected by a mutex called task_queue_lock . Once space is available on\nthe task queue, the recently created task is inserted into the task queue and the\navailability of the task is signaled by setting task_available  to 1. Within", "doc_id": "b3e1d7d6-9d2f-4a62-a632-67e4f4c594e2", "embedding": null, "doc_hash": "d80d236fcd81ee6c0715ca5351ba13ff03ac19a43b30ceeb19b1e6bac14342cd", "extra_info": null, "node_info": {"start": 827402, "end": 829788}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0ddfac00-ff5e-4702-b4fa-332efd549f8e", "3": "bed81d72-afe1-4be2-896c-2a67fd8162d9"}}, "__type__": "1"}, "bed81d72-afe1-4be2-896c-2a67fd8162d9": {"__data__": {"text": " } \n47          process_task(my_task); \n48      } \n49  } \nProgramming Notes  In this example, the producer thread creates a task and waits\nfor space on the queue. This is indicated by the variable task_available  being 0. The\ntest and update of this variable as well as insertion and extraction from the shared\nqueue are protected by a mutex called task_queue_lock . Once space is available on\nthe task queue, the recently created task is inserted into the task queue and the\navailability of the task is signaled by setting task_available  to 1. Within the\nproducer thread, the fact that the recently created task has been inserted into the\nqueue is signaled by the variable inserted  being set to 1, which allows the producer\nto produce the next task. Irrespective of whether a recently created task is\nsuccessfully inserted into the queue or not, the lock is relinquished. This allows\nconsumer threads to pick up work from the queue in case there is work on the queue\nto begin with. If the lock is not relinquished, threads would deadlock since a consumer\nwould not be able to get the lock to pick up the task and the producer would not be\nable to insert its task into the task queue. The consumer thread waits for a task to\nbecome available and executes it when available. As was the case with the producer\nthread, the consumer relinquishes the lock in each iteration of the while  loop to allow\nthe producer to insert work into the queue if there was none. \nOverheads of Locking\nLocks represent serialization points since critical sections must be executed by threads one after\nthe other. Encapsulating large segments of the program within locks can, therefore, lead to\nsignificant performance degradation. It is important to minimize the size of critical sections. For\ninstance, in the above example, the create_task and process_task  functions are left outside\nthe critical region, but insert_into_queue  and extract_from_queue  functions are left inside\nthe critical region. The former is left out in the interest of making the critical section as small as\npossible. The insert_into_queue  and extract_from_queue  functions are left inside because if\nthe lock is relinquished after updating task_available  but not inserting or extracting the task,\nother threads may gain access to the shared data structure while the insertion or extraction is in\nprogress, resulting in errors. It is therefore important to handle critical sections and shared data\nstructures with extreme care.\nAlleviating Locking Overheads\nIt is often possible to reduce the idling overhead associated with locks using an alternate\nfunction, pthread_mutex_trylock. This function attempts a lock on mutex_lock . If the lock is\nsuccessful, the function returns a zero. If it is already locked by another thread, instead of\nblocking the thread execution, it returns a value EBUSY . This allows the thread to do other work\nand to poll the mutex for a lock. Furthermore, pthread_mutex_trylock is typically much faster\nthan pthread_mutex_lock  on typical systems since it does not have to deal with queues\nassociated with locks for multiple threads waiting on the lock. The prototype of\npthread_mutex_trylock is:\n1   int \n2   pthread_mutex_trylock ( \n3       pthread_mutex_t *mutex_lock); \nWe illustrate the use of this function using the following example:\nExample 7.5 Finding k matches in a list\nWe consider the example of finding k matches to a query item in a given list. The list\nis partitioned equally among the threads. Assuming that the list has n entries, each of\nthe p threads is responsible for searching n/p entries of the list. The program segment\nfor computing this using the pthread_mutex_lock  function is as follows:\n1   void *find_entries(void *start_pointer) { \n2 \n3       /* This is the thread function */", "doc_id": "bed81d72-afe1-4be2-896c-2a67fd8162d9", "embedding": null, "doc_hash": "bc2a3cc981f6b278e20d3306a425d0d1bc91a5a146531c851c009ccebf11c8e2", "extra_info": null, "node_info": {"start": 829776, "end": 833571}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b3e1d7d6-9d2f-4a62-a632-67e4f4c594e2", "3": "f5d3fdaf-2368-4a06-81b7-dbd8103fbaad"}}, "__type__": "1"}, "f5d3fdaf-2368-4a06-81b7-dbd8103fbaad": {"__data__": {"text": "illustrate the use of this function using the following example:\nExample 7.5 Finding k matches in a list\nWe consider the example of finding k matches to a query item in a given list. The list\nis partitioned equally among the threads. Assuming that the list has n entries, each of\nthe p threads is responsible for searching n/p entries of the list. The program segment\nfor computing this using the pthread_mutex_lock  function is as follows:\n1   void *find_entries(void *start_pointer) { \n2 \n3       /* This is the thread function */ \n4 \n5       struct database_record *next_record; \n6       int count; \n7       current_pointer = start_pointer; \n8       do { \n9           next_record = find_next_entry(current_pointer); \n10          count = output_record(next_record); \n11      } while (count < requested_number_of_records); \n12  } \n13 \n14  int output_record(struct database_record *record_ptr) { \n15      int count; \n16      pthread_mutex_lock(&output_count_lock); \n17      output_count ++; \n18      count = output_count; \n19      pthread_mutex_unlock(&output_count_lock); \n20 \n21      if (count <= requested_number_of_records) \n22          print_record(record_ptr); \n23      return (count); \n24  } \nThis program segment finds an entry in its part of the database, updates the global\ncount and then finds the next entry. If the time for a lock-update count-unlock cycle is\nt1 and the time to find an entry is t2, then the total time for satisfying the query is ( t1\n+ t2) x nmax , where nmax is the maximum number of entries found by any thread. If t1\nand t2 are comparable, then locking leads to considerable overhead.\nThis locking overhead can be alleviated by using the function\npthread_mutex_trylock. Each thread now finds the next entry and tries to acquire\nthe lock and update count. If another thread already has the lock, the record is\ninserted into a local list and the thread proceeds to find other matches. When it finally\ngets the lock, it inserts all entries found locally thus far into the list (provided the\nnumber does not exceed the desired number of entries). The corresponding\noutput_record  function is as follows:\n1   int output_record(struct database_record *record_ptr) { \n2       int count; \n3       int lock_status; \n4       lock_status = pthread_mutex_trylock(&output_count_lock); \n5       if (lock_status == EBUSY) { \n6           insert_into_local_list(record_ptr); \n7           return(0); \n8       } \n9       else { \n10          count = output_count; \n11          output_count += number_on_local_list + 1; \n12          pthread_mutex_unlock(&output_count_lock); \n13          print_records(record_ptr, local_list, \n14                requested_number_of_records - count); \n15          return(count + number_on_local_list +", "doc_id": "f5d3fdaf-2368-4a06-81b7-dbd8103fbaad", "embedding": null, "doc_hash": "7fe678bace5ebedc56b275bafe6a2061772d59047b208a39682236733a90e80e", "extra_info": null, "node_info": {"start": 833580, "end": 836327}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bed81d72-afe1-4be2-896c-2a67fd8162d9", "3": "835a55d7-23e7-4bac-9b9f-4c280b2a1d47"}}, "__type__": "1"}, "835a55d7-23e7-4bac-9b9f-4c280b2a1d47": {"__data__": {"text": "    else { \n10          count = output_count; \n11          output_count += number_on_local_list + 1; \n12          pthread_mutex_unlock(&output_count_lock); \n13          print_records(record_ptr, local_list, \n14                requested_number_of_records - count); \n15          return(count + number_on_local_list + 1); \n16      } \n17  } \nProgramming Notes  Examining this function closely, we notice that if the lock for\nupdating the global count is not available, the function inserts the current record into\na local list and returns. If the lock is available, it increments the global count by the\nnumber of records on the local list, and then by one (for the current record). It then\nunlocks the associated lock and proceeds to print as many records as are required\nusing the function print_records .\nPerformance Notes  The time for execution of this version is less than the time for\nthe first one on two counts: first, as mentioned, the time for executing a\npthread_mutex_trylock is typically much smaller than that for a\npthread_mutex_lock . Second, since multiple records may be inserted on each lock,\nthe number of locking operations is also reduced. The number of records actually\nsearched (across all threads) may be slightly larger than the number of records\nactually desired (since there may be entries in the local lists that may never be\nprinted). However, since this time would otherwise have been spent idling for the lock\nanyway, this overhead does not cause a slowdown. \nThe above example illustrates the use of the function pthread_mutex_trylock instead of\npthread_mutex_lock . The general use of the function is in reducing idling overheads associated\nwith mutex-locks. If the computation is such that the critical section can be delayed and other\ncomputations can be performed in the interim, pthread mutex_trylock_ is the function of\nchoice. Another determining factor, as has been mentioned, is the fact that for most\nimplementations pthread_mutex_trylock is a much cheaper function than\npthread_mutex_lock . In fact, for highly optimized codes, even when a pthread_mutex_lock  is\nrequired, a pthread_mutex_trylock inside a loop may often be desirable, since if the lock is\nacquired within the first few calls, it would be cheaper than a pthread_mutex_lock .\n7.5.2 Condition Variables for Synchronization\nAs we noted in the previous section, indiscriminate use of locks can result in idling overhead\nfrom blocked threads. While the function pthread_mutex_trylock alleviates this overhead, it\nintroduces the overhead of polling for availability of locks. For example, if the producer-\nconsumer example is rewritten using pthread_mutex_trylock instead of pthread_mutex_lock ,\nthe producer and consumer threads would have to periodically poll for availability of lock (and\nsubsequently availability of buffer space or tasks on queue). A natural solution to this problem\nis to suspend the execution of the producer until space becomes available (an interrupt driven\nmechanism as opposed to a polled mechanism). The availability of space is signaled by the\nconsumer thread that consumes the task. The functionality to accomplish this is provided by a\ncondition variable .\nA condition variable is a data object used for synchronizing threads. This variable allows a\nthread to block itself until specified data reaches a predefined state. In the producer-consumer\ncase, the shared variable task_available  must become 1 before the consumer threads can be\nsignaled. The boolean condition task_available == 1  is referred to as a predicate. A condition\nvariable", "doc_id": "835a55d7-23e7-4bac-9b9f-4c280b2a1d47", "embedding": null, "doc_hash": "55a12b5f042a577170ac32606e9c6e1c0ccc7e30eb35b56dcefbe78a06b75c89", "extra_info": null, "node_info": {"start": 836543, "end": 840118}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f5d3fdaf-2368-4a06-81b7-dbd8103fbaad", "3": "5c870abf-bde8-4072-8732-c7d4041bc885"}}, "__type__": "1"}, "5c870abf-bde8-4072-8732-c7d4041bc885": {"__data__": {"text": "until space becomes available (an interrupt driven\nmechanism as opposed to a polled mechanism). The availability of space is signaled by the\nconsumer thread that consumes the task. The functionality to accomplish this is provided by a\ncondition variable .\nA condition variable is a data object used for synchronizing threads. This variable allows a\nthread to block itself until specified data reaches a predefined state. In the producer-consumer\ncase, the shared variable task_available  must become 1 before the consumer threads can be\nsignaled. The boolean condition task_available == 1  is referred to as a predicate. A condition\nvariable is associated with this predicate. When the predicate becomes true, the condition\nvariable is used to signal one or more threads waiting on the condition. A single condition\nvariable may be associated with more than one predicate. However, this is strongly discouraged\nsince it makes the program difficult to debug.\nA condition variable always has a mutex associated with it. A thread locks this mutex and tests\nthe predicate defined on the shared variable (in this case task_available ); if the predicate is\nnot true, the thread waits on the condition variable associated with the predicate using the\nfunction pthread_cond_wait . The prototype of this function is: 1   int pthread_cond_wait(pthread_cond_t *cond, \n2       pthread_mutex_t *mutex); \nA call to this function blocks the execution of the thread until it receives a signal from another\nthread or is interrupted by an OS signal. In addition to blocking the thread, the\npthread_cond_wait  function releases the lock on mutex . This is important because otherwise\nno other thread will be able to work on the shared variable task_available  and the predicate\nwould never be satisfied. When the thread is released on a signal, it waits to reacquire the lock\non mutex  before resuming execution. It is convenient to think of each condition variable as\nbeing associated with a queue. Threads performing a condition wait on the variable relinquish\ntheir lock and enter the queue. When the condition is signaled (using pthread_cond_signal ),\none of these threads in the queue is unblocked, and when the mutex becomes available, it is\nhanded to this thread (and the thread becomes runnable).\nIn the context of our producer-consumer example, the producer thread produces the task and,\nsince the lock on mutex  has been relinquished (by waiting consumers), it can insert its task on\nthe queue and set task_available  to 1 after locking mutex . Since the predicate has now been\nsatisfied, the producer must wake up one of the consumer threads by signaling it. This is done\nusing the function pthread_cond_signal , whose prototype is as follows:\n1       int pthread_cond_signal(pthread_cond_t *cond); \nThe function unblocks at least one thread that is currently waiting on the condition variable\ncond. The producer then relinquishes its lock on mutex  by explicitly calling\npthread_mutex_unlock , allowing one of the blocked consumer threads to consume the task.\nBefore we rewrite our producer-consumer example using condition variables, we need to\nintroduce two more function calls for initializing and destroying condition variables,\npthread_cond_init  and pthread_cond_destroy  respectively. The prototypes of these calls are\nas follows:\n1   int pthread_cond_init(pthread_cond_t *cond, \n2       const pthread_condattr_t *attr); \n3   int pthread_cond_destroy(pthread_cond_t *cond); \nThe function pthread_cond_init  initializes a condition variable (pointed to by cond) whose\nattributes are defined in the attribute object attr. Setting this pointer to NULL assigns default\nattributes for condition variables. If at some point in a program a condition variable is no longer\nrequired, it can be discarded using the function pthread_cond_destroy", "doc_id": "5c870abf-bde8-4072-8732-c7d4041bc885", "embedding": null, "doc_hash": "ee70aa8e7b93c79049cfbe94f031c38a33699b5bf5d1a569fe71ce712e3cf6f6", "extra_info": null, "node_info": {"start": 839795, "end": 843629}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "835a55d7-23e7-4bac-9b9f-4c280b2a1d47", "3": "db8af660-28c8-4400-a466-ae2a1f784609"}}, "__type__": "1"}, "db8af660-28c8-4400-a466-ae2a1f784609": {"__data__": {"text": "are\nas follows:\n1   int pthread_cond_init(pthread_cond_t *cond, \n2       const pthread_condattr_t *attr); \n3   int pthread_cond_destroy(pthread_cond_t *cond); \nThe function pthread_cond_init  initializes a condition variable (pointed to by cond) whose\nattributes are defined in the attribute object attr. Setting this pointer to NULL assigns default\nattributes for condition variables. If at some point in a program a condition variable is no longer\nrequired, it can be discarded using the function pthread_cond_destroy . These functions for\nmanipulating condition variables enable us to rewrite our producer-consumer segment as\nfollows:\nExample 7.6 Producer-consumer using condition variables\nCondition variables can be used to block execution of the producer thread when the\nwork queue is full and the consumer thread when the work queue is empty. We use\ntwo condition variables cond_queue_empty and cond_queue_full  for specifying\nempty and full queues respectively. The predicate associated with cond_queue_empty\nis task_available == 0 , and cond_queue_full  is asserted when task_available\n== 1.\nThe producer queue locks the mutex task_queue_cond_lock  associated with the\nshared variable task_available . It checks to see if task_available  is 0 (i.e., queue\nis empty). If this is the case, the producer inserts the task into the work queue and\nsignals any waiting consumer threads to wake up by signaling the condition variable\ncond_queue_full . It subsequently proceeds to create additional tasks. If\ntask_available  is 1 (i.e., queue is full), the producer performs a condition wait on\nthe condition variable cond_queue_empty (i.e., it waits for the queue to become\nempty). The reason for implicitly releasing the lock on task_queue_cond_lock\nbecomes clear at this point. If the lock is not released, no consumer will be able to\nconsume the task and the queue would never be empty. At this point, the producer\nthread is blocked. Since the lock is available to the consumer, the thread can consume\nthe task and signal the condition variable cond_queue_empty when the task has been\ntaken off the work queue.\nThe consumer thread locks the mutex task_queue_cond_lock  to check if the shared\nvariable task_available  is 1. If not, it performs a condition wait on\ncond_queue_full . (Note that this signal is generated from the producer when a task\nis inserted into the work queue.) If there is a task available, the consumer takes it off\nthe work queue and signals the producer. In this way, the producer and consumer\nthreads operate by signaling each other. It is easy to see that this mode of operation\nis similar to an interrupt-based operation as opposed to a polling-based operation of\npthread_mutex_trylock. The program segment for accomplishing this producer-\nconsumer behavior is as follows:\n1   pthread_cond_t cond_queue_empty, cond_queue_full; \n2   pthread_mutex_t task_queue_cond_lock; \n3   int task_available; \n4 \n5   /* other data structures here */ \n6 \n7   main() { \n8       /* declarations and initializations */ \n9       task_available = 0; \n10      pthread_init(); \n11      pthread_cond_init(&cond_queue_empty, NULL); \n12      pthread_cond_init(&cond_queue_full, NULL); \n13      pthread_mutex_init(&task_queue_cond_lock, NULL); \n14      /* create and join producer and consumer threads */ \n15  } \n16 \n17  void *producer(void *producer_thread_data) { \n18      int inserted; \n19      while", "doc_id": "db8af660-28c8-4400-a466-ae2a1f784609", "embedding": null, "doc_hash": "effec8635328ce069f9c24f96b4c6813af2c03c3fa53dbed28108c532b45df0a", "extra_info": null, "node_info": {"start": 843733, "end": 847140}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5c870abf-bde8-4072-8732-c7d4041bc885", "3": "23dd34fb-9491-4907-98a3-60cd0980d95d"}}, "__type__": "1"}, "23dd34fb-9491-4907-98a3-60cd0980d95d": {"__data__": {"text": "  task_available = 0; \n10      pthread_init(); \n11      pthread_cond_init(&cond_queue_empty, NULL); \n12      pthread_cond_init(&cond_queue_full, NULL); \n13      pthread_mutex_init(&task_queue_cond_lock, NULL); \n14      /* create and join producer and consumer threads */ \n15  } \n16 \n17  void *producer(void *producer_thread_data) { \n18      int inserted; \n19      while (!done()) { \n20          create_task(); \n21          pthread_mutex_lock(&task_queue_cond_lock); \n22          while (task_available == 1) \n23              pthread_cond_wait(&cond_queue_empty, \n24                  &task_queue_cond_lock); \n25          insert_into_queue(); \n26          task_available = 1; \n27          pthread_cond_signal(&cond_queue_full); \n28          pthread_mutex_unlock(&task_queue_cond_lock); \n29      } \n30  } \n31 \n32  void *consumer(void *consumer_thread_data) { \n33      while (!done()) { \n34          pthread_mutex_lock(&task_queue_cond_lock); \n35          while (task_available == 0) \n36              pthread_cond_wait(&cond_queue_full, \n37                  &task_queue_cond_lock); \n38          my_task = extract_from_queue(); \n39          task_available = 0; \n40          pthread_cond_signal(&cond_queue_empty); \n41          pthread_mutex_unlock(&task_queue_cond_lock); \n42          process_task(my_task); \n43      } \n44  } \nProgramming Notes  An important point to note about this program segment is that\nthe predicate associated with a condition variable is checked in a loop. One might\nexpect that when cond_queue_full  is asserted, the value of task_available  must be\n1. However, it is a good practice to check for the condition in a loop because the\nthread might be woken up due to other reasons (such as an OS signal). In other\ncases, when the condition variable is signaled using a condition broadcast (signaling\nall waiting threads instead of just one), one of the threads that got the lock earlier\nmight invalidate the condition. In the example of multiple producers and multiple\nconsumers, a task available on the work queue might be consumed by one of the\nother consumers.\nPerformance Notes  When a thread performs a condition wait, it takes itself off the\nrunnable list \u2013 consequently, it does not use any CPU cycles until it is woken up. This\nis in contrast to a mutex lock which consumes CPU cycles as it polls for the lock. \nIn the above example, each task could be consumed by only one consumer thread. Therefore,\nwe choose to signal one blocked thread at a time. In some other computations, it may be\nbeneficial to wake all threads that are waiting on the condition variable as opposed to a single\nthread. This can be done using the function pthread_cond_broadcast .\n1   int", "doc_id": "23dd34fb-9491-4907-98a3-60cd0980d95d", "embedding": null, "doc_hash": "236ea226a8032b863c5b9731274d522cf538cd8f12ff2854e70df8e0914f4c6e", "extra_info": null, "node_info": {"start": 847293, "end": 849981}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "db8af660-28c8-4400-a466-ae2a1f784609", "3": "9c964db4-c302-4aa9-ba47-2a2bbb589766"}}, "__type__": "1"}, "9c964db4-c302-4aa9-ba47-2a2bbb589766": {"__data__": {"text": "performs a condition wait, it takes itself off the\nrunnable list \u2013 consequently, it does not use any CPU cycles until it is woken up. This\nis in contrast to a mutex lock which consumes CPU cycles as it polls for the lock. \nIn the above example, each task could be consumed by only one consumer thread. Therefore,\nwe choose to signal one blocked thread at a time. In some other computations, it may be\nbeneficial to wake all threads that are waiting on the condition variable as opposed to a single\nthread. This can be done using the function pthread_cond_broadcast .\n1   int pthread_cond_broadcast(pthread_cond_t *cond); \nAn example of this is in the producer-consumer scenario with large work queues and multiple\ntasks being inserted into the work queue on each insertion cycle. This is left as an exercise for\nthe reader (Exercise 7.2). Another example of the use of pthread_cond_broadcast  is in the\nimplementation of barriers illustrated in Section 7.8.2 .\nIt is often useful to build time-outs into condition waits. Using the function\npthread_cond_timedwait , a thread can perform a wait on a condition variable until a specified\ntime expires. At this point, the thread wakes up by itself if it does not receive a signal or a\nbroadcast. The prototype for this function is:\n1   int pthread_cond_timedwait(pthread_cond_t *cond, \n2       pthread_mutex_t *mutex, \n3       const struct timespec *abstime); \nIf the absolute time abstime  specified expires before a signal or broadcast is received, the\nfunction returns an error message. It also reacquires the lock on mutex  when it becomes\navailable.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.6 Controlling Thread and Synchronization Attributes\nIn our discussion thus far, we have noted that entities such as threads and synchronization\nvariables can have several attributes associated with them. For example, different threads may\nbe scheduled differently (round-robin, prioritized, etc.), they may have different stack sizes,\nand so on. Similarly, a synchronization variable such as a mutex-lock may be of different types.\nThe Pthreads API allows a programmer to change the default attributes of entities using\nattributes objects .\nAn attributes object is a data-structure that describes entity (thread, mutex, condition variable)\nproperties. When creating a thread or a synchronization variable, we can specify the attributes\nobject that determines the properties of the entity. Once created, the thread or synchronization\nvariable's properties are largely fixed (Pthreads allows the user to change the priority of the\nthread). Subsequent changes to attributes objects do not change the properties of entities\ncreated using the attributes object prior to the change. There are several advantages of using\nattributes objects. First, it separates the issues of program semantics and implementation.\nThread properties are specified by the user. How these are implemented at the system level is\ntransparent to the user. This allows for greater portability across operating systems. Second,\nusing attributes objects improves modularity and readability of the programs. Third, it allows\nthe user to modify the program easily. For instance, if the user wanted to change the scheduling\nfrom round robin to time-sliced for all threads, they would only need to change the specific\nattribute in the attributes object.\nTo create an attributes object with the desired properties, we must first create an object with\ndefault properties and then modify the object as required. We look at Pthreads functions for\naccomplishing this for threads and synchronization variables.\n7.6.1 Attributes Objects for Threads\nThe function pthread_attr_init  lets us create an attributes object for threads. The prototype\nof this function is1   int \n2   pthread_attr_init (", "doc_id": "9c964db4-c302-4aa9-ba47-2a2bbb589766", "embedding": null, "doc_hash": "849b5abe0d61dc39af97b0eae9d6dc3652bf7aaa99b0dc3ff6740883964c8bcb", "extra_info": null, "node_info": {"start": 849801, "end": 853588}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "23dd34fb-9491-4907-98a3-60cd0980d95d", "3": "f5fe9527-e8aa-412e-be89-97291aa15f45"}}, "__type__": "1"}, "f5fe9527-e8aa-412e-be89-97291aa15f45": {"__data__": {"text": "to change the scheduling\nfrom round robin to time-sliced for all threads, they would only need to change the specific\nattribute in the attributes object.\nTo create an attributes object with the desired properties, we must first create an object with\ndefault properties and then modify the object as required. We look at Pthreads functions for\naccomplishing this for threads and synchronization variables.\n7.6.1 Attributes Objects for Threads\nThe function pthread_attr_init  lets us create an attributes object for threads. The prototype\nof this function is1   int \n2   pthread_attr_init ( \n3       pthread_attr_t *attr); \nThis function initializes the attributes object attr to the default values. Upon successful\ncompletion, the function returns a 0, otherwise it returns an error code. The attributes object\nmay be destroyed using the function pthread_attr_destroy . The prototype of this function is:\n1   int \n2   pthread_attr_destroy ( \n3       pthread_attr_t *attr); \nThe call returns a 0 on successful removal of the attributes object attr. Individual properties\nassociated with the attributes object can be changed using the following functions:\npthread_attr_setdetachstate , pthread_attr_setguardsize_np ,\npthread_attr_setstacksize , pthread_attr_setinheritsched ,\npthread_attr_setschedpolicy , and pthread_attr_setschedparam. These functions can be\nused to set the detach state in a thread attributes object, the stack guard size, the stack size,\nwhether scheduling policy is inherited from the creating thread, the scheduling policy (in case it\nis not inherited), and scheduling parameters, respectively. We refer the reader to the Pthreads\nmanuals for a detailed description of these functions. For most parallel programs, default thread\nproperties are generally adequate.\n7.6.2 Attributes Objects for Mutexes\nThe Pthreads API supports three different kinds of locks. All of these locks use the same\nfunctions for locking and unlocking; however, the type of lock is determined by the lock\nattribute. The mutex lock used in examples thus far is called a normal mutex . This is the\ndefault type of lock. Only a single thread is allowed to lock a normal mutex once at any point in\ntime. If a thread with a lock attempts to lock it again, the second locking call results in a\ndeadlock.\nConsider the following example of a thread searching for an element in a binary tree. To ensure\nthat other threads are not changing the tree during the search process, the thread locks the tree\nwith a single mutex tree_lock . The search function is as follows:\n1   search_tree(void *tree_ptr) \n2   { \n3       struct node *node_pointer; \n4       node_pointer = (struct node *) tree_ptr; \n5       pthread_mutex_lock(&tree_lock); \n6       if (is_search_node(node_pointer) == 1) { \n7           /* solution is found here */ \n8           print_node(node_pointer); \n9           pthread_mutex_unlock(&tree_lock); \n10          return(1); \n11      } \n12      else { \n13          if (tree_ptr -> left != NULL) \n14              search_tree((void *) tree_ptr -> left); \n15          if (tree_ptr -> right != NULL) \n16              search_tree((void *) tree_ptr -> right); \n17      } \n18     ", "doc_id": "f5fe9527-e8aa-412e-be89-97291aa15f45", "embedding": null, "doc_hash": "081b7ab40fb06bbcb18a4740d5a1a10f4af204db2028738d716ca48ee2e17af5", "extra_info": null, "node_info": {"start": 853561, "end": 856731}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9c964db4-c302-4aa9-ba47-2a2bbb589766", "3": "755aabfb-a502-44c4-b4bc-8a48eb0ad053"}}, "__type__": "1"}, "755aabfb-a502-44c4-b4bc-8a48eb0ad053": {"__data__": {"text": "\n10          return(1); \n11      } \n12      else { \n13          if (tree_ptr -> left != NULL) \n14              search_tree((void *) tree_ptr -> left); \n15          if (tree_ptr -> right != NULL) \n16              search_tree((void *) tree_ptr -> right); \n17      } \n18      printf(\"Search unsuccessful\\n\"); \n19      pthread_mutex_unlock(&tree_lock); \n20  } \nIf tree_lock  is a normal mutex, the first recursive call to the function search_tree ends in a\ndeadlock since a thread attempts to lock a mutex that it holds a lock on. For addressing such\nsituations, the Pthreads API supports a recursive mutex . A recursive mutex allows a single\nthread to lock a mutex multiple times. Each time a thread locks the mutex, a lock counter is\nincremented. Each unlock decrements the counter. For any other thread to be able to\nsuccessfully lock a recursive mutex, the lock counter must be zero (i.e., each lock by another\nthread must have a corresponding unlock). A recursive mutex is useful when a thread function\nneeds to call itself recursively.\nIn addition to normal and recursive mutexes, a third kind of mutex called an errorcheck\nmutex  is also supported. The operation of an errorcheck mutex is similar to a normal mutex in\nthat a thread can lock a mutex only once. However, unlike a normal mutex, when a thread\nattempts a lock on a mutex it has already locked, instead of deadlocking it returns an error.\nTherefore, an errorcheck mutex is more useful for debugging purposes.\nThe type of mutex can be specified using a mutex attribute object. To create and initialize a\nmutex attribute object to default values, Pthreads provides the function\npthread_mutexattr_init . The prototype of the function is:\n1   int \n2   pthread_mutexattr_init ( \n3       pthread_mutexattr_t   *attr); \nThis creates and initializes a mutex attributes object attr. The default type of mutex is a\nnormal mutex. Pthreads provides the function pthread_mutexattr_settype_np  for setting the\ntype of mutex specified by the mutex attributes object. The prototype for this function is:\n1   int \n2   pthread_mutexattr_settype_np ( \n3       pthread_mutexattr_t   *attr, \n4       int type); \nHere, type specifies the type of the mutex and can take one of the following values\ncorresponding to the three mutex types \u2013 normal, recursive, or errorcheck:\nPTHREAD_MUTEX_NORMAL_ NP\nPTHREAD_MUTEX_RECURSIVE_NP\nPTHREAD_MUTEX_ERRORCHECK_NP\nA mutex-attributes object can be destroyed using the pthread_attr_destroy  that takes the\nmutex attributes object attr as its only argument.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.7 Thread Cancellation\nConsider a simple program to evaluate a set of positions in a chess game. Assume that there\nare k moves, each being evaluated by an independent thread. If at any point of time, a position\nis established to be of a certain quality, the other positions that are known to be of worse\nquality must stop being evaluated. In other words, the threads evaluating the corresponding\nboard positions must be canceled. Posix threads provide this cancellation feature in the function\npthread_cancel . The prototype of this function is:\n1   int \n2  ", "doc_id": "755aabfb-a502-44c4-b4bc-8a48eb0ad053", "embedding": null, "doc_hash": "8e5d4e5daaa387c5063b513bddc6836ae7b08f88c98ea07a6ed93109fc15e6b0", "extra_info": null, "node_info": {"start": 857071, "end": 860198}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f5fe9527-e8aa-412e-be89-97291aa15f45", "3": "fbd6f09e-29a1-4af6-a774-f0f01e64e4f9"}}, "__type__": "1"}, "fbd6f09e-29a1-4af6-a774-f0f01e64e4f9": {"__data__": {"text": "]\n  \n7.7 Thread Cancellation\nConsider a simple program to evaluate a set of positions in a chess game. Assume that there\nare k moves, each being evaluated by an independent thread. If at any point of time, a position\nis established to be of a certain quality, the other positions that are known to be of worse\nquality must stop being evaluated. In other words, the threads evaluating the corresponding\nboard positions must be canceled. Posix threads provide this cancellation feature in the function\npthread_cancel . The prototype of this function is:\n1   int \n2   pthread_cancel ( \n3       pthread_t   thread); \nHere, thread is the handle to the thread to be canceled. A thread may cancel itself or cancel\nother threads. When a call to this function is made, a cancellation is sent to the specified\nthread. It is not guaranteed that the specified thread will receive or act on the cancellation.\nThreads can protect themselves against cancellation. When a cancellation is actually performed,\ncleanup functions are invoked for reclaiming the thread data structures. After this the thread is\ncanceled. This process is similar to termination of a thread using the pthread_exit  call. This is\nperformed independently of the thread that made the original request for cancellation. The\npthread_cancel  function returns after a cancellation has been sent. The cancellation may itself\nbe performed later. The function returns a 0 on successful completion. This does not imply that\nthe requested thread has been canceled; it implies that the specified thread is a valid thread for\ncancellation.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.8 Composite Synchronization Constructs\nWhile the Pthreads API provides a basic set of synchronization constructs, often, there is a need\nfor higher level constructs. These higher level constructs can be built using basic\nsynchronization constructs. In this section, we look at some of these constructs along with their\nperformance aspects and applications.\n7.8.1 Read-Write Locks\nIn many applications, a data structure is read frequently but written infrequently. For such\nscenarios, it is useful to note that multiple reads can proceed without any coherence problems.\nHowever, writes must be serialized. This points to an alternate structure called a read-write\nlock. A thread reading a shared data item acquires a read lock on the variable. A read lock is\ngranted when there are other threads that may already have read locks. If there is a write lock\non the data (or if there are queued write locks), the thread performs a condition wait. Similarly,\nif there are multiple threads requesting a write lock, they must perform a condition wait. Using\nthis principle, we design functions for read locks mylib_rwlock_rlock , write locks\nmylib_rwlock_wlock , and unlocking mylib_rwlock_unlock .\nThe read-write locks illustrated are based on a data structure called mylib_rwlock_t . This\nstructure maintains a count of the number of readers, the writer (a 0/1 integer specifying\nwhether a writer is present), a condition variable readers_proceed  that is signaled when\nreaders can proceed, a condition variable writer_proceed  that is signaled when one of the\nwriters can proceed, a count pending_writers  of pending writers, and a mutex\nread_write_lock  associated with the shared data structure. The function mylib_rwlock_init\nis used to initialize various components of this data structure.\nThe function mylib rwlock rlock  attempts a read lock on the data structure. It checks to see\nif there is a write lock or pending writers. If so, it performs a condition wait on the condition\nvariable readers proceed, otherwise it increments the count of readers and proceeds to grant\na read lock. The", "doc_id": "fbd6f09e-29a1-4af6-a774-f0f01e64e4f9", "embedding": null, "doc_hash": "d830edfbf32843851dce7fde76ed4329dae038cd424f0cac22d3f4789b5e1f4c", "extra_info": null, "node_info": {"start": 859891, "end": 863601}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "755aabfb-a502-44c4-b4bc-8a48eb0ad053", "3": "0e218f96-dca5-49ca-9795-a02d51658d16"}}, "__type__": "1"}, "0e218f96-dca5-49ca-9795-a02d51658d16": {"__data__": {"text": "that is signaled when one of the\nwriters can proceed, a count pending_writers  of pending writers, and a mutex\nread_write_lock  associated with the shared data structure. The function mylib_rwlock_init\nis used to initialize various components of this data structure.\nThe function mylib rwlock rlock  attempts a read lock on the data structure. It checks to see\nif there is a write lock or pending writers. If so, it performs a condition wait on the condition\nvariable readers proceed, otherwise it increments the count of readers and proceeds to grant\na read lock. The function mylib_rwlock_wlock  attempts a write lock on the data structure. It\nchecks to see if there are readers or writers; if so, it increments the count of pending writers\nand performs a condition wait on the condition variable writer_proceed . If there are no\nreaders or writer, it grants a write lock and proceeds.\nThe function mylib_rwlock_unlock  unlocks a read or write lock. It checks to see if there is a\nwrite lock, and if so, it unlocks the data structure by setting the writer field to 0. If there are\nreaders, it decrements the number of readers readers . If there are no readers left and there\nare pending writers, it signals one of the writers to proceed (by signaling writer_proceed ). If\nthere are no pending writers but there are pending readers, it signals all the reader threads to\nproceed. The code for initializing and locking/unlocking is as follows:1   typedef struct { \n2       int readers; \n3       int writer; \n4       pthread_cond_t readers_proceed; \n5       pthread_cond_t writer_proceed; \n6       int pending_writers; \n7       pthread_mutex_t read_write_lock; \n8   } mylib_rwlock_t; \n9 \n10 \n11  void mylib_rwlock_init (mylib_rwlock_t *l) { \n12     l -> readers = l -> writer = l -> pending_writers = 0; \n13     pthread_mutex_init(&(l -> read_write_lock), NULL); \n14     pthread_cond_init(&(l -> readers_proceed), NULL); \n15     pthread_cond_init(&(l -> writer_proceed), NULL); \n16  } \n17 \n18  void mylib_rwlock_rlock(mylib_rwlock_t *l) { \n19     /* if there is a write lock or pending writers, perform condition \n20     wait.. else increment count of readers and grant read lock */ \n21 \n22     pthread_mutex_lock(&(l -> read_write_lock)); \n23     while ((l -> pending_writers > 0) || (l -> writer > 0)) \n24         pthread_cond_wait(&(l -> readers_proceed), \n25            &(l -> read_write_lock)); \n26     l -> readers ++; \n27     pthread_mutex_unlock(&(l -> read_write_lock)); \n28  } \n29 \n30 \n31  void mylib_rwlock_wlock(mylib_rwlock_t *l) { \n32     /* if there are readers or writers, increment pending writers \n33     count and wait. On being woken, decrement pending writers \n34     count and increment writer count */ \n35 \n36     pthread_mutex_lock(&(l -> read_write_lock)); \n37     while ((l -> writer > 0) || (l -> readers > 0)) { \n38        ", "doc_id": "0e218f96-dca5-49ca-9795-a02d51658d16", "embedding": null, "doc_hash": "bf6804d737d5dff77746aabde052345c6e63df91feb4ab15bd359b54de038a6e", "extra_info": null, "node_info": {"start": 863594, "end": 866443}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "fbd6f09e-29a1-4af6-a774-f0f01e64e4f9", "3": "39ff2375-5d64-4a92-bac1-72c0dda2921d"}}, "__type__": "1"}, "39ff2375-5d64-4a92-bac1-72c0dda2921d": {"__data__": {"text": "\n28  } \n29 \n30 \n31  void mylib_rwlock_wlock(mylib_rwlock_t *l) { \n32     /* if there are readers or writers, increment pending writers \n33     count and wait. On being woken, decrement pending writers \n34     count and increment writer count */ \n35 \n36     pthread_mutex_lock(&(l -> read_write_lock)); \n37     while ((l -> writer > 0) || (l -> readers > 0)) { \n38         l -> pending_writers ++; \n39         pthread_cond_wait(&(l -> writer_proceed), \n40            &(l -> read_write_lock)); \n41     } \n42     l -> pending_writers --; \n43     l -> writer ++ \n44     pthread_mutex_unlock(&(l -> read_write_lock)); \n45  } \n46 \n47 \n48  void mylib_rwlock_unlock(mylib_rwlock_t *l) { \n49     /* if there is a write lock then unlock, else if there are \n50     read locks, decrement count of read locks. If the count \n51     is 0 and there is a pending writer, let it through, else \n52     if there are pending readers, let them all go through */ \n53 \n54     pthread_mutex_lock(&(l -> read_write_lock)); \n55     if (l -> writer > 0) \n56        l -> writer = 0; \n57     else if (l -> readers > 0) \n58        l -> readers --; \n59     pthread_mutex_unlock(&(l -> read_write_lock)); \n60     if ((l -> readers == 0) && (l -> pending_writers > 0)) \n61        pthread_cond_signal(&(l -> writer_proceed)); \n62     else if (l -> readers > 0) \n63        pthread_cond_broadcast(&(l -> readers_proceed)); \n64  } \nWe now illustrate the use of read-write locks with some examples.\nExample 7.7 Using read-write locks for computing the minimum\nof a list of numbers\nA simple use of read-write locks is in computing the minimum of a list of numbers. In\nour earlier implementation, we associated a lock with the minimum value. Each thread\nlocked this object and updated the minimum value, if necessary. In general, the\nnumber of times the value is examined is greater than the number of times it is\nupdated. Therefore, it is beneficial to allow multiple reads using a read lock and write\nafter a write lock only if needed. The corresponding program segment is as follows:\n1   void *find_min_rw(void *list_ptr) { \n2       int *partial_list_pointer, my_min, i; \n3       my_min = MIN_INT; \n4       partial_list_pointer = (int *) list_ptr; \n5       for (i = 0; i < partial_list_size; i++) \n6           if (partial_list_pointer[i] < my_min) \n7               my_min = partial_list_pointer[i]; \n8       /* lock the mutex associated with minimum_value and \n9       update the variable as required */ \n10      mylib_rwlock_rlock(&read_write_lock); \n11      if (my_min < minimum_value) { \n12         ", "doc_id": "39ff2375-5d64-4a92-bac1-72c0dda2921d", "embedding": null, "doc_hash": "01f82b34379547159b5d8ec7bb978c84370c61708ea5021a10415a8a39d894af", "extra_info": null, "node_info": {"start": 866639, "end": 869203}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0e218f96-dca5-49ca-9795-a02d51658d16", "3": "f9f19ec5-3f91-4818-8a2a-ac5e7821c805"}}, "__type__": "1"}, "f9f19ec5-3f91-4818-8a2a-ac5e7821c805": {"__data__": {"text": "i < partial_list_size; i++) \n6           if (partial_list_pointer[i] < my_min) \n7               my_min = partial_list_pointer[i]; \n8       /* lock the mutex associated with minimum_value and \n9       update the variable as required */ \n10      mylib_rwlock_rlock(&read_write_lock); \n11      if (my_min < minimum_value) { \n12          mylib_rwlock_unlock(&read_write_lock); \n13          mylib_rwlock_wlock(&read_write_lock); \n14          minimum_value = my_min; \n15      } \n16      /* and unlock the mutex */ \n17      mylib_rwlock_unlock(&read_write_lock); \n18      pthread_exit(0); \n19  } \nProgramming Notes  In this example, each thread computes the minimum element\nin its partial list. It then attempts a read lock on the lock associated with the global\nminimum value. If the global minimum value is greater than the locally minimum\nvalue (thus requiring an update), the read lock is relinquished and a write lock is\nsought. Once the write lock has been obtained, the global minimum can be updated.\nThe performance gain obtained from read-write locks is influenced by the number of\nthreads and the number of updates (write locks) required. In the extreme case when\nthe first value of the global minimum is also the true minimum value, no write locks\nare subsequently sought. In this case, the version using read-write locks performs\nbetter. In contrast, if each thread must update the global minimum, the read locks are\nsuperfluous and add overhead to the program. \nExample 7.8 Using read-write locks for implementing hash\ntables\nA commonly used operation in applications ranging from database query to state\nspace search is the search of a key in a database. The database is organized as a hash\ntable. In our example, we assume that collisions are handled by chaining colliding\nentries into linked lists. Each list has a lock associated with it. This lock ensures that\nlists are not being updated and searched at the same time. We consider two versions\nof this program: one using mutex locks and one using read-write locks developed in\nthis section.\nThe mutex lock version of the program hashes the key into the table, locks the mutex\nassociated with the table index, and proceeds to search/update within the linked list.\nThe thread function for doing this is as follows:\n1   manipulate_hash_table(int entry) { \n2       int table_index, found; \n3       struct list_entry *node, *new_node; \n4 \n5       table_index = hash(entry); \n6       pthread_mutex_lock(&hash_table[table_index].list_lock); \n7       found = 0; \n8       node = hash_table[table_index].next; \n9       while ((node != NULL) && (!found)) { \n10          if (node -> value == entry) \n11              found = 1; \n12          else \n13              node = node -> next; \n14      } \n15      pthread_mutex_unlock(&hash_table[table_index].list_lock); \n16      if (found) \n17          return(1); \n18      else \n19         ", "doc_id": "f9f19ec5-3f91-4818-8a2a-ac5e7821c805", "embedding": null, "doc_hash": "46e641037b3db33d264fac82b14e085910a52cd06fbf68e06c620124126ebede", "extra_info": null, "node_info": {"start": 869248, "end": 872129}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "39ff2375-5d64-4a92-bac1-72c0dda2921d", "3": "d503dc02-9a9b-46d0-a04a-93b0d914879a"}}, "__type__": "1"}, "d503dc02-9a9b-46d0-a04a-93b0d914879a": {"__data__": {"text": "         if (node -> value == entry) \n11              found = 1; \n12          else \n13              node = node -> next; \n14      } \n15      pthread_mutex_unlock(&hash_table[table_index].list_lock); \n16      if (found) \n17          return(1); \n18      else \n19          insert_into_hash_table(entry); \n20  } \nHere, the function insert_into_hash_table  must lock\nhash_table[table_index].list_lock  before performing the actual insertion. When\na large fraction of the queries are found in the hash table (i.e., they do not need to be\ninserted), these searches are serialized. It is easy to see that multiple threads can be\nsafely allowed to search the hash table and only updates to the table must be\nserialized. This can be accomplished using read-write locks. We can rewrite the\nmanipulate_hash table  function as follows:\n1   manipulate_hash_table(int entry) \n2   { \n3       int table_index, found; \n4       struct list_entry *node, *new_node; \n5 \n6       table_index = hash(entry); \n7       mylib_rwlock_rlock(&hash_table[table_index].list_lock); \n8       found = 0; \n9       node = hash_table[table_index].next; \n10      while ((node != NULL) && (!found)) { \n11          if (node -> value == entry) \n12              found = 1; \n13          else \n14              node = node -> next; \n15      } \n16      mylib_rwlock_rlock(&hash_table[table_index].list_lock); \n17      if (found) \n18          return(1); \n19      else \n20          insert_into_hash_table(entry); \n21  } \nHere, the function insert_into_hash_table  must first get a write lock on\nhash_table[table_index].list_lock  before performing actual insertion.\nProgramming Notes  In this example, we assume that the list_lock  field has been\ndefined to be of type mylib_rwlock_t  and all read-write locks associated with the\nhash tables have been initialized using the function mylib_rwlock_init . Using mylib\nrwlock_rlock_ instead of a mutex lock allows multiple threads to search respective\nentries concurrently. Thus, if the number of successful searches outnumber insertions,\nthis formulation is likely to yield better performance. Note that the insert\ninto_hash_table  function must be suitably modified to use write locks (instead of\nmutex locks as before). \nIt is important to identify situations where read-write locks offer advantages over normal locks.\nSince read-write locks offer no advantage over normal mutexes for writes, they are beneficial\nonly when there are a significant number of read operations. Furthermore, as the critical section\nbecomes larger, read-write locks offer more advantages. This is because the serialization\noverhead paid by normal mutexes is higher. Finally, since read-write locks rely on condition\nvariables, the underlying thread system must provide fast condition wait, signal, and broadcast\nfunctions. It is possible to do a simple analysis to understand the relative merits of read-write\nlocks (Exercise", "doc_id": "d503dc02-9a9b-46d0-a04a-93b0d914879a", "embedding": null, "doc_hash": "b9c3e0d281066762d1497589b00325e81b2dda7dc2aee8d022af5e5e7b8792c1", "extra_info": null, "node_info": {"start": 872213, "end": 875116}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f9f19ec5-3f91-4818-8a2a-ac5e7821c805", "3": "3e764b53-33d8-491f-b48b-7cacf8263cc3"}}, "__type__": "1"}, "3e764b53-33d8-491f-b48b-7cacf8263cc3": {"__data__": {"text": "situations where read-write locks offer advantages over normal locks.\nSince read-write locks offer no advantage over normal mutexes for writes, they are beneficial\nonly when there are a significant number of read operations. Furthermore, as the critical section\nbecomes larger, read-write locks offer more advantages. This is because the serialization\noverhead paid by normal mutexes is higher. Finally, since read-write locks rely on condition\nvariables, the underlying thread system must provide fast condition wait, signal, and broadcast\nfunctions. It is possible to do a simple analysis to understand the relative merits of read-write\nlocks (Exercise 7.7).\n7.8.2 Barriers\nAn important and often used construct in threaded (as well as other parallel) programs is a\nbarrier . A barrier call is used to hold a thread until all other threads participating in the barrier\nhave reached the barrier. Barriers can be implemented using a counter, a mutex and a\ncondition variable. (They can also be implemented simply using mutexes; however, such\nimplementations suffer from the overhead of busy-wait.) A single integer is used to keep track\nof the number of threads that have reached the barrier. If the count is less than the total\nnumber of threads, the threads execute a condition wait. The last thread entering (and setting\nthe count to the number of threads) wakes up all the threads using a condition broadcast. The\ncode for accomplishing this is as follows:1   typedef struct { \n2       pthread_mutex_t count_lock; \n3       pthread_cond_t ok_to_proceed; \n4       int count; \n5   } mylib_barrier_t; \n6 \n7   void mylib_init_barrier(mylib_barrier_t *b) { \n8       b -> count = 0; \n9       pthread_mutex_init(&(b -> count_lock), NULL); \n10      pthread_cond_init(&(b -> ok_to_proceed), NULL); \n11  } \n12 \n13  void mylib_barrier (mylib_barrier_t *b, int num_threads) { \n14      pthread_mutex_lock(&(b -> count_lock)); \n15      b -> count ++; \n16      if (b -> count == num_threads) { \n17          b -> count = 0; \n18          pthread_cond_broadcast(&(b -> ok_to_proceed)); \n19      } \n20      else \n21          while (pthread_cond_wait(&(b -> ok_to_proceed), \n22              &(b -> count_lock)) != 0); \n23      pthread_mutex_unlock(&(b -> count_lock)); \n24  } \nIn the above implementation of a barrier, threads enter the barrier and stay until the broadcast\nsignal releases them. The threads are released one by one since the mutex count_lock  is\npassed among them one after the other. The trivial lower bound on execution time of this\nfunction is therefore O (n) for n threads. This implementation of a barrier can be speeded up\nusing multiple barrier variables.\nLet us consider an alternate barrier implementation in which there are n/2 condition variable-\nmutex pairs for implementing a barrier for n threads. The barrier works as follows: at the first\nlevel, threads are paired up and each pair of threads shares a single condition variable-mutex\npair. A designated member of the pair waits for both threads to arrive at the pairwise barrier.\nOnce this happens, all the designated members are organized into", "doc_id": "3e764b53-33d8-491f-b48b-7cacf8263cc3", "embedding": null, "doc_hash": "8c679ac18d8f6afa466e22ac7828dc86f331c57afcf9d8e0e09b5dfc3d2c518e", "extra_info": null, "node_info": {"start": 874708, "end": 877818}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d503dc02-9a9b-46d0-a04a-93b0d914879a", "3": "910c6062-ae22-43ea-bf2d-b156d3d0924d"}}, "__type__": "1"}, "910c6062-ae22-43ea-bf2d-b156d3d0924d": {"__data__": {"text": "of this\nfunction is therefore O (n) for n threads. This implementation of a barrier can be speeded up\nusing multiple barrier variables.\nLet us consider an alternate barrier implementation in which there are n/2 condition variable-\nmutex pairs for implementing a barrier for n threads. The barrier works as follows: at the first\nlevel, threads are paired up and each pair of threads shares a single condition variable-mutex\npair. A designated member of the pair waits for both threads to arrive at the pairwise barrier.\nOnce this happens, all the designated members are organized into pairs, and this process\ncontinues until there is only one thread. At this point, we know that all threads have reached\nthe barrier point. We must release all threads at this point. However, releasing them requires\nsignaling all n/2 condition variables. We use the same hierarchical strategy for doing this. The\ndesignated thread in a pair signals the respective condition variables.\n1   typedef struct barrier_node { \n2       pthread_mutex_t count_lock; \n3       pthread_cond_t ok_to_proceed_up; \n4       pthread_cond_t ok_to_proceed_down; \n5       int count; \n6   } mylib_barrier_t_internal; \n7 \n8   typedef struct barrier_node mylog_logbarrier_t[MAX_THREADS]; \n9   pthread_t p_threads[MAX_THREADS]; \n10  pthread_attr_t attr; \n11 \n12  void mylib_init_barrier(mylog_logbarrier_t b) { \n13      int i; \n14      for (i = 0; i < MAX_THREADS; i++) { \n15          b[i].count = 0; \n16          pthread_mutex_init(&(b[i].count_lock), NULL); \n17          pthread_cond_init(&(b[i].ok_to_proceed_up), NULL); \n18          pthread_cond_init(&(b[i].ok_to_proceed_down), NULL); \n19      } \n20  } \n21 \n22  void mylib_logbarrier (mylog_logbarrier_t b, int num_threads, \n23             int thread_id) { \n24      int i, base, index; \n25      i=2; \n26      base = 0; \n27 \n28      do { \n29          index = base + thread_id / i; \n30          if (thread_id % i == 0) { \n31              pthread_mutex_lock(&(b[index].count_lock)); \n32              b[index].count ++; \n33              while (b[index].count < 2) \n34                  pthread_cond_wait(&(b[index].ok_to_proceed_up), \n35                      &(b[index].count_lock)); \n36                  pthread_mutex_unlock(&(b[index].count_lock)); \n37              } \n38              else { \n39                 ", "doc_id": "910c6062-ae22-43ea-bf2d-b156d3d0924d", "embedding": null, "doc_hash": "77d9a48f15926de03489ee4baa617a35afca203b1e42b99bb66323d4db8f6649", "extra_info": null, "node_info": {"start": 877891, "end": 880212}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3e764b53-33d8-491f-b48b-7cacf8263cc3", "3": "3b985f5f-2318-4e08-81db-38d5de3de25b"}}, "__type__": "1"}, "3b985f5f-2318-4e08-81db-38d5de3de25b": {"__data__": {"text": "       pthread_cond_wait(&(b[index].ok_to_proceed_up), \n35                      &(b[index].count_lock)); \n36                  pthread_mutex_unlock(&(b[index].count_lock)); \n37              } \n38              else { \n39                  pthread_mutex_lock(&(b[index].count_lock)); \n40                  b[index].count ++; \n41                  if (b[index].count == 2) \n42                      pthread_cond_signal(&(b[index].ok_to_proceed_up)); \n43                  while (pthread_cond_wait(&(b[index].ok_to_proceed_down), \n44                      &(b[index].count_lock)) != 0); \n45                  pthread_mutex_unlock(&(b[index].count_lock)); \n46                  break; \n47              } \n48              base = base + num_threads/i; \n49              i=i*2; \n50      } while (i <= num_threads); \n51      i=i/2; \n52      for (; i > 1; i = i / 2) { \n53          base = base - num_threads/i; \n54          index = base + thread_id / i; \n55          pthread_mutex_lock(&(b[index].count_lock)); \n56          b[index].count = 0; \n57          pthread_cond_signal(&(b[index].ok_to_proceed_down)); \n58          pthread_mutex_unlock(&(b[index].count_lock)); \n59      } \n60  } \nIn this implementation of a barrier, we visualize the barrier as a binary tree. Threads arrive at\nthe leaf nodes of this tree. Consider an instance of a barrier with eight threads. Threads 0 and 1\nare paired up on a single leaf node. One of these threads is designated as the representative of\nthe pair at the next level in the tree. In the above example, thread 0 is considered the\nrepresentative and it waits on the condition variable ok_to_proceed_up for thread 1 to catch\nup. All even numbered threads proceed to the next level in the tree. Now thread 0 is paired up\nwith thread 2 and thread 4 with thread 6. Finally thread 0 and 4 are paired. At this point,\nthread 0 realizes that all threads have reached the desired barrier point and releases threads by\nsignaling the condition ok_to_proceed_down . When all threads are released, the barrier is\ncomplete.\nIt is easy to see that there are n - 1 nodes in the tree for an n thread barrier. Each node\ncorresponds to two condition variables, one for releasing the thread up and one for releasing it\ndown, one lock, and a count of number of threads reaching the node. The tree", "doc_id": "3b985f5f-2318-4e08-81db-38d5de3de25b", "embedding": null, "doc_hash": "134de9d6716b0d27554c54948a1d74445aee8b8b69d1ec7c19ca703ce8c878c3", "extra_info": null, "node_info": {"start": 880571, "end": 882866}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "910c6062-ae22-43ea-bf2d-b156d3d0924d", "3": "e37ff5b0-2466-4ff3-83ac-48e9408d92a5"}}, "__type__": "1"}, "e37ff5b0-2466-4ff3-83ac-48e9408d92a5": {"__data__": {"text": "0 is paired up\nwith thread 2 and thread 4 with thread 6. Finally thread 0 and 4 are paired. At this point,\nthread 0 realizes that all threads have reached the desired barrier point and releases threads by\nsignaling the condition ok_to_proceed_down . When all threads are released, the barrier is\ncomplete.\nIt is easy to see that there are n - 1 nodes in the tree for an n thread barrier. Each node\ncorresponds to two condition variables, one for releasing the thread up and one for releasing it\ndown, one lock, and a count of number of threads reaching the node. The tree nodes are\nlinearly laid out in the array mylog_logbarrier_t  with the n/2 leaf nodes taking the first n/2\nelements, the n/4 tree nodes at the next higher level taking the next n/4 nodes and so on.\nIt is interesting to study the performance of this program. Since threads in the linear barrier are\nreleased one after the other, it is reasonable to expect runtime to be linear in the number of\nthreads even on multiple processors. In Figure 7.3 , we plot the runtime of 1000 barriers in a\nsequence on a 32 processor SGI Origin 2000. The linear runtime of the sequential barrier is\nclearly reflected in the runtime. The logarithmic barrier executing on a single processor does\njust as much work asymptotically as a sequential barrier (albeit with a higher constant).\nHowever, on a parallel machine, in an ideal case when threads are assigned so that subtrees of\nthe binary barrier tree are assigned to different processors, the time grows as O(n/p + log p).\nWhile this is difficult to achieve without being able to examine or assign blocks of threads\ncorresponding to subtrees to individual processors, the logarithmic barrier displays significantly\nbetter performance than the serial barrier. Its performance tends to be linear in n as n becomes\nlarge for a given number of processors. This is because the n/p term starts to dominate the log\np term in the execution time. This is observed both from observations as well as from analytical\nintuition.\nFigure 7.3. Execution time of 1000 sequential and logarithmic barriers\nas a function of number of threads on a 32 processor SGI Origin 2000.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.9 Tips for Designing Asynchronous Programs\nWhen designing multithreaded applications, it is important to remember that one cannot\nassume any order of execution with respect to other threads. Any such order must be explicitly\nestablished using the synchronization mechanisms discussed above: mutexes, condition\nvariables, and joins. In addition, the system may provide other means of synchronization.\nHowever, for portability reasons, we discourage the use of these mechanisms.\nIn many thread libraries, threads are switched at semi-deterministic intervals. Such libraries\nare more forgiving of synchronization errors in programs. These libraries are called slightly\nasynchronous  libraries. On the other hand, kernel threads (threads supported by the kernel)\nand threads scheduled on multiple processors are less forgiving. The programmer must\ntherefore not make any assumptions regarding the level of asynchrony in the threads library.\nLet us look at some common errors that arise from incorrect assumptions on relative execution\ntimes of threads:\nSay, a thread T1 creates another thread T2. T2 requires some data from thread T1. This\ndata is transferred using a global memory location. However, thread T1 places the data in\nthe location after creating thread T2. The implicit assumption here is that T1 will not be\nswitched until it blocks; or that T2 will get to the point at which it uses the data only after\nT1 has stored it there. Such assumptions may lead to errors since it is possible that T1\ngets switched as soon as it creates T2. In such a situation,", "doc_id": "e37ff5b0-2466-4ff3-83ac-48e9408d92a5", "embedding": null, "doc_hash": "cbb14fff789c61be261f758f39c3d967d31930e55a2d9b456a5ecd7c62155ed7", "extra_info": null, "node_info": {"start": 882531, "end": 886289}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3b985f5f-2318-4e08-81db-38d5de3de25b", "3": "08adeb84-66ad-4a69-81a0-3b3838d61071"}}, "__type__": "1"}, "08adeb84-66ad-4a69-81a0-3b3838d61071": {"__data__": {"text": "assumptions on relative execution\ntimes of threads:\nSay, a thread T1 creates another thread T2. T2 requires some data from thread T1. This\ndata is transferred using a global memory location. However, thread T1 places the data in\nthe location after creating thread T2. The implicit assumption here is that T1 will not be\nswitched until it blocks; or that T2 will get to the point at which it uses the data only after\nT1 has stored it there. Such assumptions may lead to errors since it is possible that T1\ngets switched as soon as it creates T2. In such a situation, T1 will receive uninitialized\ndata.Assume, as before, that thread T1 creates T2 and that it needs to pass data to thread T2\nwhich resides on its stack. It passes this data by passing a pointer to the stack location to\nthread T2. Consider the scenario in which T1 runs to completion before T2 gets scheduled.\nIn this case, the stack frame is released and some other thread may overwrite the space\npointed to formerly by the stack frame. In this case, what thread T2 reads from the\nlocation may be invalid data. Similar problems may exist with global variables.We strongly discourage the use of scheduling techniques as means of synchronization. It is\nespecially difficult to keep track of scheduling decisions on parallel machines. Further, as\nthe number of processors change, these issues may change depending on the thread\nscheduling policy. It may happen that higher priority threads are actually waiting while\nlower priority threads are running.We recommend the following rules of thumb which help minimize the errors in threaded\nprograms.\nSet up all the requirements for a thread before actually creating the thread. This includes\ninitializing the data, setting thread attributes, thread priorities, mutex-attributes, etc.\nOnce you create a thread, it is possible that the newly created thread actually runs to\ncompletion before the creating thread gets scheduled again.When there is a producer-consumer relation between two threads for certain data items,\nmake sure the producer thread places the data before it is consumed and that\nintermediate buffers are guaranteed to not overflow.\nAt the consumer end, make sure that the data lasts at least until all potential consumers\nhave consumed the data. This is particularly relevant for stack variables.\nWhere possible, define and use group synchronizations and data replication. This can\nimprove program performance significantly.\nWhile these simple tips provide guidelines for writing error-free threaded programs, extreme\ncaution must be taken to avoid race conditions and parallel overheads associated with\nsynchronization.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.10 OpenMP: a Standard for Directive Based Parallel\nProgramming\nIn the first part of this chapter, we studied the use of threaded APIs for programming shared\naddress space machines. While standardization and support for these APIs has come a long\nway, their use is still predominantly restricted to system programmers as opposed to\napplication programmers. One of the reasons for this is that APIs such as Pthreads are\nconsidered to be low-level primitives. Conventional wisdom indicates that a large class of\napplications can be efficiently supported by higher level constructs (or directives) which rid the\nprogrammer of the mechanics of manipulating threads. Such directive-based languages have\nexisted for a long time, but only recently have standardization efforts succeeded in the form of\nOpenMP. OpenMP is an API that can be used with FORTRAN, C, and C++ for programming\nshared address space machines. OpenMP directives provide support for concurrency,\nsynchronization, and data handling while obviating the need for explicitly setting up mutexes,\ncondition variables, data scope, and initialization. We use the OpenMP C API in the rest of this\nchapter.\n7.10.1 The OpenMP Programming Model\nWe initiate the OpenMP programming model with the aid of a", "doc_id": "08adeb84-66ad-4a69-81a0-3b3838d61071", "embedding": null, "doc_hash": "66fd36aea41ad942bf808453fe2ba804f15d7b4555f23576c66e14ff8128e9a2", "extra_info": null, "node_info": {"start": 886293, "end": 890228}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e37ff5b0-2466-4ff3-83ac-48e9408d92a5", "3": "27bb71a8-79a5-4e9c-926f-110559e863be"}}, "__type__": "1"}, "27bb71a8-79a5-4e9c-926f-110559e863be": {"__data__": {"text": "for a long time, but only recently have standardization efforts succeeded in the form of\nOpenMP. OpenMP is an API that can be used with FORTRAN, C, and C++ for programming\nshared address space machines. OpenMP directives provide support for concurrency,\nsynchronization, and data handling while obviating the need for explicitly setting up mutexes,\ncondition variables, data scope, and initialization. We use the OpenMP C API in the rest of this\nchapter.\n7.10.1 The OpenMP Programming Model\nWe initiate the OpenMP programming model with the aid of a simple program. OpenMP\ndirectives in C and C++ are based on the #pragma  compiler directives. The directive itself\nconsists of a directive name followed by clauses.1   #pragma omp directive [clause list] \nOpenMP programs execute serially until they encounter the parallel  directive. This directive is\nresponsible for creating a group of threads. The exact number of threads can be specified in the\ndirective, set using an environment variable, or at runtime using OpenMP functions. The main\nthread that encounters the parallel  directive becomes the master  of this group of threads and\nis assigned the thread id 0 within the group. The parallel  directive has the following\nprototype:\n1   #pragma omp parallel [clause list] \n2   /* structured block */ \n3 \nEach thread created by this directive executes the structured block  specified by the parallel\ndirective. The clause list is used to specify conditional parallelization, number of threads, and\ndata handling.\nConditional Parallelization:  The clause if (scalar expression)  determines whether\nthe parallel construct results in creation of threads. Only one if clause can be used with a\nparallel directive.\nDegree of Concurrency:  The clause num_threads (integer expression) specifies the\nnumber of threads that are created by the parallel  directive.\nData Handling:  The clause private (variable list)  indicates that the set of variables\nspecified is local to each thread \u2013 i.e., each thread has its own copy of each variable in the\nlist. The clause firstprivate (variable list)  is similar to the private clause, except\nthe values of variables on entering the threads are initialized to corresponding values\nbefore the parallel directive. The clause shared (variable list) indicates that all variables in\nthe list are shared across all the threads, i.e., there is only one copy. Special care must be\ntaken while handling these variables by threads to ensure serializability.\nIt is easy to understand the concurrency model of OpenMP when viewed in the context of the\ncorresponding Pthreads translation. In Figure 7.4, we show one possible translation of an\nOpenMP program to a Pthreads program. The interested reader may note that such a\ntranslation can easily be automated through a Yacc or CUP script.\nFigure 7.4. A sample OpenMP program along with its Pthreads\ntranslation that might be performed by an OpenMP compiler.\nExample 7.9 Using the parallel directive\n1   #pragma omp parallel if (is_parallel == 1) num_threads(8) \\ \n2                        private (a) shared (b) firstprivate(c) \n3   { \n4       /* structured block */ \n5   } \nHere, if the value of the variable is_parallel equals one, eight threads are created.\nEach of these threads gets private copies of variables a and c, and shares a single\nvalue of variable b. Furthermore, the value of each copy of c is initialized to the value\nof c before the parallel directive. \n\nThe default state of a variable is specified by the clause default (shared)  or default (none) .\nThe clause default (shared)  implies that, by", "doc_id": "27bb71a8-79a5-4e9c-926f-110559e863be", "embedding": null, "doc_hash": "518512d6f8a329aaa3b24ce7bd752d15582812523cadcb14210e73a8678e0dc0", "extra_info": null, "node_info": {"start": 890230, "end": 893818}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "08adeb84-66ad-4a69-81a0-3b3838d61071", "3": "73f863c6-904c-4d13-ba81-8687ad836722"}}, "__type__": "1"}, "73f863c6-904c-4d13-ba81-8687ad836722": {"__data__": {"text": "private (a) shared (b) firstprivate(c) \n3   { \n4       /* structured block */ \n5   } \nHere, if the value of the variable is_parallel equals one, eight threads are created.\nEach of these threads gets private copies of variables a and c, and shares a single\nvalue of variable b. Furthermore, the value of each copy of c is initialized to the value\nof c before the parallel directive. \n\nThe default state of a variable is specified by the clause default (shared)  or default (none) .\nThe clause default (shared)  implies that, by default, a variable is shared by all the threads.\nThe clause default (none)  implies that the state of each variable used in a thread must be\nexplicitly specified. This is generally recommended, to guard against errors arising from\nunintentional concurrent access to shared data.\nJust as firstprivate  specifies how multiple local copies of a variable are initialized inside a\nthread, the reduction  clause specifies how multiple local copies of a variable at different\nthreads are combined into a single copy at the master when threads exit. The usage of the\nreduction  clause is reduction (operator: variable list). This clause performs a\nreduction on the scalar variables specified in the list using the operator . The variables in the\nlist are implicitly specified as being private to threads. The operator  can be one of +, *, -,\n&, |, ^, && , and ||.\nExample 7.10 Using the reduction clause\n1       #pragma omp parallel reduction(+: sum) num_threads(8) \n2       { \n3           /* compute local sums here */ \n4       } \n5       /* sum here contains sum of all local instances of sums */ \nIn this example, each of the eight threads gets a copy of the variable sum. When the\nthreads exit, the sum of all of these local copies is stored in the single copy of the\nvariable (at the master thread). \nIn addition to these data handling clauses, there is one other clause, copyin. We will describe\nthis clause in Section 7.10.4  after we discuss data scope in greater detail.\nWe can now use the parallel  directive along with the clauses to write our first OpenMP\nprogram. We introduce two functions to facilitate this. The omp_get_num_threads() function\nreturns the number of threads in the parallel region and the omp_get_thread_num()  function\nreturns the integer i.d. of each thread (recall that the master thread has an i.d. 0).\nExample 7.11 Computing PI using OpenMP directives\nOur first OpenMP example follows from Example 7.2 , which presented a Pthreads\nprogram for the same problem. The parallel directive specifies that all variables except\nnpoints , the total number of random points in two dimensions across all threads, are\nlocal. Furthermore, the directive specifies that there are eight threads, and the value\nof sum after all threads complete execution is the sum of local values at each thread.\nThe function omp_get_num_threads  is used to determine the total number of threads.\nAs in Example 7.2, a for loop generates the required number of random points (in\ntwo dimensions) and determines how many of them are within the prescribed circle of\nunit diameter.\n1   /* ****************************************************** \n2      An OpenMP version of a threaded program to compute PI. \n3      ****************************************************** */ \n4 \n5       #pragma omp parallel default(private) shared (npoints) \\ \n6                            reduction(+: sum) num_threads(8) \n7       { \n8   ", "doc_id": "73f863c6-904c-4d13-ba81-8687ad836722", "embedding": null, "doc_hash": "aba8a940fbda47d42fac98a09f76da5067b610d1ab78ee96f12c5d0212e0c8ac", "extra_info": null, "node_info": {"start": 893860, "end": 897300}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "27bb71a8-79a5-4e9c-926f-110559e863be", "3": "69487513-8247-4c52-8dd2-69b0056d84e1"}}, "__type__": "1"}, "69487513-8247-4c52-8dd2-69b0056d84e1": {"__data__": {"text": "dimensions) and determines how many of them are within the prescribed circle of\nunit diameter.\n1   /* ****************************************************** \n2      An OpenMP version of a threaded program to compute PI. \n3      ****************************************************** */ \n4 \n5       #pragma omp parallel default(private) shared (npoints) \\ \n6                            reduction(+: sum) num_threads(8) \n7       { \n8         num_threads = omp_get_num_threads(); \n9         sample_points_per_thread = npoints / num_threads; \n10        sum = 0; \n11        for (i = 0; i < sample_points_per_thread; i++) { \n12          rand_no_x =(double)(rand_r(&seed))/(double)((2<<14)-1); \n13          rand_no_y =(double)(rand_r(&seed))/(double)((2<<14)-1); \n14          if (((rand_no_x - 0.5) * (rand_no_x - 0.5) + \n15              (rand_no_y - 0.5) * (rand_no_y - 0.5)) < 0.25) \n16              sum ++; \n17        } \n18      } \nNote that this program is much easier to write in terms of specifying creation and termination of\nthreads compared to the corresponding POSIX threaded program.\n7.10.2 Specifying Concurrent Tasks in OpenMP\nThe parallel  directive can be used in conjunction with other directives to specify concurrency\nacross iterations and tasks. OpenMP provides two directives \u2013 for and sections  \u2013 to specify\nconcurrent iterations and tasks.\nThe for Directive\nThe for directive is used to split parallel iteration spaces across threads. The general form of a\nfor directive is as follows:\n1       #pragma omp for [clause list] \n2           /* for loop */ \n3 \nThe clauses that can be used in this context are: private , firstprivate , lastprivate,\nreduction , schedule , nowait,and ordered . The first four clauses deal with data handling and\nhave identical semantics as in the case of the parallel  directive. The lastprivate clause deals\nwith how multiple local copies of a variable are written back into a single copy at the end of the\nparallel for loop. When using a for loop (or sections  directive as we shall see) for farming\nwork to threads, it is sometimes desired that the last iteration (as defined by serial execution)\nof the for loop update the value of a variable. This is accomplished using the lastprivate\ndirective.\nExample 7.12 Using the for directive for computing p\nRecall from Example 7.11  that each iteration of the for loop is independent, and can\nbe executed concurrently. In such situations, we can simplify the program using the\nfor directive. The modified code segment is as follows:\n1     #pragma omp parallel default(private) shared (npoints) \\ \n2                          reduction(+: sum) num_threads(8) \n3     { \n4       sum=0; \n5       #pragma omp for \n6       for (i = 0; i < npoints; i++) { \n7        ", "doc_id": "69487513-8247-4c52-8dd2-69b0056d84e1", "embedding": null, "doc_hash": "9fafcdad616b5ab1e2df000cd674c5f75846114d9e9024c610fe465f40f5e97d", "extra_info": null, "node_info": {"start": 897394, "end": 900142}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "73f863c6-904c-4d13-ba81-8687ad836722", "3": "37ea7c09-7bdd-405d-843c-a176029fa909"}}, "__type__": "1"}, "37ea7c09-7bdd-405d-843c-a176029fa909": {"__data__": {"text": "the program using the\nfor directive. The modified code segment is as follows:\n1     #pragma omp parallel default(private) shared (npoints) \\ \n2                          reduction(+: sum) num_threads(8) \n3     { \n4       sum=0; \n5       #pragma omp for \n6       for (i = 0; i < npoints; i++) { \n7         rand_no_x =(double)(rand_r(&seed))/(double)((2<<14)-1); \n8         rand_no_y =(double)(rand_r(&seed))/(double)((2<<14)-1); \n9         if (((rand_no_x - 0.5) * (rand_no_x - 0.5) + \n10            (rand_no_y - 0.5) * (rand_no_y - 0.5)) < 0.25) \n11            sum ++; \n12      } \n13    } \nThe for directive in this example specifies that the for loop immediately following the\ndirective must be executed in parallel, i.e., split across various threads. Notice that\nthe loop index goes from 0 to npoints  in this case, as opposed to\nsample_points_per_thread  in Example 7.11. The loop index for the for directive is\nassumed to be private, by default. It is interesting to note that the only difference\nbetween this OpenMP segment and the corresponding serial code is the two directives.\nThis example illustrates how simple it is to convert many serial programs into\nOpenMP-based threaded programs. \nAssigning Iterations to Threads\nThe schedule  clause of the for directive deals with the assignment of iterations to threads. The\ngeneral form of the schedule  directive is schedule(scheduling_class[, parameter]) .\nOpenMP supports four scheduling classes: static, dynamic , guided, and runtime .\nExample 7.13 Scheduling classes in OpenMP \u2013 matrix\nmultiplication.\nWe explore various scheduling classes in the context of dense matrix multiplication.\nThe code for multiplying two matrices a and b to yield matrix c is as follows:\n1       for (i = 0; i < dim; i++) { \n2           for (j = 0; j < dim; j++) { \n3               c(i,j) = 0; \n4               for (k = 0; k < dim; k++) { \n5                   c(i,j) += a(i, k) * b(k, j); \n6               } \n7           } \n8       } \nThe code segment above specifies a three-dimensional iteration space providing us\nwith an ideal example for studying various scheduling classes in OpenMP. \nStatic  The general form of the static scheduling class is schedule(static[,  chunk-size]) .\nThis technique splits the iteration space into equal chunks of size chunk-size  and assigns them\nto threads in a round-robin fashion. When no chunk-size  is specified, the iteration space is\nsplit into as many chunks as there are threads and one chunk is assigned to each thread.\nExample 7.14 Static scheduling of loops in matrix multiplication\nThe following modification of the matrix-multiplication program causes the outermost\niteration to be split statically", "doc_id": "37ea7c09-7bdd-405d-843c-a176029fa909", "embedding": null, "doc_hash": "fa699e57e6ebf9367f8eed8a39b477a4a7549f0962a275240c4ec200e812b9fe", "extra_info": null, "node_info": {"start": 900275, "end": 902958}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "69487513-8247-4c52-8dd2-69b0056d84e1", "3": "2b4e9b0a-9e67-4b4b-be54-a7a5f0cfed39"}}, "__type__": "1"}, "2b4e9b0a-9e67-4b4b-be54-a7a5f0cfed39": {"__data__": {"text": "OpenMP. \nStatic  The general form of the static scheduling class is schedule(static[,  chunk-size]) .\nThis technique splits the iteration space into equal chunks of size chunk-size  and assigns them\nto threads in a round-robin fashion. When no chunk-size  is specified, the iteration space is\nsplit into as many chunks as there are threads and one chunk is assigned to each thread.\nExample 7.14 Static scheduling of loops in matrix multiplication\nThe following modification of the matrix-multiplication program causes the outermost\niteration to be split statically across threads as illustrated in Figure 7.5(a) .\n1   #pragma omp parallel default(private) shared (a, b, c, dim) \\ \n2                        num_threads(4) \n3       #pragma omp for schedule(static) \n4       for (i = 0; i < dim; i++) { \n5           for (j = 0; j < dim; j++) { \n6               c(i,j) = 0; \n7               for (k = 0; k < dim; k++) { \n8                   c(i,j) += a(i, k) * b(k, j); \n9               } \n10          } \n11      } \nFigure 7.5. Three different schedules using the static\nscheduling class of OpenMP.\nSince there are four threads in all, if dim = 128 , the size of each partition is 32\ncolumns, since we have not specified the chunk size. Using schedule(static, 16)\nresults in the partitioning of the iteration space illustrated in Figure 7.5(b) . Another\nexample of the split illustrated in Figure 7.5(c)  results when each for loop in the\nprogram in Example 7.13  is parallelized across threads with a schedule(static) and\nnested parallelism is enabled (see Section 7.10.6 ). \n\nDynamic  Often, because of a number of reasons, ranging from heterogeneous computing\nresources to non-uniform processor loads, equally partitioned workloads take widely varying\nexecution times. For this reason, OpenMP has a dynamic  scheduling class. The general form of\nthis class is schedule(dynamic[, chunk-size]) . The iteration space is partitioned into chunks\ngiven by chunk-size . However, these are assigned to threads as they become idle. This takes\ncare of the temporal imbalances resulting from static scheduling. If no chunk-size  is specified,\nit defaults to a single iteration per chunk.\nGuided  Consider the partitioning of an iteration space of 100 iterations with a chunk size of 5.\nThis corresponds to 20 chunks. If there are 16 threads, in the best case, 12 threads get one\nchunk each and the remaining four threads get two chunks. Consequently, if there are as many\nprocessors as threads, this assignment results in considerable idling. The solution to this\nproblem (also referred to as an edge effect ) is to reduce the chunk size as we proceed through\nthe computation. This is the principle of the guided scheduling class. The general form of this\nclass is schedule(guided[, chunk-size]).In this class, the chunk size is reduced\nexponentially as each chunk is dispatched to a thread. The chunk-size  refers to the smallest\nchunk that should be dispatched. Therefore, when the number of iterations left is less than\nchunk-size , the", "doc_id": "2b4e9b0a-9e67-4b4b-be54-a7a5f0cfed39", "embedding": null, "doc_hash": "34e7e0662aa6298bc277e1b49ff397647d66aa9c45ad2982231ef099f3bd922d", "extra_info": null, "node_info": {"start": 902680, "end": 905706}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "37ea7c09-7bdd-405d-843c-a176029fa909", "3": "9e7378f9-53e5-47dc-81a5-e0cc06cb78c3"}}, "__type__": "1"}, "9e7378f9-53e5-47dc-81a5-e0cc06cb78c3": {"__data__": {"text": "threads, this assignment results in considerable idling. The solution to this\nproblem (also referred to as an edge effect ) is to reduce the chunk size as we proceed through\nthe computation. This is the principle of the guided scheduling class. The general form of this\nclass is schedule(guided[, chunk-size]).In this class, the chunk size is reduced\nexponentially as each chunk is dispatched to a thread. The chunk-size  refers to the smallest\nchunk that should be dispatched. Therefore, when the number of iterations left is less than\nchunk-size , the entire set of iterations is dispatched at once. The value of chunk-size  defaults\nto one if none is specified.\nRuntime  Often it is desirable to delay scheduling decisions until runtime. For example, if one\nwould like to see the impact of various scheduling strategies to select the best one, the\nscheduling can be set to runtime . In this case the environment variable OMP_SCHEDULE\ndetermines the scheduling class and the chunk size.\nWhen no scheduling class is specified with the omp for  directive, the actual scheduling\ntechnique is not specified and is implementation dependent. The for directive places additional\nrestrictions on the for loop that follows. For example, it must not have a break statement, the\nloop control variable must be an integer, the initialization expression of the for loop must be an\ninteger assignment, the logical expression must be one of <, \n, >, or \n , and the increment\nexpression must have integer increments or decrements only. For more details on these\nrestrictions, we refer the reader to the OpenMP manuals.\nSynchronization Across Multiple for Directives\nOften, it is desirable to have a sequence of for-directives within a parallel construct that do not\nexecute an implicit barrier at the end of each for directive. OpenMP provides a clause \u2013 nowait,\nwhich can be used with a for directive to indicate that the threads can proceed to the next\nstatement without waiting for all other threads to complete the for loop execution. This is\nillustrated in the following example:\nExample 7.15 Using the nowait clause\nConsider the following example in which variable name needs to be looked up in two\nlists \u2013 current_list  and past_list . If the name exists in a list, it must be processed\naccordingly. The name might exist in both lists. In this case, there is no need to wait\nfor all threads to complete execution of the first loop before proceeding to the second\nloop. Consequently, we can use the nowait clause to save idling and synchronization\noverheads as follows:\n1       #pragma omp parallel \n2       { \n3           #pragma omp for nowait \n4               for (i = 0; i < nmax; i++) \n5                   if (isEqual(name, current_list[i]) \n6                       processCurrentName(name); \n7           #pragma omp for \n8               for (i = 0; i < mmax; i++) \n9                   if (isEqual(name, past_list[i]) \n10                      processPastName(name); \n11      } \nThe sections  Directive\nThe for directive is suited to partitioning iteration spaces across threads. Consider now a\nscenario in which there are three tasks ( taskA , taskB , and taskC ) that need to be executed.\nAssume that these tasks are independent of", "doc_id": "9e7378f9-53e5-47dc-81a5-e0cc06cb78c3", "embedding": null, "doc_hash": "3810c239c36a56aa2ac8cb722bcbbf92a1712d9a52c1e9ec677d2a13cc90bde2", "extra_info": null, "node_info": {"start": 905720, "end": 908948}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2b4e9b0a-9e67-4b4b-be54-a7a5f0cfed39", "3": "5827db1f-ab9d-4a48-8b97-d1b96fd81e59"}}, "__type__": "1"}, "5827db1f-ab9d-4a48-8b97-d1b96fd81e59": {"__data__": {"text": "= 0; i < mmax; i++) \n9                   if (isEqual(name, past_list[i]) \n10                      processPastName(name); \n11      } \nThe sections  Directive\nThe for directive is suited to partitioning iteration spaces across threads. Consider now a\nscenario in which there are three tasks ( taskA , taskB , and taskC ) that need to be executed.\nAssume that these tasks are independent of each other and therefore can be assigned to\ndifferent threads. OpenMP supports such non-iterative parallel task assignment using the\nsections  directive. The general form of the sections  directive is as follows:\n1   #pragma omp sections [clause list] \n2   { \n3       [#pragma omp section \n4           /* structured block */ \n5       ] \n6       [#pragma omp section \n7           /* structured block */ \n8       ] \n9       ... \n10  } \nThis sections  directive assigns the structured block corresponding to each section to one\nthread (indeed more than one section can be assigned to a single thread). The clause list\nmay include the following clauses \u2013 private , firstprivate , lastprivate, reduction , and no\nwait. The syntax and semantics of these clauses are identical to those in the case of the for\ndirective. The lastprivate clause, in this case, specifies that the last section (lexically) of the\nsections  directive updates the value of the variable. The nowait clause specifies that there is\nno implicit synchronization among all threads at the end of the sections  directive.\nFor executing the three concurrent tasks taskA , taskB , and taskC , the corresponding sections\ndirective is as follows:\n1       #pragma omp parallel \n2       { \n3           #pragma omp sections \n4           { \n5               #pragma omp section \n6               { \n7                   taskA(); \n8               } \n9               #pragma omp section \n10              { \n11                  taskB(); \n12              } \n13              #pragma omp section \n14              { \n15                  taskC(); \n16              } \n17          } \n18      } \nIf there are three threads, each section (in this case, the associated task) is assigned to one\nthread. At the end of execution of the assigned section, the threads synchronize (unless the\nnowait clause is used). Note that it is illegal to branch in and out of section  blocks.\nMerging Directives\nIn our discussion thus far, we use the directive parallel  to create concurrent threads, and for\nand sections  to farm out work to threads. If there was no parallel  directive specified, the\nfor and sections  directives would execute serially (all work is farmed to a single thread, the\nmaster thread). Consequently, for and sections", "doc_id": "5827db1f-ab9d-4a48-8b97-d1b96fd81e59", "embedding": null, "doc_hash": "444c60c897011ba29511afac0098a52fd631f186e9d215976c92d1c68faf5489", "extra_info": null, "node_info": {"start": 909130, "end": 911784}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9e7378f9-53e5-47dc-81a5-e0cc06cb78c3", "3": "7eb64382-983a-4aa5-ad2c-1db74912024e"}}, "__type__": "1"}, "7eb64382-983a-4aa5-ad2c-1db74912024e": {"__data__": {"text": "(in this case, the associated task) is assigned to one\nthread. At the end of execution of the assigned section, the threads synchronize (unless the\nnowait clause is used). Note that it is illegal to branch in and out of section  blocks.\nMerging Directives\nIn our discussion thus far, we use the directive parallel  to create concurrent threads, and for\nand sections  to farm out work to threads. If there was no parallel  directive specified, the\nfor and sections  directives would execute serially (all work is farmed to a single thread, the\nmaster thread). Consequently, for and sections  directives are generally preceded by the\nparallel  directive. OpenMP allows the programmer to merge the parallel  directives to\nparallel for  and parallel sections , re-spectively. The clause list for the merged directive\ncan be from the clause lists of either the parallel  or for / sections  directives.\nFor example:\n1       #pragma omp parallel default (private) shared (n) \n2       { \n3           #pragma omp for \n4           for (i = 0 < i < n; i++) { \n5           /* body of parallel for loop */ \n6           } \n7       } \nis identical to:\n1   #pragma omp parallel for default (private) shared (n) \n2   { \n3       for (i = 0 < i < n; i++) { \n4       /* body of parallel for loop */ \n5       } \n6   } \n7 \nand:\n1       #pragma omp parallel \n2       { \n3           #pragma omp sections \n4           { \n5               #pragma omp section \n6               { \n7                   taskA(); \n8               } \n9               #pragma omp section \n10              { \n11                 taskB(); \n12              } \n13              /* other sections here */ \n14          } \n15      } \nis identical to:\n1       #pragma omp parallel sections \n2       { \n3           #pragma omp section \n4           { \n5               taskA(); \n6           } \n7           #pragma omp section \n8           { \n9               taskB(); \n10          } \n11          /* other sections here */ \n12      } \nNesting parallel  Directives\nLet us revisit Program 7.13. To split each of the for loops across various threads, we would\nmodify the program as follows:\n1   #pragma omp parallel for default(private) shared (a, b, c, dim) \\ \n2                        num_threads(2) \n3", "doc_id": "7eb64382-983a-4aa5-ad2c-1db74912024e", "embedding": null, "doc_hash": "087c25730c4c4ccd848c8dcc6b073f1eb1693dc48b9e5ec80e1a87a3ad5ce056", "extra_info": null, "node_info": {"start": 911579, "end": 913814}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5827db1f-ab9d-4a48-8b97-d1b96fd81e59", "3": "1a8d7753-df0a-4de6-b540-eb7c703bc8ed"}}, "__type__": "1"}, "1a8d7753-df0a-4de6-b540-eb7c703bc8ed": {"__data__": {"text": "     taskB(); \n10          } \n11          /* other sections here */ \n12      } \nNesting parallel  Directives\nLet us revisit Program 7.13. To split each of the for loops across various threads, we would\nmodify the program as follows:\n1   #pragma omp parallel for default(private) shared (a, b, c, dim) \\ \n2                        num_threads(2) \n3       for (i = 0; i < dim; i++) { \n4       #pragma omp parallel for default(private) shared (a, b, c, dim) \\ \n5                        num_threads(2) \n6           for (j = 0; j < dim; j++) { \n7               c(i,j) = 0; \n8               #pragma omp parallel for default(private) \\ \n9                        shared (a, b, c, dim) num_threads(2) \n10              for (k = 0; k < dim; k++) { \n11                  c(i,j) += a(i, k) * b(k, j); \n12              } \n13          } \n14      } \nWe start by making a few observations about how this segment is written. Instead of nesting\nthree for directives inside a single parallel  directive, we have used three parallel for\ndirectives. This is because OpenMP does not allow for, sections , and single directives that\nbind to the same parallel  directive to be nested. Furthermore, the code as written only\ngenerates a logical team of threads on encountering a nested parallel  directive. The newly\ngenerated logical team is still executed by the same thread corresponding to the outer\nparallel  directive. To generate a new set of threads, nested parallelism must be enabled using\nthe OMP_NESTED  environment variable. If the OMP_NESTED  environment variable is set to FALSE ,\nthen the inner parallel  region is serialized and executed by a single thread. If the OMP_NESTED\nenvironment variable is set to TRUE, nested parallelism is enabled. The default state of this\nenvironment variable is FALSE , i.e., nested parallelism is disabled. OpenMP environment\nvariables are discussed in greater detail in Section 7.10.6 .\nThere are a number of other restrictions associated with the use of synchronization constructs in\nnested parallelism. We refer the reader to the OpenMP manual for a discussion of these\nrestrictions.\n7.10.3 Synchronization Constructs in OpenMP\nIn Section 7.5, we described the need for coordinating the execution of multiple threads. This\nmay be the result of a desired execution order, the atomicity of a set of instructions, or the need\nfor serial execution of code segments. The Pthreads API supports mutexes and condition\nvariables. Using these we implemented a range of higher level functionality in the form of read-\nwrite locks, barriers, monitors, etc. The OpenMP standard provides this high-level functionality\nin an easy-to-use API. In this section, we will explore these directives and their use.\nSynchronization Point: The barrier  Directive\nA barrier is one of the most frequently used synchronization primitives. OpenMP provides", "doc_id": "1a8d7753-df0a-4de6-b540-eb7c703bc8ed", "embedding": null, "doc_hash": "8db77d3691e312122df05c0bbed42deddf95c9860f81cb6375150f5bfd1e998d", "extra_info": null, "node_info": {"start": 914064, "end": 916914}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7eb64382-983a-4aa5-ad2c-1db74912024e", "3": "82e33b3a-0203-4e9a-8a58-f107b8f328dd"}}, "__type__": "1"}, "82e33b3a-0203-4e9a-8a58-f107b8f328dd": {"__data__": {"text": "of a desired execution order, the atomicity of a set of instructions, or the need\nfor serial execution of code segments. The Pthreads API supports mutexes and condition\nvariables. Using these we implemented a range of higher level functionality in the form of read-\nwrite locks, barriers, monitors, etc. The OpenMP standard provides this high-level functionality\nin an easy-to-use API. In this section, we will explore these directives and their use.\nSynchronization Point: The barrier  Directive\nA barrier is one of the most frequently used synchronization primitives. OpenMP provides a\nbarrier  directive, whose syntax is as follows:\n1       #pragma omp barrier \nOn encountering this directive, all threads in a team wait until others have caught up, and then\nrelease. When used with nested parallel  directives, the barrier  directive binds to the closest\nparallel  directive. For executing barriers conditionally, it is important to note that a barrier\ndirective must be enclosed in a compound statement that is conditionally executed. This is\nbecause pragmas are compiler directives and not a part of the language. Barriers can also be\neffected by ending and restarting parallel  regions. However, there is usually a higher\noverhead associated with this. Consequently, it is not the method of choice for implementing\nbarriers.\nSingle Thread Executions: The single and master Directives\nOften, a computation within a parallel section needs to be performed by just one thread. A\nsimple example of this is the computation of the mean of a list of numbers. Each thread can\ncompute a local sum of partial lists, add these local sums to a shared global sum, and have one\nthread compute the mean by dividing this global sum by the number of entries in the list. The\nlast step can be accomplished using a single directive.\nA single directive specifies a structured block that is executed by a single (arbitrary) thread.\nThe syntax of the single directive is as follows:\n1   #pragma omp single [clause list] \n2            structured block \nThe clause list can take clauses private , firstprivate , and nowait. These clauses have the\nsame semantics as before. On encountering the single block, the first thread enters the block.\nAll the other threads proceed to the end of the block. If the nowait clause has been specified at\nthe end of the block, then the other threads proceed; otherwise they wait at the end of the\nsingle block for the thread to finish executing the block. This directive is useful for computing\nglobal data as well as performing I/O.\nThe master directive is a specialization of the single directive in which only the master thread\nexecutes the structured block. The syntax of the master directive is as follows:\n1   #pragma omp master \n2            structured block \nIn contrast to the single directive, there is no implicit barrier associated with the master\ndirective.\nCritical Sections: The critical  and atomic Directives\nIn our discussion of Pthreads, we had examined the use of locks to protect critical regions \u2013\nregions that must be executed serially, one thread at a time. In addition to explicit lock\nmanagement ( Section 7.10.5 ), OpenMP provides a critical  directive for implementing critical\nregions. The syntax of a critical  directive is:\n1   #pragma omp critical [(name)] \n2            structured block \nHere, the optional identifier name can be used to identify a critical region. The use of name\nallows different threads to execute different code while being protected from each other.\nExample 7.16 Using the critical directive for producer-consumer\nthreads\nConsider a producer-consumer scenario in which a producer thread generates a task\nand inserts it", "doc_id": "82e33b3a-0203-4e9a-8a58-f107b8f328dd", "embedding": null, "doc_hash": "e5ca83400be3958bd4bdff78863cbb16e2c42b0120364b75ca332c75f89dad48", "extra_info": null, "node_info": {"start": 916658, "end": 920348}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1a8d7753-df0a-4de6-b540-eb7c703bc8ed", "3": "25634cd6-5273-42b1-a2b1-00da0db4754e"}}, "__type__": "1"}, "25634cd6-5273-42b1-a2b1-00da0db4754e": {"__data__": {"text": "lock\nmanagement ( Section 7.10.5 ), OpenMP provides a critical  directive for implementing critical\nregions. The syntax of a critical  directive is:\n1   #pragma omp critical [(name)] \n2            structured block \nHere, the optional identifier name can be used to identify a critical region. The use of name\nallows different threads to execute different code while being protected from each other.\nExample 7.16 Using the critical directive for producer-consumer\nthreads\nConsider a producer-consumer scenario in which a producer thread generates a task\nand inserts it into a task-queue. The consumer thread extracts tasks from the queue\nand executes them one at a time. Since there is concurrent access to the task-queue,\nthese accesses must be serialized using critical blocks. Specifically, the tasks of\ninserting and extracting from the task-queue must be serialized. This can be\nimplemented as follows:\n1       #pragma omp parallel sections \n2       { \n3           #pragma parallel section \n4           { \n5               /* producer thread */ \n6               task = produce_task(); \n7               #pragma omp critical ( task_queue) \n8               { \n9                   insert_into_queue(task); \n10              } \n11          } \n12          #pragma parallel section \n13          { \n14              /* consumer thread */ \n15              #pragma omp critical ( task_queue) \n16              { \n17                  task = extract_from_queue(task); \n18              } \n19              consume_task(task); \n20          } \n21      } \nNote that queue full and queue empty conditions must be explicitly handled here in\nfunctions insert_into_queue  and extract_from_queue . \nThe critical  directive ensures that at any point in the execution of the program, only one\nthread is within a critical section specified by a certain name. If a thread is already inside a\ncritical section (with a name), all others must wait until it is done before entering the named\ncritical section. The name field is optional. If no name is specified, the critical section maps to a\ndefault name that is the same for all unnamed critical sections. The names of critical sections\nare global across the program.\nIt is easy to see that the critical  directive is a direct application of the corresponding mutex\nfunction in Pthreads. The name field maps to the name of the mutex on which the lock is\nperformed. As is the case with Pthreads, it is important to remember that critical  sections\nrepresent serialization points in the program and therefore we must reduce the size of the\ncritical sections as much as possible (in terms of execution time) to get good performance.\nThere are some obvious safeguards that must be noted while using the critical  directive. The\nblock  of instructions must represent a structured block, i.e., no jumps are permitted into or out\nof the block. It is easy to see that the former would result in non-critical access and the latter in\nan unreleased lock, which could cause the threads to wait indefinitely.\nOften, a critical section consists", "doc_id": "25634cd6-5273-42b1-a2b1-00da0db4754e", "embedding": null, "doc_hash": "e0a1bdc224a365e9d412a3e5a4d9a2d7e11b4c432463853952ae02347102fda3", "extra_info": null, "node_info": {"start": 920374, "end": 923428}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "82e33b3a-0203-4e9a-8a58-f107b8f328dd", "3": "82a91afa-a974-4adc-96bb-f05e71d7ac83"}}, "__type__": "1"}, "82a91afa-a974-4adc-96bb-f05e71d7ac83": {"__data__": {"text": "it is important to remember that critical  sections\nrepresent serialization points in the program and therefore we must reduce the size of the\ncritical sections as much as possible (in terms of execution time) to get good performance.\nThere are some obvious safeguards that must be noted while using the critical  directive. The\nblock  of instructions must represent a structured block, i.e., no jumps are permitted into or out\nof the block. It is easy to see that the former would result in non-critical access and the latter in\nan unreleased lock, which could cause the threads to wait indefinitely.\nOften, a critical section consists simply of an update to a single memory location, for example,\nincrementing or adding to an integer. OpenMP provides another directive, atomic, for such\natomic updates to memory locations. The atomic directive specifies that the memory location\nupdate in the following instruction should be performed as an atomic operation. The update\ninstruction can be one of the following forms:\n1   x binary_operation = expr \n2   x++ \n3   ++x \n4   x--\n5   --x \nHere, expr is a scalar expression that does not include a reference to x, x itself is an lvalue of\nscalar type, and binary_operation is one of {+, *, -, /, &, ,||, \n , \n,}. It is important to\nnote that the atomic directive only atomizes the load and store of the scalar variable. The\nevaluation of the expression is not atomic. Care must be taken to ensure that there are no race\nconditions hidden therein. This also explains why the expr term in the atomic directive cannot\ncontain the updated variable itself. All atomic directives can be replaced by critical  directives\nprovided they have the same name. However, the availability of atomic hardware instructions\nmay optimize the performance of the program, compared to translation to critical  directives.\nIn-Order Execution: The ordered  Directive\nIn many circumstances, it is necessary to execute a segment of a parallel loop in the order in\nwhich the serial version would execute it. For example, consider a for loop in which, at some\npoint, we compute the cumulative sum in array cumul_sum  of a list stored in array list. The\narray cumul_sum  can be computed using a for loop over index i serially by executing\ncumul_sum[i] = cumul_sum[i-1] + list[i] . When executing this for loop across threads, it\nis important to note that cumul_sum[i]  can be computed only after cumul_sum[i-1]  has been\ncomputed. Therefore, the statement would have to executed within an ordered  block.\nThe syntax of the ordered  directive is as follows:\n1   #pragma omp ordered \n2       structured block \nSince the ordered  directive refers to the in-order execution of a for loop, it must be within the\nscope of a for or parallel for  directive. Furthermore, the for or parallel for  directive\nmust have the ordered  clause specified to indicate that the loop contains an ordered  block.\nExample 7.17 Computing the cumulative sum of a list using the\nordered directive\nAs we have just seen, to compute the cumulative sum of i numbers of a list, we can\nadd the current number to the cumulative sum of i-1 numbers of the list. This loop\nmust, however, be executed in order. Furthermore, the cumulative sum of the first\nelement is simply the element itself. We can therefore write the following code\nsegment using the ordered  directive.\n1       cumul_sum[0] = list[0]; \n2       #pragma omp parallel for private (i) \\ \n3                   shared (cumul_sum, list, n) ordered \n4       for (i = 1; i < n; i++) \n5   ", "doc_id": "82a91afa-a974-4adc-96bb-f05e71d7ac83", "embedding": null, "doc_hash": "3395f3cbca4eea1bff5c46b5bd9f83488f2ebe282bb3d78d5ea2dff2f39f6f11", "extra_info": null, "node_info": {"start": 923370, "end": 926899}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "25634cd6-5273-42b1-a2b1-00da0db4754e", "3": "263940b7-26b5-42e9-b0e2-988360a4524e"}}, "__type__": "1"}, "263940b7-26b5-42e9-b0e2-988360a4524e": {"__data__": {"text": "This loop\nmust, however, be executed in order. Furthermore, the cumulative sum of the first\nelement is simply the element itself. We can therefore write the following code\nsegment using the ordered  directive.\n1       cumul_sum[0] = list[0]; \n2       #pragma omp parallel for private (i) \\ \n3                   shared (cumul_sum, list, n) ordered \n4       for (i = 1; i < n; i++) \n5       { \n6           /* other processing on list[i] if needed */ \n7 \n8           #pragma omp ordered \n9           { \n10              cumul_sum[i] = cumul_sum[i-1] + list[i]; \n11          } \n12      } \nIt is important to note that the ordered  directive represents an ordered serialization point in\nthe program. Only a single thread can enter an ordered block when all prior threads (as\ndetermined by loop indices) have exited. Therefore, if large portions of a loop are enclosed in\nordered  directives, corresponding speedups suffer. In the above example, the parallel\nformulation is expected to be no faster than the serial formulation unless there is significant\nprocessing associated with list[i]  outside the ordered  directive. A single for directive is\nconstrained to have only one ordered  block in it.\nMemory Consistency: The flush  Directive\nThe flush  directive provides a mechanism for making memory consistent across threads. While\nit would appear that such a directive is superfluous for shared address space machines, it is\nimportant to note that variables may often be assigned to registers and register-allocated\nvariables may be inconsistent. In such cases, the flush  directive provides a memory fence by\nforcing a variable to be written to or read from the memory system. All write operations to\nshared variables must be committed to memory at a flush and all references to shared variables\nafter a fence must be satisfied from the memory. Since private variables are relevant only to a\nsingle thread, the flush  directive applies only to shared variables.\nThe syntax of the flush  directive is as follows:\n1   #pragma omp flush[(list)] \nThe optional list specifies the variables that need to be flushed. The default is that all shared\nvariables are flushed.\nSeveral OpenMP directives have an implicit flush . Specifically, a flush  is implied at a barrier ,\nat the entry and exit of critical , ordered , parallel , parallel  for, and parallel sections\nblocks and at the exit of for, sections , and single blocks. A flush  is not implied if a nowait\nclause is present. It is also not implied at the entry of for, sections , and single blocks and at\nentry or exit of a master block.\n7.10.4 Data Handling in OpenMP\nOne of the critical factors influencing program performance is the manipulation of data by\nthreads. We have briefly discussed OpenMP support for various data classes such as private ,\nshared, firstprivate , and lastprivate. We now examine these in greater detail, with a view\nto understanding how these classes should be used. We identify the following heuristics to guide\nthe process:\nIf a thread initializes and uses a variable (such as loop indices) and no other thread\naccesses the data, then a local copy of the variable should be made for the thread. Such\ndata should be specified as private .\nIf a thread repeatedly reads a variable that has been initialized earlier in the program, it is\nbeneficial to make a copy of the variable and inherit the value at the time of thread\ncreation. This way, when a thread is scheduled on the processor,", "doc_id": "263940b7-26b5-42e9-b0e2-988360a4524e", "embedding": null, "doc_hash": "bed2ffa013133b032a8502a43b2f1a36b3bbb57d72e37a8ebe5b4761b995a227", "extra_info": null, "node_info": {"start": 927147, "end": 930608}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "82a91afa-a974-4adc-96bb-f05e71d7ac83", "3": "64924814-4dca-4b66-9b0e-8a65a5598cd9"}}, "__type__": "1"}, "64924814-4dca-4b66-9b0e-8a65a5598cd9": {"__data__": {"text": "We now examine these in greater detail, with a view\nto understanding how these classes should be used. We identify the following heuristics to guide\nthe process:\nIf a thread initializes and uses a variable (such as loop indices) and no other thread\naccesses the data, then a local copy of the variable should be made for the thread. Such\ndata should be specified as private .\nIf a thread repeatedly reads a variable that has been initialized earlier in the program, it is\nbeneficial to make a copy of the variable and inherit the value at the time of thread\ncreation. This way, when a thread is scheduled on the processor, the data can reside at the\nsame processor (in its cache if possible) and accesses will not result in interprocessor\ncommunication. Such data should be specified as firstprivate .\nIf multiple threads manipulate a single piece of data, one must explore ways of breaking\nthese manipulations into local operations followed by a single global operation. For\nexample, if multiple threads keep a count of a certain event, it is beneficial to keep local\ncounts and to subsequently accrue it using a single summation at the end of the parallel\nblock. Such operations are supported by the reduction  clause.\nIf multiple threads manipulate different parts of a large data structure, the programmer\nshould explore ways of breaking it into smaller data structures and making them private to\nthe thread manipulating them.\nAfter all the above techniques have been explored and exhausted, remaining data items\nmay be shared among various threads using the clause shared.\nIn addition to private , shared, firstprivate , and lastprivate, OpenMP supports one\nadditional data class called threadprivate .\nThe threadprivate  and copyin Directives  Often, it is useful to make a set of objects locally\navailable to a thread in such a way that these objects persist through parallel and serial blocks\nprovided the number of threads remains the same. In contrast to private  variables, these\nvariables are useful for maintaining persistent objects across parallel regions, which would\notherwise have to be copied into the master thread's data space and reinitialized at the next\nparallel block. This class of variables is supported in OpenMP using the threadprivate\ndirective. The syntax of the directive is as follows:\n1   #pragma omp threadprivate(variable_list) \nThis directive implies that all variables in variable_list  are local to each thread and are\ninitialized once before they are accessed in a parallel region. Furthermore, these variables\npersist across different parallel regions provided dynamic adjustment of the number of threads\nis disabled and the number of threads is the same.\nSimilar to firstprivate , OpenMP provides a mechanism for assigning the same value to\nthreadprivate  variables across all threads in a parallel region. The syntax of the clause, which\ncan be used with parallel  directives, is copyin(variable_list).\n7.10.5 OpenMP Library Functions\nIn addition to directives, OpenMP also supports a number of functions that allow a programmer\nto control the execution of threaded programs. As we shall notice, these functions are similar to\ncorresponding Pthreads functions; however, they are generally at a higher level of abstraction,\nmaking them easier to use.\nControlling Number of Threads and Processors\nThe following OpenMP functions relate to the concurrency and number of processors used by a\nthreaded program:\n1   #include <omp.h> \n2 \n3   void omp_set_num_threads (int num_threads); \n4   int omp_get_num_threads (); \n5   int omp_get_max_threads (); \n6   int omp_get_thread_num (); \n7   int omp_get_num_procs (); \n8   int omp_in_parallel(); \nThe function omp_set_num_threads  sets the default number of threads that will be created on\nencountering the next parallel  directive provided the num_threads clause is", "doc_id": "64924814-4dca-4b66-9b0e-8a65a5598cd9", "embedding": null, "doc_hash": "5da4d88deba2fb1f1907ee34e64a8927cbaf04a6f6924576d9eb1b5be25801e6", "extra_info": null, "node_info": {"start": 930379, "end": 934227}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "263940b7-26b5-42e9-b0e2-988360a4524e", "3": "6b758866-7a1c-414a-9e1d-81ca3c7452fa"}}, "__type__": "1"}, "6b758866-7a1c-414a-9e1d-81ca3c7452fa": {"__data__": {"text": "a\nthreaded program:\n1   #include <omp.h> \n2 \n3   void omp_set_num_threads (int num_threads); \n4   int omp_get_num_threads (); \n5   int omp_get_max_threads (); \n6   int omp_get_thread_num (); \n7   int omp_get_num_procs (); \n8   int omp_in_parallel(); \nThe function omp_set_num_threads  sets the default number of threads that will be created on\nencountering the next parallel  directive provided the num_threads clause is not used in the\nparallel  directive. This function must be called outsid the scope of a parallel region and\ndynamic adjustment of threads must be enabled (using either the OMP_DYNAMIC environment\nvariable discussed in Section 7.10.6  or the omp_set_dynamic  library function).\nThe omp_get_num_threads  function returns the number of threads participating in a team. It\nbinds to the closest parallel directive and in the absence of a parallel directive, returns 1 (for\nmaster thread). The omp_get_max_threads  function returns the maximum number of threads\nthat could possibly be created by a parallel  directive encountered, which does not have a\nnum_threads clause. The omp_get_thread_num  returns a unique thread i.d. for each thread in a\nteam. This integer lies between 0 (for the master thread) and omp get_num_threads() -1 . The\nomp_get_num_procs  function returns the number of processors that are available to execute the\nthreaded program at that point. Finally, the function omp_in_parallel  returns a non-zero value\nif called from within the scope of a parallel region, and zero otherwise.\nControlling and Monitoring Thread Creation\nThe following OpenMP functions allow a programmer to set and monitor thread creation:\n1   #include <omp.h> \n2 \n3   void omp_set_dynamic (int dynamic_threads); \n4   int omp_get_dynamic (); \n5   void omp_set_nested (int nested); \n6   int omp_get_nested (); \nThe omp_set_dynamic  function allows the programmer to dynamically alter the number of\nthreads created on encountering a parallel region. If the value dynamic_threads  evaluates to\nzero, dynamic adjustment is disabled, otherwise it is enabled. The function must be called\noutside the scope of a parallel region. The corresponding state, i.e., whether dynamic\nadjustment is enabled or disabled, can be queried using the function omp_get_dynamic , which\nreturns a non-zero value if dynamic adjustment is enabled, and zero otherwise.\nThe omp_set_nested  enables nested parallelism if the value of its argument, nested, is non-\nzero, and disables it otherwise. When nested parallelism is disabled, any nested parallel regions\nsubsequently encountered are serialized. The state of nested parallelism can be queried using\nthe omp_get_nested  function, which returns a non-zero value if nested parallelism is enabled,\nand zero otherwise.\nMutual Exclusion\nWhile OpenMP provides support for critical sections and atomic updates, there are situations\nwhere it is more convenient to use an explicit lock. For such programs, OpenMP provides\nfunctions for initializing, locking, unlocking, and discarding locks. The lock data structure in\nOpenMP is of type omp_lock_t . The following functions are defined:\n1   #include <omp.h> \n2 \n3   void omp_init_lock (omp_lock_t *lock); \n4   void omp_destroy_lock (omp_lock_t *lock); \n5   void omp_set_lock (omp_lock_t *lock); \n6   void omp_unset_lock (omp_lock_t", "doc_id": "6b758866-7a1c-414a-9e1d-81ca3c7452fa", "embedding": null, "doc_hash": "561db7486eb834d2097eed1b3eb7242d1f52090f9ad20f02a48707b4f84db974", "extra_info": null, "node_info": {"start": 934394, "end": 937701}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "64924814-4dca-4b66-9b0e-8a65a5598cd9", "3": "826606ee-124b-48c5-98cd-a0eed855df04"}}, "__type__": "1"}, "826606ee-124b-48c5-98cd-a0eed855df04": {"__data__": {"text": "is more convenient to use an explicit lock. For such programs, OpenMP provides\nfunctions for initializing, locking, unlocking, and discarding locks. The lock data structure in\nOpenMP is of type omp_lock_t . The following functions are defined:\n1   #include <omp.h> \n2 \n3   void omp_init_lock (omp_lock_t *lock); \n4   void omp_destroy_lock (omp_lock_t *lock); \n5   void omp_set_lock (omp_lock_t *lock); \n6   void omp_unset_lock (omp_lock_t *lock); \n7   int omp_test_lock (omp_lock_t *lock); \nBefore a lock can be used, it must be initialized. This is done using the omp_init_lock  function.\nWhen a lock is no longer needed, it must be discarded using the function omp_destroy_lock. It\nis illegal to initialize a previously initialized lock and destroy an uninitialized lock. Once a lock\nhas been initialized, it can be locked and unlocked using the functions omp_set_lock  and\nomp_unset_lock . On locking a previously unlocked lock, a thread gets exclusive access to the\nlock. All other threads must wait on this lock when they attempt an omp_set_lock . Only a\nthread owning a lock can unlock it. The result of a thread attempting to unlock a lock owned by\nanother thread is undefined. Both of these operations are illegal prior to initialization or after\nthe destruction of a lock. The function omp_test_lock  can be used to attempt to set a lock. If\nthe function returns a non-zero value, the lock has been successfully set, otherwise the lock is\ncurrently owned by another thread.\nSimilar to recursive mutexes in Pthreads, OpenMP also supports nestable locks that can be\nlocked multiple times by the same thread. The lock object in this case is omp_nest_lock_t  and\nthe corresponding functions for handling a nested lock are:\n1   #include <omp.h> \n2 \n3   void omp_init_nest_lock (omp_nest_lock_t *lock); \n4   void omp_destroy_nest_lock (omp_nest_lock_t *lock); \n5   void omp_set_nest_lock (omp_nest_lock_t *lock); \n6   void omp_unset_nest_lock (omp_nest_lock_t *lock); \n7   int omp_test_nest_lock (omp_nest_lock_t *lock); \nThe semantics of these functions are similar to corresponding functions for simple locks. Notice\nthat all of these functions have directly corresponding mutex calls in Pthreads.\n7.10.6 Environment Variables in OpenMP\nOpenMP provides additional environment variables that help control execution of parallel\nprograms. These environment variables include the following.\nOMP_NUM_THREADS  This environment variable specifies the default number of threads created\nupon entering a parallel  region. The number of threads can be changed using either the\nomp_set_num_threads  function or the num_threads clause in the parallel  directive. Note that\nthe number of threads can be changed dynamically only if the variable OMP_SET_DYNAMIC  is set\nto TRUE or if the function omp_set_dynamic  has been called with a non-zero argument. For\nexample, the following command, when typed into csh prior to execution of the program, sets\nthe default number of threads to 8.\n1   setenv OMP_NUM_THREADS 8 \nOMP_DYNAMIC This variable, when set to TRUE, allows the number of threads to be controlled at\nruntime using the omp_set_num threads  function or the num_threads clause. Dynamic control\nof number of threads can be disabled by calling the omp_set_dynamic  function with a zero\nargument.\nOMP_NESTED  This", "doc_id": "826606ee-124b-48c5-98cd-a0eed855df04", "embedding": null, "doc_hash": "592a88261b8a7197146fddf3da827828dac41a15d655bb0f3e0055b94104d40e", "extra_info": null, "node_info": {"start": 937683, "end": 940991}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6b758866-7a1c-414a-9e1d-81ca3c7452fa", "3": "a3cf7615-d6db-4a16-9115-6b94a573de65"}}, "__type__": "1"}, "a3cf7615-d6db-4a16-9115-6b94a573de65": {"__data__": {"text": " has been called with a non-zero argument. For\nexample, the following command, when typed into csh prior to execution of the program, sets\nthe default number of threads to 8.\n1   setenv OMP_NUM_THREADS 8 \nOMP_DYNAMIC This variable, when set to TRUE, allows the number of threads to be controlled at\nruntime using the omp_set_num threads  function or the num_threads clause. Dynamic control\nof number of threads can be disabled by calling the omp_set_dynamic  function with a zero\nargument.\nOMP_NESTED  This variable, when set to TRUE, enables nested parallelism, unless it is disabled by\ncalling the omp_set_nested  function with a zero argument.\nOMP_SCHEDULE  This environment variable controls the assignment of iteration spaces associated\nwith for directives that use the runtime  scheduling class. The variable can take values static,\ndynamic , and guided along with optional chunk size. For example, the following assignment:\n1   setenv OMP_SCHEDULE \"static,4\" \nspecifies that by default, all for directives use static scheduling with a chunk size of 4. Other\nexamples of assignments include:\n1   setenv OMP_SCHEDULE \"dynamic\" \n2   setenv OMP_SCHEDULE \"guided\" \nIn each of these cases, a default chunk size of 1 is used.\n7.10.7 Explicit Threads versus OpenMP Based Programming\nOpenMP provides a layer on top of native threads to facilitate a variety of thread-related tasks.\nUsing directives provided by OpenMP, a programmer is rid of the tasks of initializing attributes\nobjects, setting up arguments to threads, partitioning iteration spaces, etc. This convenience is\nespecially useful when the underlying problem has a static and/or regular task graph. The\noverheads associated with automated generation of threaded code from directives have been\nshown to be minimal in the context of a variety of applications.\nHowever, there are some drawbacks to using directives as well. An artifact of explicit threading\nis that data exchange is more apparent. This helps in alleviating some of the overheads from\ndata movement, false sharing, and contention. Explicit threading also provides a richer API in\nthe form of condition waits, locks of different types, and increased flexibility for building\ncomposite synchronization operations as illustrated in Section 7.8. Finally, since explicit\nthreading is used more widely than OpenMP, tools and support for Pthreads programs is easier\nto find.\nA programmer must weigh all these considerations before deciding on an API for programming.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n7.11 Bibliographic Remarks\nA number of excellent references exist for both explicit thread-based and OpenMP-based\nprogramming. Lewis and Berg [ LB97, LB95a ] provide a detailed guide to programming with\nPthreads. Kleiman, Shah, and Smaalders [ KSS95 ] provide an excellent description of thread\nsystems as well as programming using threads. Several other books have also addressed\nprogramming and system software issues related to multithreaded programming [ NBF96,\nBut97 , Gal95 , Lew91 , RRRR96 , ND96 ].\nMany other thread APIs and systems have also been developed and are commonly used in a\nvariety of applications. These include Java threads [ Dra96, MK99 , Hyd99 , Lea99 ], Microsoft\nthread APIs [ PG98 , CWP98 , Wil00 , BW97 ], and the Solaris threads API [ KSS95 , Sun95 ]. Thread\nsystems have a long and rich history of research dating back to the days of the HEP Denelcor\n[HLM84] from both the software as well as the hardware viewpoints. More recently, software\nsystems such as Cilk [ BJK+95, LRZ95 ],", "doc_id": "a3cf7615-d6db-4a16-9115-6b94a573de65", "embedding": null, "doc_hash": "29386cce54aad87207e89f3ae734f395d8f6aaff6f075835f0ba337663fb9db2", "extra_info": null, "node_info": {"start": 940936, "end": 944465}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "826606ee-124b-48c5-98cd-a0eed855df04", "3": "ff76814b-288c-4a74-af5b-9c0e292e16ee"}}, "__type__": "1"}, "ff76814b-288c-4a74-af5b-9c0e292e16ee": {"__data__": {"text": "].\nMany other thread APIs and systems have also been developed and are commonly used in a\nvariety of applications. These include Java threads [ Dra96, MK99 , Hyd99 , Lea99 ], Microsoft\nthread APIs [ PG98 , CWP98 , Wil00 , BW97 ], and the Solaris threads API [ KSS95 , Sun95 ]. Thread\nsystems have a long and rich history of research dating back to the days of the HEP Denelcor\n[HLM84] from both the software as well as the hardware viewpoints. More recently, software\nsystems such as Cilk [ BJK+95, LRZ95 ], OxfordBSP [ HDM97 ], Active Threads [ Wei97 ], and Earth\nManna [ HMT+96] have been developed. Hardware support for multithreading has been explored\nin the Tera computer system [ RS90a ], MIT Alewife [ ADJ+91], Horizon [ KS88 ], simultaneous\nmultithreading [ TEL95 , Tul96 ], multiscalar architecture [ Fra93 ], and superthreaded architecture\n[TY96], among others.\nThe performance aspects of threads have also been explored. Early work on the performance\ntradeoffs of multithreaded processors was reported in [ Aga89, SBCV90 , Aga91 , CGL92 , LB95b ].\nConsistency models for shared memory have been extensively studied. Other areas of active\nresearch include runtime systems, compiler support, object-based extensions, performance\nevaluation, and software development tools. There have also been efforts aimed at supporting\nsoftware shared memory across networks of workstations. All of these are only tangentially\nrelated to the issue of programming using threads.\nDue to its relative youth, relatively few texts exist for programming in OpenMP [ CDK+00]. The\nOpenMP standard and an extensive set of resources is available at http://www.openmp.org . A\nnumber of other articles (and special issues) have addressed issues relating to OpenMP\nperformance, compilation, and interoperability [ Bra97 , CM98 , DM98 , LHZ98 , Thr99 ].\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n7.1 Estimate the time taken for each of the following in Pthreads:\nThread creation.\nThread join.\nSuccessful lock.\nSuccessful unlock.\nSuccessful trylock.\nUnsuccessful trylock.\nCondition wait.\nCondition signal.\nCondition broadcast.\nIn each case, carefully document the method used to compute the time for each of these\nfunction calls. Also document the machine on which these observations were made.\n7.2 Implement a multi-access threaded queue with multiple threads inserting and multiple\nthreads extracting from the queue. Use mutex-locks to synchronize access to the queue.\nDocument the time for 1000 insertions and 1000 extractions each by 64 insertion threads\n(producers) and 64 extraction threads (consumers).\n7.3 Repeat Problem 7.2 using condition variables (in addition to mutex locks). Document\nthe time for the same test case as above. Comment on the difference in the times.\n7.4 A simple streaming media player consists of a thread monitoring a network port for\narriving data, a decompressor thread for decompressing packets and generating frames in\na video sequence, and a rendering thread that displays frames at programmed intervals.\nThe three threads must communicate via shared buffers \u2013 an in-buffer between the\nnetwork and decompressor, and an out-buffer between the decompressor and renderer.\nImplement this simple threaded framework. The network thread calls a dummy function\nlisten_to_port  to gather data from the network. For the sake of this program, this\nfunction generates a random string of bytes of desired length. The decompressor thread\ncalls function decompress , which takes in data from the in-buffer and returns a frame of\npredetermined size. For this exercise, generate a frame with random bytes. Finally the\nrender thread picks frames from the out buffer and calls the", "doc_id": "ff76814b-288c-4a74-af5b-9c0e292e16ee", "embedding": null, "doc_hash": "739c30ac907d69541404a2f1cc8844f1ef642267b0518d16c51e97b2fc612d85", "extra_info": null, "node_info": {"start": 944475, "end": 948153}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a3cf7615-d6db-4a16-9115-6b94a573de65", "3": "8a052764-3b55-4a26-b35d-94bf04549b35"}}, "__type__": "1"}, "8a052764-3b55-4a26-b35d-94bf04549b35": {"__data__": {"text": "buffers \u2013 an in-buffer between the\nnetwork and decompressor, and an out-buffer between the decompressor and renderer.\nImplement this simple threaded framework. The network thread calls a dummy function\nlisten_to_port  to gather data from the network. For the sake of this program, this\nfunction generates a random string of bytes of desired length. The decompressor thread\ncalls function decompress , which takes in data from the in-buffer and returns a frame of\npredetermined size. For this exercise, generate a frame with random bytes. Finally the\nrender thread picks frames from the out buffer and calls the display function. This function\ntakes a frame as an argument, and for this exercise, it does nothing. Implement this\nthreaded framework using condition variables. Note that you can easily change the three\ndummy functions to make a meaningful streaming media decompressor.\n7.5 Illustrate the use of recursive locks using a binary tree search algorithm. The program\ntakes in a large list of numbers. The list is divided across multiple threads. Each thread\ntries to insert its elements into the tree by using a single lock associated with the tree.\nShow that the single lock becomes a bottleneck even for a moderate number of threads.\n7.6 Improve the binary tree search program by associating a lock with each node in the\ntree (as opposed to a single lock with the entire tree). A thread locks a node when it reads\nor writes it. Examine the performance properties of this implementation.\n7.7 Improve the binary tree search program further by using read-write locks. A thread\nread-locks a node before reading. It write-locks a node only when it needs to write into the\ntree node. Implement the program and document the range of program parameters where\nread-write locks actually yield performance improvements over regular locks.\n7.8 Implement a threaded hash table in which collisions are resolved by chaining.\nImplement the hash table so that there is a single lock associated with a block of k hash-\ntable entries. Threads attempting to read/write an element in a block must first lock the\ncorresponding block. Examine the performance of your implementation as a function of k.\n7.9 Change the locks to read-write locks in the hash table and use write locks only when\ninserting an entry into the linked list. Examine the performance of this program as a\nfunction of k. Compare the performance to that obtained using regular locks.\n7.10 Write a threaded program for computing the Sieve of Eratosthenes. Think through\nthe threading strategy carefully before implementing it. It is important to realize, for\ninstance, that you cannot eliminate multiples of 6 from the sieve until you have eliminated\nmultiples of 3 (at which point you would realize that you did not need to eliminate\nmultiples of 6 in the first place). A pipelined (assembly line) strategy with the current\nsmallest element forming the next station in the assembly line is one way to think about\nthe problem.\n7.11 Write a threaded program for solving a 15-puzzle. The program takes an initial\nposition and keeps an open list of outstanding positions. This list is sorted on the\n\"goodness\" measure of the boards. A simple goodness measure is the Manhattan distance\n(i.e., the sum of x -displacement and y-displacement of every tile from where it needs to\nbe). This open list is a work queue implemented as a heap. Each thread extracts work (a\nboard) from the work queue, expands it to all possible successors, and inserts the\nsuccessors into the work queue if it has not already been encountered. Use a hash table\n(from Problem 7.9) to keep track of entries that have been previously encountered. Plot\nthe speedup of your program with the number of threads. You can compute the speedups\nfor some reference board that is the same for various thread counts.\n7.12 Modify the above program so that you now have multiple open lists (say k). Now\neach thread picks a random open list", "doc_id": "8a052764-3b55-4a26-b35d-94bf04549b35", "embedding": null, "doc_hash": "a8a714e2ea1f9129bf2e494ae40d030b4811315ffdcb1c2675f36bb61a5ae2cb", "extra_info": null, "node_info": {"start": 948052, "end": 952001}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ff76814b-288c-4a74-af5b-9c0e292e16ee", "3": "ab99e35d-f964-4225-aa34-6c86154eb4c9"}}, "__type__": "1"}, "ab99e35d-f964-4225-aa34-6c86154eb4c9": {"__data__": {"text": "as a heap. Each thread extracts work (a\nboard) from the work queue, expands it to all possible successors, and inserts the\nsuccessors into the work queue if it has not already been encountered. Use a hash table\n(from Problem 7.9) to keep track of entries that have been previously encountered. Plot\nthe speedup of your program with the number of threads. You can compute the speedups\nfor some reference board that is the same for various thread counts.\n7.12 Modify the above program so that you now have multiple open lists (say k). Now\neach thread picks a random open list and tries to pick a board from the random list and\nexpands and inserts it back into another, randomly selected list. Plot the speedup of your\nprogram with the number of threads. Compare your performance with the previous case.\nMake sure you use your locks and trylocks carefully to minimize serialization overheads.\n7.13 Implement and test the OpenMP program for computing a matrix-matrix product in\nExample 7.14. Use the OMP_NUM_THREADS environment variable to control the number\nof threads and plot the performance with varying numbers of threads. Consider three\ncases in which (i) only the outermost loop is parallelized; (ii) the outer two loops are\nparallelized; and (iii) all three loops are parallelized. What is the observed result from\nthese three cases?\n7.14 Consider a simple loop that calls a function dummy  containing a programmable delay.\nAll invocations of the function are independent of the others. Partition this loop across four\nthreads using static, dynamic , and guided scheduling. Use different parameters for static\nand guided scheduling. Document the result of this experiment as the delay within the\ndummy  function becomes large.\n7.15 Consider a sparse matrix stored in the compressed row format (you may find a\ndescription of this format on the web or any suitable text on sparse linear algebra). Write\nan OpenMP program for computing the product of this matrix with a vector. Download\nsample matrices from the Matrix Market ( http://math.nist.gov/MatrixMarket/) and test the\nperformance of your implementation as a function of matrix size and number of threads.\n7.16 Implement a producer-consumer framework in OpenMP using sections  to create a\nsingle producer  task and a single consumer  task. Ensure appropriate synchronization\nusing locks. Test your program for a varying number of producers and consumers.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 8. Dense Matrix Algorithms\nAlgorithms involving matrices and vectors are applied in several numerical and non-numerical\ncontexts. This chapter discusses some key algorithms for dense  or full matrices  that have no\nor few known usable zero entries. We deal specifically with square matrices for pedagogical\nreasons, but the algorithms in this chapter, wherever applicable, can easily be adapted for\nrectangular matrices as well.\nDue to their regular structure, parallel computations involving matrices and vectors readily lend\nthemselves to data-decomposition ( Section 3.2.2). Depending on the computation at hand, the\ndecomposition may be induced by partitioning the input, the output, or the intermediate data.\nSection 3.4.1 describes in detail the various schemes of partitioning matrices for parallel\ncomputation. The algorithms discussed in this chapter use one- and two-dimensional block,\ncyclic, and block-cyclic partitionings. For the sake of brevity, we will henceforth refer to one-\nand two-dimensional partitionings as 1-D and 2-D partitionings, respectively.\nAnother characteristic of most of the algorithms described in this chapter is that they use one\ntask per process. As a result of a one-to-one mapping of tasks to processes, we do not usually\nrefer to the tasks explicitly and", "doc_id": "ab99e35d-f964-4225-aa34-6c86154eb4c9", "embedding": null, "doc_hash": "b532ec51f09a9f4903a03bec2137f6fc9a36a175bc27dd8ef538a5fbc438bd24", "extra_info": null, "node_info": {"start": 952044, "end": 955795}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8a052764-3b55-4a26-b35d-94bf04549b35", "3": "e2686c30-65db-48e5-930a-2836cbbdc129"}}, "__type__": "1"}, "e2686c30-65db-48e5-930a-2836cbbdc129": {"__data__": {"text": "various schemes of partitioning matrices for parallel\ncomputation. The algorithms discussed in this chapter use one- and two-dimensional block,\ncyclic, and block-cyclic partitionings. For the sake of brevity, we will henceforth refer to one-\nand two-dimensional partitionings as 1-D and 2-D partitionings, respectively.\nAnother characteristic of most of the algorithms described in this chapter is that they use one\ntask per process. As a result of a one-to-one mapping of tasks to processes, we do not usually\nrefer to the tasks explicitly and decompose or partition the problem directly into processes.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n8.1 Matrix-Vector Multiplication\nThis section addresses the problem of multiplying a dense n x n matrix A with an n x 1 vector x\nto yield the n x 1 result vector y. Algorithm 8.1  shows a serial algorithm for this problem. The\nsequential algorithm requires n2 multiplications and additions. Assuming that a multiplication\nand addition pair takes unit time, the sequential run time is\nEquation 8.1\nAt least three distinct parallel formulations of matrix-vector multiplication are possible,\ndepending on whether rowwise 1-D, columnwise 1-D, or a 2-D partitioning is used.\nAlgorithm 8.1 A serial algorithm for multiplying an n x n matrix A with\nan n x 1 vector x to yield an n x 1 product vector y.\n1.   procedure  MAT_VECT ( A, x, y) \n2.   begin \n3.      for i := 0 to n - 1 do \n4.      begin \n5.         y[i]:=0; \n6.         for j := 0 to n - 1 do \n7.             y[i] := y[i] + A[i, j] x x[j]; \n8.      endfor; \n9.   end MAT_VECT \n8.1.1 Rowwise 1-D Partitioning\nThis section details the parallel algorithm for matrix-vector multiplication using rowwise block\n1-D partitioning. The parallel algorithm for columnwise block 1-D partitioning is similar\n(Problem 8.2) and has a similar expression for parallel run time. Figure 8.1 describes the\ndistribution and movement of data for matrix-vector multiplication with block 1-D partitioning.\nFigure 8.1. Multiplication of an n x n matrix with an n x 1 vector using\nrowwise block 1-D partitioning. For the one-row-per-process case, p =\nn.\nOne Row Per Process\nFirst, consider the case in which the n x n matrix is partitioned among n processes so that each\nprocess stores one complete row of the matrix. The n x 1 vector x is distributed such that each\nprocess owns one of its elements. The initial distribution of the matrix and the vector for\nrowwise block 1-D partitioning is shown in Figure 8.1(a) . Process P i initially owns x[i] and A[i,\n0], A[i, 1], ..., A[i, n-1] and is responsible for computing y[i]. Vector x is multiplied with each\nrow of the matrix ( Algorithm 8.1 ); hence, every process needs the entire vector. Since each\nprocess starts with only one element of x, an all-to-all broadcast is required to distribute all the\nelements to all the processes. Figure 8.1(b)  illustrates this communication step. After the vector\nx is distributed among the processes ( Figure 8.1(c) ), process P i computes\n (lines 6 and 7 of Algorithm 8.1 ). As Figure 8.1(d)  shows, the result\nvector y is stored exactly the way the starting vector x", "doc_id": "e2686c30-65db-48e5-930a-2836cbbdc129", "embedding": null, "doc_hash": "dc8451144824908505ee1b3a4e79d735468b3534e865f4f40973933b025dd6d9", "extra_info": null, "node_info": {"start": 955807, "end": 958932}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ab99e35d-f964-4225-aa34-6c86154eb4c9", "3": "8b8d12c5-09d2-493a-baba-5fe97928fc6b"}}, "__type__": "1"}, "8b8d12c5-09d2-493a-baba-5fe97928fc6b": {"__data__": {"text": "Vector x is multiplied with each\nrow of the matrix ( Algorithm 8.1 ); hence, every process needs the entire vector. Since each\nprocess starts with only one element of x, an all-to-all broadcast is required to distribute all the\nelements to all the processes. Figure 8.1(b)  illustrates this communication step. After the vector\nx is distributed among the processes ( Figure 8.1(c) ), process P i computes\n (lines 6 and 7 of Algorithm 8.1 ). As Figure 8.1(d)  shows, the result\nvector y is stored exactly the way the starting vector x was stored.\nParallel Run Time  Starting with one vector element per process, the all-to-all broadcast of the\nvector elements among n processes requires time Q(n) on any architecture ( Table 4.1 ). The\nmultiplication of a single row of A with x is also performed by each process in time Q(n). Thus,\nthe entire procedure is completed by n processes in time Q(n), resulting in a process-time\nproduct of Q(n2). The parallel algorithm is cost-optimal because the complexity of the serial\nalgorithm is Q(n2).\nUsing Fewer than n Processes\nConsider the case in which p processes are used such that p < n, and the matrix is partitioned\namong the processes by using block 1-D partitioning. Each process initially stores n/p complete\nrows of the matrix and a portion of the vector of size n/p. Since the vector x must be multiplied\nwith each row of the matrix, every process needs the entire vector (that is, all the portions\nbelonging to separate processes). This again requires an all-to-all broadcast as shown in Figure\n8.1(b)  and (c). The all-to-all broadcast takes place among p processes and involves messages\nof size n/p. After this communication step, each process multiplies its n/p rows with the vector x\nto produce n/p elements of the result vector. Figure 8.1(d)  shows that the result vector y is\ndistributed in the same format as that of the starting vector x.\nParallel Run Time  According to Table 4.1 , an all-to-all broadcast of messages of size n/p\namong p processes takes time ts log p + tw(n/ p)( p - 1). For large p, this can be approximated\nby ts log p + twn. After the communication, each process spends time n2/p multiplying its n/p\nrows with the vector. Thus, the parallel run time of this procedure is\nEquation 8.2\nThe process-time product for this parallel formulation is n2 + ts p log p + twnp. The algorithm is\ncost-optimal for p = O(n).\nScalability Analysis  We now derive the isoefficiency function for matrix-vector multiplication\nalong the lines of the analysis in Section 5.4.2  by considering the terms of the overhead\nfunction one at a time. Consider the parallel run time given by Equation 8.2  for the hypercube\narchitecture. The relation To = pTP - W gives the following expression for the overhead function\nof matrix-vector multiplication on a hypercube with block 1-D partitioning:\nEquation 8.3\nRecall from Chapter 5  that the central relation that determines the isoefficiency function of a\nparallel algorithm is W = KTo (Equation 5.14 ), where K = E/(1 - E) and E is the desired\nefficiency. Rewriting this relation for matrix-vector multiplication, first with only the ts term of\nTo,\nEquation 8.4\nEquation 8.4  gives the isoefficiency term with respect to message startup time. Similarly, for the\ntw term of the overhead function,\nSince W = n2 (Equation 8.1 ), we derive an expression for W in terms of p, K , and tw (that is,\nthe isoefficiency function due to tw) as follows:\nEquation", "doc_id": "8b8d12c5-09d2-493a-baba-5fe97928fc6b", "embedding": null, "doc_hash": "814fbc5719c0a382cfc6c7129414248551a350924cf0b9f914e4227ddc04890e", "extra_info": null, "node_info": {"start": 958957, "end": 962409}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e2686c30-65db-48e5-930a-2836cbbdc129", "3": "037f7d86-a40f-43ae-94c8-5f5743fe94d6"}}, "__type__": "1"}, "037f7d86-a40f-43ae-94c8-5f5743fe94d6": {"__data__": {"text": "a\nparallel algorithm is W = KTo (Equation 5.14 ), where K = E/(1 - E) and E is the desired\nefficiency. Rewriting this relation for matrix-vector multiplication, first with only the ts term of\nTo,\nEquation 8.4\nEquation 8.4  gives the isoefficiency term with respect to message startup time. Similarly, for the\ntw term of the overhead function,\nSince W = n2 (Equation 8.1 ), we derive an expression for W in terms of p, K , and tw (that is,\nthe isoefficiency function due to tw) as follows:\nEquation 8.5\nNow consider the degree of concurrency of this parallel algorithm. Using 1-D partitioning, a\nmaximum of n processes can be used to multiply an n x n matrix with an n x 1 vector. In other\nwords, p is O(n), which yields the following condition:\nEquation 8.6\nThe overall asymptotic isoefficiency function can be determined by comparing Equations 8.4,\n8.5, and 8.6. Among the three, Equations 8.5 and 8.6 give the highest asymptotic rate at which\nthe problem size must increase with the number of processes to maintain a fixed efficiency. This\nrate of Q(p2) is the asymptotic isoefficiency function of the parallel matrix-vector multiplication\nalgorithm with 1-D partitioning.\n8.1.2 2-D Partitioning\nThis section discusses parallel matrix-vector multiplication for the case in which the matrix is\ndistributed among the processes using a block 2-D partitioning. Figure 8.2 shows the\ndistribution of the matrix and the distribution and movement of vectors among the processes.\nFigure 8.2. Matrix-vector multiplication with block 2-D partitioning.\nFor the one-element-per-process case, p = n2 if the matrix size is n x\nn.\nOne Element Per Process\nWe start with the simple case in which an n x n matrix is partitioned among n2 processes such\nthat each process owns a single element. The n x 1 vector x is distributed only in the last\ncolumn of n processes, each of which owns one element of the vector. Since the algorithm\nmultiplies the elements of the vector x with the corresponding elements in each row of the\nmatrix, the vector must be distributed such that the ith element of the vector is available to the\nith element of each row of the matrix. The communication steps for this are shown in Figure\n8.2(a)  and (b). Notice the similarity of Figure 8.2  to Figure 8.1 . Before the multiplication, the\nelements of the matrix and the vector must be in the same relative locations as in Figure 8.1(c) .\nHowever, the vector communication steps differ between various partitioning strategies. With 1-\nD partitioning, the elements of the vector cross only the horizontal partition-boundaries ( Figure\n8.1), but for 2-D partitioning, the vector elements cross both horizontal and vertical partition-\nboundaries ( Figure 8.2 ).\nAs Figure 8.2(a)  shows, the first communication step for the 2-D partitioning aligns the vector x\nalong the principal diagonal of the matrix. Often, the vector is stored along the diagonal instead\nof the last column, in which case this step is not required. The second step copies the vector\nelements from each diagonal process to all the processes in the corresponding column. As\nFigure 8.2(b) shows, this step consists of n simultaneous one-to-all broadcast operations, one in\neach column of processes. After these two communication steps, each process multiplies its\nmatrix element with the corresponding element of x. To obtain the result vector y, the products\ncomputed for each row must be added, leaving the sums in the last column of processes. Figure\n8.2(c)  shows this step, which requires an all-to-one reduction (", "doc_id": "037f7d86-a40f-43ae-94c8-5f5743fe94d6", "embedding": null, "doc_hash": "4301349adcbe12e9114bef7d3f789e6c61d2d4425712d5b6def4f35a1810ac56", "extra_info": null, "node_info": {"start": 962439, "end": 965983}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8b8d12c5-09d2-493a-baba-5fe97928fc6b", "3": "a461839d-bc7f-4b45-b277-9dfb5c29708b"}}, "__type__": "1"}, "a461839d-bc7f-4b45-b277-9dfb5c29708b": {"__data__": {"text": "not required. The second step copies the vector\nelements from each diagonal process to all the processes in the corresponding column. As\nFigure 8.2(b) shows, this step consists of n simultaneous one-to-all broadcast operations, one in\neach column of processes. After these two communication steps, each process multiplies its\nmatrix element with the corresponding element of x. To obtain the result vector y, the products\ncomputed for each row must be added, leaving the sums in the last column of processes. Figure\n8.2(c)  shows this step, which requires an all-to-one reduction ( Section 4.1 ) in each row with the\nlast process of the row as the destination. The parallel matrix-vector multiplication is complete\nafter the reduction step.\nParallel Run Time  Three basic communication operations are used in this algorithm: one-to-\none communication to align the vector along the main diagonal, one-to-all broadcast of each\nvector element among the n  processes of each column, and all-to-one reduction in each row.\nEach of these operations takes time Q(log n). Since each process performs a single\nmultiplication in constant time, the overall parallel run time of this algorithm is Q(n). The cost\n(process-time product) is Q(n2 log n); hence, the algorithm is not cost-optimal.\nUsing Fewer than n2 Processes\nA cost-optimal parallel implementation of matrix-vector multiplication with block 2-D\npartitioning of the matrix can be obtained if the granularity of computation at each process is\nincreased by using fewer than n2 processes.\nConsider a logical two-dimensional mesh of p processes in which each process owns an\n block of the matrix. The vector is distributed in portions of \n  elements in\nthe last process-column only. Figure 8.2  also illustrates the initial data-mapping and the various\ncommunication steps for this case. The entire vector must be distributed on each row of\nprocesses before the multiplication can be performed. First, the vector is aligned along the main\ndiagonal. For this, each process in the rightmost column sends its \n vector elements to the\ndiagonal process in its row. Then a columnwise one-to-all broadcast of these \n  elements\ntakes place. Each process then performs n2/p multiplications and locally adds the \n  sets of\nproducts. At the end of this step, as shown in Figure 8.2(c) , each process has \n  partial\nsums that must be accumulated along each row to obtain the result vector. Hence, the last step\nof the algorithm is an all-to-one reduction of the \n values in each row, with the rightmost\nprocess of the row as the destination.\nParallel Run Time  The first step of sending a message of size \n  from the rightmost\nprocess of a row to the diagonal process ( Figure 8.2(a) ) takes time \n . We can\nperform the columnwise one-to-all broadcast in at most time \n  by using\nthe procedure described in Section 4.1.3 . Ignoring the time to perform additions, the final\nrowwise all-to-one reduction also takes the same amount of time. Assuming that a\nmultiplication and addition pair takes unit time, each process spends approximately n2/p time in\ncomputation. Thus, the parallel run time for this procedure is as follows:\nEquation 8.7\n\nScalability Analysis  By using Equations 8.1 and 8.7, and applying the relation To = pTp - W\n(Equation 5.1 ), we get the following expression for the overhead function of this parallel\nalgorithm:\nEquation 8.8\nWe now perform an approximate isoefficiency analysis along the lines of Section 5.4.2  by\nconsidering the terms of the overhead function one at a time (see Problem 8.4 for a more\nprecise isoefficiency analysis). For the ts term of the overhead function, Equation", "doc_id": "a461839d-bc7f-4b45-b277-9dfb5c29708b", "embedding": null, "doc_hash": "af797481f864d7ae839c9d5e091b7b4c207028ac10e2aed290b188f4048b4b58", "extra_info": null, "node_info": {"start": 965904, "end": 969549}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "037f7d86-a40f-43ae-94c8-5f5743fe94d6", "3": "30c49d18-8015-459f-9536-498dd5fa452d"}}, "__type__": "1"}, "30c49d18-8015-459f-9536-498dd5fa452d": {"__data__": {"text": "run time for this procedure is as follows:\nEquation 8.7\n\nScalability Analysis  By using Equations 8.1 and 8.7, and applying the relation To = pTp - W\n(Equation 5.1 ), we get the following expression for the overhead function of this parallel\nalgorithm:\nEquation 8.8\nWe now perform an approximate isoefficiency analysis along the lines of Section 5.4.2  by\nconsidering the terms of the overhead function one at a time (see Problem 8.4 for a more\nprecise isoefficiency analysis). For the ts term of the overhead function, Equation 5.14  yields\nEquation 8.9\nEquation 8.9  gives the isoefficiency term with respect to the message startup time. We can\nobtain the isoefficiency function due to tw by balancing the term \n  log p with the problem\nsize n2. Using the isoefficiency relation of Equation 5.14 , we get the following:\nEquation 8.10\nFinally, considering that the degree of concurrency of 2-D partitioning is n2 (that is, a maximum\nof n2 processes can be used), we arrive at the following relation:\nEquation 8.11\nAmong Equations 8.9, 8.10, and 8.11, the one with the largest right-hand side expression\ndetermines the overall isoefficiency function of this parallel algorithm. To simplify the analysis,\nwe ignore the impact of the constants and consider only the asymptotic rate of the growth of\nproblem size that is necessary to maintain constant efficiency. The asymptotic isoefficiency term\ndue to tw (Equation 8.10) clearly dominates the ones due to ts (Equation 8.9 ) and due to\nconcurrency ( Equation 8.11 ). Therefore, the overall asymptotic isoefficiency function is given by\nQ(p log2 p).\nThe isoefficiency function also determines the criterion for cost-optimality ( Section 5.4.3 ). With\nan isoefficiency function of Q(p log2 p), the maximum number of processes that can be used\ncost-optimally for a given problem size W is determined by the following relations:\nEquation 8.12\nIgnoring the lower-order terms,\nSubstituting log n for log p in Equation 8.12 ,\nEquation 8.13\nThe right-hand side of Equation 8.13  gives an asymptotic upper bound on the number of\nprocesses that can be used cost-optimally for an n x n matrix-vector multiplication with a 2-D\npartitioning of the matrix.\nComparison of 1-D and 2-D Partitionings\nA comparison of Equations 8.2 and 8.7 shows that matrix-vector multiplication is faster with\nblock 2-D partitioning of the matrix than with block 1-D partitioning for the same number of\nprocesses. If the number of processes is greater than n, then the 1-D partitioning cannot be\nused. However, even if the number of processes is less than or equal to n, the analysis in this\nsection suggests that 2-D partitioning is preferable.\nAmong the two partitioning schemes, 2-D partitioning has a better (smaller) asymptotic\nisoefficiency function. Thus, matrix-vector multiplication is more scalable with 2-D partitioning;\nthat is, it can deliver the same efficiency on more processes with 2-D partitioning than with 1-D\npartitioning.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n8.2 Matrix-Matrix Multiplication\nThis section discusses parallel algorithms for multiplying two n x n dense, square matrices A\nand B to yield the product matrix C = A x B. All parallel matrix multiplication algorithms in this\nchapter are based on the conventional serial algorithm shown in Algorithm 8.2 . If we assume\nthat an addition and multiplication pair (line 8) takes unit time, then the sequential run time", "doc_id": "30c49d18-8015-459f-9536-498dd5fa452d", "embedding": null, "doc_hash": "542b179a1f5453ab80e51f1d05c2cc0a7e2a43bf178afe88bbad7dc6ae1351ac", "extra_info": null, "node_info": {"start": 969598, "end": 973004}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a461839d-bc7f-4b45-b277-9dfb5c29708b", "3": "c56210c6-3939-4bff-abeb-31847e6e38d5"}}, "__type__": "1"}, "c56210c6-3939-4bff-abeb-31847e6e38d5": {"__data__": {"text": "efficiency on more processes with 2-D partitioning than with 1-D\npartitioning.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n8.2 Matrix-Matrix Multiplication\nThis section discusses parallel algorithms for multiplying two n x n dense, square matrices A\nand B to yield the product matrix C = A x B. All parallel matrix multiplication algorithms in this\nchapter are based on the conventional serial algorithm shown in Algorithm 8.2 . If we assume\nthat an addition and multiplication pair (line 8) takes unit time, then the sequential run time of\nthis algorithm is n3. Matrix multiplication algorithms with better asymptotic sequential\ncomplexities are available, for example Strassen's algorithm. However, for the sake of\nsimplicity, in this book we assume that the conventional algorithm is the best available serial\nalgorithm. Problem 8.5 explores the performance of parallel matrix multiplication regarding\nStrassen's method as the base algorithm.\nAlgorithm 8.2 The conventional serial algorithm for multiplication of\ntwo n x n matrices.\n1.   procedure  MAT_MULT ( A, B, C) \n2.   begin \n3.      for i := 0 to n - 1 do \n4.          for j := 0 to n - 1 do \n5.              begin \n6.                 C[i, j] := 0; \n7.                 for k := 0 to n - 1 do \n8.                     C[i, j] := C[i, j] + A[i, k] x B[k, j]; \n9.              endfor; \n10.  end MAT_MULT \nAlgorithm 8.3 The block matrix multiplication algorithm for n x n\nmatrices with a block size of ( n/q) x (n/q).\n1.   procedure  BLOCK_MAT_MULT ( A, B, C) \n2.   begin \n3.      for i := 0 to q - 1 do \n4.          for j := 0 to q - 1 do \n5.              begin \n6.                 Initialize all elements of Ci,j to zero; \n7.                 for k := 0 to q - 1 do \n8.                     Ci,j := Ci,j + Ai,k x Bk,j; \n9.              endfor; \n10.  end BLOCK_MAT_MULT \nA concept that is useful in matrix multiplication as well as in a variety of other matrix\nalgorithms is that of block matrix operations. We can often express a matrix computation\ninvolving scalar algebraic operations on all its elements in terms of identical matrix algebraic\noperations on blocks or submatrices of the original matrix. Such algebraic operations on the\nsubmatrices are called block matrix operations . For example, an n x n matrix A can be\nregarded as a q x q array of blocks Ai,j (0 \n i, j < q) such that each block is an ( n/q) x (n/q)\nsubmatrix. The matrix multiplication algorithm in Algorithm 8.2  can then be rewritten as\nAlgorithm 8.3 , in which the multiplication and addition operations on line 8 are matrix\nmultiplication and matrix addition, respectively.", "doc_id": "c56210c6-3939-4bff-abeb-31847e6e38d5", "embedding": null, "doc_hash": "eadb0d787be05c0aef6b39b4e5aee81b3fcc2a8654bdfc689214ba1b36e38e5d", "extra_info": null, "node_info": {"start": 973009, "end": 975605}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "30c49d18-8015-459f-9536-498dd5fa452d", "3": "8e21687b-08b3-4e71-bb77-5c5a41ff7da5"}}, "__type__": "1"}, "8e21687b-08b3-4e71-bb77-5c5a41ff7da5": {"__data__": {"text": "on blocks or submatrices of the original matrix. Such algebraic operations on the\nsubmatrices are called block matrix operations . For example, an n x n matrix A can be\nregarded as a q x q array of blocks Ai,j (0 \n i, j < q) such that each block is an ( n/q) x (n/q)\nsubmatrix. The matrix multiplication algorithm in Algorithm 8.2  can then be rewritten as\nAlgorithm 8.3 , in which the multiplication and addition operations on line 8 are matrix\nmultiplication and matrix addition, respectively. Not only are the final results of Algorithm 8.2\nand 8.3 identical, but so are the total numbers of scalar additions and multiplications performed\nby each. Algorithm 8.2  performs n3 additions and multiplications, and Algorithm 8.3  performs q\n3 matrix multiplications, each involving ( n/q) x (n/q) matrices and requiring ( n/q)3 additions\nand multiplications. We can use p processes to implement the block version of matrix\nmultiplication in parallel by choosing \n  and computing a distinct Ci,j block at each\nprocess.\nIn the following sections, we describe a few ways of parallelizing Algorithm 8.3 . Each of the\nfollowing parallel matrix multiplication algorithms uses a block 2-D partitioning of the matrices.\n8.2.1 A Simple Parallel Algorithm\nConsider two n x n matrices A and B partitioned into p blocks Ai,j and Bi,j \n  of size\n each. These blocks are mapped onto a \n  logical mesh of processes.\nThe processes are labeled from P 0,0 to \n . Process P i,j initially stores Ai,j and Bi,j and\ncomputes block Ci,j of the result matrix. Computing submatrix Ci,j requires all submatrices Ai,k\nand Bk,j for \n . To acquire all the required blocks, an all-to-all broadcast of matrix\nA's blocks is performed in each row of processes, and an all-to-all broadcast of matrix B's blocks\nis performed in each column. After P i,j acquires \n  and \n, it performs the submatrix multiplication and addition step of lines 7 and 8 in Algorithm 8.3 .\nPerformance and Scalability Analysis  The algorithm requires two all-to-all broadcast steps\n(each consisting of \n  concurrent broadcasts in all rows and columns of the process mesh)\namong groups of \n  processes. The messages consist of submatrices of n2/p elements. From\nTable 4.1 , the total communication time is \n . After the\ncommunication step, each process computes a submatrix Ci,j, which requires \n  multiplications\nof \n  submatrices (lines 7 and 8 of Algorithm 8.3  with \n . This takes a total\nof time \n . Thus, the parallel run time is approximately\nEquation 8.14\nThe process-time product is \n , and the parallel algorithm is cost-optimal\nfor p = O(n2).\nThe isoefficiency functions due to ts and tw are ts p log p and 8( tw)3p3/2, respectively. Hence, the\noverall isoefficiency function due to the communication overhead is Q(p3/2). This algorithm can\nuse a maximum of n2 processes; hence, p \n n2 or n3 \n p3/2. Therefore, the isoefficiency\nfunction due to concurrency is also Q(p3/2).\nA notable drawback of this algorithm is its excessive memory requirements. At the end of the\ncommunication phase, each process has \n  blocks of both matrices A and B. Since each block\nrequires Q(n2/p) memory, each process requires Q\n  memory. The total memory\nrequirement over all the processes is Q\n , which is \n  times the memory requirement of\nthe sequential algorithm.\n8.2.2 Cannon's", "doc_id": "8e21687b-08b3-4e71-bb77-5c5a41ff7da5", "embedding": null, "doc_hash": "30cf226bef15128d49e68ec3d458ecf90bd8f30a1edeb5306876196ae8fd0818", "extra_info": null, "node_info": {"start": 975638, "end": 978949}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c56210c6-3939-4bff-abeb-31847e6e38d5", "3": "fdf55cc8-d1d4-4b99-b459-841b5bc33b81"}}, "__type__": "1"}, "fdf55cc8-d1d4-4b99-b459-841b5bc33b81": {"__data__": {"text": "a maximum of n2 processes; hence, p \n n2 or n3 \n p3/2. Therefore, the isoefficiency\nfunction due to concurrency is also Q(p3/2).\nA notable drawback of this algorithm is its excessive memory requirements. At the end of the\ncommunication phase, each process has \n  blocks of both matrices A and B. Since each block\nrequires Q(n2/p) memory, each process requires Q\n  memory. The total memory\nrequirement over all the processes is Q\n , which is \n  times the memory requirement of\nthe sequential algorithm.\n8.2.2 Cannon's Algorithm\nCannon's algorithm is a memory-efficient version of the simple algorithm presented in Section\n8.2.1 . To study this algorithm, we again partition matrices A and B into p square blocks. We\nlabel the processes from P 0,0 to \n , and initially assign submatrices Ai,j and Bi,j to\nprocess P i,j. Although every process in the i th row requires all \n  submatrices\n, it is possible to schedule the computations of the \n  processes of the i th\nrow such that, at any given time, each process is using a different Ai,k. These blocks can be\nsystematically rotated among the processes after every submatrix multiplication so that every\nprocess gets a fresh Ai,k after each rotation. If an identical schedule is applied to the columns,\nthen no process holds more than one block of each matrix at any time, and the total memory\nrequirement of the algorithm over all the processes is Q(n2). Cannon's algorithm is based on\nthis idea. The scheduling for the multiplication of submatrices on separate processes in\nCannon's algorithm is illustrated in Figure 8.3 for 16 processes.\nFigure 8.3. The communication steps in Cannon's algorithm on 16\nprocesses.\nThe first communication step of the algorithm aligns the blocks of A and B in such a way that\neach process multiplies its local submatrices. As Figure 8.3(a)  shows, this alignment is achieved\nfor matrix A by shifting all submatrices Ai,j to the left (with wraparound) by i steps. Similarly, as\nshown in Figure 8.3(b) , all submatrices Bi,j are shifted up (with wraparound) by j steps. These\nare circular shift operations ( Section 4.6 ) in each row and column of processes, which leave\nprocess P i,j with submatrices \n  and \n . Figure 8.3(c)  shows the blocks of\nA and B after the initial alignment, when each process is ready for the first submatrix\nmultiplication. After a submatrix multiplication step, each block of A moves one step left and\neach block of B moves one step up (again with wraparound), as shown in Figure 8.3(d) . A\nsequence of \n  such submatrix multiplications and single-step shifts pairs up each Ai,k and Bk,j\nfor \n  at P i,j. This completes the multiplication of matrices A and B.\nPerformance Analysis  The initial alignment of the two matrices ( Figure 8.3(a)  and (b))\ninvolves a rowwise and a columnwise circular shift. In any of these shifts, the maximum\ndistance over which a block shifts is \n . The two shift operations require a total of time 2( ts\n+ twn2/p) (Table 4.1 ). Each of the \n  single-step shifts in the compute-and-shift phase of the\nalgorithm takes time ts + twn2/p. Thus, the total communication time (for both matrices) during\nthis phase of the algorithm is \n . For large enough p on a network with sufficient\nbandwidth, the communication time for the initial alignment can be disregarded in comparison\nwith the time spent in communication during the compute-and-shift phase.\nEach process performs \n  multiplications of \n  submatrices. Assuming that a\nmultiplication and addition pair takes unit", "doc_id": "fdf55cc8-d1d4-4b99-b459-841b5bc33b81", "embedding": null, "doc_hash": "c72ed78b703295bb2353206114d73a0ae29ac7905e651e28ab728bc3d72d7e51", "extra_info": null, "node_info": {"start": 978927, "end": 982431}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8e21687b-08b3-4e71-bb77-5c5a41ff7da5", "3": "df85b1d9-6271-4885-afd9-7c04c5bde21f"}}, "__type__": "1"}, "df85b1d9-6271-4885-afd9-7c04c5bde21f": {"__data__": {"text": "ts\n+ twn2/p) (Table 4.1 ). Each of the \n  single-step shifts in the compute-and-shift phase of the\nalgorithm takes time ts + twn2/p. Thus, the total communication time (for both matrices) during\nthis phase of the algorithm is \n . For large enough p on a network with sufficient\nbandwidth, the communication time for the initial alignment can be disregarded in comparison\nwith the time spent in communication during the compute-and-shift phase.\nEach process performs \n  multiplications of \n  submatrices. Assuming that a\nmultiplication and addition pair takes unit time, the total time that each process spends in\ncomputation is n3/p. Thus, the approximate overall parallel run time of this algorithm is\nEquation 8.15\nThe cost-optimality condition for Cannon's algorithm is identical to that for the simple algorithm\npresented in Section 8.2.1. As in the simple algorithm, the isoefficiency function of Cannon's\nalgorithm is Q(p3/2).\n8.2.3 The DNS Algorithm\nThe matrix multiplication algorithms presented so far use block 2-D partitioning of the input and\nthe output matrices and use a maximum of n2 processes for n x n matrices. As a result, these\nalgorithms have a parallel run time of W(n) because there are Q(n3) operations in the serial\nalgorithm. We now present a parallel algorithm based on partitioning intermediate data that\ncan use up to n3 processes and that performs matrix multiplication in time Q(log n) by using\nW(n3/log n) processes. This algorithm is known as the DNS algorithm because it is due to Dekel,\nNassimi, and Sahni.\nWe first introduce the basic idea, without concern for inter-process communication. Assume that\nn3 processes are available for multiplying two n x n matrices. These processes are arranged in a\nthree-dimensional n x n x n logical array. Since the matrix multiplication algorithm performs n3\nscalar multiplications, each of the n3 processes is assigned a single scalar multiplication. The\nprocesses are labeled according to their location in the array, and the multiplication A[i, k] x\nB[k, j] is assigned to process P i,j,k (0 \n i, j, k < n). After each process performs a single\nmultiplication, the contents of P i,j,0, Pi,j,1, ..., P i,j,n-1 are added to obtain C [i, j]. The additions\nfor all C [i, j] can be carried out simultaneously in log n steps each. Thus, it takes one step to\nmultiply and log n steps to add; that is, it takes time Q(log n) to multiply the n x n matrices by\nthis algorithm.\nWe now describe a practical parallel implementation of matrix multiplication based on this idea.\nAs Figure 8.4  shows, the process arrangement can be visualized as n planes of n x n processes\neach. Each plane corresponds to a different value of k. Initially, as shown in Figure 8.4(a) , the\nmatrices are distributed among the n2 processes of the plane corresponding to k = 0 at the base\nof the three-dimensional process array. Process P i,j,0 initially owns A[i, j] and B[i, j].\nFigure 8.4. The communication steps in the DNS algorithm while\nmultiplying 4 x 4 matrices A and B on 64 processes. The shaded\nprocesses in part (c) store elements of the first row of A and the\nshaded processes in part (d) store elements of the first column of B.\nThe vertical column of processes P i,j,* computes the dot product of row A [i, *] and column B[*,\nj]. Therefore, rows of A and columns of B need to be moved appropriately so that each", "doc_id": "df85b1d9-6271-4885-afd9-7c04c5bde21f", "embedding": null, "doc_hash": "0c8d95ca0c5d10d78fadcb3a816a33af92998d0fbf7551f6b54383e89de9645d", "extra_info": null, "node_info": {"start": 982388, "end": 985756}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "fdf55cc8-d1d4-4b99-b459-841b5bc33b81", "3": "3f5827d6-36e5-4069-b915-36c000972820"}}, "__type__": "1"}, "3f5827d6-36e5-4069-b915-36c000972820": {"__data__": {"text": "initially owns A[i, j] and B[i, j].\nFigure 8.4. The communication steps in the DNS algorithm while\nmultiplying 4 x 4 matrices A and B on 64 processes. The shaded\nprocesses in part (c) store elements of the first row of A and the\nshaded processes in part (d) store elements of the first column of B.\nThe vertical column of processes P i,j,* computes the dot product of row A [i, *] and column B[*,\nj]. Therefore, rows of A and columns of B need to be moved appropriately so that each vertical\ncolumn of processes P i,j,* has row A[i, *] and column B[*, j]. More precisely, process P i,j,k\nshould have A[i, k] and B[k, j].\nThe communication pattern for distributing the elements of matrix A among the processes is\nshown in Figure 8.4(a) \u2013(c). First, each column of A moves to a different plane such that the j th\ncolumn occupies the same position in the plane corresponding to k = j as it initially did in the\nplane corresponding to k = 0. The distribution of A after moving A[i, j] from P i,j,0 to P i,j,j is\nshown in Figure 8.4(b) . Now all the columns of A are replicated n times in their respective\nplanes by a parallel one-to-all broadcast along the j axis. The result of this step is shown in\nFigure 8.4(c) , in which the n processes P i,0,j, Pi,1,j, ..., P i,n-1,j receive a copy of A[i, j] from P i,j,j.\nAt this point, each vertical column of processes P i,j,* has row A[i, *]. More precisely, process\nPi,j,k has A[i, k].\nFor matrix B, the communication steps are similar, but the roles of i and j in process subscripts\nare switched. In the first one-to-one communication step, B[i, j] is moved from P i,j,0 to P i,j,i.\nThen it is broadcast from P i,j,i among P 0,j,i, P1,j,i, ..., P n-1,j,i. The distribution of B after this one-\nto-all broadcast along the i axis is shown in Figure 8.4(d) . At this point, each vertical column of\nprocesses P i,j,* has column B[*, j]. Now process P i,j,k has B[k, j], in addition to A[i, k].\nAfter these communication steps, A[i, k] and B[k, j] are multiplied at P i,j,k. Now each element\nC[i, j] of the product matrix is obtained by an all-to-one reduction along the k axis. During this\nstep, process P i,j,0 accumulates the results of the multiplication from processes P i,j,1, ..., P i,j,n-1.\nFigure 8.4  shows this step for C[0, 0].\nThe DNS algorithm has three main communication steps: (1) moving the columns of A and the\nrows of B to their respective planes, (2) performing one-to-all broadcast along the j axis for A\nand along the i axis for B, and (3) all-to-one reduction along the k axis. All these operations are\nperformed within groups of n processes and take time Q(log n). Thus, the parallel run time for\nmultiplying two n x n matrices using the DNS algorithm on n3 processes is Q(log n).\nDNS Algorithm with Fewer than n3 Processes\nThe DNS algorithm is not cost-optimal for n3 processes, since its process-time product of Q(n3\nlog n) exceeds the Q(n3) sequential complexity of matrix multiplication. We now present a", "doc_id": "3f5827d6-36e5-4069-b915-36c000972820", "embedding": null, "doc_hash": "d8a9c376f3118322f52d43d9f5a89ab94950929e1bd6f96d996031e45d1cc5e5", "extra_info": null, "node_info": {"start": 985839, "end": 988810}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "df85b1d9-6271-4885-afd9-7c04c5bde21f", "3": "06e0d21c-96d1-4ff9-a3db-4c497e2b6d3d"}}, "__type__": "1"}, "06e0d21c-96d1-4ff9-a3db-4c497e2b6d3d": {"__data__": {"text": "along the i axis for B, and (3) all-to-one reduction along the k axis. All these operations are\nperformed within groups of n processes and take time Q(log n). Thus, the parallel run time for\nmultiplying two n x n matrices using the DNS algorithm on n3 processes is Q(log n).\nDNS Algorithm with Fewer than n3 Processes\nThe DNS algorithm is not cost-optimal for n3 processes, since its process-time product of Q(n3\nlog n) exceeds the Q(n3) sequential complexity of matrix multiplication. We now present a cost-\noptimal version of this algorithm that uses fewer than n3 processes. Another variant of the DNS\nalgorithm that uses fewer than n3 processes is described in Problem 8.6.\nAssume that the number of processes p is equal to q3 for some q < n. To implement the DNS\nalgorithm, the two matrices are partitioned into blocks of size ( n/q) x (n/q). Each matrix can\nthus be regarded as a q x q two-dimensional square array of blocks. The implementation of this\nalgorithm on q3 processes is very similar to that on n3 processes. The only difference is that now\nwe operate on blocks rather than on individual elements. Since 1 \n  q \n n, the number of\nprocesses can vary between 1 and n3.\nPerformance Analysis  The first one-to-one communication step is performed for both A and B,\nand takes time ts + tw(n/q)2 for each matrix. The second step of one-to-all broadcast is also\nperformed for both matrices and takes time t s log q + tw(n/q)2 log q for each matrix. The final\nall-to-one reduction is performed only once (for matrix C)and takes time ts log q + tw(n/q)2 log\nq. The multiplication of ( n/q) x (n/q) submatrices by each process takes time ( n/q)3. We can\nignore the communication time for the first one-to-one communication step because it is much\nsmaller than the communication time of one-to-all broadcasts and all-to-one reduction. We can\nalso ignore the computation time for addition in the final reduction phase because it is of a\nsmaller order of magnitude than the computation time for multiplying the submatrices. With\nthese assumptions, we get the following approximate expression for the parallel run time of the\nDNS algorithm:\n\nSince q = p1/3, we get\nEquation 8.16\nThe total cost of this parallel algorithm is n3 + ts p log p + twn2 p1/3 log p. The isoefficiency\nfunction is Q(p(log p)3). The algorithm is cost-optimal for n3 = W(p(log p)3), or p = O(n3/(log\nn)3).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n \n8.3 Solving a System of Linear Equations\nThis section discusses the problem of solving a system of linear equations of the form\nIn matrix notation, this system is written as Ax = b . Here A is a dense n x n matrix of\ncoefficients such that A [i , j ] = ai , j , b is an n x 1 vector [ b 0 , b 1 , ..., bn -1 ]T , and x is the\ndesired solution vector [ x 0 , x 1 , ..., xn -1 ]T . We will make all subsequent references to ai , j by\nA [i , j ] and xi by x [i ].\nA system of equations Ax = b is usually solved in two stages. First, through a series of algebraic\nmanipulations, the original system of equations is reduced to an upper-triangular system of the\nform\nWe write this as Ux = y , where U is a unit upper-triangular matrix \u2013 one in which all\nsubdiagonal entries are zero and all principal diagonal", "doc_id": "06e0d21c-96d1-4ff9-a3db-4c497e2b6d3d", "embedding": null, "doc_hash": "6a5cb1f53f83768a1d872162d01705d4e3dc1a150cc1b913a3f119e429e24198", "extra_info": null, "node_info": {"start": 988786, "end": 992002}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3f5827d6-36e5-4069-b915-36c000972820", "3": "d5914db6-6075-46e9-a658-cefb2f47cb4c"}}, "__type__": "1"}, "d5914db6-6075-46e9-a658-cefb2f47cb4c": {"__data__": {"text": "]T , and x is the\ndesired solution vector [ x 0 , x 1 , ..., xn -1 ]T . We will make all subsequent references to ai , j by\nA [i , j ] and xi by x [i ].\nA system of equations Ax = b is usually solved in two stages. First, through a series of algebraic\nmanipulations, the original system of equations is reduced to an upper-triangular system of the\nform\nWe write this as Ux = y , where U is a unit upper-triangular matrix \u2013 one in which all\nsubdiagonal entries are zero and all principal diagonal entries are equal to one. Formally, U [i ,\nj ] = 0 if i > j , otherwise U [i , j ] = ui , j . Furthermore, U [i , i ] = 1 for 0 \n  i < n . In the\nsecond stage of solving a system of linear equations, the upper-triangular system is solved for\nthe variables in reverse order from x [n - 1] to x [0] by a procedure known as back-\nsubstitution  (Section 8.3.3 ).\nWe discuss parallel formulations of the classical Gaussian elimination method for upper-\ntriangularization in Sections 8.3.1  and 8.3.2  . In Section 8.3.1  , we describe a straightforward\nGaussian elimination algorithm assuming that the coefficient matrix is nonsingular, and its rows\nand columns are permuted in a way that the algorithm is numerically stable. Section 8.3.2\ndiscusses the case in which a numerically stable solution of the system of equations requires\npermuting the columns of the matrix during the execution of the Gaussian elimination\nalgorithm.\nAlthough we discuss Gaussian elimination in the context of upper-triangularization, a similar\nprocedure can be used to factorize matrix A as the product of a lower-triangular matrix L and a\nunit upper-triangular matrix U so that A = L x U . This factorization is commonly referred to as\nLU factorization  . Performing LU factorization (rather than upper-triangularization) is\nparticularly useful if multiple systems of equations with the same left-hand side Ax need to be\nsolved. Algorithm 3.3  gives a procedure for column-oriented LU factorization.\n8.3.1 A Simple Gaussian Elimination Algorithm\nThe serial Gaussian elimination algorithm has three nested loops. Several variations of the\nalgorithm exist, depending on the order in which the loops are arranged. Algorithm 8.4  shows\none variation of Gaussian elimination, which we will adopt for parallel implementation in the\nremainder of this section. This program converts a system of linear equations Ax = b to a unit\nupper-triangular system Ux = y . We assume that the matrix U shares storage with A and\noverwrites the upper-triangular portion of A . The element A [k , j ] computed on line 6 of\nAlgorithm 8.4  is actually U [k , j ]. Similarly, the element A [k , k ] equated to 1 on line 8 is U\n[k , k ]. Algorithm 8.4  assumes that A [k , k ] \n 0 when it is used as a divisor on lines 6 and\n7.\nAlgorithm 8.4 A serial Gaussian elimination algorithm that converts\nthe system of linear equations Ax = b to a unit upper-triangular\nsystem Ux = y . The matrix U occupies the upper-triangular locations\nof A . This algorithm assumes that A [k , k ] \n 0 when it is used as a\ndivisor on lines 6 and 7.\n1.   procedure  GAUSSIAN_ELIMINATION ( A, b, y) \n2.   begin \n3.      for k := 0 to n - 1 do           /* Outer loop */ \n4.      begin \n5.       ", "doc_id": "d5914db6-6075-46e9-a658-cefb2f47cb4c", "embedding": null, "doc_hash": "d8b0a9df4f9580fd13b9eb778630a4cac95deda22a521107a267fc5493866036", "extra_info": null, "node_info": {"start": 992026, "end": 995242}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "06e0d21c-96d1-4ff9-a3db-4c497e2b6d3d", "3": "bc90069c-86b0-48fb-9d7b-34a5530bec3f"}}, "__type__": "1"}, "bc90069c-86b0-48fb-9d7b-34a5530bec3f": {"__data__": {"text": "linear equations Ax = b to a unit upper-triangular\nsystem Ux = y . The matrix U occupies the upper-triangular locations\nof A . This algorithm assumes that A [k , k ] \n 0 when it is used as a\ndivisor on lines 6 and 7.\n1.   procedure  GAUSSIAN_ELIMINATION ( A, b, y) \n2.   begin \n3.      for k := 0 to n - 1 do           /* Outer loop */ \n4.      begin \n5.         for j := k + 1 to n - 1 do \n6.             A[k, j] := A[k, j]/A[k, k];  /* Division step */ \n7.         y[k] := b[k]/A[k, k]; \n8.         A[k, k] := 1; \n9.         for i := k + 1 to n - 1 do \n10.        begin \n11.           for j := k + 1 to n - 1 do \n12.               A[i, j] := A[i, j] - A[i, k] x A[k, j]; /* Elimination step */ \n13.           b[i] := b[i] - A[i, k] x y[k]; \n14.           A[i, k] := 0; \n15.        endfor;          /* Line 9 */ \n16.     endfor;             /* Line 3 */ \n17.  end GAUSSIAN_ELIMINATION \nIn this section, we will concentrate only on the operations on matrix A in Algorithm 8.4  . The\noperations on vector b on lines 7 and 13 of the program are straightforward to implement.\nHence, in the rest of the section, we will ignore these steps. If the steps on lines 7, 8, 13, and\n14 are not performed, then Algorithm 8.4  leads to the LU factorization of A as a product L x U .\nAfter the termination of the procedure, L is stored in the lower-triangular part of A , and U\noccupies the locations above the principal diagonal.\nFor k varying from 0 to n - 1, the Gaussian elimination procedure systematically eliminates\nvariable x [k ] from equations k + 1 to n - 1 so that the matrix of coefficients becomes upper-\ntriangular. As shown in Algorithm 8.4  , in the k th iteration of the outer loop (starting on line 3),\nan appropriate multiple of the k th equation is subtracted from each of the equations k + 1 to n\n- 1 (loop starting on line 9). The multiples of the k th equation (or the k th row of matrix A ) are\nchosen such that the k th coefficient becomes zero in equations k + 1 to n - 1 eliminating x [k ]\nfrom these equations. A typical computation of the Gaussian elimination procedure in the k th\niteration of the outer loop is shown in Figure 8.5  . The k th iteration of the outer loop does not\ninvolve any computation on rows 1 to k - 1 or columns 1 to k - 1. Thus, at this stage, only the\nlower-right ( n - k ) x (n - k ) submatrix of A (the shaded portion in Figure 8.5 ) is\ncomputationally active.\nFigure 8.5. A typical computation in Gaussian elimination.\nGaussian elimination involves approximately n 2 /2 divisions (line 6) and approximately ( n 3", "doc_id": "bc90069c-86b0-48fb-9d7b-34a5530bec3f", "embedding": null, "doc_hash": "2929da4bc274dd69307f6814cb534b00e2e42ca092510c40ef096f4fcd38a773", "extra_info": null, "node_info": {"start": 995380, "end": 997937}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d5914db6-6075-46e9-a658-cefb2f47cb4c", "3": "5ea9fce6-3c7e-4439-bf0e-df1bfd52dcdf"}}, "__type__": "1"}, "5ea9fce6-3c7e-4439-bf0e-df1bfd52dcdf": {"__data__": {"text": "elimination procedure in the k th\niteration of the outer loop is shown in Figure 8.5  . The k th iteration of the outer loop does not\ninvolve any computation on rows 1 to k - 1 or columns 1 to k - 1. Thus, at this stage, only the\nlower-right ( n - k ) x (n - k ) submatrix of A (the shaded portion in Figure 8.5 ) is\ncomputationally active.\nFigure 8.5. A typical computation in Gaussian elimination.\nGaussian elimination involves approximately n 2 /2 divisions (line 6) and approximately ( n 3 /3)\n- (n 2 /2) subtractions and multiplications (line 12). In this section, we assume that each scalar\narithmetic operation takes unit time. With this assumption, the sequential run time of the\nprocedure is approximately 2 n 3 /3 (for large n ); that is,\nEquation 8.17\nParallel Implementation with 1-D Partitioning\nWe now consider a parallel implementation of Algorithm 8.4  , in which the coefficient matrix is\nrowwise 1-D partitioned among the processes. A parallel implementation of this algorithm with\ncolumnwise 1-D partitioning is very similar, and its details can be worked out based on the\nimplementation using rowwise 1-D partitioning (Problems 8.8 and 8.9).\nWe first consider the case in which one row is assigned to each process, and the n x n coefficient\nmatrix A is partitioned along the rows among n processes labeled from P 0 to P n -1 . In this\nmapping, process P i initially stores elements A [i , j ] for 0 \n j < n . Figure 8.6  illustrates this\nmapping of the matrix onto the processes for n = 8. The figure also illustrates the computation\nand communication that take place in the iteration of the outer loop when k = 3.\nFigure 8.6. Gaussian elimination steps during the iteration\ncorresponding to k = 3 for an 8 x 8 matrix partitioned rowwise among\neight processes.\nAlgorithm 8.4  and Figure 8.5  show that A [k , k + 1], A [k , k + 2], ..., A [k , n - 1] are divided\nby A [k , k ] (line 6) at the beginning of the k th iteration. All matrix elements participating in\nthis operation (shown by the shaded portion of the matrix in Figure 8.6(a)  ) belong to the same\nprocess. So this step does not require any communication. In the second computation step of\nthe algorithm (the elimination step of line 12), the modified (after division) elements of the k th\nrow are used by all other rows of the active part of the matrix. As Figure 8.6(b)  shows, this\nrequires a one-to-all broadcast of the active part of the k th row to the processes storing rows k\n+ 1 to n - 1. Finally, the computation A [i , j ] := A [i , j ] - A [i , k ] x A [k , j ] takes place in\nthe remaining active portion of the matrix, which is shown shaded in Figure 8.6(c)  .\nThe computation step corresponding to Figure 8.6(a)  in the k th iteration requires n - k - 1\ndivisions at process P k . Similarly, the computation step of Figure 8.6(c)  involves n - k - 1\nmultiplications and subtractions in the k th iteration at all processes P i , such that k < i < n .\nAssuming a single arithmetic operation takes unit time, the total time spent in computation in\nthe k th iteration is 3( n - k - 1). Note that when P k is performing the divisions, the remaining p\n- 1 processes are idle, and while processes P k + 1, ..., P n -1 are performing the elimination step,\nprocesses P 0 , ..., P k are idle.", "doc_id": "5ea9fce6-3c7e-4439-bf0e-df1bfd52dcdf", "embedding": null, "doc_hash": "6c1d22abc57e5ee565ee05d8737e0369b91056d859f054e72287419fb5f554c7", "extra_info": null, "node_info": {"start": 997797, "end": 1001076}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bc90069c-86b0-48fb-9d7b-34a5530bec3f", "3": "9c68c128-815b-4593-a691-a8cccb744925"}}, "__type__": "1"}, "9c68c128-815b-4593-a691-a8cccb744925": {"__data__": {"text": ". Similarly, the computation step of Figure 8.6(c)  involves n - k - 1\nmultiplications and subtractions in the k th iteration at all processes P i , such that k < i < n .\nAssuming a single arithmetic operation takes unit time, the total time spent in computation in\nthe k th iteration is 3( n - k - 1). Note that when P k is performing the divisions, the remaining p\n- 1 processes are idle, and while processes P k + 1, ..., P n -1 are performing the elimination step,\nprocesses P 0 , ..., P k are idle. Thus, the total time spent during the computation steps shown in\nFigures 8.6(a)  and (c)  in this parallel implementation of Gaussian elimination is\n, which is equal to 3 n (n - 1)/2.\nThe communication step of Figure 8.6(b)  takes time ( ts +tw (n -k -1)) log n (Table 4.1 ). Hence,\nthe total communication time over all iterations is \n  log n , which is equal\nto ts n log n + tw (n (n - 1)/2) log n . The overall parallel run time of this algorithm is\nEquation 8.18\nSince the number of processes is n , the cost, or the process-time product, is Q (n 3 log n ) due\nto the term associated with tw in Equation 8.18  . This cost is asymptotically higher than the\nsequential run time of this algorithm (Equation 8.17  ). Hence, this parallel implementation is\nnot cost-optimal.\nPipelined Communication and Computation  We now present a parallel implementation of\nGaussian elimination that is cost-optimal on n processes.\nIn the parallel Gaussian elimination algorithm just presented, the n iterations of the outer loop\nof Algorithm 8.4  execute sequentially. At any given time, all processes work on the same\niteration. The ( k + 1)th iteration starts only after all the computation and communication for\nthe k th iteration is complete. The performance of the algorithm can be improved substantially if\nthe processes work asynchronously; that is, no process waits for the others to finish an iteration\nbefore starting the next one. We call this the asynchronous  or pipelined  version of Gaussian\nelimination. Figure 8.7  illustrates the pipelined Algorithm 8.4 for a 5 x 5 matrix partitioned\nalong the rows onto a logical linear array of five processes.\nFigure 8.7. Pipelined Gaussian elimination on a 5 x 5 matrix\npartitioned with one row per process.\nDuring the k th iteration of Algorithm 8.4  , process P k broadcasts part of the k th row of the\nmatrix to processes P k +1 , ..., P n -1 (Figure 8.6(b)  ). Assuming that the processes form a logical\nlinear array, and P k +1 is the first process to receive the k th row from process P k . Then process\nPk +1 must forward this data to P k +2 . However, after forwarding the k th row to P k +2 , process\nPk +1 need not wait to perform the elimination step (line 12) until all the processes up to P n -1\nhave received the k th row. Similarly, P k +2 can start its computation as soon as it has\nforwarded the k th row to P k +3 , and so on. Meanwhile, after completing the computation for\nthe k th iteration, P k +1 can perform the division step (line 6), and start the broadcast of the ( k\n+ 1)th row by sending it to P k +2 .\nIn pipelined Gaussian elimination, each process independently performs the following sequence\nof actions repeatedly until all n iterations are complete. For the sake of simplicity, we assume\nthat steps (1) and (2) take the same amount of time (this assumption does not affect", "doc_id": "9c68c128-815b-4593-a691-a8cccb744925", "embedding": null, "doc_hash": "e57f124d3e631c282769889439c1babec0cfaf37b6bfb08e8e5ddc27c9d79d2b", "extra_info": null, "node_info": {"start": 1001075, "end": 1004428}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5ea9fce6-3c7e-4439-bf0e-df1bfd52dcdf", "3": "b4031009-0fd8-4291-b4e9-c3763e75817d"}}, "__type__": "1"}, "b4031009-0fd8-4291-b4e9-c3763e75817d": {"__data__": {"text": "as soon as it has\nforwarded the k th row to P k +3 , and so on. Meanwhile, after completing the computation for\nthe k th iteration, P k +1 can perform the division step (line 6), and start the broadcast of the ( k\n+ 1)th row by sending it to P k +2 .\nIn pipelined Gaussian elimination, each process independently performs the following sequence\nof actions repeatedly until all n iterations are complete. For the sake of simplicity, we assume\nthat steps (1) and (2) take the same amount of time (this assumption does not affect the\n1.\nanalysis):\nIf a process has any data destined for other processes, it sends those data to the\nappropriate process.1.\nIf the process can perform some computation using the data it has, it does so.2.\nOtherwise, the process waits to receive data to be used for one of the above actions.3.\nFigure 8.7  shows the 16 steps in the pipelined parallel execution of Gaussian elimination for a 5\nx 5 matrix partitioned along the rows among five processes. As Figure 8.7(a)  shows, the first\nstep is to perform the division on row 0 at process P 0 . The modified row 0 is then sent to P 1\n(Figure 8.7(b)  ), which forwards it to P 2 (Figure 8.7(c)  ). Now P 1 is free to perform the\nelimination step using row 0 (Figure 8.7(d)  ). In the next step (Figure 8.7(e)  ), P 2 performs the\nelimination step using row 0. In the same step, P 1 , having finished its computation for iteration\n0, starts the division step of iteration 1. At any given time, different stages of the same iteration\ncan be active on different processes. For instance, in Figure 8.7(h)  , process P 2 performs the\nelimination step of iteration 1 while processes P 3 and P 4 are engaged in communication for the\nsame iteration. Furthermore, more than one iteration may be active simultaneously on different\nprocesses. For instance, in Figure 8.7(i)  , process P 2 is performing the division step of iteration\n2 while process P 3 is performing the elimination step of iteration 1.\nWe now show that, unlike the synchronous algorithm in which all processes work on the same\niteration at a time, the pipelined or the asynchronous version of Gaussian elimination is cost-\noptimal. As Figure 8.7  shows, the initiation of consecutive iterations of the outer loop of\nAlgorithm 8.4  is separated by a constant number of steps. A total of n such iterations are\ninitiated. The last iteration modifies only the bottom-right corner element of the coefficient\nmatrix; hence, it completes in a constant time after its initiation. Thus, the total number of\nsteps in the entire pipelined procedure is Q (n ) (Problem 8.7). In any step, either O (n )\nelements are communicated between directly-connected processes, or a division step is\nperformed on O (n ) elements of a row, or an elimination step is performed on O (n ) elements\nof a row. Each of these operations take O (n ) time. Hence, the entire procedure consists of Q (n\n) steps of O( n ) complexity each, and its parallel run time is O (n 2 ). Since n processes are\nused, the cost is O (n 3 ), which is of the same order as the sequential complexity of Gaussian\nelimination. Hence, the pipelined version of parallel Gaussian elimination with 1-D partitioning\nof the coefficient matrix is cost-optimal.\nBlock 1-D Partitioning with Fewer than n Processes  The preceding pipelined\nimplementation of parallel Gaussian elimination can be easily adapted for the case in which n >\np . Consider an n x n matrix", "doc_id": "b4031009-0fd8-4291-b4e9-c3763e75817d", "embedding": null, "doc_hash": "978bd09ecf6a181c5b975cfc7dbcbe519f6a33182d5014cd0d895e8efd73a8bc", "extra_info": null, "node_info": {"start": 1004400, "end": 1007834}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9c68c128-815b-4593-a691-a8cccb744925", "3": "46676383-9b39-4406-94c2-f41198863fb8"}}, "__type__": "1"}, "46676383-9b39-4406-94c2-f41198863fb8": {"__data__": {"text": "of O( n ) complexity each, and its parallel run time is O (n 2 ). Since n processes are\nused, the cost is O (n 3 ), which is of the same order as the sequential complexity of Gaussian\nelimination. Hence, the pipelined version of parallel Gaussian elimination with 1-D partitioning\nof the coefficient matrix is cost-optimal.\nBlock 1-D Partitioning with Fewer than n Processes  The preceding pipelined\nimplementation of parallel Gaussian elimination can be easily adapted for the case in which n >\np . Consider an n x n matrix partitIoned among p processes ( p < n ) such that each process is\nassigned n /p contiguous rows of the matrix. Figure 8.8  illustrates the communication steps in a\ntypical iteration of Gaussian elimination with such a mapping. As the figure shows, the k th\niteration of the algorithm requires that the active part of the k th row be sent to the processes\nstoring rows k + 1, k + 2, ..., n - 1.\nFigure 8.8. The communication in the Gaussian elimination iteration\ncorresponding to k = 3 for an 8 x 8 matrix distributed among four\nprocesses using block 1-D partitioning.\nFigure 8.9(a)  shows that, with block 1-D partitioning, a process with all rows belonging to the\nactive part of the matrix performs ( n - k - 1)n /p multiplications and subtractions during the\nelimination step of the k th iteration. Note that in the last ( n /p ) - 1 iterations, no process has\nall active rows, but we ignore this anomaly. If the pipelined version of the algorithm is used,\nthen the number of arithmetic operations on a maximally-loaded process in the k th iteration\n(2(n - k - 1)n /p ) is much higher than the number of words communicated ( n - k - 1) by a\nprocess in the same iteration. Thus, for sufficiently large values of n with respect to p ,\ncomputation dominates communication in each iteration. Assuming that each scalar\nmultiplication and subtraction pair takes unit time, the total parallel run time of this algorithm\n(ignoring communication overhead) is \n, which is approximately equal to n\n3 /p .\nFigure 8.9. Computation load on different processes in block and cyclic\n1-D partitioning of an 8 x 8 matrix on four processes during the\nGaussian elimination iteration corresponding to k = 3.\nThe process-time product of this algorithm is n 3 , even if the communication costs are ignored.\nThus, the cost of the parallel algorithm is higher than the sequential run time (Equation 8.17  )\nby a factor of 3/2. This inefficiency of Gaussian elimination with block 1-D partitioning is due to\nprocess idling resulting from an uneven load distribution. As Figure 8.9(a)  shows for an 8 x 8\nmatrix and four processes, during the iteration corresponding to k = 3 (in the outer loop of\nAlgorithm 8.4  ), one process is completely idle, one is partially loaded, and only two processes\nare fully active. By the time half of the iterations of the outer loop are over, only half the\nprocesses are active. The remaining idle processes make the parallel algorithm costlier than the\nsequential algorithm.\nThis problem can be alleviated if the matrix is partitioned among the processes using cyclic 1-D\nmapping as shown in Figure 8.9(b)  . With the cyclic 1-D partitioning, the difference between the\ncomputational loads of a maximally loaded process and the least loaded process in any iteration\nis of at most one row (that is, O (n ) arithmetic operations). Since there are n iterations, the\ncumulative overhead due to process idling is only O (n 2 p ) with a cyclic mapping, compared to\nQ (n 3 ) with a block", "doc_id": "46676383-9b39-4406-94c2-f41198863fb8", "embedding": null, "doc_hash": "d1e86e3bf8f9e8e55ee65499617e2c958a2a3863fd31b84047ad1c1a2146cf74", "extra_info": null, "node_info": {"start": 1007831, "end": 1011345}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b4031009-0fd8-4291-b4e9-c3763e75817d", "3": "968c8d7c-468b-4bfc-aaaa-0c7b877806b9"}}, "__type__": "1"}, "968c8d7c-468b-4bfc-aaaa-0c7b877806b9": {"__data__": {"text": "algorithm.\nThis problem can be alleviated if the matrix is partitioned among the processes using cyclic 1-D\nmapping as shown in Figure 8.9(b)  . With the cyclic 1-D partitioning, the difference between the\ncomputational loads of a maximally loaded process and the least loaded process in any iteration\nis of at most one row (that is, O (n ) arithmetic operations). Since there are n iterations, the\ncumulative overhead due to process idling is only O (n 2 p ) with a cyclic mapping, compared to\nQ (n 3 ) with a block mapping (Problem 8.12).\nParallel Implementation with 2-D Partitioning\nWe now describe a parallel implementation of Algorithm 8.4  in which the n x n matrix A is\nmapped onto an n x n mesh of processes such that process P i , j initially stores A [i , j ]. The\ncommunication and computation steps in the iteration of the outer loop corresponding to k = 3\nare illustrated in Figure 8.10  for n = 8. Algorithm 8.4  and Figures 8.5  and 8.10  show that in the\nk th iteration of the outer loop, A [k , k ] is required by processes P k , k +1 , Pk , k +2 , ..., P k , n -1\nto divide A [k , k + 1], A [k , k + 2], ..., A [k , n - 1], respectively. After the division on line 6,\nthe modified elements of the k th row are used to perform the elimination step by all the other\nrows in the active part of the matrix. The modified (after the division on line 6) elements of the\nk th row are used by all other rows of the active part of the matrix. Similarly, the elements of\nthe k th column are used by all other columns of the active part of the matrix for the elimination\nstep. As Figure 8.10  shows, the communication in the k th iteration requires a one-to-all\nbroadcast of A [i , k ] along the i th row (Figure 8.10(a)  ) for k \n i < n , and a one-to-all\nbroadcast of A [k , j ] along the j th column (Figure 8.10(c)  ) for k < j < n . Just like the 1-D\npartitioning case, a non-cost-optimal parallel formulation results if these broadcasts are\nperformed synchronously on all processes (Problem 8.11).\nFigure 8.10. Various steps in the Gaussian elimination iteration\ncorresponding to k = 3 for an 8 x 8 matrix on 64 processes arranged in\na logical two-dimensional mesh.\nPipelined Communication and Computation  Based on our experience with Gaussian\nelimination using 1-D partitioning of the coefficient matrix, we develop a pipelined version of\nthe algorithm using 2-D partitioning.\nAs Figure 8.10  shows, in the k th iteration of the outer loop (lines 3\u201316 of Algorithm 8.4  ), A [k\n, k ] is sent to the right from P k , k to Pk , k +1 to P k , k +2 , and so on, until it reaches P k , n -1 .\nProcess P k , k +1 performs the division A [k , k + 1]/ A [k , k ] as soon as it receives A [k , k ]\nfrom P k , k . It does not have to wait for A [k , k ] to reach all the way up to P k , n -1 before\nperforming its local computation. Similarly, any subsequent process P k , j of the k th row can\nperform its division as soon as it receives A [k , k ]. After performing the division, A [k , j ] is\nready to be communicated downward in the j th column. As A [k , j ] moves down, each process\nit passes is free to use it for computation. Processes in the j th column need not wait until A", "doc_id": "968c8d7c-468b-4bfc-aaaa-0c7b877806b9", "embedding": null, "doc_hash": "f43c4491093070605ad2f91703e3c96442ba3a85641bacde3b9f2b6e1e55f698", "extra_info": null, "node_info": {"start": 1011354, "end": 1014543}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "46676383-9b39-4406-94c2-f41198863fb8", "3": "36a82600-daf3-498d-bb8d-a57f5a743221"}}, "__type__": "1"}, "36a82600-daf3-498d-bb8d-a57f5a743221": {"__data__": {"text": "receives A [k , k ]\nfrom P k , k . It does not have to wait for A [k , k ] to reach all the way up to P k , n -1 before\nperforming its local computation. Similarly, any subsequent process P k , j of the k th row can\nperform its division as soon as it receives A [k , k ]. After performing the division, A [k , j ] is\nready to be communicated downward in the j th column. As A [k , j ] moves down, each process\nit passes is free to use it for computation. Processes in the j th column need not wait until A [k ,\nj ] reaches the last process of the column. Thus, P i , j performs the elimination step A [i , j ] :=\nA [i , j ] - A [i , k ] x A [k , j ] as soon as A [i , k ] and A [k , j ] are available. Since some\nprocesses perform the computation for a given iteration earlier than other processes, they start\nworking on subsequent iterations sooner.\nThe communication and computation can be pipelined in several ways. We present one such\nscheme in Figure 8.11  . In Figure 8.11(a) , the iteration of the outer loop for k = 0 starts at\nprocess P 0,0 , when P 0,0 sends A [0, 0] to P 0,1 . Upon receiving A [0, 0], P 0,1 computes A [0, 1]\n:= A [0, 1]/ A [0, 0] (Figure 8.11(b)  ). Now P 0,1 forwards A [0, 0] to P 0,2 and also sends the\nupdated A [0, 1] down to P 1,1 (Figure 8.11(c)  ). At the same time, P 1,0 sends A [1, 0] to P 1,1 .\nHaving received A [0, 1] and A [1, 0], P 1,1 performs the elimination step A [1, 1] := A [1, 1] - A\n[1, 0] x A [0, 1], and having received A [0, 0], P 0,2 performs the division step A [0, 2] := A [0,\n2]/A [0, 0] (Figure 8.11(d)  ). After this computation step, another set of processes (that is,\nprocesses P 0,2 , P1,1 , and P 2,0 ) is ready to initiate communication (Figure 8.11(e)  ).\nFigure 8.11. Pipelined Gaussian elimination for a 5 x 5 matrix with 25\nprocesses.\nAll processes performing communication or computation during a particular iteration lie along a\ndiagonal in the bottom-left to top-right direction (for example, P 0,2 , P1,1 , and P 2,0 performing\ncommunication in Figure 8.11(e)  and P 0,3 , P1,2 , and P 2,1 performing computation in Figure\n8.11(f)  ). As the parallel algorithm progresses, this diagonal moves toward the bottom-right\ncorner of the logical 2-D mesh. Thus, the computation and communication for each iteration\nmoves through the mesh from top-left to bottom-right as a \"front.\" After the front\ncorresponding to a certain iteration passes through a process, the process is free to perform\nsubsequent iterations. For instance, in Figure 8.11(g)  , after the front for k = 0 has passed P 1,1 ,\nit initiates the iteration for k = 1 by sending A [1, 1] to P 1,2 . This initiates a front for k = 1,\nwhich closely follows the front for k = 0. Similarly, a third front for k = 2 starts at P 2,2 (Figure\n8.11(m)  ). Thus, multiple fronts that correspond to different iterations are active\nsimultaneously.\nEvery step of an iteration, such as division,", "doc_id": "36a82600-daf3-498d-bb8d-a57f5a743221", "embedding": null, "doc_hash": "77c4c8bed8b9dc73cd0ffeb19a55b6b9ed5731a92dad785fcd48f32eb8fd9890", "extra_info": null, "node_info": {"start": 1014580, "end": 1017494}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "968c8d7c-468b-4bfc-aaaa-0c7b877806b9", "3": "ac5dabf2-cd55-4a9d-9126-baf8ccd82ad5"}}, "__type__": "1"}, "ac5dabf2-cd55-4a9d-9126-baf8ccd82ad5": {"__data__": {"text": "the process is free to perform\nsubsequent iterations. For instance, in Figure 8.11(g)  , after the front for k = 0 has passed P 1,1 ,\nit initiates the iteration for k = 1 by sending A [1, 1] to P 1,2 . This initiates a front for k = 1,\nwhich closely follows the front for k = 0. Similarly, a third front for k = 2 starts at P 2,2 (Figure\n8.11(m)  ). Thus, multiple fronts that correspond to different iterations are active\nsimultaneously.\nEvery step of an iteration, such as division, elimination, or transmitting a value to a neighboring\nprocess, is a constant-time operation. Therefore, a front moves a single step closer to the\nbottom-right corner of the matrix in constant time (equivalent to two steps of Figure 8.11  ). The\nfront for k = 0 takes time Q (n ) to reach P n - 1,n - 1 after its initiation at P 0,0 . The algorithm\ninitiates n fronts for the n iterations of the outer loop. Each front lags behind the previous one\nby a single step. Thus, the last front passes the bottom-right corner of the matrix Q (n ) steps\nafter the first one. The total time elapsed between the first front starting at P 0,0 and the last one\nfinishing is Q (n ). The procedure is complete after the last front passes the bottom-right corner\nof the matrix; hence, the total parallel run time is Q (n ). Since n 2 process are used, the cost of\nthe pipelined version of Gaussian elimination is Q (n 3 ), which is the same as the sequential run\ntime of the algorithm. Hence, the pipelined version of Gaussian elimination with 2-D partitioning\nis cost-optimal.\n2-D Partitioning with Fewer than  n2 Processes  Consider the case in which p processes are\nused so that p < n 2 and the matrix is mapped onto a \n mesh by using block 2-D\npartitioning. Figure 8.12  illustrates that a typical parallel Gaussian iteration involves a rowwise\nand a columnwise communication of \n  values. Figure 8.13(a) illustrates the load\ndistribution in block 2-D mapping for n = 8 and p = 16.\nFigure 8.12. The communication steps in the Gaussian elimination\niteration corresponding to k = 3 for an 8 x 8 matrix on 16 processes of\na two-dimensional mesh.\nFigure 8.13. Computational load on different processes in block and\ncyclic 2-D mappings of an 8 x 8 matrix onto 16 processes during the\nGaussian elimination iteration corresponding to k = 3.\nFigures 8.12 and 8.13(a)  show that a process containing a completely active part of the matrix\nperforms n 2 /p multiplications and subtractions, and communicates \n  words along its row\nand its column (ignoring the fact that in the last \n  iterations, the active part of the\nmatrix becomes smaller than the size of a block, and no process contains a completely active\npart of the matrix). If the pipelined version of the algorithm is used, the number of arithmetic\noperations per process (2 n 2 /p ) is an order of magnitude higher than the number of words\ncommunicated per process (\n) in each iteration. Thus, for sufficiently large values of n 2\nwith respect to p , the communication in each iteration is dominated by computation. Ignoring\nthe communication cost and assuming that each scalar arithmetic operation takes unit time, the\ntotal parallel run time of this algorithm is (2 n 2 /p ) x n , which is equal to 2 n 3 /p . The process-\ntime product is 2 n 3 , which is three times the cost of the serial algorithm (Equation 8.17  ). As a\nresult, there is an upper bound of 1/3 on the efficiency of the parallel algorithm.\nAs in the case of a block", "doc_id": "ac5dabf2-cd55-4a9d-9126-baf8ccd82ad5", "embedding": null, "doc_hash": "7b53113d426362543a0c60666f905982f092ddefcbf480b29cfca5bdf89331df", "extra_info": null, "node_info": {"start": 1017493, "end": 1020952}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "36a82600-daf3-498d-bb8d-a57f5a743221", "3": "c29e5705-7c84-449e-b712-e3e0cf7f7d30"}}, "__type__": "1"}, "c29e5705-7c84-449e-b712-e3e0cf7f7d30": {"__data__": {"text": "in each iteration. Thus, for sufficiently large values of n 2\nwith respect to p , the communication in each iteration is dominated by computation. Ignoring\nthe communication cost and assuming that each scalar arithmetic operation takes unit time, the\ntotal parallel run time of this algorithm is (2 n 2 /p ) x n , which is equal to 2 n 3 /p . The process-\ntime product is 2 n 3 , which is three times the cost of the serial algorithm (Equation 8.17  ). As a\nresult, there is an upper bound of 1/3 on the efficiency of the parallel algorithm.\nAs in the case of a block 1-D mapping, the inefficiency of Gaussian elimination with a block 2-D\npartitioning of the matrix is due to process idling resulting from an uneven load distribution.\nFigure 8.13(a)  shows the active part of an 8 x 8 matrix of coefficients in the iteration of the\nouter loop for k = 3 when the matrix is block 2-D partitioned among 16 processes. As shown in\nthe figure, seven out of 16 processes are fully idle, five are partially loaded, and only four are\nfully active. By the time half of the iterations of the outer loop have been completed, only one-\nfourth of the processes are active. The remaining idle processes make the parallel algorithm\nmuch costlier than the sequential algorithm.\nThis problem can be alleviated if the matrix is partitioned in a 2-D cyclic fashion as shown in\nFigure 8.13(b)  . With the cyclic 2-D partitioning, the maximum difference in computational load\nbetween any two processes in any iteration is that of one row and one column update. For\nexample, in Figure 8.13(b)  , n 2 /p matrix elements are active in the bottom-right process, and\n(n - 1)2 /p elements are active in the top-left process. The difference in workload between any\ntwo processes is at most Q (\n) in any iteration, which contributes Q (\n ) to the\noverhead function. Since there are n iterations, the cumulative overhead due to process idling is\nonly Q \n  with cyclic mapping in contrast to Q (n 3 ) with block mapping (Problem 8.12).\nIn practical parallel implementations of Gaussian elimination and LU factorization, a block-cyclic\nmapping is used to reduce the overhead due to message startup time associated with a pure\ncyclic mapping and to obtain better serial CPU utilization by performing block-matrix operations\n(Problem 8.15).\nFrom the discussion in this section, we conclude that pipelined parallel Gaussian elimination for\nan n x n matrix takes time Q (n 3 /p ) on p processes with both 1-D and 2-D partitioning\nschemes. 2-D partitioning can use more processes ( O (n 2 )) than 1-D partitioning ( O (n )) for\nan n x n coefficient matrix. Hence, an implementation with 2-D partitioning is more scalable.\n8.3.2 Gaussian Elimination with Partial Pivoting\nThe Gaussian elimination algorithm in Algorithm 8.4  fails if any diagonal entry A [k , k ] of the\nmatrix of coefficients is close or equal to zero. To avoid this problem and to ensure the\nnumerical stability of the algorithm, a technique called partial pivoting  is used. At the\nbeginning of the outer loop in the k th iteration, this method selects a column i (called the pivot\ncolumn) such that A [k , i ] is the largest in magnitude among all A [k , j ] such that k \n j < n\n. It then exchanges the k th and the i th columns before starting the iteration. These columns\ncan either be exchanged explicitly by physically moving them into each other's locations, or\nthey can be exchanged implicitly by simply maintaining an n x 1 permutation vector to keep\ntrack of the new indices of", "doc_id": "c29e5705-7c84-449e-b712-e3e0cf7f7d30", "embedding": null, "doc_hash": "f7b0bd40951a2ca2ce4655ad59d405cb85dd764c1e03fc4424990df393b24c42", "extra_info": null, "node_info": {"start": 1020886, "end": 1024403}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ac5dabf2-cd55-4a9d-9126-baf8ccd82ad5", "3": "8f6e8d3b-77ff-481a-9c2e-f07de5bd7f5f"}}, "__type__": "1"}, "8f6e8d3b-77ff-481a-9c2e-f07de5bd7f5f": {"__data__": {"text": "technique called partial pivoting  is used. At the\nbeginning of the outer loop in the k th iteration, this method selects a column i (called the pivot\ncolumn) such that A [k , i ] is the largest in magnitude among all A [k , j ] such that k \n j < n\n. It then exchanges the k th and the i th columns before starting the iteration. These columns\ncan either be exchanged explicitly by physically moving them into each other's locations, or\nthey can be exchanged implicitly by simply maintaining an n x 1 permutation vector to keep\ntrack of the new indices of the columns of A . If partial pivoting is performed with an implicit\nexchange of column indices, then the factors L and U are not exactly triangular matrices, but\ncolumnwise permutations of triangular matrices.\nAssuming that columns are exchanged explicitly, the value of A [k , k ] used as the divisor on\nline 6 of Algorithm 8.4  (after exchanging columns k and i ) is greater than or equal to any A [k ,\nj ] that it divides in the k th iteration. Partial pivoting in Algorithm 8.4  results in a unit upper-\ntriangular matrix in which all elements above the principal diagonal have an absolute value of\nless than one.\n1-D Partitioning\nPerforming partial pivoting is straightforward with rowwise partitioning as discussed in Section\n8.3.1  . Before performing the divide operation in the k th iteration, the process storing the k th\nrow makes a comparison pass over the active portion of this row, and selects the element with\nthe largest absolute value as the divisor. This element determines the pivot column, and all\nprocesses must know the index of this column. This information can be passed on to the rest of\nthe processes along with the modified (after the division) elements of the k th row. The\ncombined pivot-search and division step takes time Q (n - k - 1) in the k th iteration, as in case\nof Gaussian elimination without pivoting. Thus, partial pivoting has no significant effect on the\nperformance of Algorithm 8.4  if the coefficient matrix is partitioned along the rows.\nNow consider a columnwise 1-D partitioning of the coefficient matrix. In the absence of\npivoting, parallel implementations of Gaussian elimination with rowwise and columnwise 1-D\npartitioning are almost identical (Problem 8.9). However, the two are significantly different if\npartial pivoting is performed.\nThe first difference is that, unlike rowwise partitioning, the pivot search is distributed in\ncolumnwise partitioning. If the matrix size is n x n and the number of processes is p , then the\npivot search in columnwise partitioning involves two steps. During pivot search for the k th\niteration, first each process determines the maximum of the n /p (or fewer) elements of the k th\nrow that it stores. The next step is to find the maximum of the resulting p (or fewer) values, and\nto distribute the maximum among all processes. Each pivot search takes time Q (n /p ) + Q (log\np ). For sufficiently large values of n with respect to p , this is less than the time Q (n ) it takes\nto perform a pivot search with rowwise partitioning. This seems to suggest that a columnwise\npartitioning is better for partial pivoting that a rowwise partitioning. However, the following\nfactors favor rowwise partitioning.\nFigure 8.7  shows how communication and computation \"fronts\" move from top to bottom in the\npipelined version of Gaussian elimination with rowwise 1-D partitioning. Similarly, the\ncommunication and computation fronts move from left to right in case of columnwise 1-D\npartitioning. This means that the ( k + 1)th row is not ready for pivot search for the ( k +", "doc_id": "8f6e8d3b-77ff-481a-9c2e-f07de5bd7f5f", "embedding": null, "doc_hash": "ec3ebbe527c28e75fd599765c5fd088f25938967988fe862979acbead280c45a", "extra_info": null, "node_info": {"start": 1024413, "end": 1028026}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c29e5705-7c84-449e-b712-e3e0cf7f7d30", "3": "e2d50b56-6c66-479f-8bb8-43afe8c946b4"}}, "__type__": "1"}, "e2d50b56-6c66-479f-8bb8-43afe8c946b4": {"__data__": {"text": "search with rowwise partitioning. This seems to suggest that a columnwise\npartitioning is better for partial pivoting that a rowwise partitioning. However, the following\nfactors favor rowwise partitioning.\nFigure 8.7  shows how communication and computation \"fronts\" move from top to bottom in the\npipelined version of Gaussian elimination with rowwise 1-D partitioning. Similarly, the\ncommunication and computation fronts move from left to right in case of columnwise 1-D\npartitioning. This means that the ( k + 1)th row is not ready for pivot search for the ( k + 1)th\niteration (that is, it is not fully updated) until the front corresponding to the k th iteration\nreaches the rightmost process. As a result, the ( k + 1)th iteration cannot start until the entire k\nth iteration is complete. This effectively eliminates pipelining, and we are therefore forced to use\nthe synchronous version with poor efficiency.\nWhile performing partial pivoting, columns of the coefficient matrix may or may not be explicitly\nexchanged. In either case, the performance of Algorithm 8.4  is adversely affected with\ncolumnwise 1-D partitioning. Recall that cyclic or block-cyclic mappings result in a better load\nbalance in Gaussian elimination than a block mapping. A cyclic mapping ensures that the active\nportion of the matrix is almost uniformly distributed among the processes at every stage of\nGaussian elimination. If pivot columns are not exchanged explicitly, then this condition may\ncease to hold. After a pivot column is used, it no longer stays in the active portion of the matrix.\nAs a result of pivoting without explicit exchange, columns are arbitrarily removed from the\ndifferent processes' active portions of the matrix. This randomness may disturb the uniform\ndistribution of the active portion. On the other hand, if columns belonging to different processes\nare exchanged explicitly, then this exchange requires communication between the processes. A\nrowwise 1-D partitioning neither requires communication for exchanging columns, nor does it\nlose the load-balance if columns are not exchanged explicitly.\n2-D Partitioning\nIn the case of 2-D partitioning of the coefficient matrix, partial pivoting seriously restricts\npipelining, although it does not completely eliminate it. Recall that in the pipelined version of\nGaussian elimination with 2-D partitioning, fronts corresponding to various iterations move\nfrom top-left to bottom-right. The pivot search for the ( k + 1)th iteration can commence as\nsoon as the front corresponding to the k th iteration has moved past the diagonal of the active\nmatrix joining its top-right and bottom-left corners.\nThus, partial pivoting may lead to considerable performance degradation in parallel Gaussian\nelimination with 2-D partitioning. If numerical considerations allow, it may be possible to\nreduce the performance loss due to partial pivoting. We can restrict the search for the pivot in\nthe k th iteration to a band of q columns (instead of all n - k columns). In this case, the i th\ncolumn is selected as the pivot in the k th iteration if A [k , i ] is the largest element in a band\nof q elements of the active part of the i th row. This restricted partial pivoting not only reduces\nthe communication cost, but also permits limited pipelining. By restricting the number of\ncolumns for pivot search to q , an iteration can start as soon as the previous iteration has\nupdated the first q + 1 columns.\nAnother way to get around the loss of pipelining due to partial pivoting in Gaussian elimination\nwith 2-D partitioning is to use fast algorithms for one-to-all broadcast, such as those described\nin Section 4.7.1 . With 2-D partitioning of the n x n", "doc_id": "e2d50b56-6c66-479f-8bb8-43afe8c946b4", "embedding": null, "doc_hash": "6cd60263a61b9dbacaa8bf7b1e743983a71da9189c2d837b7b8c134c13149c7c", "extra_info": null, "node_info": {"start": 1027998, "end": 1031699}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8f6e8d3b-77ff-481a-9c2e-f07de5bd7f5f", "3": "3370ad48-11e5-491f-a2a4-998124489f56"}}, "__type__": "1"}, "3370ad48-11e5-491f-a2a4-998124489f56": {"__data__": {"text": "q elements of the active part of the i th row. This restricted partial pivoting not only reduces\nthe communication cost, but also permits limited pipelining. By restricting the number of\ncolumns for pivot search to q , an iteration can start as soon as the previous iteration has\nupdated the first q + 1 columns.\nAnother way to get around the loss of pipelining due to partial pivoting in Gaussian elimination\nwith 2-D partitioning is to use fast algorithms for one-to-all broadcast, such as those described\nin Section 4.7.1 . With 2-D partitioning of the n x n coefficient matrix on p processes, a process\nspends time Q (\n) in communication in each iteration of the pipelined version of Gaussian\nelimination. Disregarding the message startup time ts , a non-pipelined version that performs\nexplicit one-to-all broadcasts using the algorithm of Section 4.1  spends time Q ((\n ) log p )\ncommunicating in each iteration. This communication time is higher than that of the pipelined\nversion. The one-to-all broadcast algorithms described in Section 4.7.1  take time Q (\n ) in\neach iteration (disregarding the startup time). This time is asymptotically equal to the per-\niteration communication time of the pipelined algorithm. Hence, using a smart algorithm to\nperform one-to-all broadcast, even non-pipelined parallel Gaussian elimination can attain\nperformance comparable to that of the pipelined algorithm. However, the one-to-all broadcast\nalgorithms described in Section 4.7.1  split a message into smaller parts and route them\nseparately. For these algorithms to be effective, the sizes of the messages should be large\nenough; that is, n should be large compared to p .\nAlthough pipelining and pivoting do not go together in Gaussian elimination with 2-D\npartitioning, the discussion of 2-D partitioning in this section is still useful. With some\nmodification, it applies to the Cholesky factorization algorithm (Algorithm 8.6  in Problem 8.16),\nwhich does not require pivoting. Cholesky factorization applies only to symmetric, positive\ndefinite matrices. A real n x n matrix A is positive definite  if xT Ax > 0 for any n x 1 nonzero,\nreal vector x . The communication pattern in Cholesky factorization is quite similar to that of\nGaussian elimination (Problem 8.16), except that, due to symmetric lower and upper-triangular\nhalves in the matrix, Cholesky factorization uses only one triangular half of the matrix.\n8.3.3 Solving a Triangular System: Back-Substitution\nWe now briefly discuss the second stage of solving a system of linear equations. After the full\nmatrix A has been reduced to an upper-triangular matrix U with ones along the principal\ndiagonal, we perform back-substitution to determine the vector x . A sequential back-\nsubstitution algorithm for solving an upper-triangular system of equations Ux = y is shown in\nAlgorithm 8.5  .\nStarting with the last equation, each iteration of the main loop (lines 3\u20138) of Algorithm 8.5\ncomputes the values of a variable and substitutes the variable's value back into the remaining\nequations. The program performs approximately n 2 /2 multiplications and subtractions. Note\nthat the number of arithmetic operations in back-substitution is less than that in Gaussian\nelimination by a factor of Q (n ). Hence, if back-substitution is used in conjunction with\nGaussian elimination, it is best to use the matrix partitioning scheme that is the most efficient\nfor parallel Gaussian elimination.\nAlgorithm 8.5 A serial algorithm for back-substitution. U is an upper-\ntriangular matrix with all entries of the principal diagonal equal to\none, and", "doc_id": "3370ad48-11e5-491f-a2a4-998124489f56", "embedding": null, "doc_hash": "b7c932c706a623c85b9f9f649b5fbc14b9ed055be65a276e2aab1b2470123aba", "extra_info": null, "node_info": {"start": 1031713, "end": 1035313}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e2d50b56-6c66-479f-8bb8-43afe8c946b4", "3": "6712f101-8220-4e6a-a6dd-df52d3c8178b"}}, "__type__": "1"}, "6712f101-8220-4e6a-a6dd-df52d3c8178b": {"__data__": {"text": "The program performs approximately n 2 /2 multiplications and subtractions. Note\nthat the number of arithmetic operations in back-substitution is less than that in Gaussian\nelimination by a factor of Q (n ). Hence, if back-substitution is used in conjunction with\nGaussian elimination, it is best to use the matrix partitioning scheme that is the most efficient\nfor parallel Gaussian elimination.\nAlgorithm 8.5 A serial algorithm for back-substitution. U is an upper-\ntriangular matrix with all entries of the principal diagonal equal to\none, and all subdiagonal entries equal to zero.\n1.   procedure  BACK_SUBSTITUTION ( U, x, y) \n2.   begin \n3.      for k := n - 1 downto 0 do /* Main loop */ \n4.          begin \n5.             x[k] := y[k]; \n6.             for i := k - 1 downto 0 do \n7.                 y[i] := y[i] - x[k] x U[i, k]; \n8.          endfor; \n9.   end BACK_SUBSTITUTION \nConsider a rowwise block 1-D mapping of the n x n matrix U onto p processes. Let the vector y\nbe distributed uniformly among all the processes. The value of the variable solved in a typical\niteration of the main loop (line 3) must be sent to all the processes with equations involving\nthat variable. This communication can be pipelined (Problem 8.22). If so, the time to perform\nthe computations of an iteration dominates the time that a process spends in communication in\nan iteration. In every iteration of a pipelined implementation, a process receives (or generates)\nthe value of a variable and sends that value to another process. Using the value of the variable\nsolved in the current iteration, a process also performs up to n /p multiplications and\nsubtractions (lines 6 and 7). Hence, each step of a pipelined implementation requires a constant\namount of time for communication and time Q (n /p ) for computation. The algorithm\nterminates in Q (n ) steps (Problem 8.22), and the parallel run time of the entire algorithm is Q\n(n 2 /p ).\nIf the matrix is partitioned by using 2-D partitioning on a \n logical mesh of processes,\nand the elements of the vector are distributed along one of the columns of the process mesh,\nthen only the \n  processes containing the vector perform any computation. Using pipelining to\ncommunicate the appropriate elements of U to the process containing the corresponding\nelements of y for the substitution step (line 7), the algorithm can be executed in time Q\n (Problem 8.22). Thus, the cost of parallel back-substitution with 2-D mapping is Q\n. The algorithm is not cost-optimal because its sequential cost is only Q (n 2 ). However,\nthe entire process of solving the linear system, including upper-triangularization using Gaussian\nelimination, is still cost-optimal for \n  because the sequential complexity of the entire\nprocess is Q (n 3 ).\n8.3.4 Numerical Considerations in Solving Systems of Linear Equations\nA system of linear equations of the form Ax = b can be solved by using a factorization algorithm\nto express A as the product of a lower-triangular matrix L , and a unit upper-triangular matrix U\n. The system of equations is then rewritten as LU x = b , and is solved in two steps. First, the\nlower-triangular system Ly = b is solved for y . Second, the upper-triangular system Ux = y is\nsolved for x", "doc_id": "6712f101-8220-4e6a-a6dd-df52d3c8178b", "embedding": null, "doc_hash": "a889ad53dd08e98a29184865b3659f111f96a918a0ec3c6e38346d2ee6fda70f", "extra_info": null, "node_info": {"start": 1035314, "end": 1038552}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3370ad48-11e5-491f-a2a4-998124489f56", "3": "0808bf88-d232-4e29-ae4d-fdf81cd8497e"}}, "__type__": "1"}, "0808bf88-d232-4e29-ae4d-fdf81cd8497e": {"__data__": {"text": "the sequential complexity of the entire\nprocess is Q (n 3 ).\n8.3.4 Numerical Considerations in Solving Systems of Linear Equations\nA system of linear equations of the form Ax = b can be solved by using a factorization algorithm\nto express A as the product of a lower-triangular matrix L , and a unit upper-triangular matrix U\n. The system of equations is then rewritten as LU x = b , and is solved in two steps. First, the\nlower-triangular system Ly = b is solved for y . Second, the upper-triangular system Ux = y is\nsolved for x .\nThe Gaussian elimination algorithm given in Algorithm 8.4  effectively factorizes A into L and U .\nHowever, it also solves the lower-triangular system Ly = b on the fly by means of steps on lines\n7 and 13. Algorithm 8.4  gives what is called a row-oriented  Gaussian elimination algorithm. In\nthis algorithm, multiples of rows are subtracted from other rows. If partial pivoting, as\ndescribed in Section 8.3.2  , is incorporated into this algorithm, then the resulting upper-\ntriangular matrix U has all its elements less than or equal to one in magnitude. The lower-\ntriangular matrix L , whether implicit or explicit, may have elements with larger numerical\nvalues. While solving the system Ax = b , the triangular system Ly = b is solved first. If L\ncontains large elements, then rounding errors can occur while solving for y due to the finite\nprecision of floating-point numbers in the computer. These errors in y are propagated through\nthe solution of Ux = y .\nAn alternate form of Gaussian elimination is the column-oriented  form that can be obtained\nfrom Algorithm 8.4  by reversing the roles of rows and columns. In the column-oriented\nalgorithm, multiples of columns are subtracted from other columns, pivot search is also\nperformed along the columns, and numerical stability is guaranteed by row interchanges, if\nneeded. All elements of the lower-triangular matrix L generated by the column-oriented\nalgorithm have a magnitude less than or equal to one. This minimizes numerical error while\nsolving Ly = b , and results in a significantly smaller error in the overall solution than the row-\noriented algorithm. Algorithm 3.3  gives a procedure for column-oriented LU factorization.\nFrom a practical point of view, the column-oriented Gaussian elimination algorithm is more\nuseful than the row-oriented algorithm. We have chosen to present the row-oriented algorithm\nin detail in this chapter because it is more intuitive. It is easy to see that the system of linear\nequations resulting from the subtraction of a multiple of an equation from other equations is\nequivalent to the original system. The entire discussion on the row-oriented algorithm of\nAlgorithm 8.4  presented in this section applies to the column-oriented algorithm with the roles\nof rows and columns reversed. For example, columnwise 1-D partitioning is more suitable than\nrowwise 1-D partitioning for the column-oriented algorithm with partial pivoting.\n[ Team LiB ]\n \n\n[ Team LiB ]\n  \n8.4 Bibliographic Remarks\nMatrix transposition with 1-D partitioning is essentially an all-to-all personalized communication\nproblem [ Ede89 ]. Hence, all the references in Chapter 4  for all-to-all personalized\ncommunication apply directly to matrix transposition. The recursive transposition algorithm,\npopularly known as RTA, was first reported by Eklundh [ Ekl72]. Its adaptations for hypercubes\nhave been described by Bertsekas and Tsitsiklis [ BT97], Fox and Furmanski [ FF86], Johnsson\n[Joh87 ], and McBryan and Van de Velde [ MdV87 ] for one-port communication on each process.\nJohnsson", "doc_id": "0808bf88-d232-4e29-ae4d-fdf81cd8497e", "embedding": null, "doc_hash": "3a3b6f5a118a3c1426c6d05cebea094536a52fb054de80323fe2cd87d930d974", "extra_info": null, "node_info": {"start": 1038585, "end": 1042177}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6712f101-8220-4e6a-a6dd-df52d3c8178b", "3": "c1bc2a4c-01ce-46d7-ad94-5adbb3faedce"}}, "__type__": "1"}, "c1bc2a4c-01ce-46d7-ad94-5adbb3faedce": {"__data__": {"text": "is essentially an all-to-all personalized communication\nproblem [ Ede89 ]. Hence, all the references in Chapter 4  for all-to-all personalized\ncommunication apply directly to matrix transposition. The recursive transposition algorithm,\npopularly known as RTA, was first reported by Eklundh [ Ekl72]. Its adaptations for hypercubes\nhave been described by Bertsekas and Tsitsiklis [ BT97], Fox and Furmanski [ FF86], Johnsson\n[Joh87 ], and McBryan and Van de Velde [ MdV87 ] for one-port communication on each process.\nJohnsson [ Joh87 ] also discusses parallel RTA for hypercubes that permit simultaneous\ncommunication on all channels. Further improvements on the hypercube RTA have been\nsuggested by Ho and Raghunath [ HR91], Johnsson and Ho [ JH88], Johnsson [ Joh90 ], and Stout\nand Wagar [ SW87 ].\nA number of sources of parallel dense linear algebra algorithms, including those for matrix-\nvector multiplication and matrix multiplication, are available [ CAHH91, GPS90 , GL96a , Joh87 ,\nMod88 , OS85 ]. Since dense matrix multiplication is highly computationally intensive, there has\nbeen a great deal of interest in developing parallel formulations of this algorithm and in testing\nits performance on various parallel architectures [ Akl89, Ber89 , CAHH91 , Can69 , Cha79 , CS88 ,\nDNS81 , dV89, FJL+88, FOH87 , GK91 , GL96a , Hip89 , HJE91 , Joh87 , PV80, Tic88 ]. Some of the\nearly parallel formulations of matrix multiplication were developed by Cannon [ Can69 ], Dekel,\nNassimi, and Sahni [ DNS81 ], and Fox et al. [FOH87 ]. Variants and improvements of these\nalgorithms have been presented by Berntsen [ Ber89 ], and by Ho, Johnsson, and Edelman\n[HJE91 ]. In particular, Berntsen [ Ber89 ] presents an algorithm that has strictly smaller\ncommunication overhead than Cannon's algorithm, but has a smaller degree of concurrency.\nHo, Johnsson, and Edelman [ HJE91] present another variant of Cannon's algorithm for a\nhypercube that permits communication on all channels simultaneously. This algorithm, while\nreducing communication, also reduces the degree of concurrency. Gupta and Kumar [ GK91]\npresent a detailed scalability analysis of several matrix multiplication algorithms. They present\nan analysis to determine the best algorithm to multiply two n x n matrices on a p-process\nhypercube for different ranges of n, p and the hardware-related constants. They also show that\nthe improvements suggested by Berntsen and Ho et al. do not improve the overall scalability of\nmatrix multiplication on a hypercube.\nParallel algorithms for LU factorization and solving dense systems of linear equations have been\ndiscussed by several researchers [ Ber84, BT97, CG87 , Cha87 , Dav86 , DHvdV93 , FJL+88, Gei85 ,\nGH86 , GPS90 , GR88 , Joh87 , LD90, Lei92 , Mod88 , Mol86  OR88 , Ort88 , OS86 , PR85, Rob90 ,\nSaa86 , Vav89 ]. Geist and Heath [ GH85 , GH86 ], and Heath [ Hea85 ] specifically concentrate on\nparallel dense Cholesky factorization. Parallel algorithms for solving triangular systems have\nalso been studied in detail [ EHHR88, HR88 , LC88, LC89, RO88 , Rom87 ]. Demmel, Heath, and\nvan der Vorst [DHvdV93 ] present a comprehensive survey of parallel matrix computations\nconsidering numerical implications in detail.\nA portable software implementation of all matrix and", "doc_id": "c1bc2a4c-01ce-46d7-ad94-5adbb3faedce", "embedding": null, "doc_hash": "55cd84b773d259247d6b19ea644d6cecebac749c29ac5220707ba13e497a25a4", "extra_info": null, "node_info": {"start": 1042162, "end": 1045440}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0808bf88-d232-4e29-ae4d-fdf81cd8497e", "3": "f7e65ac7-d522-407c-a838-7807804831ca"}}, "__type__": "1"}, "f7e65ac7-d522-407c-a838-7807804831ca": {"__data__": {"text": "PR85, Rob90 ,\nSaa86 , Vav89 ]. Geist and Heath [ GH85 , GH86 ], and Heath [ Hea85 ] specifically concentrate on\nparallel dense Cholesky factorization. Parallel algorithms for solving triangular systems have\nalso been studied in detail [ EHHR88, HR88 , LC88, LC89, RO88 , Rom87 ]. Demmel, Heath, and\nvan der Vorst [DHvdV93 ] present a comprehensive survey of parallel matrix computations\nconsidering numerical implications in detail.\nA portable software implementation of all matrix and vector operations discussed in this\nchapter, and many more, is available as PBLAS (parallel basic linear algebra subroutines)\n[C+95]. The ScaLAPACK library [ B+97] uses PBLAS to implement a variety of linear algebra\nroutines of practical importance, including procedures for various methods of matrix\nfactorizations and solving systems of linear equations.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n \nProblems\n8.1 Consider the two algorithms for all-to-all personalized communication in Section 4.5.3  .\nWhich method would you use on a 64-node parallel computer with Q (p ) bisection width for\ntransposing a 1024 x 1024 matrix with the 1-D partitioning if ts = 100 \u00b5 s and tw = 1\u00b5 s? Why?\n8.2 Describe a parallel formulation of matrix-vector multiplication in which the matrix is 1-D\nblock-partitioned along the columns and the vector is equally partitioned among all the\nprocesses. Show that the parallel run time is the same as in case of rowwise 1-D block\npartitioning.\nHint:  The basic communication operation used in the case of columnwise 1-D partitioning is\nall-to-all reduction, as opposed to all-to-all broadcast in the case of rowwise 1-D partitioning.\nProblem 4.8 describes all-to-all reduction.\n8.3 Section 8.1.2  describes and analyzes matrix-vector multiplication with 2-D partitioning. If\n, then suggest ways of improving the parallel run time to\n. Is the improved method more scalable than the one used in\nSection 8.1.2  ?\n8.4 The overhead function for multiplying an n x n 2-D partitioned matrix with an n x 1 vector\nusing p processes is \n  (Equation 8.8  ). Substituting this expression in\nEquation 5.14  yields a quadratic equation in n . Using this equation, determine the precise\nisoefficiency function for the parallel algorithm and compare it with Equations 8.9  and 8.10 .\nDoes this comparison alter the conclusion that the term associated with tw is responsible for the\noverall isoefficiency function of this parallel algorithm?\n8.5 Strassen's method [AHU74  , CLR90  ] for matrix multiplication is an algorithm based on the\ndivide-and-conquer technique. The sequential complexity of multiplying two n x n matrices\nusing Strassen's algorithm is Q (n 2.81 ). Consider the simple matrix multiplication algorithm\n(Section 8.2.1  ) for multiplying two n x n matrices using p processes. Assume that the\n submatrices are multiplied using Strassen's algorithm at each process. Derive\nan expression for the parallel run time of this algorithm. Is the parallel algorithm cost-optimal?\n8.6 (DNS algorithm with fewer than  n 3 processes [  DNS81 ] ) Section 8.2.3  describes a\nparallel formulation of the DNS algorithm that uses fewer than n 3 processes. Another variation\nof this algorithm works with p = n 2 q processes, where 1 \n q \n n . Here the process\narrangement is regarded as a q x q x q logical", "doc_id": "f7e65ac7-d522-407c-a838-7807804831ca", "embedding": null, "doc_hash": "fa210a567b57d404251743fa6cb5774ce0859ee15783d169643a0e0140796081", "extra_info": null, "node_info": {"start": 1045480, "end": 1048772}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c1bc2a4c-01ce-46d7-ad94-5adbb3faedce", "3": "5b4d3233-e376-462a-b0f0-0c4763563f66"}}, "__type__": "1"}, "5b4d3233-e376-462a-b0f0-0c4763563f66": {"__data__": {"text": "Assume that the\n submatrices are multiplied using Strassen's algorithm at each process. Derive\nan expression for the parallel run time of this algorithm. Is the parallel algorithm cost-optimal?\n8.6 (DNS algorithm with fewer than  n 3 processes [  DNS81 ] ) Section 8.2.3  describes a\nparallel formulation of the DNS algorithm that uses fewer than n 3 processes. Another variation\nof this algorithm works with p = n 2 q processes, where 1 \n q \n n . Here the process\narrangement is regarded as a q x q x q logical three-dimensional array of \"superprocesses,\" in\nwhich each superprocess is an ( n /q ) x (n /q ) mesh of processes. This variant can be viewed\nas identical to the block variant described in Section 8.2.3  , except that the role of each process\nis now assumed by an ( n /q ) x (n /q ) logical mesh of processes. This means that each block\nmultiplication of ( n /q ) x (n /q ) submatrices is performed in parallel by ( n /q )2 processes\nrather than by a single process. Any of the algorithms described in Sections 8.2.1  or 8.2.2  can\nbe used to perform this multiplication.\nDerive an expression for the parallel run time for this variant of the DNS algorithm in terms of\nn , p , ts , and tw . Compare the expression with Equation 8.16  . Discuss the relative merits and\ndrawbacks of the two variations of the DNS algorithm for fewer than n 3 processes.\n8.7 Figure 8.7  shows that the pipelined version of Gaussian elimination requires 16 steps for a\n5 x 5 matrix partitioned rowwise on five processes. Show that, in general, the algorithm\nillustrated in this figure completes in 4( n - 1) steps for an n x n matrix partitioned rowwise with\none row assigned to each process.\n8.8 Describe in detail a parallel implementation of the Gaussian elimination algorithm of\nAlgorithm 8.4  without pivoting if the n x n coefficient matrix is partitioned columnwise among p\nprocesses. Consider both pipelined and non-pipelined implementations. Also consider the cases\np = n and p < n .\nHint:  The parallel implementation of Gaussian elimination described in Section 8.3.1  shows\nhorizontal and vertical communication on a logical two-dimensional mesh of processes (Figure\n8.12 ). A rowwise partitioning requires only the vertical part of this communication. Similarly,\ncolumnwise partitioning performs only the horizontal part of this communication.\n8.9 Derive expressions for the parallel run times of all the implementations in Problem 8.8. Is\nthe run time of any of these parallel implementations significantly different from the\ncorresponding implementation with rowwise 1-D partitioning?\n8.10 Rework Problem 8.9 with partial pivoting. In which implementations are the parallel run\ntimes significantly different for rowwise and columnwise partitioning?\n8.11 Show that Gaussian elimination on an n x n matrix 2-D partitioned on an n x n logical\nmesh of processes is not cost-optimal if the 2 n one-to-all broadcasts are performed\nsynchronously.\n8.12 Show that the cumulative idle time over all the processes in the Gaussian elimination\nalgorithm is Q (n 3 ) for a block mapping, whether the n x n matrix is partitioned along one or\nboth dimensions. Show that this idle time is reduced to Q (n 2 p ) for cyclic 1-D mapping and Q\n  for cyclic 2-D mapping.\n8.13 Prove that the isoefficiency function of the asynchronous version of the Gaussian\nelimination with 2-D mapping is Q (p 3/2 ) if pivoting is not performed.\n8.14 Derive", "doc_id": "5b4d3233-e376-462a-b0f0-0c4763563f66", "embedding": null, "doc_hash": "d316506f2e6fac3beb7b78716b3fef367e02b99e20b0aedc91a9af87002ee348", "extra_info": null, "node_info": {"start": 1048765, "end": 1052192}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f7e65ac7-d522-407c-a838-7807804831ca", "3": "f4bc6c1d-5d37-45ed-8735-ca10e2fe0744"}}, "__type__": "1"}, "f4bc6c1d-5d37-45ed-8735-ca10e2fe0744": {"__data__": {"text": "Show that the cumulative idle time over all the processes in the Gaussian elimination\nalgorithm is Q (n 3 ) for a block mapping, whether the n x n matrix is partitioned along one or\nboth dimensions. Show that this idle time is reduced to Q (n 2 p ) for cyclic 1-D mapping and Q\n  for cyclic 2-D mapping.\n8.13 Prove that the isoefficiency function of the asynchronous version of the Gaussian\nelimination with 2-D mapping is Q (p 3/2 ) if pivoting is not performed.\n8.14 Derive precise expressions for the parallel run time of Gaussian elimination with and\nwithout partial pivoting if the n x n matrix of coefficients is partitioned among p processes of a\nlogical square two-dimensional mesh in the following formats:\nRowwise block 1-D partitioning.a.\nRowwise cyclic 1-D partitioning.b.\nColumnwise block 1-D partitioning.c.\nColumnwise cyclic 1-D partitioning.d.\n8.15 Rewrite Algorithm 8.4  in terms of block matrix operations as discussed at the beginning\nof Section 8.2  . Consider Gaussian elimination of an n x n matrix partitioned into a q x q array\nof submatrices, where each submatrix is of size of n /q x n /q . This array of blocks is mapped\nonto a logical \n  mesh of processes in a cyclic manner, resulting in a 2-D block cyclic\nmapping of the original matrix onto the mesh. Assume that \n  and that n is\ndivisible by q , which in turn is divisible by \n . Derive expressions for the parallel run time for\nboth synchronous and pipelined versions of Gaussian elimination.\nHint:  The division step A [k , j ] := A [k , j ]/A [k , k ] is replaced by submatrix operation\n, where Ak , k is the lower triangular part of the k th diagonal submatrix.\n8.16 (Cholesky factorization)  Algorithm 8.6 describes a row-oriented version of the\nCholesky factorization algorithm for factorizing a symmetric positive definite matrix into the\nform A = UT U . Cholesky factorization does not require pivoting. Describe a pipelined parallel\nformulation of this algorithm that uses 2-D partitioning of the matrix on a square mesh of\nprocesses. Draw a picture similar to Figure 8.11  .\nAlgorithm 8.6 A row-oriented Cholesky factorization algorithm.\n1.   procedure  CHOLESKY ( A) \n2.   begin \n3.      for k := 0 to n - 1 do \n4.          begin \n5.             \n6.             for j := k + 1 to n - 1 do \n7.                 A[k, j] := A[k, j]/A[k, k]; \n8.             for i := k + 1 to n - 1 do \n9.                 for j := i to n - 1 do \n10.                    A[i, j] := A[i, j] - A[k, i] x A[k, j]; \n11.         endfor;       /*Line3*/ \n12.  end CHOLESKY \n8.17 (Scaled speedup)  Scaled speedup (Section 5.7  ) is defined as the speedup obtained\nwhen the problem size is increased linearly with the number of processes; that is, if W is\nchosen as a base problem size for a single process,", "doc_id": "f4bc6c1d-5d37-45ed-8735-ca10e2fe0744", "embedding": null, "doc_hash": "c02326de04a18d68f81b579b82d9e90ea605f1bcda2e830a954d3b8073370fd1", "extra_info": null, "node_info": {"start": 1052224, "end": 1054992}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5b4d3233-e376-462a-b0f0-0c4763563f66", "3": "58431ef2-58ed-4313-9ed1-32cbc99c9894"}}, "__type__": "1"}, "58431ef2-58ed-4313-9ed1-32cbc99c9894": {"__data__": {"text": "do \n10.                    A[i, j] := A[i, j] - A[k, i] x A[k, j]; \n11.         endfor;       /*Line3*/ \n12.  end CHOLESKY \n8.17 (Scaled speedup)  Scaled speedup (Section 5.7  ) is defined as the speedup obtained\nwhen the problem size is increased linearly with the number of processes; that is, if W is\nchosen as a base problem size for a single process, then\nEquation 8.19\nFor the simple matrix multiplication algorithm described in Section 8.2.1  , plot the standard\nand scaled speedup curves for the base problem of multiplying 16 x 16 matrices. Use p = 1, 4,\n16, 64, and 256. Assume that ts = 10 and tw = 1 in Equation 8.14  .\n8.18 Plot a third speedup curve for Problem 8.17, in which the problem size is scaled up\naccording to the isoefficiency function, which is Q (p 3/2 ). Use the same values of ts and tw .\nHint:  The scaled speedup under this method of scaling is\n8.19 Plot the efficiency curves for the simple matrix multiplication algorithm corresponding to\nthe standard speedup curve (Problem 8.17), the scaled speedup curve (Problem 8.17), and the\nspeedup curve when the problem size is increased according to the isoefficiency function\n(Problem 8.18).\n8.20 A drawback of increasing the number of processes without increasing the total workload\nis that the speedup does not increase linearly with the number of processes, and the efficiency\ndrops monotonically. Based on your experience with Problems 8.17 and 8.19, discuss whether\nusing scaled speedup instead of standard speedup solves the problem in general. What can you\nsay about the isoefficiency function of a parallel algorithm whose scaled speedup curve matches\nthe speedup curve determined by increasing the problem size according to the isoefficiency\nfunction?\n8.21 (Time-constrained scaling)  Assume that ts = 10 and tw = 1 in the expression of parallel\nexecution time (Equation 8.14  ) of the matrix-multiplication algorithm discussed in Section\n8.2.1  . For p = 1, 4, 16, 64, 256, 1024, and 4096, what is the largest problem that can be\nsolved if the total run time is not to exceed 512 time units? In general, is it possible to solve an\narbitrarily large problem in a fixed amount of time, provided that an unlimited number of\nprocesses is available? Give a brief explanation.\n8.22 Describe a pipelined algorithm for performing back-substitution to solve a triangular\nsystem of equations of the form Ux = y , where the n x n unit upper-triangular matrix U is 2-D\npartitioned onto an n x n mesh of processes. Give an expression for the parallel run time of the\nalgorithm. Modify the algorithm to work on fewer than n 2 processes, and derive an expression\nfor the parallel execution time of the modified algorithm.\n8.23 Consider the parallel algorithm given in Algorithm 8.7  for multiplying two n x n matrices\nA and B to obtain the product matrix C . Assume that it takes time tlocal for a memory read or\nwrite operation on a matrix element and time tc to add and multiply two numbers. Determine\nthe parallel run time for this algorithm on an n 2 -processor CREW PRAM. Is this parallel\nalgorithm cost-optimal?\n8.24 Assuming that concurrent read accesses to a memory location are serialized on an EREW\nPRAM, derive the parallel run time of the algorithm given in Algorithm 8.7  on an n 2 -processor\nEREW PRAM. Is this algorithm cost-optimal on an EREW PRAM?\nAlgorithm", "doc_id": "58431ef2-58ed-4313-9ed1-32cbc99c9894", "embedding": null, "doc_hash": "e16e884cd6413a89799824cbcefd251889ce344ae6cb7f25a4d02cb07c7bb4ff", "extra_info": null, "node_info": {"start": 1055122, "end": 1058468}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f4bc6c1d-5d37-45ed-8735-ca10e2fe0744", "3": "a1c99f96-93f0-4dd4-a7e2-4e75fb35bfb9"}}, "__type__": "1"}, "a1c99f96-93f0-4dd4-a7e2-4e75fb35bfb9": {"__data__": {"text": "matrix C . Assume that it takes time tlocal for a memory read or\nwrite operation on a matrix element and time tc to add and multiply two numbers. Determine\nthe parallel run time for this algorithm on an n 2 -processor CREW PRAM. Is this parallel\nalgorithm cost-optimal?\n8.24 Assuming that concurrent read accesses to a memory location are serialized on an EREW\nPRAM, derive the parallel run time of the algorithm given in Algorithm 8.7  on an n 2 -processor\nEREW PRAM. Is this algorithm cost-optimal on an EREW PRAM?\nAlgorithm 8.7 An algorithm for multiplying two n x n matrices A and B\non a CREW PRAM, yielding matrix C = A x B .1.   procedure  MAT_MULT_CREW_PRAM ( A, B, C, n) \n2.   begin \n3.      Organize the n2 processes into a logical mesh of n x n; \n4.      for each process P i,j do \n5.      begin \n6.         C[i, j] := 0; \n7.         for k := 0 to n - 1 do \n8.             C[i, j] := C[i, j] + A[i, k] x B[k, j]; \n9.      endfor; \n10.  end MAT_MULT_CREW_PRAM \n8.25 Consider a shared-address-space parallel computer with n 2 processors. Assume that each\nprocessor has some local memory, and A [i , j ] and B [i , j ] are stored in the local memory of\nprocessor P i , j . Furthermore, processor P i , j computes C [i , j ] in its local memory. Assume\nthat it takes time tlocal + tw to perform a read or write operation on nonlocal memory and time\ntlocal on local memory. Derive an expression for the parallel run time of the algorithm in\nAlgorithm 8.7  on this parallel computer.\n8.26 Algorithm 8.7  can be modified so that the parallel run time on an EREW PRAM is less than\nthat in Problem 8.24. The modified program is shown in Algorithm 8.8  . What is the parallel\nrun time of Algorithm 8.8 on an EREW PRAM and a shared-address-space parallel computer\nwith memory access times as described in Problems 8.24 and 8.25? Is the algorithm cost-\noptimal on these architectures?\n8.27 Consider an implementation of Algorithm 8.8  on a shared-address-space parallel\ncomputer with fewer than n 2 (say, p ) processors and with memory access times as described\nin Problem 8.25. What is the parallel runtime?\nAlgorithm 8.8 An algorithm for multiplying two n x n matrices A and B\non an EREW PRAM, yielding matrix C = A x B .\n1.   procedure  MAT_MULT_EREW_PRAM ( A, B, C, n) \n2.   begin \n3.      Organize the n2 processes into a logical mesh of n x n; \n4.      for each process P i,j do \n5.      begin \n6.         C[i, j] := 0; \n7.         for k := 0 to n - 1 do \n8.             C[i, j] := C[i, j] + \n                          A[i, (i + j + k) mod n] x B[(i + j + k) mod n, j]; \n9.      endfor; \n10.  end MAT_MULT_EREW_PRAM", "doc_id": "a1c99f96-93f0-4dd4-a7e2-4e75fb35bfb9", "embedding": null, "doc_hash": "91bf569f2fb19d153cf837c5c51e64b001c48b397f31ade189d1a65716cb9bd6", "extra_info": null, "node_info": {"start": 1058291, "end": 1060910}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "58431ef2-58ed-4313-9ed1-32cbc99c9894", "3": "44c390c0-a1fe-41bd-9001-4924bf1d239d"}}, "__type__": "1"}, "44c390c0-a1fe-41bd-9001-4924bf1d239d": {"__data__": {"text": "        C[i, j] := 0; \n7.         for k := 0 to n - 1 do \n8.             C[i, j] := C[i, j] + \n                          A[i, (i + j + k) mod n] x B[(i + j + k) mod n, j]; \n9.      endfor; \n10.  end MAT_MULT_EREW_PRAM \n8.28 Consider the implementation of the parallel matrix multiplication algorithm presented in\nSection 8.2.1  on a shared-address-space computer with memory access times as given in\nProblem 8.25. In this algorithm, each processor first receives all the data it needs into its local\nmemory, and then performs the computation. Derive the parallel run time of this algorithm.\nCompare the performance of this algorithm with that in Problem 8.27.\n8.29 Use the results of Problems 8.23\u20138.28 to comment on the viability of the PRAM model as\na platform for parallel algorithm design. Also comment on the relevance of the message-\npassing model for shared-address-space computers.\n[ Team LiB ]\n \n\n[ Team LiB ]\n  \nChapter 9. Sorting\nSorting is one of the most common operations performed by a computer. Because sorted data\nare easier to manipulate than randomly-ordered data, many algorithms require sorted data.\nSorting is of additional importance to parallel computing because of its close relation to the task\nof routing data among processes, which is an essential part of many parallel algorithms. Many\nparallel sorting algorithms have been investigated for a variety of parallel computer\narchitectures. This chapter presents several parallel sorting algorithms for PRAM, mesh,\nhypercube, and general shared-address-space and message-passing architectures.\nSorting is defined as the task of arranging an unordered collection of elements into\nmonotonically increasing (or decreasing) order. Specifically, let S = <a1, a2, ..., an > be a\nsequence of n elements in arbitrary order; sorting transforms S into a monotonically increasing\nsequence \n such that \n  for 1 \n  i \n j \n n, and S' is a permutation of\nS.\nSorting algorithms are categorized as internal  or external . In internal sorting, the number of\nelements to be sorted is small enough to fit into the process's main memory. In contrast,\nexternal sorting algorithms use auxiliary storage (such as tapes and hard disks) for sorting\nbecause the number of elements to be sorted is too large to fit into memory. This chapter\nconcentrates on internal sorting algorithms only.\nSorting algorithms can be categorized as comparison-based  and noncomparison-based . A\ncomparison-based algorithm sorts an unordered sequence of elements by repeatedly comparing\npairs of elements and, if they are out of order, exchanging them. This fundamental operation of\ncomparison-based sorting is called compare-exchange . The lower bound on the sequential\ncomplexity of any sorting algorithms that is comparison-based is Q(n log n), where n is the\nnumber of elements to be sorted. Noncomparison-based algorithms sort by using certain known\nproperties of the elements (such as their binary representation or their distribution). The lower-\nbound complexity of these algorithms is Q(n). We concentrate on comparison-based sorting\nalgorithms in this chapter, although we briefly discuss some noncomparison-based sorting\nalgorithms in Section 9.6.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n9.1 Issues in Sorting on Parallel Computers\nParallelizing a sequential", "doc_id": "44c390c0-a1fe-41bd-9001-4924bf1d239d", "embedding": null, "doc_hash": "8f84d4067504c83ff459a59c50a2daf671c32d37d04d64176c77c8ce57e61041", "extra_info": null, "node_info": {"start": 1061236, "end": 1064527}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a1c99f96-93f0-4dd4-a7e2-4e75fb35bfb9", "3": "72bfda1e-80e5-4cee-b696-58707b8c5744"}}, "__type__": "1"}, "72bfda1e-80e5-4cee-b696-58707b8c5744": {"__data__": {"text": "log n), where n is the\nnumber of elements to be sorted. Noncomparison-based algorithms sort by using certain known\nproperties of the elements (such as their binary representation or their distribution). The lower-\nbound complexity of these algorithms is Q(n). We concentrate on comparison-based sorting\nalgorithms in this chapter, although we briefly discuss some noncomparison-based sorting\nalgorithms in Section 9.6.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n9.1 Issues in Sorting on Parallel Computers\nParallelizing a sequential sorting algorithm involves distributing the elements to be sorted onto\nthe available processes. This process raises a number of issues that we must address in order to\nmake the presentation of parallel sorting algorithms clearer.\n9.1.1 Where the Input and Output Sequences are Stored\nIn sequential sorting algorithms, the input and the sorted sequences are stored in the process's\nmemory. However, in parallel sorting there are two places where these sequences can reside.\nThey may be stored on only one of the processes, or they may be distributed among the\nprocesses. The latter approach is particularly useful if sorting is an intermediate step in another\nalgorithm. In this chapter, we assume that the input and sorted sequences are distributed\namong the processes.\nConsider the precise distribution of the sorted output sequence among the processes. A general\nmethod of distribution is to enumerate the processes and use this enumeration to specify a\nglobal ordering for the sorted sequence. In other words, the sequence will be sorted with\nrespect to this process enumeration. For instance, if Pi comes before Pj in the enumeration, all\nthe elements stored in Pi will be smaller than those stored in Pj . We can enumerate the\nprocesses in many ways. For certain parallel algorithms and interconnection networks, some\nenumerations lead to more efficient parallel formulations than others.\n9.1.2 How Comparisons are Performed\nA sequential sorting algorithm can easily perform a compare-exchange on two elements\nbecause they are stored locally in the process's memory. In parallel sorting algorithms, this step\nis not so easy. If the elements reside on the same process, the comparison can be done easily.\nBut if the elements reside on different processes, the situation becomes more complicated.\nOne Element Per Process\nConsider the case in which each process holds only one element of the sequence to be sorted. At\nsome point in the execution of the algorithm, a pair of processes ( Pi, Pj) may need to compare\ntheir elements, ai and aj. After the comparison, Pi will hold the smaller and Pj the larger of { ai,\naj}. We can perform comparison by having both processes send their elements to each other.\nEach process compares the received element with its own and retains the appropriate element.\nIn our example, Pi will keep the smaller and Pj will keep the larger of { ai, aj}. As in the\nsequential case, we refer to this operation as compare-exchange . As Figure 9.1 illustrates,\neach compare-exchange operation requires one comparison step and one communication step.\nFigure 9.1. A parallel compare-exchange operation. Processes Pi and Pj\nsend their elements to each other. Process Pi keeps min{ ai, aj}, and Pj\nkeeps max{ ai , aj}.\nIf we assume that processes Pi and Pj are neighbors, and the communication channels are\nbidirectional, then the communication cost of a compare-exchange step is ( ts + tw), where ts\nand tw are message-startup time and per-word transfer time, respectively. In commercially\navailable message-passing computers, ts is significantly larger than tw, so the communication\ntime is dominated by ts. Note that in today's parallel computers it takes more time to send an\nelement from one", "doc_id": "72bfda1e-80e5-4cee-b696-58707b8c5744", "embedding": null, "doc_hash": "94df5afdec0ee98a584f6f5ab8a6a86635f033ef261ad7ef989cadb14e8b4f8a", "extra_info": null, "node_info": {"start": 1064193, "end": 1067934}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "44c390c0-a1fe-41bd-9001-4924bf1d239d", "3": "58ae74ba-0330-4317-be2f-c8fa23eff7e2"}}, "__type__": "1"}, "58ae74ba-0330-4317-be2f-c8fa23eff7e2": {"__data__": {"text": "their elements to each other. Process Pi keeps min{ ai, aj}, and Pj\nkeeps max{ ai , aj}.\nIf we assume that processes Pi and Pj are neighbors, and the communication channels are\nbidirectional, then the communication cost of a compare-exchange step is ( ts + tw), where ts\nand tw are message-startup time and per-word transfer time, respectively. In commercially\navailable message-passing computers, ts is significantly larger than tw, so the communication\ntime is dominated by ts. Note that in today's parallel computers it takes more time to send an\nelement from one process to another than it takes to compare the elements. Consequently, any\nparallel sorting formulation that uses as many processes as elements to be sorted will deliver\nvery poor performance because the overall parallel run time will be dominated by interprocess\ncommunication.\nMore than One Element Per Process\nA general-purpose parallel sorting algorithm must be able to sort a large sequence with a\nrelatively small number of processes. Let p be the number of processes P0, P1, ..., Pp-1, and let n\nbe the number of elements to be sorted. Each process is assigned a block of n/p elements, and\nall the processes cooperate to sort the sequence. Let A0, A1, ... A p-1 be the blocks assigned to\nprocesses P0, P1, ... Pp-1, respectively. We say that Ai \n Aj if every element of Ai is less than or\nequal to every element in Aj. When the sorting algorithm finishes, each process P i holds a set \nsuch that \n  for i \n j, and \n .\nAs in the one-element-per-process case, two processes Pi and Pj may have to redistribute their\nblocks of n/p elements so that one of them will get the smaller n/p elements and the other will\nget the larger n/p elements. Let Ai and Aj be the blocks stored in processes Pi and Pj. If the\nblock of n/p elements at each process is already sorted, the redistribution can be done\nefficiently as follows. Each process sends its block to the other process. Now, each process\nmerges the two sorted blocks and retains only the appropriate half of the merged block. We\nrefer to this operation of comparing and splitting two sorted blocks as compare-split . The\ncompare-split operation is illustrated in Figure 9.2.\nFigure 9.2. A compare-split operation. Each process sends its block of\nsize n/p to the other process. Each process merges the received block\nwith its own block and retains only the appropriate half of the merged\nblock. In this example, process Pi retains the smaller elements and\nprocess Pj retains the larger elements.\nIf we assume that processes Pi and Pj are neighbors and that the communication channels are\nbidirectional, then the communication cost of a compare-split operation is ( ts + twn/p). As the\nblock size increases, the significance of ts decreases, and for sufficiently large blocks it can be\nignored. Thus, the time required to merge two sorted blocks of n/p elements is Q(n/p).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n9.2 Sorting Networks\nIn the quest for fast sorting methods, a number of networks have been designed that sort n\nelements in time significantly smaller than Q(n log n). These sorting networks are based on a\ncomparison network model, in which many comparison operations are performed\nsimultaneously.\nThe key component of these networks is a comparator . A comparator is a device with two\ninputs x and y and two outputs x' and y'. For an increasing comparator , x' = min{ x, y} and\ny' = max{ x, y}; for a decreasing comparator  x' = max{ x, y} and y' = min{ x, y}. Figure", "doc_id": "58ae74ba-0330-4317-be2f-c8fa23eff7e2", "embedding": null, "doc_hash": "f81f2a0ccb695eda1a7bd51b424271ea20500dc38a4f391b39ccaba44de2c346", "extra_info": null, "node_info": {"start": 1067905, "end": 1071399}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "72bfda1e-80e5-4cee-b696-58707b8c5744", "3": "46e4f9bc-9f32-4847-b066-0427be7867e2"}}, "__type__": "1"}, "46e4f9bc-9f32-4847-b066-0427be7867e2": {"__data__": {"text": "networks have been designed that sort n\nelements in time significantly smaller than Q(n log n). These sorting networks are based on a\ncomparison network model, in which many comparison operations are performed\nsimultaneously.\nThe key component of these networks is a comparator . A comparator is a device with two\ninputs x and y and two outputs x' and y'. For an increasing comparator , x' = min{ x, y} and\ny' = max{ x, y}; for a decreasing comparator  x' = max{ x, y} and y' = min{ x, y}. Figure 9.3\ngives the schematic representation of the two types of comparators. As the two elements enter\nthe input wires of the comparator, they are compared and, if necessary, exchanged before they\ngo to the output wires. We denote an increasing comparator by \n  and a decreasing comparator\nby \n . A sorting network is usually made up of a series of columns, and each column contains a\nnumber of comparators connected in parallel. Each column of comparators performs a\npermutation, and the output obtained from the final column is sorted in increasing or\ndecreasing order. Figure 9.4 illustrates a typical sorting network. The depth  of a network is the\nnumber of columns it contains. Since the speed of a comparator is a technology-dependent\nconstant, the speed of the network is proportional to its depth.\nFigure 9.3. A schematic representation of comparators: (a) an\nincreasing comparator, and (b) a decreasing comparator.\nFigure 9.4. A typical sorting network. Every sorting network is made\nup of a series of columns, and each column contains a number of\ncomparators connected in parallel.\nWe can convert any sorting network into a sequential sorting algorithm by emulating the\ncomparators in software and performing the comparisons of each column sequentially. The\ncomparator is emulated by a compare-exchange operation, where x and y are compared and, if\nnecessary, exchanged.\nThe following section describes a sorting network that sorts n elements in Q(log2 n) time. To\nsimplify the presentation, we assume that n is a power of two.\n9.2.1 Bitonic Sort\nA bitonic sorting network sorts n elements in Q(log2 n) time. The key operation of the bitonic\nsorting network is the rearrangement of a bitonic sequence into a sorted sequence. A bitonic\nsequence  is a sequence of elements < a0, a1, ..., an-1> with the property that either (1) there\nexists an index i, 0 \n i \n n - 1, such that < a0, ..., ai > is monotonically increasing and < ai\n+1, ..., an-1> is monotonically decreasing, or (2) there exists a cyclic shift of indices so that (1)\nis satisfied. For example, <1, 2, 4, 7, 6, 0> is a bitonic sequence, because it first increases and\nthen decreases. Similarly, <8, 9, 2, 1, 0, 4> is another bitonic sequence, because it is a cyclic\nshift of <0, 4, 8, 9, 2, 1>.\nWe present a method to rearrange a bitonic sequence to obtain a monotonically increasing\nsequence. Let s = <a0, a1, ..., an-1> be a bitonic sequence such that a0 \n a1 \n ... \n  an/2-1\nand an/2 \n an/2+1 \n ... \n  an-1. Consider the following subsequences of s:\nEquation 9.1\nIn sequence s1, there is an element bi = min{ ai, an/2+i} such that all the elements before bi are\nfrom the increasing part of the original sequence and all the elements after bi are from the\ndecreasing part. Also, in sequence s2, the element \n  is such that all", "doc_id": "46e4f9bc-9f32-4847-b066-0427be7867e2", "embedding": null, "doc_hash": "1373898a98f31ea81e5c02b916d8934393ac674be7af66aaa4a39f3fb3c4f72b", "extra_info": null, "node_info": {"start": 1071471, "end": 1074765}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "58ae74ba-0330-4317-be2f-c8fa23eff7e2", "3": "df147f34-70d8-4747-a4fe-ccf00e0b76dd"}}, "__type__": "1"}, "df147f34-70d8-4747-a4fe-ccf00e0b76dd": {"__data__": {"text": "monotonically increasing\nsequence. Let s = <a0, a1, ..., an-1> be a bitonic sequence such that a0 \n a1 \n ... \n  an/2-1\nand an/2 \n an/2+1 \n ... \n  an-1. Consider the following subsequences of s:\nEquation 9.1\nIn sequence s1, there is an element bi = min{ ai, an/2+i} such that all the elements before bi are\nfrom the increasing part of the original sequence and all the elements after bi are from the\ndecreasing part. Also, in sequence s2, the element \n  is such that all the\nelements before \n  are from the decreasing part of the original sequence and all the elements\nafter \n  are from the increasing part. Thus, the sequences s1 and s2 are bitonic sequences.\nFurthermore, every element of the first sequence is smaller than every element of the second\nsequence. The reason is that bi is greater than or equal to all elements of s1, \n is less than or\nequal to all elements of s2, and \n  is greater than or equal to bi. Thus, we have reduced the\ninitial problem of rearranging a bitonic sequence of size n to that of rearranging two smaller\nbitonic sequences and concatenating the results. We refer to the operation of splitting a bitonic\nsequence of size n into the two bitonic sequences defined by Equation 9.1  as a bitonic split .\nAlthough in obtaining s1 and s2 we assumed that the original sequence had increasing and\ndecreasing sequences of the same length, the bitonic split operation also holds for any bitonic\nsequence (Problem 9.3).\nWe can recursively obtain shorter bitonic sequences using Equation 9.1 for each of the bitonic\nsubsequences until we obtain subsequences of size one. At that point, the output is sorted in\nmonotonically increasing order. Since after each bitonic split operation the size of the problem\nis halved, the number of splits required to rearrange the bitonic sequence into a sorted\nsequence is log n. The procedure of sorting a bitonic sequence using bitonic splits is called\nbitonic merge . The recursive bitonic merge procedure is illustrated in Figure 9.5.\nFigure 9.5. Merging a 16-element bitonic sequence through a series of\nlog 16 bitonic splits.\nWe now have a method for merging a bitonic sequence into a sorted sequence. This method is\neasy to implement on a network of comparators. This network of comparators, known as a\nbitonic merging network , it is illustrated in Figure 9.6. The network contains log n columns.\nEach column contains n/2 comparators and performs one step of the bitonic merge. This\nnetwork takes as input the bitonic sequence and outputs the sequence in sorted order. We\ndenote a bitonic merging network with n inputs by \n BM[n]. If we replace the \n  comparators\nin Figure 9.6  by \n  comparators, the input will be sorted in monotonically decreasing order;\nsuch a network is denoted by \n BM[n].\nFigure 9.6. A bitonic merging network for n = 16. The input wires are\nnumbered 0, 1 ..., n - 1, and the binary representation of these\nnumbers is shown. Each column of comparators is drawn separately;\nthe entire figure represents a \n BM[16] bitonic merging network. The\nnetwork takes a bitonic sequence and outputs it in sorted order.\nArmed with the bitonic merging network, consider the task of sorting n unordered elements.\nThis is done by repeatedly merging bitonic sequences of increasing length, as illustrated in\nFigure 9.7 .\nFigure 9.7. A schematic representation of a network that converts an\ninput sequence into a bitonic sequence. In this example, \n BM[k] and\nBM[k] denote bitonic merging networks of input size k that use \nand \n  comparators, respectively. The last merging network", "doc_id": "df147f34-70d8-4747-a4fe-ccf00e0b76dd", "embedding": null, "doc_hash": "e69d4cba8ee0938c96aaacff1f33486ed08e10788cbd73ebe983abfb320f8a2d", "extra_info": null, "node_info": {"start": 1074791, "end": 1078344}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "46e4f9bc-9f32-4847-b066-0427be7867e2", "3": "5e85fb98-cf78-4f6f-934c-9f17b7b382cb"}}, "__type__": "1"}, "5e85fb98-cf78-4f6f-934c-9f17b7b382cb": {"__data__": {"text": "bitonic merging network. The\nnetwork takes a bitonic sequence and outputs it in sorted order.\nArmed with the bitonic merging network, consider the task of sorting n unordered elements.\nThis is done by repeatedly merging bitonic sequences of increasing length, as illustrated in\nFigure 9.7 .\nFigure 9.7. A schematic representation of a network that converts an\ninput sequence into a bitonic sequence. In this example, \n BM[k] and\nBM[k] denote bitonic merging networks of input size k that use \nand \n  comparators, respectively. The last merging network (\nBM[16]) sorts the input. In this example, n = 16.\nLet us now see how this method works. A sequence of two elements x and y forms a bitonic\nsequence, since either x \n y, in which case the bitonic sequence has x and y in the increasing\npart and no elements in the decreasing part, or x \n y, in which case the bitonic sequence has x\nand y in the decreasing part and no elements in the increasing part. Hence, any unsorted\nsequence of elements is a concatenation of bitonic sequences of size two. Each stage of the\nnetwork shown in Figure 9.7  merges adjacent bitonic sequences in increasing and decreasing\norder. According to the definition of a bitonic sequence, the sequence obtained by concatenating\nthe increasing and decreasing sequences is bitonic. Hence, the output of each stage in the\nnetwork in Figure 9.7 is a concatenation of bitonic sequences that are twice as long as those at\nthe input. By merging larger and larger bitonic sequences, we eventually obtain a bitonic\nsequence of size n. Merging this sequence sorts the input. We refer to the algorithm embodied\nin this method as bitonic sort  and the network as a bitonic sorting network . The first three\nstages of the network in Figure 9.7 are shown explicitly in Figure 9.8 . The last stage of Figure\n9.7 is shown explicitly in Figure 9.6 .\nFigure 9.8. The comparator network that transforms an input\nsequence of 16 unordered numbers into a bitonic sequence. In\ncontrast to Figure 9.6 , the columns of comparators in each bitonic\nmerging network are drawn in a single box, separated by a dashed\nline.\nThe last stage of an n-element bitonic sorting network contains a bitonic merging network with\nn inputs. This has a depth of log n. The other stages perform a complete sort of n/2 elements.\nHence, the depth, d(n), of the network in Figure 9.7  is given by the following recurrence\nrelation:\nEquation 9.2\nSolving Equation 9.2 , we obtain \n . This network\ncan be implemented on a serial computer, yielding a Q(n log2 n) sorting algorithm. The bitonic\nsorting network can also be adapted and used as a sorting algorithm for parallel computers. In\nthe next section, we describe how this can be done for hypercube-and mesh-connected parallel\ncomputers.\n9.2.2 Mapping Bitonic Sort to a Hypercube and a Mesh\nIn this section we discuss how the bitonic sort algorithm can be mapped on general-purpose\nparallel computers. One of the key aspects of the bitonic algorithm is that it is communication\nintensive, and a proper mapping of the algorithm must take into account the topology of the\nunderlying interconnection network. For this reason, we discuss how the bitonic sort algorithm\ncan be mapped onto the interconnection network of a hypercube- and mesh-connected parallel\ncomputers.\nThe bitonic sorting network for sorting n elements contains log n stages, and stage i consists of i\ncolumns of n/2 comparators. As Figures 9.6 and 9.8 show, each column of comparators\nperforms compare-exchange operations on n wires. On a parallel computer, the compare-\nexchange function is performed", "doc_id": "5e85fb98-cf78-4f6f-934c-9f17b7b382cb", "embedding": null, "doc_hash": "7e343f14af646c3627c8477a826a2b3d89276bda4c1aa56c1b1a336869c9c424", "extra_info": null, "node_info": {"start": 1078259, "end": 1081855}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "df147f34-70d8-4747-a4fe-ccf00e0b76dd", "3": "73e961b6-276d-4211-b841-b9a91e280495"}}, "__type__": "1"}, "73e961b6-276d-4211-b841-b9a91e280495": {"__data__": {"text": "mapping of the algorithm must take into account the topology of the\nunderlying interconnection network. For this reason, we discuss how the bitonic sort algorithm\ncan be mapped onto the interconnection network of a hypercube- and mesh-connected parallel\ncomputers.\nThe bitonic sorting network for sorting n elements contains log n stages, and stage i consists of i\ncolumns of n/2 comparators. As Figures 9.6 and 9.8 show, each column of comparators\nperforms compare-exchange operations on n wires. On a parallel computer, the compare-\nexchange function is performed by a pair of processes.\nOne Element Per Process\nIn this mapping, each of the n processes contains one element of the input sequence.\nGraphically, each wire of the bitonic sorting network represents a distinct process. During each\nstep of the algorithm, the compare-exchange operations performed by a column of comparators\nare performed by n/2 pairs of processes. One important question is how to map processes to\nwires in order to minimize the distance that the elements travel during a compare-exchange\noperation. If the mapping is poor, the elements travel a long distance before they can be\ncompared, which will degrade performance. Ideally, wires that perform a compare-exchange\nshould be mapped onto neighboring processes. Then the parallel formulation of bitonic sort will\nhave the best possible performance over all the formulations that require n processes.\nTo obtain a good mapping, we must further investigate the way that input wires are paired\nduring each stage of bitonic sort. Consider Figures 9.6 and 9.8, which show the full bitonic\nsorting network for n = 16. In each of the (1 + log 16)(log 16)/2 = 10 comparator columns,\ncertain wires compare-exchange their elements. Focus on the binary representation of the wire\nlabels. In any step, the compare-exchange operation is performed between two wires only if\ntheir labels differ in exactly one bit. During each of the four stages, wires whose labels differ in\nthe least-significant bit perform a compare-exchange in the last step of each stage. During the\nlast three stages, wires whose labels differ in the second-least-significant bit perform a\ncompare-exchange in the second-to-last step of each stage. In general, wires whose labels\ndiffer in the ith least-significant bit perform a compare-exchange (log n - i + 1) times. This\nobservation helps us efficiently map wires onto processes by mapping wires that perform\ncompare-exchange operations more frequently to processes that are close to each other.\nHypercube  Mapping wires onto the processes of a hypercube-connected parallel computer is\nstraightforward. Compare-exchange operations take place between wires whose labels differ in\nonly one bit. In a hypercube, processes whose labels differ in only one bit are neighbors\n(Section 2.4.3). Thus, an optimal mapping of input wires to hypercube processes is the one that\nmaps an input wire with label l to a process with label l where l = 0, 1, ..., n - 1.\nConsider how processes are paired for their compare-exchange steps in a d-dimensional\nhypercube (that is, p = 2d). In the final stage of bitonic sort, the input has been converted into\na bitonic sequence. During the first step of this stage, processes that differ only in the dth bit of\nthe binary representation of their labels (that is, the most significant bit) compare-exchange\ntheir elements. Thus, the compare-exchange operation takes place between processes along the\ndth dimension. Similarly, during the second step of the algorithm, the compare-exchange\noperation takes place among the processes along the ( d - 1)th dimension. In general, during the\nith step of the final stage, processes communicate along the ( d -", "doc_id": "73e961b6-276d-4211-b841-b9a91e280495", "embedding": null, "doc_hash": "879c6b97a58045e65db308be18c51f7930f32f92e9e3233fd9c93edde09deb0c", "extra_info": null, "node_info": {"start": 1081840, "end": 1085560}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5e85fb98-cf78-4f6f-934c-9f17b7b382cb", "3": "c04efdd7-d0dc-4e52-903f-f629f9e77a6f"}}, "__type__": "1"}, "c04efdd7-d0dc-4e52-903f-f629f9e77a6f": {"__data__": {"text": "In the final stage of bitonic sort, the input has been converted into\na bitonic sequence. During the first step of this stage, processes that differ only in the dth bit of\nthe binary representation of their labels (that is, the most significant bit) compare-exchange\ntheir elements. Thus, the compare-exchange operation takes place between processes along the\ndth dimension. Similarly, during the second step of the algorithm, the compare-exchange\noperation takes place among the processes along the ( d - 1)th dimension. In general, during the\nith step of the final stage, processes communicate along the ( d - (i - 1))th dimension. Figure 9.9\nillustrates the communication during the last stage of the bitonic sort algorithm.\nFigure 9.9. Communication during the last stage of bitonic sort. Each\nwire is mapped to a hypercube process; each connection represents a\ncompare-exchange between processes.\nA bitonic merge of sequences of size 2k can be performed on a k-dimensional subcube, with\neach such sequence assigned to a different subcube (Problem 9.5). Furthermore, during the ith\nstep of this bitonic merge, the processes that compare their elements are neighbors along the\n(k - (i - 1))th dimension. Figure 9.10 is a modification of Figure 9.7 , showing the\ncommunication characteristics of the bitonic sort algorithm on a hypercube.\nFigure 9.10. Communication characteristics of bitonic sort on a\nhypercube. During each stage of the algorithm, processes\ncommunicate along the dimensions shown.\nThe bitonic sort algorithm for a hypercube is shown in Algorithm 9.1 . The algorithm relies on\nthe functions comp_exchange_max(i)  and comp_exchange_min(i) . These functions compare the\nlocal element with the element on the nearest process along the ith dimension and retain either\nthe minimum or the maximum of the two elements. Problem 9.6 explores the correctness of\nAlgorithm 9.1 .\nAlgorithm 9.1 Parallel formulation of bitonic sort on a hypercube with\nn = 2d processes. In this algorithm, label  is the process's label and d is\nthe dimension of the hypercube.\n1.   procedure  BITONIC_SORT( label, d) \n2.   begin \n3.      for i := 0 to d - 1 do \n4.          for j := i downto 0 do \n5.              if (i + 1)st bit of label \n jth bit of label then \n6.                   comp_exchange max(j) ; \n7.              else \n8.                  comp_exchange min(j) ; \n9.   end BITONIC_SORT \nDuring each step of the algorithm, every process performs a compare-exchange operation. The\nalgorithm performs a total of (1 + log n)(log n)/2 such steps; thus, the parallel run time is\nEquation 9.3\nThis parallel formulation of bitonic sort is cost optimal with respect to the sequential\nimplementation of bitonic sort (that is, the process-time product is Q(n log2 n)), but it is not\ncost-optimal with respect to an optimal comparison-based sorting algorithm, which has a serial\ntime complexity of Q(n log n).\nMesh  Consider how the input wires of the bitonic sorting network can be mapped efficiently\nonto an n-process mesh. Unfortunately, the connectivity of a mesh is lower than that of a\nhypercube, so it is impossible to map wires to processes such that each compare-exchange\noperation occurs only between neighboring processes. Instead, we map wires such that the\nmost frequent compare-exchange operations occur between neighboring", "doc_id": "c04efdd7-d0dc-4e52-903f-f629f9e77a6f", "embedding": null, "doc_hash": "4b101b55527c429a35e5a8393876f34977b152d40e367cdf660897059e681560", "extra_info": null, "node_info": {"start": 1085528, "end": 1088855}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "73e961b6-276d-4211-b841-b9a91e280495", "3": "c0920de5-d7f8-452a-b122-6cde6ef71f63"}}, "__type__": "1"}, "c0920de5-d7f8-452a-b122-6cde6ef71f63": {"__data__": {"text": "but it is not\ncost-optimal with respect to an optimal comparison-based sorting algorithm, which has a serial\ntime complexity of Q(n log n).\nMesh  Consider how the input wires of the bitonic sorting network can be mapped efficiently\nonto an n-process mesh. Unfortunately, the connectivity of a mesh is lower than that of a\nhypercube, so it is impossible to map wires to processes such that each compare-exchange\noperation occurs only between neighboring processes. Instead, we map wires such that the\nmost frequent compare-exchange operations occur between neighboring processes.\nThere are several ways to map the input wires onto the mesh processes. Some of these are\nillustrated in Figure 9.11. Each process in this figure is labeled by the wire that is mapped onto\nit. Of these three mappings, we concentrate on the row-major shuffled mapping, shown in\nFigure 9.11(c). We leave the other two mappings as exercises (Problem 9.7).\nFigure 9.11. Different ways of mapping the input wires of the bitonic\nsorting network to a mesh of processes: (a) row-major mapping, (b)\nrow-major snakelike mapping, and (c) row-major shuffled mapping.\nThe advantage of row-major shuffled mapping is that processes that perform compare-\nexchange operations reside on square subsections of the mesh whose size is inversely related to\nthe frequency of compare-exchanges. For example, processes that perform compare-exchange\nduring every stage of bitonic sort (that is, those corresponding to wires that differ in the least-\nsignificant bit) are neighbors. In general, wires that differ in the ith least-significant bit are\nmapped onto mesh processes that are \n communication links away. The compare-\nexchange steps of the last stage of bitonic sort for the row-major shuffled mapping are shown in\nFigure 9.12. Note that each earlier stage will have only some of these steps.\nFigure 9.12. The last stage of the bitonic sort algorithm for n = 16 on a\nmesh, using the row-major shuffled mapping. During each step,\nprocess pairs compare-exchange their elements. Arrows indicate the\npairs of processes that perform compare-exchange operations.\nDuring the (1 + log n)(log n)/2 steps of the algorithm, processes that are a certain distance\napart compare-exchange their elements. The distance between processes determines the\ncommunication overhead of the parallel formulation. The total amount of communication\nperformed by each process is \n, which is Q(\n) (Problem 9.7).\nDuring each step of the algorithm, each process performs at most one comparison; thus, the\ntotal computation performed by each process is Q(log2 n). This yields a parallel run time of\nThis is not a cost-optimal formulation, because the process-time product is Q(n1.5), but the\nsequential complexity of sorting is Q(n log n). Although the parallel formulation for a hypercube\nwas optimal with respect to the sequential complexity of bitonic sort, the formulation for mesh\nis not. Can we do any better? No. When sorting n elements, one per mesh process, for certain\ninputs the element stored in the process at the upper-left corner will end up in the process at\nthe lower-right corner. For this to happen, this element must travel along \ncommunication links before reaching its destination. Thus, the run time of sorting on a mesh is\nbounded by W(\n). Our parallel formulation achieves this lower bound; thus, it is\nasymptotically optimal for the mesh architecture.\nA Block of Elements Per Process\nIn the parallel formulations of the bitonic sort algorithm presented so far, we assumed there\nwere as many processes as elements to be sorted. Now we consider the case in which", "doc_id": "c0920de5-d7f8-452a-b122-6cde6ef71f63", "embedding": null, "doc_hash": "8c2edce22c25f41f0fd11d920331db75d2eca0fee9db72599f08ce54ba3a0898", "extra_info": null, "node_info": {"start": 1088889, "end": 1092503}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c04efdd7-d0dc-4e52-903f-f629f9e77a6f", "3": "8662682a-3132-4979-95f0-827158a584b0"}}, "__type__": "1"}, "8662682a-3132-4979-95f0-827158a584b0": {"__data__": {"text": "in the process at the upper-left corner will end up in the process at\nthe lower-right corner. For this to happen, this element must travel along \ncommunication links before reaching its destination. Thus, the run time of sorting on a mesh is\nbounded by W(\n). Our parallel formulation achieves this lower bound; thus, it is\nasymptotically optimal for the mesh architecture.\nA Block of Elements Per Process\nIn the parallel formulations of the bitonic sort algorithm presented so far, we assumed there\nwere as many processes as elements to be sorted. Now we consider the case in which the\nnumber of elements to be sorted is greater than the number of processes.\nLet p be the number of processes and n be the number of elements to be sorted, such that p  <\nn. Each process is assigned a block of n/p elements and cooperates with the other processes to\nsort them. One way to obtain a parallel formulation with our new setup is to think of each\nprocess as consisting of n/p smaller processes. In other words, imagine emulating n/p processes\nby using a single process. The run time of this formulation will be greater by a factor of n/p\nbecause each process is doing the work of n/p processes. This virtual process approach ( Section\n5.3) leads to a poor parallel implementation of bitonic sort. To see this, consider the case of a\nhypercube with p processes. Its run time will be Q((n log2 n)/p), which is not cost-optimal\nbecause the process-time product is Q(n log2 n).\nAn alternate way of dealing with blocks of elements is to use the compare-split operation\npresented in Section 9.1. Think of the ( n/p)-element blocks as elements to be sorted using\ncompare-split operations. The problem of sorting the p blocks is identical to that of performing\na bitonic sort on the p blocks using compare-split operations instead of compare-exchange\noperations (Problem 9.8). Since the total number of blocks is p, the bitonic sort algorithm has a\ntotal of (1 + log p)(log p)/2 steps. Because compare-split operations preserve the initial sorted\norder of the elements in each block, at the end of these steps the n elements will be sorted. The\nmain difference between this formulation and the one that uses virtual processes is that the n/p\nelements assigned to each process are initially sorted locally, using a fast sequential sorting\nalgorithm. This initial local sort makes the new formulation more efficient and cost-optimal.\nHypercube  The block-based algorithm for a hypercube with p processes is similar to the one-\nelement-per-process case, but now we have p blocks of size n/p, instead of p elements.\nFurthermore, the compare-exchange operations are replaced by compare-split operations, each\ntaking Q(n/p) computation time and Q(n/p) communication time. Initially the processes sort\ntheir n/p elements (using merge sort) in time Q((n/p) log( n/p)) and then perform Q(log2 p)\ncompare-split steps. The parallel run time of this formulation is\nBecause the sequential complexity of the best sorting algorithm is Q(n log n), the speedup and\nefficiency are as follows:\nEquation 9.4\nFrom Equation 9.4 , for a cost-optimal formulation (log2 p)/(log n ) = O(1). Thus, this algorithm\ncan efficiently use up to \n  processes. Also from Equation 9.4 , the isoefficiency\nfunction due to both communication and extra work is Q(plog p log2 p), which is worse than any\npolynomial isoefficiency function for sufficiently large p. Hence, this parallel formulation of\nbitonic sort has poor scalability.\nMesh  The block-based mesh formulation is also similar to the one-element-per-process case.\nThe parallel run time of", "doc_id": "8662682a-3132-4979-95f0-827158a584b0", "embedding": null, "doc_hash": "381ec53cd7843f0ecba062c334c5f926aea16476abce88a7907ebbaa3cc3e8b3", "extra_info": null, "node_info": {"start": 1092500, "end": 1096095}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c0920de5-d7f8-452a-b122-6cde6ef71f63", "3": "40757c83-eb10-42e4-83ef-eb26fe81a08b"}}, "__type__": "1"}, "40757c83-eb10-42e4-83ef-eb26fe81a08b": {"__data__": {"text": "Equation 9.4 , for a cost-optimal formulation (log2 p)/(log n ) = O(1). Thus, this algorithm\ncan efficiently use up to \n  processes. Also from Equation 9.4 , the isoefficiency\nfunction due to both communication and extra work is Q(plog p log2 p), which is worse than any\npolynomial isoefficiency function for sufficiently large p. Hence, this parallel formulation of\nbitonic sort has poor scalability.\nMesh  The block-based mesh formulation is also similar to the one-element-per-process case.\nThe parallel run time of this formulation is as follows:\nNote that comparing the communication overhead of this mesh-based parallel bitonic sort\n to the communication overhead of the hypercube-based formulation ( O((n log2\np)/p)), we can see that it is higher by a factor of \n . This factor is smaller than the\n difference in the bisection bandwidth of these architectures. This illustrates that a proper\nmapping of the bitonic sort on the underlying mesh can achieve better performance than that\nachieved by simply mapping the hypercube algorithm on the mesh.\nThe speedup and efficiency are as follows:\nEquation 9.5\n\nTable 9.1. The performance of parallel formulations of bitonic sort for\nn elements on p processes.\nArchitecture Maximum Number of\nProcesses for E = Q(1)Corresponding Parallel\nRun TimeIsoefficiency\nFunction\nHypercube\n Q(plog p log2 p)\nMesh Q(log2 n) Q(n/ log n)\nRing Q(log n) Q(n) Q(2p p)\nFrom Equation 9.5 , for a cost-optimal formulation \n . Thus, this formulation can\nefficiently use up to p = Q(log2 n) processes. Also from Equation 9.5 , the isoefficiency function\n. The isoefficiency function of this formulation is exponential, and thus is even worse\nthan that for the hypercube.\nFrom the analysis for hypercube and mesh, we see that parallel formulations of bitonic sort are\nneither very efficient nor very scalable. This is primarily because the sequential algorithm is\nsuboptimal. Good speedups are possible on a large number of processes only if the number of\nelements to be sorted is very large. In that case, the efficiency of the internal sorting outweighs\nthe inefficiency of the bitonic sort. Table 9.1 summarizes the performance of bitonic sort on\nhypercube-, mesh-, and ring-connected parallel computer.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n9.3 Bubble Sort and its Variants\nThe previous section presented a sorting network that could sort n elements in a time of Q(log2\nn). We now turn our attention to more traditional sorting algorithms. Since serial algorithms\nwith Q(n log n) time complexity exist, we should be able to use Q(n) processes to sort n\nelements in time Q(log n). As we will see, this is difficult to achieve. We can, however, easily\nparallelize many sequential sorting algorithms that have Q(n2) complexity. The algorithms we\npresent are based on bubble sort .\nThe sequential bubble sort algorithm compares and exchanges adjacent elements in the\nsequence to be sorted. Given a sequence < a1, a2, ..., an >, the algorithm first performs n - 1\ncompare-exchange operations in the following order: ( a1, a2), (a2, a3), ..., ( an-1, an). This step\nmoves the largest element to the end of the sequence. The last element in the transformed\nsequence is then ignored, and the sequence of compare-exchanges is applied to the resulting\nsequence \n. The sequence is sorted after n - 1 iterations. We can improve the\nperformance of bubble sort by terminating when no exchanges take place during an iteration.\nThe bubble sort algorithm is shown in Algorithm", "doc_id": "40757c83-eb10-42e4-83ef-eb26fe81a08b", "embedding": null, "doc_hash": "945d99cdfaf6793f07abe291e83e571f454b5bccb886d33c5e0166f86eda9666", "extra_info": null, "node_info": {"start": 1096145, "end": 1099629}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8662682a-3132-4979-95f0-827158a584b0", "3": "9ed28ec0-8bb1-40fd-b83a-4580f0fb7752"}}, "__type__": "1"}, "9ed28ec0-8bb1-40fd-b83a-4580f0fb7752": {"__data__": {"text": "a2, ..., an >, the algorithm first performs n - 1\ncompare-exchange operations in the following order: ( a1, a2), (a2, a3), ..., ( an-1, an). This step\nmoves the largest element to the end of the sequence. The last element in the transformed\nsequence is then ignored, and the sequence of compare-exchanges is applied to the resulting\nsequence \n. The sequence is sorted after n - 1 iterations. We can improve the\nperformance of bubble sort by terminating when no exchanges take place during an iteration.\nThe bubble sort algorithm is shown in Algorithm 9.2 .\nAn iteration of the inner loop of bubble sort takes time Q(n), and we perform a total of Q(n)\niterations; thus, the complexity of bubble sort is Q(n2). Bubble sort is difficult to parallelize. To\nsee this, consider how compare-exchange operations are performed during each phase of the\nalgorithm (lines 4 and 5 of Algorithm 9.2). Bubble sort compares all adjacent pairs in order;\nhence, it is inherently sequential. In the following two sections, we present two variants of\nbubble sort that are well suited to parallelization.\nAlgorithm 9.2 Sequential bubble sort algorithm.\n1.   procedure  BUBBLE_SORT( n) \n2.   begin \n3.      for i := n - 1 downto 1 do \n4.          for j := 1 to i do \n5.              compare-exchange (aj, aj + 1); \n6.   end BUBBLE_SORT \n9.3.1 Odd-Even Transposition\nThe odd-even transposition  algorithm sorts n elements in n phases ( n is even), each of which\nrequires n/2 compare-exchange operations. This algorithm alternates between two phases,\ncalled the odd and even phases. Let < a1, a2, ..., an> be the sequence to be sorted. During the\nodd phase, elements with odd indices are compared with their right neighbors, and if they are\nout of sequence they are exchanged; thus, the pairs ( a1, a2), (a3, a4), ..., ( an-1, an) are\ncompare-exchanged (assuming n is even). Similarly, during the even phase, elements with even\nindices are compared with their right neighbors, and if they are out of sequence they are\nexchanged; thus, the pairs ( a2, a3), (a4, a5), ..., ( an-2, an-1) are compare-exchanged. After n\nphases of odd-even exchanges, the sequence is sorted. Each phase of the algorithm (either odd\nor even) requires Q(n) comparisons, and there are a total of n phases; thus, the sequential\ncomplexity is Q(n2). The odd-even transposition sort is shown in Algorithm 9.3 and is illustrated\nin Figure 9.13 .\nFigure 9.13. Sorting n = 8 elements, using the odd-even transposition\nsort algorithm. During each phase, n = 8 elements are compared.\nAlgorithm 9.3 Sequential odd-even transposition sort algorithm.\n1.   procedure  ODD-EVEN( n) \n2.   begin \n3.      for i := 1 to n do \n4.      begin \n5.         if i is odd then \n6.              for j := 0 to n/2 - 1 do \n7.                  compare-exchange (a2j + 1, a2j + 2); \n8.         if i is even then \n9.              for", "doc_id": "9ed28ec0-8bb1-40fd-b83a-4580f0fb7752", "embedding": null, "doc_hash": "f9ddb3f7a5a46d632806972b4b1fdcd1a0c8b92ee5abc9a8a988d469737ac7b6", "extra_info": null, "node_info": {"start": 1099609, "end": 1102463}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "40757c83-eb10-42e4-83ef-eb26fe81a08b", "3": "c0329a00-e90f-4603-84c9-4c89636a1848"}}, "__type__": "1"}, "c0329a00-e90f-4603-84c9-4c89636a1848": {"__data__": {"text": "n) \n2.   begin \n3.      for i := 1 to n do \n4.      begin \n5.         if i is odd then \n6.              for j := 0 to n/2 - 1 do \n7.                  compare-exchange (a2j + 1, a2j + 2); \n8.         if i is even then \n9.              for j := 1 to n/2 - 1 do \n10.                 compare-exchange (a2j, a2j + 1); \n11.     end for \n12.  end ODD-EVEN \nParallel Formulation\nIt is easy to parallelize odd-even transposition sort. During each phase of the algorithm,\ncompare-exchange operations on pairs of elements are performed simultaneously. Consider the\none-element-per-process case. Let n be the number of processes (also the number of elements\nto be sorted). Assume that the processes are arranged in a one-dimensional array. Element ai\ninitially resides on process Pi for i = 1, 2, ..., n. During the odd phase, each process that has an\nodd label compare-exchanges its element with the element residing on its right neighbor.\nSimilarly, during the even phase, each process with an even label compare-exchanges its\nelement with the element of its right neighbor. This parallel formulation is presented in\nAlgorithm 9.4.\nAlgorithm 9.4 The parallel formulation of odd-even transposition sort\non an n-process ring.\n1.   procedure  ODD-EVEN_PAR ( n) \n2.   begin \n3.      id := process's label \n4.      for i := 1 to n do \n5.      begin \n6.         if i is odd then \n7.             if id is odd then \n8.                 compare-exchange_min (id + 1); \n9.             else \n10.                compare-exchange_max (id - 1); \n11.        if i is even then \n12.            if id is even then \n13.                compare-exchange_min (id + 1); \n14.            else \n15.                compare-exchange_max (id - 1); \n16.     end for \n17.  end ODD-EVEN_PAR \nDuring each phase of the algorithm, the odd or even processes perform a compare- exchange\nstep with their right neighbors. As we know from Section 9.1 , this requires time Q(1). A total of\nn such phases are performed; thus, the parallel run time of this formulation is Q(n). Since the\nsequential complexity of the best sorting algorithm for n elements is Q(n log n), this formulation\nof odd-even transposition sort is not cost-optimal, because its process-time product is Q (n2).\nTo obtain a cost-optimal parallel formulation, we use fewer processes. Let p be the number of\nprocesses, where p < n. Initially, each process is assigned a block of n/p elements, which it\nsorts internally (using merge sort or quicksort) in Q((n/p) log( n/p)) time. After this, the\nprocesses execute p phases (", "doc_id": "c0329a00-e90f-4603-84c9-4c89636a1848", "embedding": null, "doc_hash": "6c2014d2d898cf9f324a349d78b30e0bed3d4d1d5b854d22589f1d6c82a8cbdf", "extra_info": null, "node_info": {"start": 1102804, "end": 1105342}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9ed28ec0-8bb1-40fd-b83a-4580f0fb7752", "3": "cabfa5ea-17e4-4623-9a5a-dca794aaef43"}}, "__type__": "1"}, "cabfa5ea-17e4-4623-9a5a-dca794aaef43": {"__data__": {"text": "the\nsequential complexity of the best sorting algorithm for n elements is Q(n log n), this formulation\nof odd-even transposition sort is not cost-optimal, because its process-time product is Q (n2).\nTo obtain a cost-optimal parallel formulation, we use fewer processes. Let p be the number of\nprocesses, where p < n. Initially, each process is assigned a block of n/p elements, which it\nsorts internally (using merge sort or quicksort) in Q((n/p) log( n/p)) time. After this, the\nprocesses execute p phases ( p/2 odd and p/2 even), performing compare-split operations. At\nthe end of these phases, the list is sorted (Problem 9.10). During each phase, Q(n/p)\ncomparisons are performed to merge two blocks, and time Q(n/p) is spent communicating.\nThus, the parallel run time of the formulation is\n\nSince the sequential complexity of sorting is Q(n log n), the speedup and efficiency of this\nformulation are as follows:\nEquation 9.6\nFrom Equation 9.6 , odd-even transposition sort is cost-optimal when p = O(log n). The\nisoefficiency function of this parallel formulation is Q(p 2p), which is exponential. Thus, it is\npoorly scalable and is suited to only a small number of processes.\n9.3.2 Shellsort\nThe main limitation of odd-even transposition sort is that it moves elements only one position at\na time. If a sequence has just a few elements out of order, and if they are Q(n) distance from\ntheir proper positions, then the sequential algorithm still requires time Q(n2) to sort the\nsequence. To make a substantial improvement over odd-even transposition sort, we need an\nalgorithm that moves elements long distances. Shellsort is one such serial sorting algorithm.\nLet n be the number of elements to be sorted and p be the number of processes. To simplify the\npresentation we will assume that the number of processes is a power of two, that is, p = 2d, but\nthe algorithm can be easily extended to work for an arbitrary number of processes as well. Each\nprocess is assigned a block of n /p elements. The processes are considered to be arranged in a\nlogical one-dimensional array, and the ordering of the processes in that array defines the global\nordering of the sorted sequence. The algorithm consists of two phases. During the first phase,\nprocesses that are far away from each other in the array compare-split their elements. Elements\nthus move long distances to get close to their final destinations in a few steps. During the\nsecond phase, the algorithm switches to an odd-even transposition sort similar to the one\ndescribed in the previous section. The only difference is that the odd and even phases are\nperformed only as long as the blocks on the processes are changing. Because the first phase of\nthe algorithm moves elements close to their final destinations, the number of odd and even\nphases performed by the second phase may be substantially smaller than p.\nInitially, each process sorts its block of n/p elements internally in Q(n/p log(n/p)) time. Then,\neach process is paired with its corresponding process in the reverse order of the array. That is,\nprocess Pi, where i < p/2, is paired with process Pp-i -1. Each pair of processes performs a\ncompare-split operation. Next, the processes are partitioned into two groups; one group has the\nfirst p/2 processes and the other group has the last p/2 processes. Now, each group is treated\nas a separate set of p/2 processes and the above scheme of process-pairing is applied to\ndetermine which processes will perform the compare-split operation. This process continues for\nd steps, until each group contains only a single process.", "doc_id": "cabfa5ea-17e4-4623-9a5a-dca794aaef43", "embedding": null, "doc_hash": "22dcd309d5d501d2a8c1b84d1e548638fb8388de3f0f51176a345e5b01749367", "extra_info": null, "node_info": {"start": 1105034, "end": 1108627}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c0329a00-e90f-4603-84c9-4c89636a1848", "3": "eeb45073-d83d-46eb-976f-2c83063aedd5"}}, "__type__": "1"}, "eeb45073-d83d-46eb-976f-2c83063aedd5": {"__data__": {"text": "of the array. That is,\nprocess Pi, where i < p/2, is paired with process Pp-i -1. Each pair of processes performs a\ncompare-split operation. Next, the processes are partitioned into two groups; one group has the\nfirst p/2 processes and the other group has the last p/2 processes. Now, each group is treated\nas a separate set of p/2 processes and the above scheme of process-pairing is applied to\ndetermine which processes will perform the compare-split operation. This process continues for\nd steps, until each group contains only a single process. The compare-split operations of the\nfirst phase are illustrated in Figure 9.14 for d = 3. Note that it is not a direct parallel\nformulation of the sequential shellsort, but it relies on similar ideas.\nFigure 9.14. An example of the first phase of parallel shellsort on an\neight-process array.\nIn the first phase of the algorithm, each process performs d = log p compare-split operations.\nIn each compare-split operation a total of p/2 pairs of processes need to exchange their locally\nstored n/p elements. The communication time required by these compare-split operations\ndepend on the bisection bandwidth of the network. In the case in which the bisection bandwidth\nis Q(p), the amount of time required by each operation is Q(n/p). Thus, the complexity of this\nphase is Q((n log p)/p). In the second phase, l odd and even phases are performed, each\nrequiring time Q(n/p). Thus, the parallel run time of the algorithm is\nEquation 9.7\nThe performance of shellsort depends on the value of l. If l is small, then the algorithm\nperforms significantly better than odd-even transposition sort; if l is Q(p), then both algorithms\nperform similarly. Problem 9.13 investigates the worst-case value of l.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n9.4 Quicksort\nAll the algorithms presented so far have worse sequential complexity than that of the lower\nbound for comparison-based sorting, Q(n log n). This section examines the quicksort\nalgorithm, which has an average complexity of Q(n log n). Quicksort is one of the most common\nsorting algorithms for sequential computers because of its simplicity, low overhead, and optimal\naverage complexity.\nQuicksort is a divide-and-conquer algorithm that sorts a sequence by recursively dividing it into\nsmaller subsequences. Assume that the n-element sequence to be sorted is stored in the array\nA[1...n]. Quicksort consists of two steps: divide and conquer. During the divide step, a\nsequence A[q...r] is partitioned (rearranged) into two nonempty subsequences A[q...s] and A[s\n+ 1... r] such that each element of the first subsequence is smaller than or equal to each\nelement of the second subsequence. During the conquer step, the subsequences are sorted by\nrecursively applying quicksort. Since the subsequences A[q...s] and A[s + 1... r] are sorted and\nthe first subsequence has smaller elements than the second, the entire sequence is sorted.\nHow is the sequence A[q...r] partitioned into two parts \u2013 one with all elements smaller than the\nother? This is usually accomplished by selecting one element x from A[q...r] and using this\nelement to partition the sequence A[q...r] into two parts \u2013 one with elements less than or equal\nto x and the other with elements greater than x.Element x is called the pivot . The quicksort\nalgorithm is presented in Algorithm 9.5. This algorithm arbitrarily chooses the first element of\nthe sequence A[q...r] as the pivot. The operation of quicksort is illustrated in", "doc_id": "eeb45073-d83d-46eb-976f-2c83063aedd5", "embedding": null, "doc_hash": "f3c30346c11f34f2142055b4eac5165e50ed10e393cafa1d7759f811ce5f8a3f", "extra_info": null, "node_info": {"start": 1108596, "end": 1112075}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "cabfa5ea-17e4-4623-9a5a-dca794aaef43", "3": "3388caeb-0857-41e6-a79f-b98db694a970"}}, "__type__": "1"}, "3388caeb-0857-41e6-a79f-b98db694a970": {"__data__": {"text": "the sequence A[q...r] partitioned into two parts \u2013 one with all elements smaller than the\nother? This is usually accomplished by selecting one element x from A[q...r] and using this\nelement to partition the sequence A[q...r] into two parts \u2013 one with elements less than or equal\nto x and the other with elements greater than x.Element x is called the pivot . The quicksort\nalgorithm is presented in Algorithm 9.5. This algorithm arbitrarily chooses the first element of\nthe sequence A[q...r] as the pivot. The operation of quicksort is illustrated in Figure 9.15 .\nFigure 9.15. Example of the quicksort algorithm sorting a sequence of\nsize n = 8.\nAlgorithm 9.5 The sequential quicksort algorithm.\n1.   procedure  QUICKSORT ( A, q, r) \n2.   begin \n3.      if q < r then \n4.      begin \n5.         x := A[q]; \n6.         s := q; \n7.         for i := q + 1 to r do \n8.             if A[i]\nx then \n9.             begin \n10.               s := s + 1; \n11.               swap( A[s], A[i]); \n12.            end if \n13.        swap( A[q], A[s]); \n14.        QUICKSORT ( A, q, s); \n15.        QUICKSORT ( A, s + 1, r); \n16.     end if \n17.  end QUICKSORT \nThe complexity of partitioning a sequence of size k is Q(k). Quicksort's performance is greatly\naffected by the way it partitions a sequence. Consider the case in which a sequence of size k is\nsplit poorly, into two subsequences of sizes 1 and k - 1. The run time in this case is given by the\nrecurrence relation T(n) = T(n - 1) + Q(n), whose solution is T(n) = Q(n2). Alternatively,\nconsider the case in which the sequence is split well, into two roughly equal-size subsequences\nof \n  and \n  elements. In this case, the run time is given by the recurrence relation T(n) =\n2T(n/2) + Q(n), whose solution is T(n) = Q(n log n). The second split yields an optimal\nalgorithm. Although quicksort can have O(n2) worst-case complexity, its average complexity is\nsignificantly better; the average number of compare-exchange operations needed by quicksort\nfor sorting a randomly-ordered input sequence is 1.4 n log n, which is asymptotically optimal.\nThere are several ways to select pivots. For example, the pivot can be the median of a small\nnumber of elements of the sequence, or it can be an element selected at random. Some pivot\nselection strategies have advantages over others for certain input sequences.\n9.4.1 Parallelizing Quicksort\nQuicksort can be parallelized in a variety of ways. First, consider a naive parallel formulation\nthat was also discussed briefly in Section 3.2.1 in the context of recursive decomposition. Lines\n14 and 15 of Algorithm 9.5  show that, during each call of QUICKSORT, the array is partitioned\ninto two parts and each part is solved recursively. Sorting the smaller arrays represents two\ncompletely independent subproblems that can be solved in parallel. Therefore, one way to\nparallelize", "doc_id": "3388caeb-0857-41e6-a79f-b98db694a970", "embedding": null, "doc_hash": "a8fefbc869334fec8434a857a4aa93c50079747da3d9cca871411a965ce59c4f", "extra_info": null, "node_info": {"start": 1112075, "end": 1114940}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "eeb45073-d83d-46eb-976f-2c83063aedd5", "3": "66169e07-5e32-436e-a6b8-aa1222217db8"}}, "__type__": "1"}, "66169e07-5e32-436e-a6b8-aa1222217db8": {"__data__": {"text": "for certain input sequences.\n9.4.1 Parallelizing Quicksort\nQuicksort can be parallelized in a variety of ways. First, consider a naive parallel formulation\nthat was also discussed briefly in Section 3.2.1 in the context of recursive decomposition. Lines\n14 and 15 of Algorithm 9.5  show that, during each call of QUICKSORT, the array is partitioned\ninto two parts and each part is solved recursively. Sorting the smaller arrays represents two\ncompletely independent subproblems that can be solved in parallel. Therefore, one way to\nparallelize quicksort is to execute it initially on a single process; then, when the algorithm\nperforms its recursive calls (lines 14 and 15), assign one of the subproblems to another process.\nNow each of these processes sorts its array by using quicksort and assigns one of its\nsubproblems to other processes. The algorithm terminates when the arrays cannot be further\npartitioned. Upon termination, each process holds an element of the array, and the sorted order\ncan be recovered by traversing the processes as we will describe later. This parallel formulation\nof quicksort uses n processes to sort n elements. Its major drawback is that partitioning the\narray A[q ...r ] into two smaller arrays, A[q ...s] and A[s + 1 ... r ], is done by a single process.\nSince one process must partition the original array A[1 ...n], the run time of this formulation is\nbounded below by W(n). This formulation is not cost-optimal, because its process-time product\nis W(n2).\nThe main limitation of the previous parallel formulation is that it performs the partitioning step\nserially. As we will see in subsequent formulations, performing partitioning in parallel is\nessential in obtaining an efficient parallel quicksort. To see why, consider the recurrence\nequation T (n) = 2T (n/2) + Q(n), which gives the complexity of quicksort for optimal pivot\nselection. The term Q(n) is due to the partitioning of the array. Compare this complexity with\nthe overall complexity of the algorithm, Q(n log n). From these two complexities, we can think\nof the quicksort algorithm as consisting of Q(log n) steps, each requiring time Q(n) \u2013 that of\nsplitting the array. Therefore, if the partitioning step is performed in time Q(1), using Q(n)\nprocesses, it is possible to obtain an overall parallel run time of Q(log n), which leads to a cost-\noptimal formulation. However, without parallelizing the partitioning step, the best we can do\n(while maintaining cost-optimality) is to use only Q(log n) processes to sort n elements in time\nQ(n) (Problem 9.14). Hence, parallelizing the partitioning step has the potential to yield a\nsignificantly faster parallel formulation.\nIn the previous paragraph, we hinted that we could partition an array of size n into two smaller\narrays in time Q(1) by using Q(n) processes. However, this is difficult for most parallel\ncomputing models. The only known algorithms are for the abstract PRAM models. Because of\ncommunication overhead, the partitioning step takes longer than Q(1) on realistic shared-\naddress-space and message-passing parallel computers. In the following sections we present\nthree distinct parallel formulations: one for a CRCW PRAM, one for a shared-address-space\narchitecture, and one for a message-passing platform. Each of these formulations parallelizes\nquicksort by performing the partitioning step in parallel.\n9.4.2 Parallel Formulation for a CRCW PRAM\nWe will now present a parallel formulation of quicksort for sorting n elements on an n-process\narbitrary CRCW", "doc_id": "66169e07-5e32-436e-a6b8-aa1222217db8", "embedding": null, "doc_hash": "4f716d717ad6db132b5677a285087005beff6351ebf35124da04afa482811c03", "extra_info": null, "node_info": {"start": 1114938, "end": 1118468}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3388caeb-0857-41e6-a79f-b98db694a970", "3": "b29d9b6c-f381-4f76-985b-21019f3c8d2f"}}, "__type__": "1"}, "b29d9b6c-f381-4f76-985b-21019f3c8d2f": {"__data__": {"text": "step takes longer than Q(1) on realistic shared-\naddress-space and message-passing parallel computers. In the following sections we present\nthree distinct parallel formulations: one for a CRCW PRAM, one for a shared-address-space\narchitecture, and one for a message-passing platform. Each of these formulations parallelizes\nquicksort by performing the partitioning step in parallel.\n9.4.2 Parallel Formulation for a CRCW PRAM\nWe will now present a parallel formulation of quicksort for sorting n elements on an n-process\narbitrary CRCW PRAM. Recall from Section 2.4.1 that an arbitrary CRCW PRAM is a concurrent-\nread, concurrent-write parallel random-access machine in which write conflicts are resolved\narbitrarily. In other words, when more than one process tries to write to the same memory\nlocation, only one arbitrarily chosen process is allowed to write, and the remaining writes are\nignored.\nExecuting quicksort can be visualized as constructing a binary tree. In this tree, the pivot is the\nroot; elements smaller than or equal to the pivot go to the left subtree, and elements larger\nthan the pivot go to the right subtree. Figure 9.16 illustrates the binary tree constructed by the\nexecution of the quicksort algorithm illustrated in Figure 9.15 . The sorted sequence can be\nobtained from this tree by performing an in-order traversal. The PRAM formulation is based on\nthis interpretation of quicksort.\nFigure 9.16. A binary tree generated by the execution of the quicksort\nalgorithm. Each level of the tree represents a different array-\npartitioning iteration. If pivot selection is optimal, then the height of\nthe tree is Q(log n), which is also the number of iterations.\nThe algorithm starts by selecting a pivot element and partitioning the array into two parts \u2013 one\nwith elements smaller than the pivot and the other with elements larger than the pivot.\nSubsequent pivot elements, one for each new subarray, are then selected in parallel. This\nformulation does not rearrange elements; instead, since all the processes can read the pivot in\nconstant time, they know which of the two subarrays (smaller or larger) the elements assigned\nto them belong to. Thus, they can proceed to the next iteration.\nThe algorithm that constructs the binary tree is shown in Algorithm 9.6. The array to be sorted\nis stored in A[1...n] and process i is assigned element A[i]. The arrays leftchild [1...n] and\nrightchild [1...n] keep track of the children of a given pivot. For each process, the local variable\nparent i stores the label of the process whose element is the pivot. Initially, all the processes\nwrite their process labels into the variable root in line 5. Because the concurrent write operation\nis arbitrary, only one of these labels will actually be written into root. The value A[root] is used\nas the first pivot and root is copied into parent i for each process i . Next, processes that have\nelements smaller than A[parent i] write their process labels into leftchild [parent i], and those with\nlarger elements write their process label into rightchild [parent i]. Thus, all processes whose\nelements belong in the smaller partition have written their labels into leftchild [parent i], and\nthose with elements in the larger partition have written their labels into rightchild [parent i].\nBecause of the arbitrary concurrent-write operations, only two values \u2013 one for leftchild [parent i]\nand one for rightchild [parent i] \u2013 are written into these locations. These two values become the\nlabels of the processes that hold the pivot elements for the next iteration, in which two smaller\narrays are being partitioned. The algorithm continues until n pivot elements are selected. A\nprocess exits when its element becomes a", "doc_id": "b29d9b6c-f381-4f76-985b-21019f3c8d2f", "embedding": null, "doc_hash": "f623d8ca1d2d1407918ad521eebc148b6c263504b3fae4235c81c3579ff6e860", "extra_info": null, "node_info": {"start": 1118470, "end": 1122201}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "66169e07-5e32-436e-a6b8-aa1222217db8", "3": "f226a730-e040-4155-b549-597868e2e230"}}, "__type__": "1"}, "f226a730-e040-4155-b549-597868e2e230": {"__data__": {"text": "the smaller partition have written their labels into leftchild [parent i], and\nthose with elements in the larger partition have written their labels into rightchild [parent i].\nBecause of the arbitrary concurrent-write operations, only two values \u2013 one for leftchild [parent i]\nand one for rightchild [parent i] \u2013 are written into these locations. These two values become the\nlabels of the processes that hold the pivot elements for the next iteration, in which two smaller\narrays are being partitioned. The algorithm continues until n pivot elements are selected. A\nprocess exits when its element becomes a pivot. The construction of the binary tree is illustrated\nin Figure 9.17. During each iteration of the algorithm, a level of the tree is constructed in time\nQ(1). Thus, the average complexity of the binary tree building algorithm is Q(log n) as the\naverage height of the tree is Q(log n) (Problem 9.16).\nFigure 9.17. The execution of the PRAM algorithm on the array shown\nin (a). The arrays leftchild  and rightchild  are shown in (c), (d), and (e)\nas the algorithm progresses. Figure (f) shows the binary tree\nconstructed by the algorithm. Each node is labeled by the process (in\nsquare brackets), and the element is stored at that process (in curly\nbrackets). The element is the pivot. In each node, processes with\nsmaller elements than the pivot are grouped on the left side of the\nnode, and those with larger elements are grouped on the right side.\nThese two groups form the two partitions of the original array. For\neach partition, a pivot element is selected at random from the two\ngroups that form the children of the node.\nAlgorithm 9.6 The binary tree construction procedure for the CRCW\nPRAM parallel quicksort formulation.\n1.   procedure  BUILD TREE ( A[1...n]) \n2.   begin \n3.      for each process i do \n4.      begin \n5.         root := i ; \n6.         parent i := root; \n7.         leftchild [i] := rightchild [i] := n + 1; \n8.      end for \n9.      repeat for each process i \n r oot do \n10.     begin \n11.        if (A[i] < A[parent i]) or \n                ( A[i]= A[parent i] and i <parent i) then \n12.        begin \n13.           leftchild [parent i] :=i ; \n14.           if i = leftchild [parent i] then exit  \n15.           else parent i := leftchild [parent i]; \n16.        end for \n17.        else \n18.        begin \n19.           rightchild [parent i] :=i; \n20.           if i = rightchild [parent i] then exit  \n21.           else parent i := rightchild [parent i]; \n22.        end else  \n23.     end repeat  \n24. end BUILD_TREE \nAfter building the binary tree, the algorithm determines the position of each element in the\nsorted array. It traverses the tree and keeps a count of the number of elements in the left and\nright subtrees of any element. Finally, each element is placed in its proper position in time Q(1),\nand the array is sorted. The algorithm that traverses the binary tree and computes the", "doc_id": "f226a730-e040-4155-b549-597868e2e230", "embedding": null, "doc_hash": "18668065ab647198a40bb158b14d90d4c3a50951df1d194083707a77b1972443", "extra_info": null, "node_info": {"start": 1122151, "end": 1125088}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b29d9b6c-f381-4f76-985b-21019f3c8d2f", "3": "2be39aa6-17c4-47fb-89b4-4a5ef8848189"}}, "__type__": "1"}, "2be39aa6-17c4-47fb-89b4-4a5ef8848189": {"__data__": {"text": "\n21.           else parent i := rightchild [parent i]; \n22.        end else  \n23.     end repeat  \n24. end BUILD_TREE \nAfter building the binary tree, the algorithm determines the position of each element in the\nsorted array. It traverses the tree and keeps a count of the number of elements in the left and\nright subtrees of any element. Finally, each element is placed in its proper position in time Q(1),\nand the array is sorted. The algorithm that traverses the binary tree and computes the position\nof each element is left as an exercise (Problem 9.15). The average run time of this algorithm is\nQ(log n) on an n -process PRAM. Thus, its overall process-time product is Q(n log n), which is\ncost-optimal.\n9.4.3 Parallel Formulation for Practical Architectures\nWe now turn our attention to a more realistic parallel architecture \u2013 that of a p-process system\nconnected via an interconnection network. Initially, our discussion will focus on developing an\nalgorithm for a shared-address-space system and then we will show how this algorithm can be\nadapted to message-passing systems.\nShared-Address-Space Parallel Formulation\nThe quicksort formulation for a shared-address-space system works as follows. Let A be an\narray of n elements that need to be sorted and p be the number of processes. Each process is\nassigned a consecutive block of n/p elements, and the labels of the processes define the global\norder of the sorted sequence. Let Ai be the block of elements assigned to process Pi .\nThe algorithm starts by selecting a pivot element, which is broadcast to all processes. Each\nprocess Pi, upon receiving the pivot, rearranges its assigned block of elements into two sub-\nblocks, one with elements smaller than the pivot Si and one with elements larger than the pivot\nLi. This local rearrangement is done in place using the collapsing the loops  approach of\nquicksort. The next step of the algorithm is to rearrange the elements of the original array A so\nthat all the elements that are smaller than the pivot (i.e., \n) are stored at the beginning\nof the array, and all the elements that are larger than the pivot (i.e., \n ) are stored at\nthe end of the array.\nOnce this global  rearrangement is done, then the algorithm proceeds to partition the processes\ninto two groups, and assign to the first group the task of sorting the smaller elements S, and to\nthe second group the task of sorting the larger elements L . Each of these steps is performed by\nrecursively calling the parallel quicksort algorithm. Note that by simultaneously partitioning\nboth the processes and the original array each group of processes can proceed independently.\nThe recursion ends when a particular sub-block of elements is assigned to only a single process,\nin which case the process sorts the elements using a serial quicksort algorithm.\nThe partitioning of processes into two groups is done according to the relative sizes of the S and\nL blocks. In particular, the first \n processes are assigned to sort the smaller\nelements S, and the rest of the processes are assigned to sort the larger elements L. Note that\nthe 0.5 term in the above formula is to ensure that the processes are assigned in the most\nbalanced fashion.\nExample 9.1 Efficient parallel quicksort\nFigure 9.18  illustrates this algorithm using an example of 20 integers and five\nprocesses. In the first step, each process locally rearranges the four elements that it is\ninitially responsible for, around the pivot element (seven in this example), so that the\nelements smaller or equal to the pivot are moved to the beginning of the locally\nassigned portion of the array (and", "doc_id": "2be39aa6-17c4-47fb-89b4-4a5ef8848189", "embedding": null, "doc_hash": "8253187fc775262707fa0ee2312792aae67092b8361f03ebfcd2fc510c2c4192", "extra_info": null, "node_info": {"start": 1125214, "end": 1128842}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f226a730-e040-4155-b549-597868e2e230", "3": "1f22a396-8cc2-49eb-9668-5ea06a3dc014"}}, "__type__": "1"}, "1f22a396-8cc2-49eb-9668-5ea06a3dc014": {"__data__": {"text": "are assigned to sort the larger elements L. Note that\nthe 0.5 term in the above formula is to ensure that the processes are assigned in the most\nbalanced fashion.\nExample 9.1 Efficient parallel quicksort\nFigure 9.18  illustrates this algorithm using an example of 20 integers and five\nprocesses. In the first step, each process locally rearranges the four elements that it is\ninitially responsible for, around the pivot element (seven in this example), so that the\nelements smaller or equal to the pivot are moved to the beginning of the locally\nassigned portion of the array (and are shaded in the figure). Once this local\nrearrangement is done, the processes perform a global rearrangement to obtain the\nthird array shown in the figure (how this is performed will be discussed shortly). In\nthe second step, the processes are partitioned into two groups. The first contains { P0,\nP1} and is responsible for sorting the elements that are smaller than or equal to\nseven, and the second group contains processes { P2, P3, P4} and is responsible for\nsorting the elements that are greater than seven. Note that the sizes of these process\ngroups were created to match the relative size of the smaller than and larger than the\npivot arrays. Now, the steps of pivot selection, local, and global rearrangement are\nrecursively repeated for each process group and sub-array, until a sub-array is\nassigned to a single process, in which case it proceeds to sort it locally. Also note that\nthese final local sub-arrays will in general be of different size, as they depend on the\nelements that were selected to act as pivots. \nFigure 9.18. An example of the execution of an efficient shared-\naddress-space quicksort algorithm.\nIn order to globally rearrange the elements of A into the smaller and larger sub-arrays we need\nto know where each element of A will end up going at the end of that rearrangement. One way\nof doing this rearrangement is illustrated at the bottom of Figure 9.19 . In this approach, S is\nobtained by concatenating the various Si blocks over all the processes, in increasing order of\nprocess label. Similarly, L is obtained by concatenating the various Li blocks in the same order.\nAs a result, for process Pi, the j th element of its Si sub-block will be stored at location\n, and the j th element of its Li sub-block will be stored at location \n .\nFigure 9.19. Efficient global rearrangement of the array.\nThese locations can be easily computed using the prefix-sum operation described in Section 4.3 .\nTwo prefix-sums are computed, one involving the sizes of the Si sub-blocks and the other the\nsizes of the Li sub-blocks. Let Q and R be the arrays of size p that store these prefix sums,\nrespectively. Their elements will be\nNote that for each process Pi, Qi is the starting location in the final array where its lower-than-\nthe-pivot element will be stored, and Ri is the ending location in the final array where its\ngreater-than-the-pivot elements will be stored. Once these locations have been determined, the\noverall rearrangement of A can be easily performed by using an auxiliary array A' of size n.\nThese steps are illustrated in Figure 9.19 . Note that the above definition of prefix-sum is slightly\ndifferent from that described in Section 4.3 , in the sense that the value that is computed for\nlocation Qi (or Ri) does not include Si (or Li) itself. This type of prefix-sum is sometimes referred\nto as non-inclusive  prefix-sum.\nAnalysis  The complexity of the shared-address-space formulation of the quicksort algorithm\ndepends on two things. The first is the amount of time it requires to split a particular array into\nthe smaller-than- and the", "doc_id": "1f22a396-8cc2-49eb-9668-5ea06a3dc014", "embedding": null, "doc_hash": "d93f58c46989dcb3412e338f5cfd2f4cb52f9634a03a25f8bb23a032171035f9", "extra_info": null, "node_info": {"start": 1128745, "end": 1132413}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2be39aa6-17c4-47fb-89b4-4a5ef8848189", "3": "cdda3407-53a0-495a-aaf7-ce576a106b8c"}}, "__type__": "1"}, "cdda3407-53a0-495a-aaf7-ce576a106b8c": {"__data__": {"text": "array A' of size n.\nThese steps are illustrated in Figure 9.19 . Note that the above definition of prefix-sum is slightly\ndifferent from that described in Section 4.3 , in the sense that the value that is computed for\nlocation Qi (or Ri) does not include Si (or Li) itself. This type of prefix-sum is sometimes referred\nto as non-inclusive  prefix-sum.\nAnalysis  The complexity of the shared-address-space formulation of the quicksort algorithm\ndepends on two things. The first is the amount of time it requires to split a particular array into\nthe smaller-than- and the greater-than-the-pivot sub-arrays, and the second is the degree to\nwhich the various pivots being selected lead to balanced partitions. In this section, to simplify\nour analysis, we will assume that pivot selection always results in balanced partitions. However,\nthe issue of proper pivot selection and its impact on the overall parallel performance is\naddressed in Section 9.4.4 .\nGiven an array of n elements and p processes, the shared-address-space formulation of the\nquicksort algorithm needs to perform four steps: (i) determine and broadcast the pivot; (ii)\nlocally rearrange the array assigned to each process; (iii) determine the locations in the globally\nrearranged array that the local elements will go to; and (iv) perform the global rearrangement.\nThe first step can be performed in time Q(log p) using an efficient recursive doubling approach\nfor shared-address-space broadcast. The second step can be done in time Q(n/p) using the\ntraditional quicksort algorithm for splitting around a pivot element. The third step can be done\nin Q(log p) using two prefix sum operations. Finally, the fourth step can be done in at least time\nQ(n/p) as it requires us to copy the local elements to their final destination. Thus, the overall\ncomplexity of splitting an n-element array is Q(n/p) + Q(log p). This process is repeated for\neach of the two subarrays recursively on half the processes, until the array is split into p parts,\nat which point each process sorts the elements of the array assigned to it using the serial\nquicksort algorithm. Thus, the overall complexity of the parallel algorithm is:\nEquation 9.8\nThe communication overhead in the above formulation is reflected in the Q(log2 p) term, which\nleads to an overall isoefficiency of Q(p log2 p). It is interesting to note that the overall scalability\nof the algorithm is determined by the amount of time required to perform the pivot broadcast\nand the prefix sum operations.\nMessage-Passing Parallel Formulation\nThe quicksort formulation for message-passing systems follows the general structure of the\nshared-address-space formulation. However, unlike the shared-address-space case in which\narray A and the globally rearranged array A' are stored in shared memory and can be accessed\nby all the processes, these arrays are now explicitly distributed among the processes. This\nmakes the task of splitting A somewhat more involved.\nIn particular, in the message-passing version of the parallel quicksort, each process stores n/p\nelements of array A. This array is also partitioned around a particular pivot element using a\ntwo-phase approach. In the first phase (which is similar to the shared-address-space\nformulation), the locally stored array Ai at process Pi is partitioned into the smaller-than- and\nlarger-than-the-pivot sub-arrays Si and Li locally. In the next phase, the algorithm first\ndetermines which processes will be responsible for recursively sorting the smaller-than-the-\npivot sub-arrays (i.e., \n) and which process will be responsible for recursively sorting\nthe larger-than-the-pivot", "doc_id": "cdda3407-53a0-495a-aaf7-ce576a106b8c", "embedding": null, "doc_hash": "9b70743abbded66665c81641ec1f1bc441fb3dfc5bb76d4c91a04d5d5b1a7fc3", "extra_info": null, "node_info": {"start": 1132424, "end": 1136065}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1f22a396-8cc2-49eb-9668-5ea06a3dc014", "3": "e52f0d76-156d-41bb-8441-b7634e00b072"}}, "__type__": "1"}, "e52f0d76-156d-41bb-8441-b7634e00b072": {"__data__": {"text": "around a particular pivot element using a\ntwo-phase approach. In the first phase (which is similar to the shared-address-space\nformulation), the locally stored array Ai at process Pi is partitioned into the smaller-than- and\nlarger-than-the-pivot sub-arrays Si and Li locally. In the next phase, the algorithm first\ndetermines which processes will be responsible for recursively sorting the smaller-than-the-\npivot sub-arrays (i.e., \n) and which process will be responsible for recursively sorting\nthe larger-than-the-pivot sub-arrays (i.e., \n ). Once this is done, then the processes\nsend their Si and Li arrays to the corresponding processes. After that, the processes are\npartitioned into the two groups, one for S and one for L, and the algorithm proceeds\nrecursively. The recursion terminates when each sub-array is assigned to a single process, at\nwhich point it is sorted locally.\nThe method used to determine which processes will be responsible for sorting S and L is\nidentical to that for the shared-address-space formulation, which tries to partition the processes\nto match the relative size of the two sub-arrays. Let pS and pL be the number of processes\nassigned to sort S and L, respectively. Each one of the pS processes will end up storing | S|/pS\nelements of the smaller-than-the-pivot sub-array, and each one of the pL processes will end up\nstoring | L|/pL elements of the larger-than-the-pivot sub-array. The method used to determine\nwhere each process Pi will send its Si and Li elements follows the same overall strategy as the\nshared-address-space formulation. That is, the various Si (or Li) sub-arrays will be stored in\nconsecutive locations in S (or L) based on the process number. The actual processes that will be\nresponsible for these elements are determined by the partition of S (or L) into pS (or pL) equal-\nsize segments, and can be computed using a prefix-sum operation. Note that each process Pi\nmay need to split its Si (or Li) sub-arrays into multiple segments and send each one to different\nprocesses. This can happen because its elements may be assigned to locations in S (or L) that\nspan more than one process. In general, each process may have to send its elements to two\ndifferent processes; however, there may be cases in which more than two partitions are\nrequired.\nAnalysis  Our analysis of the message-passing formulation of quicksort will mirror the\ncorresponding analysis of the shared-address-space formulation.\nConsider a message-passing parallel computer with p processes and O  (p) bisection bandwidth.\nThe amount of time required to split an array of size n is Q(log p) for broadcasting the pivot\nelement, Q(n/p) for splitting the locally assigned portion of the array, Q(log p) for performing\nthe prefix sums to determine the process partition sizes and the destinations of the various Si\nand Li sub-arrays, and the amount of time required for sending and receiving the various\narrays. This last step depends on how the processes are mapped on the underlying architecture\nand on the maximum number of processes that each process needs to communicate with. In\ngeneral, this communication step involves all-to-all personalized communication (because a\nparticular process may end-up receiving elements from all other processes), whose complexity\nhas a lower bound of Q(n/p). Thus, the overall complexity for the split is Q(n/p) + Q(log p),\nwhich is asymptotically similar to that of the shared-address-space formulation. As a result, the\noverall runtime is also the same as in Equation 9.8, and the algorithm has a similar isoefficiency\nfunction", "doc_id": "e52f0d76-156d-41bb-8441-b7634e00b072", "embedding": null, "doc_hash": "0f6393b932b7f142e71b94d29dd0e30f70579797675c2c567b6493dc59fe92d3", "extra_info": null, "node_info": {"start": 1136089, "end": 1139683}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "cdda3407-53a0-495a-aaf7-ce576a106b8c", "3": "5cfdde5a-2109-4317-bf65-aed75f379543"}}, "__type__": "1"}, "5cfdde5a-2109-4317-bf65-aed75f379543": {"__data__": {"text": "on the maximum number of processes that each process needs to communicate with. In\ngeneral, this communication step involves all-to-all personalized communication (because a\nparticular process may end-up receiving elements from all other processes), whose complexity\nhas a lower bound of Q(n/p). Thus, the overall complexity for the split is Q(n/p) + Q(log p),\nwhich is asymptotically similar to that of the shared-address-space formulation. As a result, the\noverall runtime is also the same as in Equation 9.8, and the algorithm has a similar isoefficiency\nfunction of Q(p log2 p).\n9.4.4 Pivot Selection\nIn the parallel quicksort algorithm, we glossed over pivot selection. Pivot selection is\nparticularly difficult, and it significantly affects the algorithm's performance. Consider the case\nin which the first pivot happens to be the largest element in the sequence. In this case, after the\nfirst split, one of the processes will be assigned only one element, and the remaining p - 1\nprocesses will be assigned n - 1 elements. Hence, we are faced with a problem whose size has\nbeen reduced only by one element but only p - 1 processes will participate in the sorting\noperation. Although this is a contrived example, it illustrates a significant problem with\nparallelizing the quicksort algorithm. Ideally, the split should be done such that each partition\nhas a non-trivial fraction of the original array.\nOne way to select pivots is to choose them at random as follows. During the ith split, one\nprocess in each of the process groups randomly selects one of its elements to be the pivot for\nthis partition. This is analogous to the random pivot selection in the sequential quicksort\nalgorithm. Although this method seems to work for sequential quicksort, it is not well suited to\nthe parallel formulation. To see this, consider the case in which a bad pivot is selected at some\npoint. In sequential quicksort, this leads to a partitioning in which one subsequence is\nsignificantly larger than the other. If all subsequent pivot selections are good, one poor pivot\nwill increase the overall work by at most an amount equal to the length of the subsequence;\nthus, it will not significantly degrade the performance of sequential quicksort. In the parallel\nformulation, however, one poor pivot may lead to a partitioning in which a process becomes\nidle, and that will persist throughout the execution of the algorithm.\nIf the initial distribution of elements in each process is uniform, then a better pivot selection\nmethod can be derived. In this case, the n/p elements initially stored at each process form a\nrepresentative sample of all n elements. In other words, the median of each n/p-element\nsubsequence is very close to the median of the entire n-element sequence. Why is this a good\npivot selection scheme under the assumption of identical initial distributions? Since the\ndistribution of elements on each process is the same as the overall distribution of the n\nelements, the median selected to be the pivot during the first step is a good approximation of\nthe overall median. Since the selected pivot is very close to the overall median, roughly half of\nthe elements in each process are smaller and the other half larger than the pivot. Therefore, the\nfirst split leads to two partitions, such that each of them has roughly n/2 elements. Similarly,\nthe elements assigned to each process of the group that is responsible for sorting the smaller-\nthan-the-pivot elements (and the group responsible for sorting the larger-than-the-pivot\nelements) have the same distribution as the n/2 smaller (or larger) elements of the original list.\nThus, the split not only maintains load balance but also preserves the assumption of uniform\nelement distribution in the process group. Therefore, the application of the same pivot", "doc_id": "5cfdde5a-2109-4317-bf65-aed75f379543", "embedding": null, "doc_hash": "7e207594e5f18dc2c57c775a1cbb6fdfc82d184826a8687380199871e8cb5d27", "extra_info": null, "node_info": {"start": 1139654, "end": 1143478}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e52f0d76-156d-41bb-8441-b7634e00b072", "3": "84b17ff6-c036-4958-b269-f3c0ea6d9069"}}, "__type__": "1"}, "84b17ff6-c036-4958-b269-f3c0ea6d9069": {"__data__": {"text": "the pivot. Therefore, the\nfirst split leads to two partitions, such that each of them has roughly n/2 elements. Similarly,\nthe elements assigned to each process of the group that is responsible for sorting the smaller-\nthan-the-pivot elements (and the group responsible for sorting the larger-than-the-pivot\nelements) have the same distribution as the n/2 smaller (or larger) elements of the original list.\nThus, the split not only maintains load balance but also preserves the assumption of uniform\nelement distribution in the process group. Therefore, the application of the same pivot selection\nscheme to the sub-groups of processes continues to yield good pivot selection.\nCan we really assume that the n/p elements in each process have the same distribution as the\noverall sequence? The answer depends on the application. In some applications, either the\nrandom or the median pivot selection scheme works well, but in others neither scheme delivers\ngood performance. Two additional pivot selection schemes are examined in Problems 9.20 and\n9.21.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n9.5 Bucket and Sample Sort\nA popular serial algorithm for sorting an array of n elements whose values are uniformly\ndistributed over an interval [ a, b] is the bucket sort  algorithm. In this algorithm, the interval\n[a, b] is divided into m equal-sized subintervals referred to as buckets , and each element is\nplaced in the appropriate bucket. Since the n elements are uniformly distributed over the\ninterval [ a, b], the number of elements in each bucket is roughly n/m. The algorithm then sorts\nthe elements in each bucket, yielding a sorted sequence. The run time of this algorithm is Q(n\nlog(n/m)).Form = Q(n), it exhibits linear run time, Q(n). Note that the reason that bucket sort\ncan achieve such a low complexity is because it assumes that the n elements to be sorted are\nuniformly distributed over an interval [ a, b].\nParallelizing bucket sort is straightforward. Let n be the number of elements to be sorted and p\nbe the number of processes. Initially, each process is assigned a block of n/p elements, and the\nnumber of buckets is selected to be m = p. The parallel formulation of bucket sort consists of\nthree steps. In the first step, each process partitions its block of n/p elements into p sub-blocks,\none for each of the p buckets. This is possible because each process knows the interval [ a, b)\nand thus the interval for each bucket. In the second step, each process sends sub-blocks to the\nappropriate processes. After this step, each process has only the elements belonging to the\nbucket assigned to it. In the third step, each process sorts its bucket internally by using an\noptimal sequential sorting algorithm.\nUnfortunately, the assumption that the input elements are uniformly distributed over an interval\n[a, b] is not realistic. In most cases, the actual input may not have such a distribution or its\ndistribution may be unknown. Thus, using bucket sort may result in buckets that have a\nsignificantly different number of elements, thereby degrading performance. In such situations\nan algorithm called sample sort  will yield significantly better performance. The idea behind\nsample sort is simple. A sample of size s is selected from the n-element sequence, and the\nrange of the buckets is determined by sorting the sample and choosing m - 1 elements from the\nresult. These elements (called splitters ) divide the sample into m equal-sized buckets. After\ndefining the buckets, the algorithm proceeds in the same way as bucket sort. The performance\nof sample sort depends on the sample size s and the way it is selected from the n-element\nsequence.\nConsider a splitter selection scheme that guarantees that the", "doc_id": "84b17ff6-c036-4958-b269-f3c0ea6d9069", "embedding": null, "doc_hash": "d46225746a564a266b693eb53b9f07a8ce28e0421a879ee64edbff90edb87536", "extra_info": null, "node_info": {"start": 1143462, "end": 1147185}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5cfdde5a-2109-4317-bf65-aed75f379543", "3": "a0cb80fc-1895-4b6e-b85d-0e1952dad5ff"}}, "__type__": "1"}, "a0cb80fc-1895-4b6e-b85d-0e1952dad5ff": {"__data__": {"text": "called sample sort  will yield significantly better performance. The idea behind\nsample sort is simple. A sample of size s is selected from the n-element sequence, and the\nrange of the buckets is determined by sorting the sample and choosing m - 1 elements from the\nresult. These elements (called splitters ) divide the sample into m equal-sized buckets. After\ndefining the buckets, the algorithm proceeds in the same way as bucket sort. The performance\nof sample sort depends on the sample size s and the way it is selected from the n-element\nsequence.\nConsider a splitter selection scheme that guarantees that the number of elements ending up in\neach bucket is roughly the same for all buckets. Let n be the number of elements to be sorted\nand m be the number of buckets. The scheme works as follows. It divides the n elements into m\nblocks of size n/m each, and sorts each block by using quicksort. From each sorted block it\nchooses m - 1 evenly spaced elements. The m(m - 1) elements selected from all the blocks\nrepresent the sample used to determine the buckets. This scheme guarantees that the number\nof elements ending up in each bucket is less than 2 n/m (Problem 9.28).\nHow can we parallelize the splitter selection scheme? Let p be the number of processes. As in\nbucket sort, set m = p; thus, at the end of the algorithm, each process contains only the\nelements belonging to a single bucket. Each process is assigned a block of n/p elements, which\nit sorts sequentially. It then chooses p - 1 evenly spaced elements from the sorted block. Each\nprocess sends its p - 1 sample elements to one process \u2013 say P0. Process P0 then sequentially\nsorts the p(p - 1) sample elements and selects the p - 1 splitters. Finally, process P0 broadcasts\nthe p - 1 splitters to all the other processes. Now the algorithm proceeds in a manner identical\nto that of bucket sort. This algorithm is illustrated in Figure 9.20.\nFigure 9.20. An example of the execution of sample sort on an array\nwith 24 elements on three processes.\nAnalysis  We now analyze the complexity of sample sort on a message-passing computer with p\nprocesses and O (p) bisection bandwidth.\nThe internal sort of n/p elements requires time Q((n/p) log( n/p)), and the selection of p - 1\nsample elements requires time Q(p). Sending p - 1 elements to process P0 is similar to a gather\noperation ( Section 4.4 ); the time required is Q(p2). The time to internally sort the p(p - 1)\nsample elements at P0 is Q(p2 log p), and the time to select p - 1 splitters is Q(p). The p - 1\nsplitters are sent to all the other processes by using one-to-all broadcast ( Section 4.1 ), which\nrequires time Q(p log p). Each process can insert  these p - 1 splitters in its local sorted block of\nsize n/p by performing p - 1 binary searches. Each process thus partitions its block into p sub-\nblocks, one for each bucket. The time required for this partitioning is Q(p log(n/p)). Each\nprocess then sends sub-blocks to the appropriate processes (that is, buckets). The\ncommunication time for this step is difficult to compute precisely, as it depends on the size of\nthe sub-blocks to be communicated. These sub-blocks can vary arbitrarily between 0 and n/p.\nThus, the upper bound on the communication time is O (n) + O (p log p).\nIf we assume that the elements stored in each process are uniformly distributed, then each sub-\nblock has roughly Q(n/p2) elements. In this case, the parallel run time is\nEquation 9.9\nIn this case, the isoefficiency function is Q(p3 log", "doc_id": "a0cb80fc-1895-4b6e-b85d-0e1952dad5ff", "embedding": null, "doc_hash": "47f28e289d8870e6b7da837f3de726439314a5bf59cdc07ee355947cb912c14f", "extra_info": null, "node_info": {"start": 1147171, "end": 1150677}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "84b17ff6-c036-4958-b269-f3c0ea6d9069", "3": "3112b129-b348-447b-a39d-4f80d933e111"}}, "__type__": "1"}, "3112b129-b348-447b-a39d-4f80d933e111": {"__data__": {"text": "buckets). The\ncommunication time for this step is difficult to compute precisely, as it depends on the size of\nthe sub-blocks to be communicated. These sub-blocks can vary arbitrarily between 0 and n/p.\nThus, the upper bound on the communication time is O (n) + O (p log p).\nIf we assume that the elements stored in each process are uniformly distributed, then each sub-\nblock has roughly Q(n/p2) elements. In this case, the parallel run time is\nEquation 9.9\nIn this case, the isoefficiency function is Q(p3 log p). If bitonic sort is used to sort the p(p - 1)\nsample elements, then the time for sorting the sample would be Q(p log p), and the isoefficiency\nwill be reduced to Q(p2 log p) (Problem 9.30).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n9.6 Other Sorting Algorithms\nAs mentioned in the introduction to this chapter, there are many sorting algorithms, and we\ncannot explore them all in this chapter. However, in this section we briefly present two\nadditional sorting algorithms that are important both practically and theoretically. Our\ndiscussion of these schemes will be brief. Refer to the bibliographic remarks ( Section 9.7) for\nreferences on these and other algorithms.\n9.6.1 Enumeration Sort\nAll the sorting algorithms presented so far are based on compare-exchange operations. This\nsection considers an algorithm based on enumeration sort , which does not use compare-\nexchange. The basic idea behind enumeration sort is to determine the rank of each element.\nThe rank of an element ai is the number of elements smaller than ai in the sequence to be\nsorted. The rank of ai can be used to place it in its correct position in the sorted sequence.\nSeveral parallel algorithms are based on enumeration sort. Here we present one such algorithm\nthat is suited to the CRCW PRAM model. This formulation sorts n elements by using n2 processes\nin time Q(1).\nAssume that concurrent writes to the same memory location of the CRCW PRAM result in the\nsum of all the values written being stored at that location ( Section 2.4.1). Consider the n2\nprocesses as being arranged in a two-dimensional grid. The algorithm consists of two steps.\nDuring the first step, each column j of processes computes the number of elements smaller than\naj . During the second step, each process P1,j of the first row places aj in its proper position as\ndetermined by its rank. The algorithm is shown in Algorithm 9.7. It uses an auxiliary array C\n[1...n] to store the rank of each element. The crucial steps of this algorithm are lines 7 and 9.\nThere, each process Pi,j, writes 1 in C [j] if the element A[i] is smaller than A[j] and writes 0\notherwise. Because of the additive-write conflict resolution scheme, the effect of these\ninstructions is to count the number of elements smaller than A[j] and thus compute its rank.\nThe run time of this algorithm is Q(1). Modifications of this algorithm for various parallel\narchitectures are discussed in Problem 9.26.\nAlgorithm 9.7 Enumeration sort on a CRCW PRAM with additive-write\nconflict resolution.\n1.   procedure  ENUM SORT ( n) \n2.   begin \n3.      for each process P1,j do \n4.          C[j] :=0; \n5.      for each process Pi,j do \n6.          if (A[i] < A[j]) or ( A[i]= A[j] and i < j) then \n7.             ", "doc_id": "3112b129-b348-447b-a39d-4f80d933e111", "embedding": null, "doc_hash": "1770b2659924dabd3433361bf01c534845fcb0379a0d784ea3fa5584e4545765", "extra_info": null, "node_info": {"start": 1150767, "end": 1154002}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a0cb80fc-1895-4b6e-b85d-0e1952dad5ff", "3": "6bcaede3-7b9b-421d-b5a8-a8e91a6bf345"}}, "__type__": "1"}, "6bcaede3-7b9b-421d-b5a8-a8e91a6bf345": {"__data__": {"text": "Enumeration sort on a CRCW PRAM with additive-write\nconflict resolution.\n1.   procedure  ENUM SORT ( n) \n2.   begin \n3.      for each process P1,j do \n4.          C[j] :=0; \n5.      for each process Pi,j do \n6.          if (A[i] < A[j]) or ( A[i]= A[j] and i < j) then \n7.              C[j] := 1; \n8.          else \n9.              C[j] := 0; \n10.     for each process P1,j do \n11.         A[C[j]] := A[j]; \n12.  end ENUM_SORT \n9.6.2 Radix Sort\nThe radix sort  algorithm relies on the binary representation of the elements to be sorted. Let b\nbe the number of bits in the binary representation of an element. The radix sort algorithm\nexamines the elements to be sorted r bits at a time, where r < b. Radix sort requires b/r\niterations. During iteration i , it sorts the elements according to their ith least significant block of\nr bits. For radix sort to work properly, each of the b/r sorts must be stable. A sorting algorithm\nis stable  if its output preserves the order of input elements with the same value. Radix sort is\nstable if it preserves the input order of any two r -bit blocks when these blocks are equal. The\nmost common implementation of the intermediate b/r radix-2r sorts uses enumeration sort\n(Section 9.6.1 ) because the range of possible values [0...2r - 1] is small. For such cases,\nenumeration sort significantly outperforms any comparison-based sorting algorithm.\nConsider a parallel formulation of radix sort for n elements on a message-passing computer\nwith n processes. The parallel radix sort algorithm is shown in Algorithm 9.8 . The main loop of\nthe algorithm (lines 3\u201317) performs the b/r enumeration sorts of the r-bit blocks. The\nenumeration sort is performed by using the prefix_sum()  and parallel_sum()  functions. These\nfunctions are similar to those described in Sections 4.1 and 4.3. During each iteration of the\ninner loop (lines 6\u201315), radix sort determines the position of the elements with an r-bit value of\nj . It does this by summing all the elements with the same value and then assigning them to\nprocesses. The variable rank holds the position of each element. At the end of the loop (line\n16), each process sends its element to the appropriate process. Process labels determine the\nglobal order of sorted elements.\nAlgorithm 9.8 A parallel radix sort algorithm, in which each element of\nthe array A[1...n] to be sorted is assigned to one process. The function\nprefix sum()  computes the prefix sum of the flag variable, and the\nfunction parallel sum()  returns the total sum of the flag variable.\n1.   procedure  RADIX SORT( A, r) \n2.   begin \n3.      for i := 0 to b/r - 1 do \n4.      begin \n5.         offset := 0 ; \n6.         for j := 0 to 2r - 1 do \n7.         begin \n8.            flag := 0 ; \n9.            if the ith least significant r-bit block of A[Pk] = j then \n10.", "doc_id": "6bcaede3-7b9b-421d-b5a8-a8e91a6bf345", "embedding": null, "doc_hash": "92e8c17b28a978fe2b0b6814c26dd5a22ff86b534729b046e96b39ece16946f9", "extra_info": null, "node_info": {"start": 1154239, "end": 1157063}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3112b129-b348-447b-a39d-4f80d933e111", "3": "c4eaf983-c17b-40dd-88a6-b6459ce896d5"}}, "__type__": "1"}, "c4eaf983-c17b-40dd-88a6-b6459ce896d5": {"__data__": {"text": "RADIX SORT( A, r) \n2.   begin \n3.      for i := 0 to b/r - 1 do \n4.      begin \n5.         offset := 0 ; \n6.         for j := 0 to 2r - 1 do \n7.         begin \n8.            flag := 0 ; \n9.            if the ith least significant r-bit block of A[Pk] = j then \n10.               flag := 1 ; \n11.           index := prefix_sum(flag)  \n12.           if flag = 1 then \n13.               rank := offset + index ; \n14.           offset := parallel_sum(flag) ; \n15.        endfor \n16.        each process Pk send its element A[Pk] to process Prank; \n17.     endfor \n18.  end RADIX_SORT \nAs shown in Sections 4.1 and 4.3, the complexity of both the parallel_sum()  and prefix_sum()\noperations is Q(log n) on a message-passing computer with n processes. The complexity of the\ncommunication step on line 16 is Q(n). Thus, the parallel run time of this algorithm is\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n9.7 Bibliographic Remarks\nKnuth [ Knu73 ] discusses sorting networks and their history. The question of whether a sorting\nnetwork could sort n elements in time O (log n) remained open for a long time. In 1983, Ajtai,\nKomlos, and Szemeredi [ AKS83 ] discovered a sorting network that could sort n elements in\ntime O (log n) by using O (n log n) comparators. Unfortunately, the constant of their sorting\nnetwork is quite large (many thousands), and thus is not practical. The bitonic sorting network\nwas discovered by Batcher [ Bat68 ], who also discovered the network for odd-even sort. These\nwere the first networks capable of sorting n elements in time O (log2 n). Stone [ Sto71 ] maps\nthe bitonic sort onto a perfect-shuffle interconnection network, sorting n elements by using n\nprocesses in time O (log2 n). Siegel [ Sie77 ]shows that bitonic sort can also be performed on the\nhypercube in time O (log2 n). The block-based hypercube formulation of bitonic sort is discussed\nin Johnsson [ Joh84 ] and Fox et al. [FJL+88]. Algorithm 9.1  is adopted from [ FJL+88]. The\nshuffled row-major indexing formulation of bitonic sort on a mesh-connected computer is\npresented by Thompson and Kung [ TK77]. They also show how the odd-even merge sort can be\nused with snakelike row-major indexing. Nassimi and Sahni [ NS79 ] present a row-major\nindexed bitonic sort formulation for a mesh with the same performance as shuffled row-major\nindexing. An improved version of the mesh odd-even merge is proposed by Kumar and\nHirschberg [ KH83]. The compare-split operation can be implemented in many ways. Baudet and\nStevenson [ BS78 ] describe one way to perform this operation. An alternative way of performing\na compare-split operation based on a bitonic sort (Problem 9.1) that requires no additional\nmemory was discovered by Hsiao and Menon [ HM80].\nThe odd-even transposition sort is described by Knuth [ Knu73 ]. Several early references to\nparallel sorting by", "doc_id": "c4eaf983-c17b-40dd-88a6-b6459ce896d5", "embedding": null, "doc_hash": "cf2efa3587ad9176cf31c0d26031ceb2279f933cf5f3dfe5b72ed9943804c9ac", "extra_info": null, "node_info": {"start": 1157105, "end": 1159946}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6bcaede3-7b9b-421d-b5a8-a8e91a6bf345", "3": "81444144-fe76-46e7-a689-f5a194530428"}}, "__type__": "1"}, "81444144-fe76-46e7-a689-f5a194530428": {"__data__": {"text": "shuffled row-major\nindexing. An improved version of the mesh odd-even merge is proposed by Kumar and\nHirschberg [ KH83]. The compare-split operation can be implemented in many ways. Baudet and\nStevenson [ BS78 ] describe one way to perform this operation. An alternative way of performing\na compare-split operation based on a bitonic sort (Problem 9.1) that requires no additional\nmemory was discovered by Hsiao and Menon [ HM80].\nThe odd-even transposition sort is described by Knuth [ Knu73 ]. Several early references to\nparallel sorting by odd-even transposition are given by Knuth [ Knu73 ] and Kung [ Kun80 ]. The\nblock-based extension of the algorithm is due to Baudet and Stevenson [ BS78 ]. Another\nvariation of block-based odd-even transposition sort that uses bitonic merge-split is described\nby DeWitt, Friedland, Hsiao, and Menon [ DFHM82]. Their algorithm uses p processes and runs\nin time O (n + n log(n/p)). In contrast to the algorithm of Baudet and Stevenson [ BS78 ], which\nis faster but requires 4 n/p storage locations in each process, the algorithm of DeWitt et al.\nrequires only ( n/p) + 1 storage locations to perform the compare-split operation.\nThe shellsort algorithm described in Section 9.3.2  is due to Fox et al. [FJL+88]. They show that,\nas n increases, the probability that the final odd-even transposition will exhibit worst-case\nperformance (in other words, will require p phases) diminishes. A different shellsort algorithm\nbased on the original sequential algorithm [ She59 ] is described by Quinn [ Qui88 ].\nThe sequential quicksort algorithm is due to Hoare [ Hoa62 ]. Sedgewick [ Sed78 ] provides a\ngood reference on the details of the implementation and how they affect its performance. The\nrandom pivot-selection scheme is described and analyzed by Robin [ Rob75]. The algorithm for\nsequence partitioning on a single process was suggested by Sedgewick [ Sed78 ] and used in\nparallel formulations by Raskin [ Ras78 ], Deminet [ Dem82 ], and Quinn [ Qui88 ]. The CRCW\nPRAM algorithm ( Section 9.4.2 ) is due to Chlebus and Vrto [ CV91 ]. Many other quicksort-based\nalgorithms for PRAM and shared-address-space parallel computers have been developed that\ncan sort n elements in time Q(log n) by using Q(n) processes. Martel and Gusfield [ MG89]\ndeveloped a quicksort algorithm for a CRCW PRAM that requires space O (n3) on the average.\nAn algorithm suited to shared-address-space parallel computers with fetch-and-add capabilities\nwas discovered by Heidelberger, Norton, and Robinson [ HNR90]. Their algorithm runs in time\nQ(log n) on the average and can be adapted for commercially available shared-address-space\ncomputers. The hypercube formulation of quicksort described in Problem 9.17 is due to Wagar\n[Wag87]. His hyperquicksort algorithm uses the median-based pivot-selection scheme and\nassumes that the elements in each process have the same distribution. His experimental results\nshow that hyperquicksort is faster than bitonic sort on a hypercube. An alternate pivot-selection\nscheme (Problem 9.20) was implemented by Fox et al. [FJL+88]. This scheme significantly\nimproves the performance of hyperquicksort when the elements are not evenly distributed in\neach process. Plaxton [ Pla89] describes a quicksort algorithm on a p-process hypercube that\nsorts n elements in time O ((n log n)/p +(n log3/2 p)/p +log3 p log(n/p)). This algorithm uses a\ntime", "doc_id": "81444144-fe76-46e7-a689-f5a194530428", "embedding": null, "doc_hash": "5d635926dfae1dfef5b01a866b89bae44d2f9c1a69fbed2e78048fe0dd67598d", "extra_info": null, "node_info": {"start": 1159636, "end": 1163034}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c4eaf983-c17b-40dd-88a6-b6459ce896d5", "3": "a87cd04d-2952-44d0-9dd1-6aed64886bf8"}}, "__type__": "1"}, "a87cd04d-2952-44d0-9dd1-6aed64886bf8": {"__data__": {"text": "His experimental results\nshow that hyperquicksort is faster than bitonic sort on a hypercube. An alternate pivot-selection\nscheme (Problem 9.20) was implemented by Fox et al. [FJL+88]. This scheme significantly\nimproves the performance of hyperquicksort when the elements are not evenly distributed in\neach process. Plaxton [ Pla89] describes a quicksort algorithm on a p-process hypercube that\nsorts n elements in time O ((n log n)/p +(n log3/2 p)/p +log3 p log(n/p)). This algorithm uses a\ntime O ((n/p) log log p + log2 p log(n/p)) parallel selection algorithm to determine the perfect\npivot selection. The mesh formulation of quicksort (Problem 9.24) is due to Singh, Kumar,\nAgha, and Tomlinson [ SKAT91a]. They also describe a modification to the algorithm that\nreduces the complexity of each step by a factor of Q(log p).\nThe sequential bucket sort algorithm was first proposed by Isaac and Singleton in 1956.\nHirschberg [ Hir78] proposed a bucket sort algorithm for the EREW PRAM model. This algorithm\nsorts n elements in the range [0... n - 1] in time Q(log n) by using n processes. A side effect of\nthis algorithm is that duplicate elements are eliminated. Their algorithm requires space Q(n2).\nHirschberg [ Hir78 ] generalizes this algorithm so that duplicate elements remain in the sorted\narray. The generalized algorithm sorts n elements in time Q(k log n) by using n1+1/k processes,\nwhere k is an arbitrary integer.\nThe sequential sample sort algorithm was discovered by Frazer and McKellar [ FM70 ]. The\nparallel sample sort algorithm ( Section 9.5 ) was discovered by Shi and Schaeffer [ SS90 ].\nSeveral parallel formulations of sample sort for different parallel architectures have been\nproposed. Abali, Ozguner, and Bataineh [ AOB93] presented a splitter selection scheme that\nguarantees that the number of elements ending up in each bucket is n/p. Their algorithm\nrequires time O ((n log n)/p + p log2 n), on average, to sort n elements on a p-process\nhypercube. Reif and Valiant [ RV87 ] present a sample sort algorithm that sorts n elements on an\nn-process hypercube-connected computer in time O (log n) with high probability. Won and\nSahni [ WS88 ] and Seidel and George [ SG88 ] present parallel formulations of a variation of\nsample sort called bin sort  [FKO86 ].\nMany other parallel sorting algorithms have been proposed. Various parallel sorting algorithms\ncan be efficiently implemented on a PRAM model or on shared-address-space computers. Akl\n[Akl85], Borodin and Hopcroft [ BH82 ], Shiloach and Vishkin [ SV81 ], and Bitton, DeWitt, Hsiao,\nand Menon [ BDHM84 ] provide a good survey of the subject. Valiant [ Val75 ] proposed a sorting\nalgorithm for a shared-address-space SIMD computer that sorts by merging. It sorts n elements\nin time O (log n log log n) by using n/2 processes. Reischuk [ Rei81 ] was the first to develop an\nalgorithm that sorted n elements in time Q(log n) for an n -process PRAM. Cole [ Col88 ]\ndeveloped a parallel merge-sort algorithm that sorts n elements in time Q(log n) on an EREW\nPRAM. Natvig [ Nat90 ] has shown that the constants hidden behind the asymptotic notation are\nvery large. In fact, the Q(log2 n) bitonic sort outperforms the Q(log n) merge sort as long as n is\nsmaller", "doc_id": "a87cd04d-2952-44d0-9dd1-6aed64886bf8", "embedding": null, "doc_hash": "192d4ade94d81aaab5510d702bb5a69676aa7ad7dbcbcd48220aa106f4b72cd3", "extra_info": null, "node_info": {"start": 1163073, "end": 1166314}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "81444144-fe76-46e7-a689-f5a194530428", "3": "320d8ab1-fc93-41f5-a823-7ac83da0740c"}}, "__type__": "1"}, "320d8ab1-fc93-41f5-a823-7ac83da0740c": {"__data__": {"text": "O (log n log log n) by using n/2 processes. Reischuk [ Rei81 ] was the first to develop an\nalgorithm that sorted n elements in time Q(log n) for an n -process PRAM. Cole [ Col88 ]\ndeveloped a parallel merge-sort algorithm that sorts n elements in time Q(log n) on an EREW\nPRAM. Natvig [ Nat90 ] has shown that the constants hidden behind the asymptotic notation are\nvery large. In fact, the Q(log2 n) bitonic sort outperforms the Q(log n) merge sort as long as n is\nsmaller than 7.6 x 1022! Plaxton [ Pla89 ] has developed a hypercube sorting algorithm, called\nsmoothsort , that runs asymptotically faster than any previously known algorithm for that\narchitecture. Leighton [ Lei85a ] proposed a sorting algorithm, called columnsort , that consists\nof a sequence of sorts followed by elementary matrix operations. Columnsort is a generalization\nof Batcher's odd-even sort. Nigam and Sahni [ NS93] presented an algorithm based on\nLeighton's columnsort for reconfigurable meshes with buses that sorts n elements on an n2-\nprocess mesh in time O (1).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n \nProblems\n9.1 Consider the following technique for performing the compare-split operation. Let x 1 ,\nx 2 , ..., xk be the elements stored at process Pi in increasing order, and let y 1 , y 2 , ..., yk\nbe the elements stored at process Pj in decreasing order. Process Pi sends x 1 to Pj .\nProcess Pj compares x 1 with y 1 and then sends the larger element back to process Pi and\nkeeps the smaller element for itself. The same procedure is repeated for pairs ( x 2 , y 2 ),\n(x 3 , y 3 ), ..., ( xk , yk ). If for any pair ( xl , yl ) for 1 \n  l \n k , xl \n yl , then no more\nexchanges are needed. Finally, each process sorts its elements. Show that this method\ncorrectly performs a compare-split operation. Analyze its run time, and compare the\nrelative merits of this method to those of the method presented in the text. Is this method\nbetter suited for MIMD or SIMD parallel computers?\n9.2 Show that the \n relation, as defined in Section 9.1  for blocks of elements, is a partial\nordering relation.\nHint:  A relation is a partial ordering  if it is reflexive, antisymmetric, and transitive.\n9.3 Consider the sequence s = {a 0 , a 1 , ..., an -1 },where n is a power of 2. In the\nfollowing cases, prove that the sequences s 1 and s 2 obtained by performing the bitonic\nsplit operation described in Section 9.2.1  , on the sequence s , satisfy the properties that\n(1) s 1 and s 2 are bitonic sequences, and (2) the elements of s 1 are smaller than the\nelements of s 2 :\ns is a bitonic sequence such that a 0 \n a 1 \n \u00b7\u00b7\u00b7 \n  an /2-1 and a n/ 2 \n an /2+1 \n \u00b7\u00b7\u00b7\n an -1 .a.\ns is a bitonic sequence such that a 0 \n a 1 \n \u00b7\u00b7\u00b7 \n  ai and ai +1 \n ai +2 \n \u00b7\u00b7\u00b7 \n  an -\n1 for some i , 0 \n  i \n n - 1.b.\ns is a bitonic sequence that becomes increasing-decreasing after shifting its elements.c.\n9.4 In the parallel formulations of bitonic sort, we assumed that we had n processes\navailable to sort n items. Show how the algorithm needs to be modified when only n /2\nprocesses are available.\n9.5 Show that, in the hypercube formulation of bitonic sort, each bitonic merge of\nsequences of size 2k is performed on a k", "doc_id": "320d8ab1-fc93-41f5-a823-7ac83da0740c", "embedding": null, "doc_hash": "b30f80404ece01e9a964ccd351d8bb94ab8abbeedee1bcc513131315b30515b8", "extra_info": null, "node_info": {"start": 1166353, "end": 1169528}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a87cd04d-2952-44d0-9dd1-6aed64886bf8", "3": "50c3a265-0522-4066-beec-9f7f8254362b"}}, "__type__": "1"}, "50c3a265-0522-4066-beec-9f7f8254362b": {"__data__": {"text": "\n \u00b7\u00b7\u00b7 \n  ai and ai +1 \n ai +2 \n \u00b7\u00b7\u00b7 \n  an -\n1 for some i , 0 \n  i \n n - 1.b.\ns is a bitonic sequence that becomes increasing-decreasing after shifting its elements.c.\n9.4 In the parallel formulations of bitonic sort, we assumed that we had n processes\navailable to sort n items. Show how the algorithm needs to be modified when only n /2\nprocesses are available.\n9.5 Show that, in the hypercube formulation of bitonic sort, each bitonic merge of\nsequences of size 2k is performed on a k -dimensional hypercube and each sequence is\nassigned to a separate hypercube.\n9.6 Show that the parallel formulation of bitonic sort shown in Algorithm 9.1  is correct. In\nparticular, show that the algorithm correctly compare-exchanges elements and that the\nelements end up in the appropriate processes.\n9.7 Consider the parallel formulation of bitonic sort for a mesh-connected parallel\ncomputer. Compute the exact parallel run time of the following formulations:\na.\nOne that uses the row-major mapping shown in Figure 9.11(a)  for a mesh with store-\nand-forward routing.a.\nOne that uses the row-major snakelike mapping shown in Figure 9.11(b)  for a mesh\nwith store-and-forward routing.b.\nOne that uses the row-major shuffled mapping shown in Figure 9.11(c)  for a mesh\nwith store-and-forward routing.c.\nAlso, determine how the above run times change when cut-through routing is used.\n9.8 Show that the block-based bitonic sort algorithm that uses compare-split operations is\ncorrect.\n9.9 Consider a ring-connected parallel computer with n processes. Show how to map the\ninput wires of the bitonic sorting network onto the ring so that the communication cost is\nminimized. Analyze the performance of your mapping. Consider the case in which only p\nprocesses are available. Analyze the performance of your parallel formulation for this case.\nWhat is the largest number of processes that can be used while maintaining a cost-optimal\nparallel formulation? What is the isoefficiency function of your scheme?\n9.10 Prove that the block-based odd-even transposition sort yields a correct algorithm.\nHint:  This problem is similar to Problem 9.8.\n9.11 Show how to apply the idea of the shellsort algorithm (Section 9.3.2  ) to a p -process\nmesh-connected computer. Your algorithm does not need to be an exact copy of the\nhypercube formulation.\n9.12 Show how to parallelize the sequential shellsort algorithm for a p -process\nhypercube. Note that the shellsort algorithm presented in Section 9.3.2  is not an exact\nparallelization of the sequential algorithm.\n9.13 Consider the shellsort algorithm presented in Section 9.3.2  . Its performance\ndepends on the value of l , which is the number of odd and even phases performed during\nthe second phase of the algorithm. Describe a worst-case initial key distribution that will\nrequire l = Q (p ) phases. What is the probability of this worst-case scenario?\n9.14 In Section 9.4.1  we discussed a parallel formulation of quicksort for a CREW PRAM\nthat is based on assigning each subproblem to a separate process. This formulation uses n\nprocesses to sort n elements. Based on this approach, derive a parallel formulation that\nuses p processes, where ( p < n ). Derive expressions for the parallel run time, efficiency,\nand isoefficiency function. What is the maximum number of processes that your parallel\nformulation can use and still remain cost-optimal?\n9.15 Derive an algorithm that traverses the binary search tree constructed by the\nalgorithm in Algorithm 9.6  and determines the position of", "doc_id": "50c3a265-0522-4066-beec-9f7f8254362b", "embedding": null, "doc_hash": "e9126e2fd3e5b5aaab04be015ddf5b0c547fd3778a03818d780665f8a5d411c6", "extra_info": null, "node_info": {"start": 1169523, "end": 1173045}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "320d8ab1-fc93-41f5-a823-7ac83da0740c", "3": "7227b1ac-6680-4309-9435-d3e8b50e6e09"}}, "__type__": "1"}, "7227b1ac-6680-4309-9435-d3e8b50e6e09": {"__data__": {"text": "of quicksort for a CREW PRAM\nthat is based on assigning each subproblem to a separate process. This formulation uses n\nprocesses to sort n elements. Based on this approach, derive a parallel formulation that\nuses p processes, where ( p < n ). Derive expressions for the parallel run time, efficiency,\nand isoefficiency function. What is the maximum number of processes that your parallel\nformulation can use and still remain cost-optimal?\n9.15 Derive an algorithm that traverses the binary search tree constructed by the\nalgorithm in Algorithm 9.6  and determines the position of each element in the sorted\narray. Your algorithm should use n processes and solve the problem in time Q (log n ) on\nan arbitrary CRCW PRAM.\n9.16 Consider the PRAM formulation of the quicksort algorithm (Section 9.4.2  ). Compute\nthe average height of the binary tree generated by the algorithm.\n9.17 Consider the following parallel quicksort algorithm that takes advantage of the\ntopology of a p -process hypercube connected parallel computer. Let n be the number of\nelements to be sorted and p = 2d be the number of processes in a d -dimensional\nhypercube. Each process is assigned a block of n /p elements, and the labels of the\nprocesses define the global order of the sorted sequence. The algorithm starts by selecting\na pivot element, which is broadcast to all processes. Each process, upon receiving the\npivot, partitions its local elements into two blocks, one with elements smaller than the\npivot and one with elements larger than the pivot. Then the processes connected along the\nd th communication link exchange appropriate blocks so that one retains elements smaller\nthan the pivot and the other retains elements larger than the pivot. Specifically, each\nprocess with a 0 in the d th bit (the most significant bit) position of the binary\nrepresentation of its process label retains the smaller elements, and each process with a 1\nin the d th bit retains the larger elements. After this step, each process in the ( d - 1)-\ndimensional hypercube whose d th label bit is 0 will have elements smaller than the pivot,\nand each process in the other ( d - 1)-dimensional hypercube will have elements larger\nthan the pivot. This procedure is performed recursively in each subcube, splitting the\nsubsequences further. After d such splits \u2013 one along each dimension \u2013 the sequence is\nsorted with respect to the global ordering imposed on the processes. This does not mean\nthat the elements at each process are sorted. Therefore, each process sorts its local\nelements by using sequential quicksort. This hypercube formulation of quicksort is shown\nin Algorithm 9.9  . The execution of the algorithm is illustrated in Figure 9.21  .\nFigure 9.21. The execution of the hypercube formulation of\nquicksort for d = 3. The three splits \u2013 one along each\ncommunication link \u2013 are shown in (a), (b), and (c). The second\ncolumn represents the partitioning of the n -element sequence\ninto subcubes. The arrows between subcubes indicate the\nmovement of larger elements. Each box is marked by the binary\nrepresentation of the process labels in that subcube. A * denotes\nthat all the binary combinations are included.\nAlgorithm 9.9 A parallel formulation of quicksort on a d -\ndimensional hypercube. B  is the n /p -element subsequence\nassigned to each process.\n1.   procedure  HYPERCUBE_QUICKSORT ( B, n) \n2.   begin \n3.      id := process's label; \n4.      for i := 1 to d do \n5.      begin \n6.         x := pivot; \n7.         partition B into B1 and B2 such that B1", "doc_id": "7227b1ac-6680-4309-9435-d3e8b50e6e09", "embedding": null, "doc_hash": "53825be63cb248297b5217edbfc171fcca81939d35bedb788b13f5ddf5193d80", "extra_info": null, "node_info": {"start": 1172948, "end": 1176481}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "50c3a265-0522-4066-beec-9f7f8254362b", "3": "ccbfbbaa-39e7-40b0-aea3-4dad7b8195cd"}}, "__type__": "1"}, "ccbfbbaa-39e7-40b0-aea3-4dad7b8195cd": {"__data__": {"text": "9.9 A parallel formulation of quicksort on a d -\ndimensional hypercube. B  is the n /p -element subsequence\nassigned to each process.\n1.   procedure  HYPERCUBE_QUICKSORT ( B, n) \n2.   begin \n3.      id := process's label; \n4.      for i := 1 to d do \n5.      begin \n6.         x := pivot; \n7.         partition B into B1 and B2 such that B1 \n x < B2; \n8.         if ith bit is 0 then \n9.         begin \n10.           send B2 to the process along the i th communication link; \n11.           C := subsequence received along the i th communication link; \n12.           B := B1 \n C ; \n13.        endif \n14.        else \n15.           send B1 to the process along the ith communication link; \n16.           C := subsequence received along the ith communication link; \n17.           B := B2 \n C ; \n18.        endelse \n19.     endfor \n20.     sort B using sequential quicksort; \n21.  end HYPERCUBE_QUICKSORT \nAnalyze the complexity of this hypercube-based parallel quicksort algorithm. Derive\nexpressions for the parallel runtime, speedup, and efficiency. Perform this analysis\nassuming that the elements that were initially assigned at each process are distributed\nuniformly.\n9.18 Consider the parallel formulation of quicksort for a d -dimensional hypercube\ndescribed in Problem 9.17. Show that after d splits \u2013 one along each communication link \u2013\nthe elements are sorted according to the global order defined by the process's labels.\n9.19 Consider the parallel formulation of quicksort for a d -dimensional hypercube\ndescribed in Problem 9.17. Compare this algorithm against the message-passing quicksort\nalgorithm described in Section 9.4.3  . Which algorithm is more scalable? Which algorithm\nis more sensitive on poor selection of pivot elements?\n9.20 An alternative way of selecting pivots in the parallel formulation of quicksort for a d -\ndimensional hypercube (Problem 9.17) is to select all the 2d - 1 pivots at once as follows:\nEach process picks a sample of l elements at random. a.\nAll processes together sort the sample of l x 2d items by using the shellsort algorithm\n(Section 9.3.2  ).b.\nChoose 2d - 1 equally distanced pivots from this list. c.\nBroadcast pivots so that all the processes know the pivots.d.\nHow does the quality of this pivot selection scheme depend on l ? Do you think l should be\na function of n ? Under what assumptions will this scheme select good pivots? Do you think\nd.\nthis scheme works when the elements are not identically distributed on each process?\nAnalyze the complexity of this scheme.\n9.21 Another pivot selection scheme for parallel quicksort for hypercube (Section 9.17) is\nas follows. During the split along the i th dimension, 2i -1 pairs of processes exchange\nelements. The pivot is selected in two steps. In the first step, each of the 2i -1 pairs of\nprocesses compute the median of their combined sequences. In the second step, the\nmedian of the 2i -1 medians is computed. This median of medians becomes the pivot for\nthe split along the i th communication link. Subsequent pivots", "doc_id": "ccbfbbaa-39e7-40b0-aea3-4dad7b8195cd", "embedding": null, "doc_hash": "9709d268a26ad3d465d62640efd86dd38920e1a63ce97a50dab8a5d7ae0e1149", "extra_info": null, "node_info": {"start": 1176727, "end": 1179756}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7227b1ac-6680-4309-9435-d3e8b50e6e09", "3": "53113cc1-1a1d-4eb2-8ec9-a70a8b6b60c8"}}, "__type__": "1"}, "53113cc1-1a1d-4eb2-8ec9-a70a8b6b60c8": {"__data__": {"text": "the complexity of this scheme.\n9.21 Another pivot selection scheme for parallel quicksort for hypercube (Section 9.17) is\nas follows. During the split along the i th dimension, 2i -1 pairs of processes exchange\nelements. The pivot is selected in two steps. In the first step, each of the 2i -1 pairs of\nprocesses compute the median of their combined sequences. In the second step, the\nmedian of the 2i -1 medians is computed. This median of medians becomes the pivot for\nthe split along the i th communication link. Subsequent pivots are selected in the same way\namong the participating subcubes. Under what assumptions will this scheme yield good\npivot selections? Is this better than the median scheme described in the text? Analyze the\ncomplexity of selecting the pivot.\nHint:  If A and B are two sorted sequences, each having n elements, then we can find the\nmedian of A \n B in time Q (log n ).\n9.22 In the parallel formulation of the quicksort algorithm on shared-address-space and\nmessage-passing architectures (Section 9.4.3  ) each iteration is followed by a barrier\nsynchronization. Is barrier synchronization necessary to ensure the correctness of the\nalgorithm? If not, then how does the performance change in the absence of barrier\nsynchronization?\n9.23 Consider the message-passing formulation of the quicksort algorithm presented in\nSection 9.4.3  . Compute the exact (that is, using ts , t w , and tc ) parallel run time and\nefficiency of the algorithm under the assumption of perfect pivots. Compute the various\ncomponents of the isoefficiency function of your formulation when\ntc = 1, tw = 1, ts = 1 a.\ntc = 1, tw = 1, ts = 10 b.\ntc = 1, tw = 10, ts = 100 c.\nfor cases in which the desired efficiency is 0.50, 0.75, and 0.95. Does the scalability of this\nformulation depend on the desired efficiency and the architectural characteristics of the\nmachine?\n9.24 Consider the following parallel formulation of quicksort for a mesh-connected\nmessage-passing parallel computer. Assume that one element is assigned to each process.\nThe recursive partitioning step consists of selecting the pivot and then rearranging the\nelements in the mesh so that those smaller than the pivot are in one part of the mesh and\nthose larger than the pivot are in the other. We assume that the processes in the mesh are\nnumbered in row-major order. At the end of the quicksort algorithm, elements are sorted\nwith respect to this order. Consider the partitioning step for an arbitrary subsequence\nillustrated in Figure 9.22(a)  . Let k be the length of this sequence, and let Pm , Pm +1 , ...,\nPm + k be the mesh processes storing it. Partitioning consists of the following four steps:\nFigure 9.22. (a) An arbitrary portion of a mesh that holds part of\nthe sequence to be sorted at some point during the execution of\nquicksort, and (b) a binary tree embedded into the same portion of\nthe mesh.\nA pivot is selected at random and sent to process Pm . Process Pm broadcasts this\npivot to all k processes by using an embedded tree, as shown in Figure 9.22(b)  . The\nroot ( Pm ) transmits the pivot toward the leaves. The tree embedding is also used in\nthe following steps.1.\nInformation is gathered at each process and passed up the tree. In particular, each\nprocess counts the number of elements smaller and larger than the pivot in both its\nleft and right subtrees. Each process knows the pivot value and therefore can\ndetermine if its element is smaller or larger. Each process propagates", "doc_id": "53113cc1-1a1d-4eb2-8ec9-a70a8b6b60c8", "embedding": null, "doc_hash": "347b3e40b9a7536a7f038a819d692ab24178cdb79a0fca24267d3c6124381862", "extra_info": null, "node_info": {"start": 1179556, "end": 1183034}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ccbfbbaa-39e7-40b0-aea3-4dad7b8195cd", "3": "058b01c2-178c-4b85-9daf-fdd7524fbcaa"}}, "__type__": "1"}, "058b01c2-178c-4b85-9daf-fdd7524fbcaa": {"__data__": {"text": "Pm . Process Pm broadcasts this\npivot to all k processes by using an embedded tree, as shown in Figure 9.22(b)  . The\nroot ( Pm ) transmits the pivot toward the leaves. The tree embedding is also used in\nthe following steps.1.\nInformation is gathered at each process and passed up the tree. In particular, each\nprocess counts the number of elements smaller and larger than the pivot in both its\nleft and right subtrees. Each process knows the pivot value and therefore can\ndetermine if its element is smaller or larger. Each process propagates two values to\nits parent: the number of elements smaller than the pivot and the number of\nelements larger than the pivot in the process's subtree. Because the tree embedded\nin the mesh is not complete, some nodes will not have left or right subtrees. At the\nend of this step, process Pm knows how many of the k elements are smaller and\nlarger than the pivot. If s is the number of the elements smaller than the pivot, then\nthe position of the pivot in the sorted sequence is Pm + s .2.\nInformation is propagated down the tree to enable each element to be moved to its\nproper position in the smaller or larger partitions. Each process in the tree receives\nfrom its parent the next empty position in the smaller and larger partitions.\nDepending on whether the element stored at each process is smaller or larger than\nthe pivot, the process propagates the proper information down to its subtrees.\nInitially, the position for elements smaller than the pivot is Pm and the position for\nelements larger than the pivot is Pm + s +1 .3.\nThe processes perform a permutation, and each element moves to the proper position\nin the smaller or larger partition.4.\nThis algorithm is illustrated in Figure 9.23  .\nFigure 9.23. Partitioning a sequence of 13 elements on a 4 x 4\nmesh: (a) row-major numbering of the mesh processes, (b) the\nelements stored in each process (the shaded element is the pivot),\n(c) the tree embedded on a portion of the mesh, (d) the number of\nsmaller or larger elements in the process of the first column after\nthe execution of the second step, (e) the destination of the smaller\nor larger elements propagated down to the processes in the first\ncolumn during the third step, (f) the destination of the elements at\nthe end of the third step, and (g) the locations of the elements\nafter one-to-one personalized communication.\nAnalyze the complexity of this mesh-based parallel quicksort algorithm. Derive expressions\nfor the parallel runtime, speedup, and efficiency. Perform this analysis assuming that the\nelements that were initially assigned at each process are distributed uniformly.\n9.25 Consider the quicksort formulation for a mesh described in Problem 9.24. Describe a\nscaled-down formulation that uses p < n processes. Analyze its parallel run time, speedup,\nand isoefficiency function.\n9.26 Consider the enumeration sort algorithm presented in Section 9.6.1  . Show how the\nalgorithm can be implemented on each of the following:\na CREW PRAMa.\na EREW PRAMb.\na hypercube-connected parallel computerc.\na mesh-connected parallel computer.d.\nAnalyze the performance of your formulations. Furthermore, show how you can extend this\nenumeration sort to a hypercube to sort n elements using p processes.\n9.27 Derive expressions for the speedup, efficiency, and isoefficiency function of the\nbucket sort parallel formulation presented in Section 9.5  . Compare these expressions with\nthe expressions for the other sorting algorithms presented in this chapter. Which parallel\nformulations perform better than bucket sort, and which perform", "doc_id": "058b01c2-178c-4b85-9daf-fdd7524fbcaa", "embedding": null, "doc_hash": "b9cb94707fd2d6d3771c57724b3d011bc221c2b17106c6c25bc08d2def781fc5", "extra_info": null, "node_info": {"start": 1183029, "end": 1186622}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "53113cc1-1a1d-4eb2-8ec9-a70a8b6b60c8", "3": "4eaf5818-351b-43b1-b1b6-1715d2c72f2a"}}, "__type__": "1"}, "4eaf5818-351b-43b1-b1b6-1715d2c72f2a": {"__data__": {"text": "EREW PRAMb.\na hypercube-connected parallel computerc.\na mesh-connected parallel computer.d.\nAnalyze the performance of your formulations. Furthermore, show how you can extend this\nenumeration sort to a hypercube to sort n elements using p processes.\n9.27 Derive expressions for the speedup, efficiency, and isoefficiency function of the\nbucket sort parallel formulation presented in Section 9.5  . Compare these expressions with\nthe expressions for the other sorting algorithms presented in this chapter. Which parallel\nformulations perform better than bucket sort, and which perform worse?\n9.28 Show that the splitter selection scheme described in Section 9.5  guarantees that the\nnumber of elements in each of the m buckets is less than 2 n /m .\n9.29 Derive expressions for the speedup, efficiency, and isoefficiency function of the\nsample sort parallel formulation presented in Section 9.5  . Derive these metrics under each\nof the following conditions: (1) the p sub-blocks at each process are of equal size, and (2)\nthe size of the p sub-blocks at each process can vary by a factor of log p .\n9.30 In the sample sort algorithm presented in Section 9.5  , all processes send p - 1\nelements to process P 0 , which sorts the p (p - 1) elements and distributes splitters to all\nthe processes. Modify the algorithm so that the processes sort the p (p - 1) elements in\nparallel using bitonic sort. How will you choose the splitters? Compute the parallel run\ntime, speedup, and efficiency of your formulation.\n9.31 How does the performance of radix sort (Section 9.6.2  ) depend on the value of r ?\nCompute the value of r that minimizes the run time of the algorithm.\n9.32 Extend the radix sort algorithm presented in Section 9.6.2  to the case in which p\nprocesses ( p < n ) are used to sort n elements. Derive expressions for the speedup,\nefficiency, and isoefficiency function for this parallel formulation. Can you devise a better\nranking mechanism?\n[ Team LiB ]\n \n\n[ Team LiB ]\n  \nChapter 10. Graph Algorithms\nGraph theory plays an important role in computer science because it provides an easy and\nsystematic way to model many problems. Many problems can be expressed in terms of graphs,\nand can be solved using standard graph algorithms. This chapter presents parallel formulations\nof some important and fundamental graph algorithms.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n10.1 Definitions and Representation\nAn undirected graph  G is a pair ( V, E), where V is a finite set of points called vertices  and E is\na finite set of edges . An edge e \n E is an unordered pair ( u, v), where u, v \n V. An edge ( u, v)\nindicates that vertices u and v are connected. Similarly, a directed graph  G, is a pair ( V, E),\nwhere V is the set of vertices as we just defined, but an edge ( u, v) \n E is an ordered pair; that\nis, it indicates that there is a connection from u to v. Figure 10.1  illustrates an undirected and a\ndirected graph. We use the term graph  to refer to both directed and undirected graphs.\nFigure 10.1. (a) An undirected graph and (b) a directed graph.\nMany definitions are common to directed and undirected graphs, although certain terms have\nslightly different meanings for each. If ( u, v) is an edge in an undirected graph, (u , v) is\nincident on  vertices u and v. However, if a graph is directed, then edge ( u, v) is incident from\nvertex u and is incident to  vertex v. For example, in Figure 10.1(a) , edge e is incident on\nvertices", "doc_id": "4eaf5818-351b-43b1-b1b6-1715d2c72f2a", "embedding": null, "doc_hash": "8659ddf6e015d5cd3065b9e7fe96b010571b09de7c81bdf525708c0c46db9979", "extra_info": null, "node_info": {"start": 1186568, "end": 1190016}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "058b01c2-178c-4b85-9daf-fdd7524fbcaa", "3": "1f3198cd-1fbb-4ef2-9c15-ff1aee802f95"}}, "__type__": "1"}, "1f3198cd-1fbb-4ef2-9c15-ff1aee802f95": {"__data__": {"text": "to refer to both directed and undirected graphs.\nFigure 10.1. (a) An undirected graph and (b) a directed graph.\nMany definitions are common to directed and undirected graphs, although certain terms have\nslightly different meanings for each. If ( u, v) is an edge in an undirected graph, (u , v) is\nincident on  vertices u and v. However, if a graph is directed, then edge ( u, v) is incident from\nvertex u and is incident to  vertex v. For example, in Figure 10.1(a) , edge e is incident on\nvertices 5 and 4, but in Figure 10.1(b) , edge f is incident from vertex 5 and incident to vertex 2.\nIf (u, v) is an edge in a undirected graph G = (V, E), vertices u and v are said to be adjacent\nto each other. If the graph is directed, vertex v is said to be adjacent to  vertex u.\nA path from a vertex v to a vertex u is a sequence < v0, v1, v2, ..., vk> of vertices where v0 = v,\nvk = u, and ( vi, vi+1) \n E for i = 0, 1, ..., k - 1. The length of a path is defined as the number of\nedges in the path. If there exists a path from v to u, then u is reachable  from v. A path is\nsimple  if all of its vertices are distinct. A path forms a cycle  if its starting and ending vertices\nare the same \u2013 that is, v0 = vk. A graph with no cycles is called acyclic . A cycle is simple  if all\nthe intermediate vertices are distinct. For example, in Figure 10.1(a) , the sequence <3, 6, 5,\n4> is a path from vertex 3 to vertex 4, and in Figure 10.1(b)  there is a directed simple cycle\n<1, 5, 6, 4, 1>. Additionally, in Figure 10.1(a) , the sequence <1, 2, 5, 3, 6, 5, 4, 1> is an\nundirected cycle that is not simple because it contains the loop <5, 3, 6, 5>.\nAn undirected graph is connected  if every pair of vertices is connected by a path. We say that\na graph G' = (V', E') is a subgraph  of G = (V, E ) if V' \n V and E' \n E. Given a set V ' \n V, the\nsubgraph of G induced  by V ' is the graph G' = (V ', E '), where E' = {(u, v) \n E|u, v \n V'}. A\ncomplete graph  is a graph in which each pair of vertices is adjacent. A forest  is an acyclic\ngraph, and a tree is a connected acyclic graph. Note that if G = (V, E) is a tree, then | E| = |V| -\n1.\nSometimes weights are associated with each edge in E. Weights are usually real numbers\nrepresenting the cost or benefit of traversing the associated edge. For example, in an electronic\ncircuit a resistor can be represented by an edge whose weight is its resistance. A graph that has\nweights associated with each edge is called a weighted graph  and is denoted by G = (V, E, w),\nwhere V and E are as we just defined and w : E \n \n is a real-valued function defined on E.\nThe weight of a graph is defined as the sum of the weights of its edges. The weight of a path is\nthe sum of the weights of its edges.\nThere are two standard methods for representing a graph in a computer program. The first\nmethod is to use a matrix, and the second method is to use a linked list.\nConsider a graph G = (V, E) with n vertices numbered 1, 2, ..., n. The adjacency matrix  of this\ngraph is an n x n array A = (ai, j), which is defined as follows:\nFigure 10.2  illustrates an adjacency", "doc_id": "1f3198cd-1fbb-4ef2-9c15-ff1aee802f95", "embedding": null, "doc_hash": "0f24492dc2065309c2844ecbecc0bd4d8c1ec04844400592d09789ed036a1a41", "extra_info": null, "node_info": {"start": 1190114, "end": 1193214}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4eaf5818-351b-43b1-b1b6-1715d2c72f2a", "3": "073d156a-3d7d-41e2-8dfc-73264e2a45db"}}, "__type__": "1"}, "073d156a-3d7d-41e2-8dfc-73264e2a45db": {"__data__": {"text": "function defined on E.\nThe weight of a graph is defined as the sum of the weights of its edges. The weight of a path is\nthe sum of the weights of its edges.\nThere are two standard methods for representing a graph in a computer program. The first\nmethod is to use a matrix, and the second method is to use a linked list.\nConsider a graph G = (V, E) with n vertices numbered 1, 2, ..., n. The adjacency matrix  of this\ngraph is an n x n array A = (ai, j), which is defined as follows:\nFigure 10.2  illustrates an adjacency matrix representation of an undirected graph. Note that the\nadjacency matrix of an undirected graph is symmetric. The adjacency matrix representation can\nbe modified to facilitate weighted graphs. In this case, A = (ai,j) is defined as follows:\nFigure 10.2. An undirected graph and its adjacency matrix\nrepresentation.\nWe refer to this modified adjacency matrix as the weighted adjacency matrix . The space\nrequired to store the adjacency matrix of a graph with n vertices is Q(n2).\nThe adjacency list  representation of a graph G = (V, E ) consists of an array Adj[1..|V|] of\nlists. For each v \n V, Adj [v] is a linked list of all vertices u such that G contains an edge ( v, u) \nE. In other words, Adj [v] is a list of all vertices adjacent to v. Figure 10.3  shows an example of\nthe adjacency list representation. The adjacency list representation can be modified to\naccommodate weighted graphs by storing the weight of each edge ( v, u) \n E in the adjacency\nlist of vertex v. The space required to store the adjacency list is Q(|E|).\nFigure 10.3. An undirected graph and its adjacency list representation.\nThe nature of the graph determines which representation should be used. A graph G = (V, E ) is\nsparse  if |E| is much smaller than O (|V|2); otherwise it is dense . The adjacency matrix\nrepresentation is useful for dense graphs, and the adjacency list representation is often more\nefficient for sparse graphs. Note that the sequential run time of an algorithm using an adjacency\nmatrix and needing to traverse all the edges of the graph is bounded below by W(|V|2) because\nthe entire array must be accessed. However, if the adjacency list representation is used, the run\ntime is bounded below by W(|V| + |E|) for the same reason. Thus, if the graph is sparse (| E| is\nmuch smaller than | V|2), the adjacency list representation is better than the adjacency matrix\nrepresentation.\nThe rest of this chapter presents several graph algorithms. The first four sections present\nalgorithms for dense graphs, and the last section discusses algorithms for sparse graphs. We\nassume that dense graphs are represented by an adjacency matrix, and sparse graphs by an\nadjacency list. Throughout this chapter, n denotes the number of vertices in the graph.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n10.2 Minimum Spanning Tree: Prim's Algorithm\nA spanning tree  of an undirected graph G is a subgraph of G that is a tree containing all the\nvertices of G. In a weighted graph, the weight of a subgraph is the sum of the weights of the\nedges in the subgraph. A minimum spanning tree  (MST) for a weighted undirected graph is a\nspanning tree with minimum weight. Many problems require finding an MST of an undirected\ngraph. For example, the minimum length of cable necessary to connect a set of computers in a\nnetwork can be determined by finding the MST of the undirected graph containing all", "doc_id": "073d156a-3d7d-41e2-8dfc-73264e2a45db", "embedding": null, "doc_hash": "0b9eeca9113af384dcd0085718bae9a644bac81abe62fd8b71b86cf204eb9a38", "extra_info": null, "node_info": {"start": 1193207, "end": 1196603}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1f3198cd-1fbb-4ef2-9c15-ff1aee802f95", "3": "71705436-a974-4326-98b8-1213641a307c"}}, "__type__": "1"}, "71705436-a974-4326-98b8-1213641a307c": {"__data__": {"text": "Tree: Prim's Algorithm\nA spanning tree  of an undirected graph G is a subgraph of G that is a tree containing all the\nvertices of G. In a weighted graph, the weight of a subgraph is the sum of the weights of the\nedges in the subgraph. A minimum spanning tree  (MST) for a weighted undirected graph is a\nspanning tree with minimum weight. Many problems require finding an MST of an undirected\ngraph. For example, the minimum length of cable necessary to connect a set of computers in a\nnetwork can be determined by finding the MST of the undirected graph containing all the\npossible connections. Figure 10.4 shows an MST of an undirected graph.\nFigure 10.4. An undirected graph and its minimum spanning tree.\nIf G is not connected, it cannot have a spanning tree. Instead, it has a spanning forest . For\nsimplicity in describing the MST algorithm, we assume that G is connected. If G is not\nconnected, we can find its connected components ( Section 10.6 ) and apply the MST algorithm\non each of them. Alternatively, we can modify the MST algorithm to output a minimum\nspanning forest.\nPrim's algorithm for finding an MST is a greedy algorithm. The algorithm begins by selecting an\narbitrary starting vertex. It then grows the minimum spanning tree by choosing a new vertex\nand edge that are guaranteed to be in a spanning tree of minimum cost. The algorithm\ncontinues until all the vertices have been selected.\nLet G = (V, E, w) be the weighted undirected graph for which the minimum spanning tree is to\nbe found, and let A = (ai, j) be its weighted adjacency matrix. Prim's algorithm is shown in\nAlgorithm 10.1. The algorithm uses the set VT to hold the vertices of the minimum spanning\ntree during its construction. It also uses an array d[1..n] in which, for each vertex v \n (V - VT ),\nd [v] holds the weight of the edge with the least weight from any vertex in VT to vertex v.\nInitially, VT contains an arbitrary vertex r that becomes the root of the MST. Furthermore, d[r]\n= 0, and for all v such that v \n (V - VT ), d[v] = w(r, v) if such an edge exists; otherwise d[v] =\n. During each iteration of the algorithm, a new vertex u is added to VT such that d[u] =\nmin{d [v]|v \n (V - VT )}. After this vertex is added, all values of d[v] such that v \n (V - VT) are\nupdated because there may now be an edge with a smaller weight between vertex v and the\nnewly added vertex u. The algorithm terminates when VT = V. Figure 10.5  illustrates the\nalgorithm. Upon termination of Prim's algorithm, the cost of the minimum spanning tree is\n. Algorithm 10.1 can be easily modified to store the edges that belong in the\nminimum spanning tree.\nFigure 10.5. Prim's minimum spanning tree algorithm. The MST is\nrooted at vertex b. For each iteration, vertices in VT as well as the\nedges selected so far are shown in bold. The array d[v] shows the\nvalues of the vertices in V - VT after they have been updated.\nIn Algorithm 10.1 , the body of the while  loop (lines 10\u201313) is executed n-1 times. Both the\ncomputation of min{ d[v]|v \n (V - VT )} (line 10), and the for loop (lines 12 and 13) execute in\nO (n) steps. Thus, the overall complexity of Prim's algorithm is Q(n2).\nAlgorithm 10.1 Prim's sequential minimum spanning tree algorithm.\n1.   procedure  PRIM_MST( V, E, w, r) \n2.   begin \n3.      VT := {r};", "doc_id": "71705436-a974-4326-98b8-1213641a307c", "embedding": null, "doc_hash": "6e07b6f06173167df03fd5e90d8e16a8ede1fd5a2e8c794a37bf737d02bed2ed", "extra_info": null, "node_info": {"start": 1196555, "end": 1199850}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "073d156a-3d7d-41e2-8dfc-73264e2a45db", "3": "5456cb88-ade6-42d5-8765-f083caad9a9e"}}, "__type__": "1"}, "5456cb88-ade6-42d5-8765-f083caad9a9e": {"__data__": {"text": "Algorithm 10.1 , the body of the while  loop (lines 10\u201313) is executed n-1 times. Both the\ncomputation of min{ d[v]|v \n (V - VT )} (line 10), and the for loop (lines 12 and 13) execute in\nO (n) steps. Thus, the overall complexity of Prim's algorithm is Q(n2).\nAlgorithm 10.1 Prim's sequential minimum spanning tree algorithm.\n1.   procedure  PRIM_MST( V, E, w, r) \n2.   begin \n3.      VT := {r}; \n4.      d[r] := 0; \n5.      for all v \n (V - VT ) do \n6.         if edge (r, v) exists set d[v] := w(r, v); \n7.         else set d[v] := \n ; \n8.      while VT \n V do \n9.      begin \n10.        find a vertex u such that d[u] :=min{ d[v]|v \n (V - VT )}; \n11.        VT := VT \n {u}; \n12.        for all v \n (V - VT ) do \n13.            d[v] := min{ d[v], w(u, v)}; \n14.     endwhile  \n15.  end PRIM_MST \nParallel Formulation\nPrim's algorithm is iterative. Each iteration adds a new vertex to the minimum spanning tree.\nSince the value of d[v] for a vertex v may change every time a new vertex u is added in VT , it\nis hard to select more than one vertex to include in the minimum spanning tree. For example, in\nthe graph of Figure 10.5, after selecting vertex b, if both vertices d and c are selected, the MST\nwill not be found. That is because, after selecting vertex d, the value of d[c] is updated from 5\nto 2. Thus, it is not easy to perform different iterations of the while  loop in parallel. However,\neach iteration can be performed in parallel as follows.\nLet p be the number of processes, and let n be the number of vertices in the graph. The set V is\npartitioned into p subsets using the 1-D block mapping ( Section 3.4.1 ). Each subset has n/p\nconsecutive vertices, and the work associated with each subset is assigned to a different\nprocess. Let Vi be the subset of vertices assigned to process Pi for i = 0, 1, ..., p - 1. Each\nprocess Pi stores the part of the array d that corresponds to Vi (that is, process Pi stores d [v]\nsuch that v \n Vi). Figure 10.6(a)  illustrates the partitioning. Each process Pi computes di[u] =\nmin{di[v]|v \n (V - VT) \n Vi} during each iteration of the while  loop. The global minimum is\nthen obtained over all di[u] by using the all-to-one reduction operation ( Section 4.1 ) and is\nstored in process P0. Process P0 now holds the new vertex u, which will be inserted into VT.\nProcess P0 broadcasts u to all processes by using one-to-all broadcast ( Section 4.1 ). The\nprocess Pi responsible for vertex u marks u as belonging to set VT. Finally, each process updates\nthe values of d[v] for its local vertices.\nFigure 10.6. The partitioning of the distance array d and the adjacency\nmatrix A among p processes.\nWhen a new vertex u is inserted into VT, the values of d[v] for v \n (V - VT) must be updated.\nThe process responsible for v must know the weight of the edge ( u, v).", "doc_id": "5456cb88-ade6-42d5-8765-f083caad9a9e", "embedding": null, "doc_hash": "ef4d2a742c10c1c2a74630c6393b65b5f3c8662a6e3c2fb41288c97bfea357b4", "extra_info": null, "node_info": {"start": 1200002, "end": 1202814}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "71705436-a974-4326-98b8-1213641a307c", "3": "c78ccfcd-5053-4382-8a39-513af65bbd21"}}, "__type__": "1"}, "c78ccfcd-5053-4382-8a39-513af65bbd21": {"__data__": {"text": "be inserted into VT.\nProcess P0 broadcasts u to all processes by using one-to-all broadcast ( Section 4.1 ). The\nprocess Pi responsible for vertex u marks u as belonging to set VT. Finally, each process updates\nthe values of d[v] for its local vertices.\nFigure 10.6. The partitioning of the distance array d and the adjacency\nmatrix A among p processes.\nWhen a new vertex u is inserted into VT, the values of d[v] for v \n (V - VT) must be updated.\nThe process responsible for v must know the weight of the edge ( u, v). Hence, each process Pi\nneeds to store the columns of the weighted adjacency matrix corresponding to set Vi of vertices\nassigned to it. This corresponds to 1-D block mapping of the matrix ( Section 3.4.1 ). The space\nto store the required part of the adjacency matrix at each process is Q(n2/p). Figure 10.6(b)\nillustrates the partitioning of the weighted adjacency matrix.\nThe computation performed by a process to minimize and update the values of d[v] during each\niteration is Q(n/p). The communication performed in each iteration is due to the all-to-one\nreduction and the one-to-all broadcast. For a p-process message-passing parallel computer, a\none-to-all broadcast of one word takes time ( ts + tw) log p (Section 4.1 ). Finding the global\nminimum of one word at each process takes the same amount of time ( Section 4.1 ). Thus, the\ntotal communication cost of each iteration is Q(log p). The parallel run time of this formulation\nis given by\nSince the sequential run time is W = Q(n2), the speedup and efficiency are as follows:\nEquation 10.1\nFrom Equation 10.1  we see that for a cost-optimal parallel formulation ( p log p)/n = O (1).\nThus, this formulation of Prim's algorithm can use only p = O (n/log n) processes. Furthermore,\nfrom Equation 10.1 , the isoefficiency function due to communication is Q(p2 log2 p). Since n\nmust grow at least as fast as p in this formulation, the isoefficiency function due to concurrency\nis Q(p2). Thus, the overall isoefficiency of this formulation is Q(p2 log2 p).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n10.3 Single-Source Shortest Paths: Dijkstra's Algorithm\nFor a weighted graph G = (V, E, w), the single-source shortest paths  problem is to find the\nshortest paths from a vertex v \n V to all other vertices in V. A shortest path  from u to v is a\nminimum-weight path. Depending on the application, edge weights may represent time, cost,\npenalty, loss, or any other quantity that accumulates additively along a path and is to be\nminimized. In the following section, we present Dijkstra's algorithm, which solves the single-\nsource shortest-paths problem on both directed and undirected graphs with non-negative\nweights.\nDijkstra's algorithm, which finds the shortest paths from a single vertex s, is similar to Prim's\nminimum spanning tree algorithm. Like Prim's algorithm, it incrementally finds the shortest\npaths from s to the other vertices of G. It is also greedy; that is, it always chooses an edge to a\nvertex that appears closest. Algorithm 10.2 shows Dijkstra's algorithm. Comparing this\nalgorithm with Prim's minimum spanning tree algorithm, we see that the two are almost\nidentical. The main difference is that, for each vertex u \n (V - VT ), Dijkstra's algorithm stores\nl[u], the minimum cost to reach vertex u from vertex s by means of vertices in VT; Prim's\nalgorithm stores d [u], the cost of", "doc_id": "c78ccfcd-5053-4382-8a39-513af65bbd21", "embedding": null, "doc_hash": "c91ada2409c29132d889058252a98c309000b2767203760554bbebae7669a68a", "extra_info": null, "node_info": {"start": 1202705, "end": 1206076}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5456cb88-ade6-42d5-8765-f083caad9a9e", "3": "8d4a9a93-7aac-4b68-9463-014212852a77"}}, "__type__": "1"}, "8d4a9a93-7aac-4b68-9463-014212852a77": {"__data__": {"text": "finds the shortest\npaths from s to the other vertices of G. It is also greedy; that is, it always chooses an edge to a\nvertex that appears closest. Algorithm 10.2 shows Dijkstra's algorithm. Comparing this\nalgorithm with Prim's minimum spanning tree algorithm, we see that the two are almost\nidentical. The main difference is that, for each vertex u \n (V - VT ), Dijkstra's algorithm stores\nl[u], the minimum cost to reach vertex u from vertex s by means of vertices in VT; Prim's\nalgorithm stores d [u], the cost of the minimum-cost edge connecting a vertex in VT to u. The\nrun time of Dijkstra's algorithm is Q(n2).\nAlgorithm 10.2 Dijkstra's sequential single-source shortest paths\nalgorithm.\n1.   procedure  DIJKSTRA_SINGLE_SOURCE_SP( V, E, w, s) \n2.   begin \n3.      VT := {s}; \n4.      for all v \n (V - VT ) do \n5.         if (s, v) exists set l[v] := w(s, v); \n6.         else set l[v] := \n ; \n7.      while VT \n V do \n8.      begin \n9.         find a vertex u such that l[u] := min{ l[v]|v \n (V - VT )}; \n10.        VT := VT \n {u}; \n11.        for all v \n (V - VT ) do \n12.            l[v] := min{ l[v], l[u] + w(u, v)}; \n13.     endwhile  \n14.  end DIJKSTRA_SINGLE_SOURCE_SP \nParallel Formulation\nThe parallel formulation of Dijkstra's single-source shortest path algorithm is very similar to the\nparallel formulation of Prim's algorithm for minimum spanning trees ( Section 10.2 ). The\nweighted adjacency matrix is partitioned using the 1-D block mapping ( Section 3.4.1 ). Each of\nthe p processes is assigned n/p consecutive columns of the weighted adjacency matrix, and\ncomputes n/p values of the array l. During each iteration, all processes perform computation\nand communication similar to that performed by the parallel formulation of Prim's algorithm.\nConsequently, the parallel performance and scalability of Dijkstra's single-source shortest path\nalgorithm is identical to that of Prim's minimum spanning tree algorithm.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n10.4 All-Pairs Shortest Paths\nInstead of finding the shortest paths from a single vertex v to every other vertex, we are\nsometimes interested in finding the shortest paths between all pairs of vertices. Formally, given\na weighted graph G(V, E, w), the all-pairs shortest paths  problem is to find the shortest paths\nbetween all pairs of vertices v i, vj \n V such that i \n j. For a graph with n vertices, the output of\nan all-pairs shortest paths algorithm is an n x n matrix D = (di,j) such that di,j is the cost of the\nshortest path from vertex v i to vertex v j.\nThe following sections present two algorithms to solve the all-pairs shortest paths problem. The\nfirst algorithm uses Dijkstra's single-source shortest paths algorithm, and the second uses\nFloyd's algorithm. Dijkstra's algorithm requires non-negative edge weights (Problem 10.4),\nwhereas Floyd's algorithm works with graphs having negative-weight edges provided they\ncontain no", "doc_id": "8d4a9a93-7aac-4b68-9463-014212852a77", "embedding": null, "doc_hash": "7c9def7c3a2a4abbf137ac1a0ce954bb0a9bc4f8b26aba5e884948561371716a", "extra_info": null, "node_info": {"start": 1206075, "end": 1208988}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c78ccfcd-5053-4382-8a39-513af65bbd21", "3": "e79cc13d-2702-4ba7-8dd1-b031e073fda8"}}, "__type__": "1"}, "e79cc13d-2702-4ba7-8dd1-b031e073fda8": {"__data__": {"text": "output of\nan all-pairs shortest paths algorithm is an n x n matrix D = (di,j) such that di,j is the cost of the\nshortest path from vertex v i to vertex v j.\nThe following sections present two algorithms to solve the all-pairs shortest paths problem. The\nfirst algorithm uses Dijkstra's single-source shortest paths algorithm, and the second uses\nFloyd's algorithm. Dijkstra's algorithm requires non-negative edge weights (Problem 10.4),\nwhereas Floyd's algorithm works with graphs having negative-weight edges provided they\ncontain no negative-weight cycles.\n10.4.1 Dijkstra's Algorithm\nIn Section 10.3 we presented Dijkstra's algorithm for finding the shortest paths from a vertex v\nto all the other vertices in a graph. This algorithm can also be used to solve the all-pairs\nshortest paths problem by executing the single-source algorithm on each process, for each\nvertex v. We refer to this algorithm as Dijkstra's all-pairs shortest paths algorithm. Since the\ncomplexity of Dijkstra's single-source algorithm is Q(n2), the complexity of the all-pairs\nalgorithm is Q(n3).\nParallel Formulations\nDijkstra's all-pairs shortest paths problem can be parallelized in two distinct ways. One\napproach partitions the vertices among different processes and has each process compute the\nsingle-source shortest paths for all vertices assigned to it. We refer to this approach as the\nsource-partitioned formulation . Another approach assigns each vertex to a set of processes\nand uses the parallel formulation of the single-source algorithm ( Section 10.3) to solve the\nproblem on each set of processes. We refer to this approach as the source-parallel\nformulation . The following sections discuss and analyze these two approaches.\nSource-Partitioned Formulation  The source-partitioned parallel formulation of Dijkstra's\nalgorithm uses n processes. Each process Pi finds the shortest paths from vertex v i to all other\nvertices by executing Dijkstra's sequential single-source shortest paths algorithm. It requires no\ninterprocess communication (provided that the adjacency matrix is replicated at all processes).\nThus, the parallel run time of this formulation is given by\nSince the sequential run time is W = Q(n3), the speedup and efficiency are as follows:\nEquation 10.2\nIt might seem that, due to the absence of communication, this is an excellent parallel\nformulation. However, that is not entirely true. The algorithm can use at most n processes.\nTherefore, the isoefficiency function due to concurrency is Q(p3), which is the overall\nisoefficiency function of the algorithm. If the number of processes available for solving the\nproblem is small (that is, n = Q(p)), then this algorithm has good performance. However, if the\nnumber of processes is greater than n, other algorithms will eventually outperform this\nalgorithm because of its poor scalability.\nSource-Parallel Formulation  The major problem with the source-partitioned parallel\nformulation is that it can keep only n processes busy doing useful work. Performance can be\nimproved if the parallel formulation of Dijkstra's single-source algorithm ( Section 10.3) is used\nto solve the problem for each vertex v. The source-parallel formulation is similar to the source-\npartitioned formulation, except that the single-source algorithm runs on disjoint subsets of\nprocesses.\nSpecifically, p  processes are divided into n partitions, each with p/n processes (this formulation\nis of interest only if p > n). Each of the n single-source shortest paths problems is solved by one\nof the n partitions. In other words, we first parallelize the all-pairs shortest paths problem by\nassigning each vertex to", "doc_id": "e79cc13d-2702-4ba7-8dd1-b031e073fda8", "embedding": null, "doc_hash": "02621e6dfef21e1a4184fb3deb757c6205f668d956e983b601ae428400e8a079", "extra_info": null, "node_info": {"start": 1208960, "end": 1212613}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8d4a9a93-7aac-4b68-9463-014212852a77", "3": "0b7563ba-0043-45d9-b15b-5be0c1bfa86c"}}, "__type__": "1"}, "0b7563ba-0043-45d9-b15b-5be0c1bfa86c": {"__data__": {"text": "algorithm ( Section 10.3) is used\nto solve the problem for each vertex v. The source-parallel formulation is similar to the source-\npartitioned formulation, except that the single-source algorithm runs on disjoint subsets of\nprocesses.\nSpecifically, p  processes are divided into n partitions, each with p/n processes (this formulation\nis of interest only if p > n). Each of the n single-source shortest paths problems is solved by one\nof the n partitions. In other words, we first parallelize the all-pairs shortest paths problem by\nassigning each vertex to a separate set of processes, and then parallelize the single-source\nalgorithm by using the set of p/n processes to solve it. The total number of processes that can\nbe used efficiently by this formulation is O (n2).\nThe analysis presented in Section 10.3  can be used to derive the performance of this\nformulation of Dijkstra's all-pairs algorithm. Assume that we have a p-process message-passing\ncomputer such that p is a multiple of n. The p processes are partitioned into n groups of size p/n\neach. If the single-source algorithm is executed on each p/n process group, the parallel run\ntime is\nEquation 10.3\nNotice the similarities between Equations 10.3 and 10.2. These similarities are not surprising\nbecause each set of p/n processes forms a different group and carries out the computation\nindependently. Thus, the time required by each set of p/n processes to solve the single-source\nproblem determines the overall run time. Since the sequential run time is W = Q(n3), the\nspeedup and efficiency are as follows:\nEquation 10.4\n\nFrom Equation 10.4  we see that for a cost-optimal formulation p log p/n2 = O (1). Hence, this\nformulation can use up to O (n2/log n) processes efficiently. Equation 10.4  also shows that the\nisoefficiency function due to communication is Q((p log p)1.5). The isoefficiency function due to\nconcurrency is Q(p1.5). Thus, the overall isoefficiency function is Q((p log p)1.5).\nComparing the two parallel formulations of Dijkstra's all-pairs algorithm, we see that the\nsource-partitioned formulation performs no communication, can use no more than n processes,\nand solves the problem in time Q(n2). In contrast, the source-parallel formulation uses up to\nn2/log n processes, has some communication overhead, and solves the problem in time Q(n log\nn) when n2/log n processes are used. Thus, the source-parallel formulation exploits more\nparallelism than does the source-partitioned formulation.\n10.4.2 Floyd's Algorithm\nFloyd's algorithm for solving the all-pairs shortest paths problem is based on the following\nobservation. Let G = (V, E, w) be the weighted graph, and let V = {v1,v2,..., vn} be the vertices\nof G. Consider a subset { v1, v2,..., vk} of vertices for some k where k \n n. For any pair of\nvertices vi , vj \n V, consider all paths from v i to v j whose intermediate vertices belong to the set\n{v1, v2,..., vk }. Let \n  be the minimum-weight path among them, and let \n  be the weight\nof \n . If vertex vk is not in the shortest path from v i to v j, then \n  is the same as \n .\nHowever, if v k is in \n , then we can break \n  into two paths \u2013 one from vi to vk and one from\nvk to vj. Each of these paths uses vertices from { v1, v2,..., vk-1}. Thus, \n .\nThese observations are expressed in the following", "doc_id": "0b7563ba-0043-45d9-b15b-5be0c1bfa86c", "embedding": null, "doc_hash": "49dbf30270029e01aaac2aa1817c3d231ab044b547c63f341576625556faa479", "extra_info": null, "node_info": {"start": 1212597, "end": 1215899}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e79cc13d-2702-4ba7-8dd1-b031e073fda8", "3": "88c00642-f60b-438c-811c-46c9f533cd70"}}, "__type__": "1"}, "88c00642-f60b-438c-811c-46c9f533cd70": {"__data__": {"text": "v j whose intermediate vertices belong to the set\n{v1, v2,..., vk }. Let \n  be the minimum-weight path among them, and let \n  be the weight\nof \n . If vertex vk is not in the shortest path from v i to v j, then \n  is the same as \n .\nHowever, if v k is in \n , then we can break \n  into two paths \u2013 one from vi to vk and one from\nvk to vj. Each of these paths uses vertices from { v1, v2,..., vk-1}. Thus, \n .\nThese observations are expressed in the following recurrence equation:\nEquation 10.5\nThe length of the shortest path from vi to vj is given by \n . In general, the solution is a matrix\n.\nFloyd's algorithm solves Equation 10.5  bottom-up in the order of increasing values of k.\nAlgorithm 10.3  shows Floyd's all-pairs algorithm. The run time of Floyd's algorithm is\ndetermined by the triple-nested for loops in lines 4\u20137. Each execution of line 7 takes time Q(1);\nthus, the complexity of the algorithm is Q(n3). Algorithm 10.3  seems to imply that we must\nstore n matrices of size n xn. However, when computing matrix D(k), only matrix D(k-1) is\nneeded. Consequently, at most two n x n matrices must be stored. Therefore, the overall space\ncomplexity is Q(n2). Furthermore, the algorithm works correctly even when only one copy of D\nis used (Problem 10.6).\nAlgorithm 10.3 Floyd's all-pairs shortest paths algorithm. This\nprogram computes the all-pairs shortest paths of the graph G = (V, E )\nwith adjacency matrix A.\n1.   procedure  FLOYD_ALL_PAIRS_SP( A) \n2.   begin \n3.      D(0) = A; \n4.      for k := 1 to n do \n5.          for i := 1 to n do \n6.              for j := 1 to n do \n7.                  \n ; \n8.   end FLOYD_ALL_PAIRS_SP \nParallel Formulation\nA generic parallel formulation of Floyd's algorithm assigns the task of computing matrix D(k) for\neach value of k to a set of processes. Let p be the number of processes available. Matrix D(k) is\npartitioned into p parts, and each part is assigned to a process. Each process computes the D(k)\nvalues of its partition. To accomplish this, a process must access the corresponding segments of\nthe kth row and column of matrix D(k-1). The following section describes one technique for\npartitioning matrix D(k). Another technique is considered in Problem 10.8.\n2-D Block Mapping  One way to partition matrix D(k) is to use the 2-D block mapping ( Section\n3.4.1 ). Specifically, matrix D(k) is divided into p blocks of size \n , and each block\nis assigned to one of the p processes. It is helpful to think of the p processes as arranged in a\nlogical grid of size \n . Note that this is only a conceptual layout and does not necessarily\nreflect the actual interconnection network. We refer to the process on the ith row and jth column\nas Pi,j. Process Pi,j is assigned a subblock of D(k) whose upper-left corner is\n and whose lower-right corner is \n . Each\nprocess updates its part of the matrix during each iteration. Figure 10.7(a)  illustrates the 2-D\nblock mapping technique.\nFigure 10.7. (a) Matrix", "doc_id": "88c00642-f60b-438c-811c-46c9f533cd70", "embedding": null, "doc_hash": "4c580b96afa8d35a5f39c247ee080232435954488a3f4ef5c7ac78f8b9faace0", "extra_info": null, "node_info": {"start": 1216020, "end": 1218978}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0b7563ba-0043-45d9-b15b-5be0c1bfa86c", "3": "d9621506-6dc6-4a04-861e-b46e1fe2f7c8"}}, "__type__": "1"}, "d9621506-6dc6-4a04-861e-b46e1fe2f7c8": {"__data__": {"text": "processes. It is helpful to think of the p processes as arranged in a\nlogical grid of size \n . Note that this is only a conceptual layout and does not necessarily\nreflect the actual interconnection network. We refer to the process on the ith row and jth column\nas Pi,j. Process Pi,j is assigned a subblock of D(k) whose upper-left corner is\n and whose lower-right corner is \n . Each\nprocess updates its part of the matrix during each iteration. Figure 10.7(a)  illustrates the 2-D\nblock mapping technique.\nFigure 10.7. (a) Matrix D(k) distributed by 2-D block mapping into\n subblocks, and (b) the subblock of D(k) assigned to process\nPi,j.\nDuring the kth iteration of the algorithm, each process Pi,j needs certain segments of the kth row\nand kth column of the D(k-1) matrix. For example, to compute \n  it must get \n  and \n .\nAs Figure 10.8  illustrates, \n  resides on a process along the same row, and element \nresides on a process along the same column as Pi,j. Segments are transferred as follows. During\nthe kth iteration of the algorithm, each of the \n  processes containing part of the kth row sends\nit to the \n  processes in the same column. Similarly, each of the \n  processes containing\npart of the kth column sends it to the \n  processes in the same row.\nFigure 10.8. (a) Communication patterns used in the 2-D block\nmapping. When computing \n , information must be sent to the\nhighlighted process from two other processes along the same row and\ncolumn. (b) The row and column of \n  processes that contain the kth\nrow and column send them along process columns and rows.\nAlgorithm 10.4  shows the parallel formulation of Floyd's algorithm using the 2-D block\nmapping. We analyze the performance of this algorithm on a p-process message-passing\ncomputer with a cross-bisection bandwidth of Q(p). During each iteration of the algorithm, the\nkth row and kth column of processes perform a one-to-all broadcast along a row or a column of\n processes. Each such process has \n  elements of the kth row or column, so it sends\n elements. This broadcast requires time Q\n . The synchronization step on\nline 7 requires time Q(log p). Since each process is assigned n2/p elements of the D(k) matrix,\nthe time to compute corresponding D(k) values is Q(n2/p). Therefore, the parallel run time of the\n2-D block mapping formulation of Floyd's algorithm is\nSince the sequential run time is W = Q(n3), the speedup and efficiency are as follows:\nEquation 10.6\nFrom Equation 10.6  we see that for a cost-optimal formulation \n ; thus, 2-D\nblock mapping can efficiently use up to O (n2/log2n) processes. Equation 10.6  can also be used\nto derive the isoefficiency function due to communication, which is Q(p1.5 log3 p). The\nisoefficiency function due to concurrency is Q(p1.5). Thus, the overall isoefficiency function is\nQ(p1.5 log3 p).\nSpeeding Things Up  In the 2-D block mapping formulation of Floyd's algorithm, a\nsynchronization step ensures that all processes have the appropriate segments of matrix D(k-1)\nbefore computing elements of matrix D(k) (line 7 in Algorithm 10.4 ). In other words, the kth\niteration starts only when the ( k - 1)th iteration has completed and the relevant parts of matrix\nD(k-1) have been transmitted to all processes. The synchronization step can be removed without\naffecting the correctness of the algorithm. To accomplish this,", "doc_id": "d9621506-6dc6-4a04-861e-b46e1fe2f7c8", "embedding": null, "doc_hash": "0e5291be9ab45b67c615249f8ce7bceee8c1bc833e34b427781d68c491127a44", "extra_info": null, "node_info": {"start": 1218893, "end": 1222243}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "88c00642-f60b-438c-811c-46c9f533cd70", "3": "4f3f4d84-c6db-427d-aef4-087e66a726f1"}}, "__type__": "1"}, "4f3f4d84-c6db-427d-aef4-087e66a726f1": {"__data__": {"text": "Things Up  In the 2-D block mapping formulation of Floyd's algorithm, a\nsynchronization step ensures that all processes have the appropriate segments of matrix D(k-1)\nbefore computing elements of matrix D(k) (line 7 in Algorithm 10.4 ). In other words, the kth\niteration starts only when the ( k - 1)th iteration has completed and the relevant parts of matrix\nD(k-1) have been transmitted to all processes. The synchronization step can be removed without\naffecting the correctness of the algorithm. To accomplish this, a process starts working on the\nkth iteration as soon as it has computed the ( k -1)th iteration and has the relevant parts of the\nD(k-1) matrix. This formulation is called pipelined 2-D block mapping . A similar technique is\nused in Section 8.3  to improve the performance of Gaussian elimination.\nAlgorithm 10.4 Floyd's parallel formulation using the 2-D block\nmapping. P*,j denotes all the processes in the jth column, and Pi,*\ndenotes all the processes in the ith row. The matrix D(0) is the\nadjacency matrix.\n1.   procedure  FLOYD_2DBLOCK( D(0)) \n2.   begin \n3.      for k := 1 to n do \n4.      begin \n5.         each process Pi,j that has a segment of the kth row of D(k-1); \n              broadcasts it to the P*,j processes; \n6.         each process Pi,j that has a segment of the kth column of D(k-1); \n              broadcasts it to the Pi,* processes; \n7.         each process waits to receive the needed segments; \n8.         each process Pi,j computes its part of the D(k) matrix; \n9.      end \n10.  end FLOYD_2DBLOCK \nConsider a p-process system arranged in a two-dimensional topology. Assume that process Pi,j\nstarts working on the kth iteration as soon as it has finished the ( k - 1)th iteration and has\nreceived the relevant parts of the D(k-1) matrix. When process Pi,j has elements of the kth row\nand has finished the ( k - 1)th iteration, it sends the part of matrix D(k-1) stored locally to\nprocesses Pi,j -1 and Pi, j +1. It does this because that part of the D(k-1) matrix is used to\ncompute the D(k) matrix. Similarly, when process Pi,j has elements of the kth column and has\nfinished the ( k - 1)th iteration, it sends the part of matrix D(k-1) stored locally to processes Pi -1, j\nand Pi +1, j. When process Pi,j receives elements of matrix D(k) from a process along its row in\nthe logical mesh, it stores them locally and forwards them to the process on the side opposite\nfrom where it received them. The columns follow a similar communication protocol. Elements of\nmatrix D(k) are not forwarded when they reach a mesh boundary. Figure 10.9 illustrates this\ncommunication and termination protocol for processes within a row (or a column).\nFigure 10.9. Communication protocol followed in the pipelined 2-D\nblock mapping formulation of Floyd's algorithm. Assume that process\n4 at time t has just computed a segment of the kth column of the D(k-1)\nmatrix. It sends the segment to processes 3 and 5. These processes\nreceive the segment at time t + 1 (where the time", "doc_id": "4f3f4d84-c6db-427d-aef4-087e66a726f1", "embedding": null, "doc_hash": "429319697b1fcd65cfebdf5dd98ea85d8e8ee96dab2e4f4e6940d5793986b8ed", "extra_info": null, "node_info": {"start": 1222246, "end": 1225254}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d9621506-6dc6-4a04-861e-b46e1fe2f7c8", "3": "bec38c44-ad6a-4061-840c-8ca4bbe10c03"}}, "__type__": "1"}, "bec38c44-ad6a-4061-840c-8ca4bbe10c03": {"__data__": {"text": "similar communication protocol. Elements of\nmatrix D(k) are not forwarded when they reach a mesh boundary. Figure 10.9 illustrates this\ncommunication and termination protocol for processes within a row (or a column).\nFigure 10.9. Communication protocol followed in the pipelined 2-D\nblock mapping formulation of Floyd's algorithm. Assume that process\n4 at time t has just computed a segment of the kth column of the D(k-1)\nmatrix. It sends the segment to processes 3 and 5. These processes\nreceive the segment at time t + 1 (where the time unit is the time it\ntakes for a matrix segment to travel over the communication link\nbetween adjacent processes). Similarly, processes farther away from\nprocess 4 receive the segment later. Process 1 (at the boundary) does\nnot forward the segment after receiving it.\nConsider the movement of values in the first iteration. In each step, \n  elements of the first\nrow are sent from process Pi,j to Pi +1,j. Similarly, elements of the first column are sent from\nprocess Pi,j to process Pi,j+1. Each such step takes time Q(\n ). After Q\n  steps, process\n gets the relevant elements of the first row and first column in time Q(n). The values of\nsuccessive rows and columns follow after time Q(n2/p) in a pipelined mode. Hence, process\n finishes its share of the shortest path computation in time Q(n3/p) + Q(n). When\nprocess \n  has finished the ( n - 1)th iteration, it sends the relevant values of the nth row\nand column to the other processes. These values reach process P1,1 in time Q(n). The overall\nparallel run time of this formulation is\n\nSince the sequential run time is W = Q(n3), the speedup and efficiency are as follows:\nEquation 10.7\nTable 10.1. The performance and scalability of the all-pairs shortest\npaths algorithms on various architectures with O (p) bisection\nbandwidth. Similar run times apply to all k - d cube architectures,\nprovided that processes are properly mapped to the underlying\nprocessors.\n\u00a0 Maximum Number of\nProcesses for E = Q(1)Corresponding Parallel\nRun TimeIsoefficiency\nFunction\nDijkstra source-\npartitionedQ(n) Q(n2) Q(p3)\nDijkstra source-\nparallelQ(n2/log n) Q(n log n) Q((p log p)1.5)\nFloyd 1-D block Q(n/log n) Q(n2 log n) Q((p log p)3)\nFloyd 2-D block Q(n2/log2 n) Q(n log2 n) Q(p1.5 log3 p)\nFloyd pipelined 2-\nD blockQ(n2) Q(n) Q(p1.5)\nFrom Equation 10.7  we see that for a cost-optimal formulation p/n2 = O (1). Thus, the pipelined\nformulation of Floyd's algorithm uses up to O (n2) processes efficiently. Also from Equation\n10.7, we can derive the isoefficiency function due to communication, which is Q(p1.5). This is the\noverall isoefficiency function as well. Comparing the pipelined formulation to the synchronized\n2-D block mapping formulation, we see that the former is significantly faster.\n10.4.3 Performance Comparisons\nThe performance of the all-pairs shortest paths algorithms previously presented is summarized\nin Table 10.1  for a parallel architecture with O (p) bisection bandwidth. Floyd's pipelined\nformulation is the most scalable and can use up to Q(n2) processes to solve the problem in time\nQ(n). Moreover, this parallel formulation performs equally well even on architectures with\nbisection bandwidth O", "doc_id": "bec38c44-ad6a-4061-840c-8ca4bbe10c03", "embedding": null, "doc_hash": "6e86d9087ddd333eda42b85e182d09f6f5a2df4467b8431f1acb4e8d7f6157d3", "extra_info": null, "node_info": {"start": 1225237, "end": 1228442}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4f3f4d84-c6db-427d-aef4-087e66a726f1", "3": "74fe005c-7f2e-45e1-990a-15d8216e671f"}}, "__type__": "1"}, "74fe005c-7f2e-45e1-990a-15d8216e671f": {"__data__": {"text": "formulation to the synchronized\n2-D block mapping formulation, we see that the former is significantly faster.\n10.4.3 Performance Comparisons\nThe performance of the all-pairs shortest paths algorithms previously presented is summarized\nin Table 10.1  for a parallel architecture with O (p) bisection bandwidth. Floyd's pipelined\nformulation is the most scalable and can use up to Q(n2) processes to solve the problem in time\nQ(n). Moreover, this parallel formulation performs equally well even on architectures with\nbisection bandwidth O \n , such as a mesh-connected computer. Furthermore, its performance\nis independent of the type of routing (store-and-forward or cut-through).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n10.5 Transitive Closure\nIn many applications we wish to determine if any two vertices in a graph are connected. This is\nusually done by finding the transitive closure of a graph. Formally, if G = (V, E) is a graph, then\nthe transitive closure  of G is defined as the graph G* = (V, E*), where E* = {(vi, vj)| there is a\npath from vi to vj in G}. We compute the transitive closure of a graph by computing the\nconnectivity matrix A*. The connectivity matrix  of G is a matrix \n  such that \nif there is a path from vi to vj or i = j, and \n  otherwise.\nTo compute A* we assign a weight of 1 to each edge of E and use any of the all-pairs shortest\npaths algorithms on this weighted graph. Matrix A* can be obtained from matrix D, where D is\nthe solution to the all-pairs shortest paths problem, as follows:\nAnother method for computing A* is to use Floyd's algorithm on the adjacency matrix of G,\nreplacing the min and + operations in line 7 of Algorithm 10.3  by logical or and logical and\noperations. In this case, we initially set ai,j = 1 if i = j or (vi, vj) \n E, and ai,j = 0 otherwise.\nMatrix A* is obtained by setting \n  if di,j = 0 and \n  otherwise. The complexity of\ncomputing the transitive closure is Q(n3).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n10.6 Connected Components\nThe connected components  of an undirected graph G = (V, E) are the maximal disjoint sets\nC1, C2, ..., Ck such that V = C1 \n C2 \n ... \n  Ck , and u, v \n Ci if and only if u is reachable from v\nand v is reachable from u. The connected components of an undirected graph are the\nequivalence classes of vertices under the \"is reachable from\" relation. For example, Figure\n10.10  shows a graph with three connected components.\nFigure 10.10. A graph with three connected components: {1, 2, 3, 4},\n{5, 6, 7}, and {8, 9}.\n10.6.1 A Depth-First Search Based Algorithm\nWe can find the connected components of a graph by performing a depth-first traversal on the\ngraph. The outcome of this depth-first traversal is a forest of depth-first trees. Each tree in the\nforest contains vertices that belong to a different connected component. Figure 10.11 illustrates\nthis algorithm. The correctness of this algorithm follows directly from the definition of a\nspanning tree (that is, a depth-first tree is also a spanning tree of a graph induced by the set of\nvertices in the depth-first tree) and from the fact that G is undirected. Assuming that the graph\nis stored using a sparse representation, the run time of this algorithm is Q(|E|) because the\ndepth-first traversal algorithm traverses all the edges in G .\nFigure", "doc_id": "74fe005c-7f2e-45e1-990a-15d8216e671f", "embedding": null, "doc_hash": "fd1d794a821d31bad9dcf91492b8b4505ad3e1542ec57c8ef6ee0249cf477a3a", "extra_info": null, "node_info": {"start": 1228435, "end": 1231724}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bec38c44-ad6a-4061-840c-8ca4bbe10c03", "3": "5181b5d7-1a27-4264-8ba9-1704de9a992d"}}, "__type__": "1"}, "5181b5d7-1a27-4264-8ba9-1704de9a992d": {"__data__": {"text": "trees. Each tree in the\nforest contains vertices that belong to a different connected component. Figure 10.11 illustrates\nthis algorithm. The correctness of this algorithm follows directly from the definition of a\nspanning tree (that is, a depth-first tree is also a spanning tree of a graph induced by the set of\nvertices in the depth-first tree) and from the fact that G is undirected. Assuming that the graph\nis stored using a sparse representation, the run time of this algorithm is Q(|E|) because the\ndepth-first traversal algorithm traverses all the edges in G .\nFigure 10.11. Part (b) is a depth-first forest obtained from depth-first\ntraversal of the graph in part (a). Each of these trees is a connected\ncomponent of the graph in part (a).\nParallel Formulation\nThe connected-component algorithm can be parallelized by partitioning the adjacency matrix of\nG into p parts and assigning each part to one of p processes. Each process Pi has a subgraph Gi\nof G, where Gi = (V, Ei) and Ei are the edges that correspond to the portion of the adjacency\nmatrix assigned to this process. In the first step of this parallel formulation, each process Pi\ncomputes the depth-first spanning forest of the graph Gi. At the end of this step, p spanning\nforests have been constructed. During the second step, spanning forests are merged pairwise\nuntil only one spanning forest remains. The remaining spanning forest has the property that two\nvertices are in the same connected component of G if they are in the same tree. Figure 10.12illustrates this algorithm.\nFigure 10.12. Computing connected components in parallel. The\nadjacency matrix of the graph G in (a) is partitioned into two parts as\nshown in (b). Next, each process gets a subgraph of G as shown in (c)\nand (e). Each process then computes the spanning forest of the\nsubgraph, as shown in (d) and (f). Finally, the two spanning trees are\nmerged to form the solution.\nTo merge pairs of spanning forests efficiently, the algorithm uses disjoint sets of edges. Assume\nthat each tree in the spanning forest of a subgraph of G is represented by a set. The sets for\ndifferent trees are pairwise disjoint. The following operations are defined on the disjoint sets:\nfind(x)  returns a pointer to the representative element of the set containing x. Each set\nhas its own unique representative.\nunion(x , y) unites the sets containing the elements x and y. The two sets are assumed to\nbe disjoint prior to the operation.\nThe spanning forests are merged as follows. Let A and B be the two spanning forests to be\nmerged. At most n - 1 edges (since A and B are forests) of one are merged with the edges of\nthe other. Suppose we want to merge forest A into forest B. For each edge ( u, v) of A, a find\noperation is performed for each vertex to determine if the two vertices are already in the same\ntree of B . If not, then the two trees (sets) of B containing u and v are united by a union\noperation. Otherwise, no union operation is necessary. Hence, merging A and B requires at\nmost 2( n - 1) find operations and ( n - 1) union operations. We can implement the disjoint-set\ndata structure by using disjoint-set forests with ranking and path compression. Using this\nimplementation, the cost of performing 2( n - 1) finds and ( n - 1) unions is O (n). A detailed\ndescription of the disjoint-set forest is beyond the scope of this book. Refer to the bibliographic\nremarks ( Section 10.8) for references.\nHaving discussed how to efficiently merge two spanning forests, we now concentrate on how to\npartition the adjacency matrix of G", "doc_id": "5181b5d7-1a27-4264-8ba9-1704de9a992d", "embedding": null, "doc_hash": "2065cf59e4e77508d37da23853d18a3e42f3e3b6043a89171660e19b72296e91", "extra_info": null, "node_info": {"start": 1231704, "end": 1235273}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "74fe005c-7f2e-45e1-990a-15d8216e671f", "3": "3ff91c1b-2b43-4ff5-b51d-2f6eafd677ce"}}, "__type__": "1"}, "3ff91c1b-2b43-4ff5-b51d-2f6eafd677ce": {"__data__": {"text": "find operations and ( n - 1) union operations. We can implement the disjoint-set\ndata structure by using disjoint-set forests with ranking and path compression. Using this\nimplementation, the cost of performing 2( n - 1) finds and ( n - 1) unions is O (n). A detailed\ndescription of the disjoint-set forest is beyond the scope of this book. Refer to the bibliographic\nremarks ( Section 10.8) for references.\nHaving discussed how to efficiently merge two spanning forests, we now concentrate on how to\npartition the adjacency matrix of G and distribute it among p processes. The next section\ndiscusses a formulation that uses 1-D block mapping. An alternative partitioning scheme is\ndiscussed in Problem 10.12.\n1-D Block Mapping  The n x n adjacency matrix is partitioned into p stripes ( Section 3.4.1).\nEach stripe is composed of n/p consecutive rows and is assigned to one of the p processes. To\ncompute the connected components, each process first computes a spanning forest for the n-\nvertex graph represented by the n/p rows of the adjacency matrix assigned to it.\nConsider a p-process message-passing computer. Computing the spanning forest based on the\n(n/p) x n adjacency matrix assigned to each process requires time Q(n2/p). The second step of\nthe algorithm\u2013the pairwise merging of spanning forests \u2013 is performed by embedding a virtual\ntree on the processes. There are log p merging stages, and each takes time Q(n). Thus, the cost\ndue to merging is Q (n log p). Finally, during each merging stage, spanning forests are sent\nbetween nearest neighbors. Recall that Q(n) edges of the spanning forest are transmitted. Thus,\nthe communication cost is Q (n log p). The parallel run time of the connected-component\nalgorithm is\nSince the sequential complexity is W = Q(n2), the speedup and efficiency are as follows:\nEquation 10.8\nFrom Equation 10.8  we see that for a cost-optimal formulation p = O (n/log n). Also from\nEquation 10.8 , we derive the isoefficiency function, which is Q(p2 log2 p). This is the isoefficiency\nfunction due to communication and due to the extra computations performed in the merging\nstage. The isoefficiency function due to concurrency is Q(p2); thus, the overall isoefficiency\nfunction is Q(p2 log2 p). The performance of this parallel formulation is similar to that of Prim's\nminimum spanning tree algorithm and Dijkstra's single-source shortest paths algorithm on a\nmessage-passing computer.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n10.7 Algorithms for Sparse Graphs\nThe parallel algorithms in the previous sections are based on the best-known algorithms for\ndense-graph problems. However, we have yet to address parallel algorithms for sparse graphs.\nRecall that a graph G = (V, E) is sparse if | E| is much smaller than | V|2. Figure 10.13 shows\nsome examples of sparse graphs.\nFigure 10.13. Examples of sparse graphs: (a) a linear graph, in which\neach vertex has two incident edges; (b) a grid graph, in which each\nvertex has four incident vertices; and (c) a random sparse graph.\nAny dense-graph algorithm works correctly on sparse graphs as well. However, if the\nsparseness of the graph is taken into account, it is usually possible to obtain significantly better\nperformance. For example, the run time of Prim's minimum spanning tree algorithm ( Section10.2) is Q(n2) regardless of the number of edges in the graph. By modifying Prim's algorithm to\nuse adjacency lists and a binary heap,", "doc_id": "3ff91c1b-2b43-4ff5-b51d-2f6eafd677ce", "embedding": null, "doc_hash": "fb23af3df55d9cb8c1030d040e49d36120723a2b8323aa29c26b8ddc022670f2", "extra_info": null, "node_info": {"start": 1235309, "end": 1238732}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5181b5d7-1a27-4264-8ba9-1704de9a992d", "3": "2bc61a05-4028-424a-93f0-4c6b75fdd57b"}}, "__type__": "1"}, "2bc61a05-4028-424a-93f0-4c6b75fdd57b": {"__data__": {"text": "vertex has two incident edges; (b) a grid graph, in which each\nvertex has four incident vertices; and (c) a random sparse graph.\nAny dense-graph algorithm works correctly on sparse graphs as well. However, if the\nsparseness of the graph is taken into account, it is usually possible to obtain significantly better\nperformance. For example, the run time of Prim's minimum spanning tree algorithm ( Section10.2) is Q(n2) regardless of the number of edges in the graph. By modifying Prim's algorithm to\nuse adjacency lists and a binary heap, the complexity of the algorithm reduces to Q(|E| log n).\nThis modified algorithm outperforms the original algorithm as long as | E|=O (n2/log n). An\nimportant step in developing sparse-graph algorithms is to use an adjacency list instead of an\nadjacency matrix. This change in representation is crucial, since the complexity of adjacency-\nmatrix-based algorithms is usually W(n2), and is independent of the number of edges.\nConversely, the complexity of adjacency-list-based algorithms is usually W(n + |E|), which\ndepends on the sparseness of the graph.\nIn the parallel formulations of sequential algorithms for dense graphs, we obtained good\nperformance by partitioning the adjacency matrix of a graph so that each process performed\nroughly an equal amount of work and communication was localized. We were able to achieve\nthis largely because the graph was dense. For example, consider Floyd's all-pairs shortest paths\nalgorithm. By assigning equal-sized blocks from the adjacency matrix to all processes, the work\nwas uniformly distributed. Moreover, since each block consisted of consecutive rows and\ncolumns, the communication overhead was limited.\nHowever, it is difficult to achieve even work distribution and low communication overhead for\nsparse graphs. Consider the problem of partitioning the adjacency list of a graph. One possible\npartition assigns an equal number of vertices and their adjacency lists to each process.\nHowever, the number of edges incident on a given vertex may vary. Hence, some processes\nmay be assigned a large number of edges while others receive very few, leading to a significant\nwork imbalance among the processes. Alternately, we can assign an equal number of edges to\neach process. This may require splitting the adjacency list of a vertex among processes. As a\nresult, the time spent communicating information among processes that store separate parts of\nthe adjacency list may increase dramatically. Thus, it is hard to derive efficient parallel\nformulations for general sparse graphs (Problems 10.14 and 10.15). However, we can often\nderive efficient parallel formulations if the sparse graph has a certain structure. For example,\nconsider the street map shown in Figure 10.14. The graph corresponding to the map is sparse:\nthe number of edges incident on any vertex is at most four. We refer to such graphs as grid\ngraphs . Other types of sparse graphs for which an efficient parallel formulation can be\ndeveloped are those corresponding to well-shaped finite element meshes, and graphs whose\nvertices have similar degrees. The next two sections present efficient algorithms for finding a\nmaximal independent set of vertices, and for computing single-source shortest paths for these\ntypes of graphs.\nFigure 10.14. A street map (a) can be represented by a graph (b). In\nthe graph shown in (b), each street intersection is a vertex and each\nedge is a street segment. The vertices of (b) are the intersections of\n(a) marked by dots.\n10.7.1 Finding a Maximal Independent Set\nConsider the problem of finding a maximal independent set (MIS) of vertices of a graph. We are\ngiven a sparse undirected graph G = (V, E). A set of vertices", "doc_id": "2bc61a05-4028-424a-93f0-4c6b75fdd57b", "embedding": null, "doc_hash": "b32eb74b3bd95dfd789d17141bfa24b706ded2816c93be812e75188199387568", "extra_info": null, "node_info": {"start": 1238730, "end": 1242440}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3ff91c1b-2b43-4ff5-b51d-2f6eafd677ce", "3": "a69052d8-9daf-4c5f-849c-a99ce8025601"}}, "__type__": "1"}, "a69052d8-9daf-4c5f-849c-a99ce8025601": {"__data__": {"text": "set of vertices, and for computing single-source shortest paths for these\ntypes of graphs.\nFigure 10.14. A street map (a) can be represented by a graph (b). In\nthe graph shown in (b), each street intersection is a vertex and each\nedge is a street segment. The vertices of (b) are the intersections of\n(a) marked by dots.\n10.7.1 Finding a Maximal Independent Set\nConsider the problem of finding a maximal independent set (MIS) of vertices of a graph. We are\ngiven a sparse undirected graph G = (V, E). A set of vertices I \n V is called independent  if no\npair of vertices in I is connected via an edge in G. An independent set is called maximal  if by\nincluding any other vertex not in I, the independence property is violated. These definitions are\nillustrated in Figure 10.15 . Note that as the example illustrates, maximal independent sets are\nnot unique. Maximal independent sets of vertices can be used to determine which computations\ncan be done in parallel in certain types of task graphs. For example, maximal independent sets\ncan be used to determine the sets of rows that can be factored concurrently in parallel\nincomplete factorization algorithms, and to compute a coloring of a graph in parallel.\nFigure 10.15. Examples of independent and maximal independent sets.\nMany algorithms have been proposed for computing a maximal independent set of vertices. The\nsimplest class of algorithms starts by initially setting I to be empty, and assigning all vertices to\na set C that acts as the candidate  set of vertices for inclusion in I . Then the algorithm proceeds\nby repeatedly moving a vertex v from C into I and removing all vertices adjacent to v from C.\nThis process terminates when C becomes empty, in which case I is a maximal independent set.\nThe resulting set I will contain an independent set of vertices, because every time we add a\nvertex into I we remove from C all the vertices whose subsequent inclusion will violate the\nindependence condition. Also, the resulting set is maximal, because any other vertex that is not\nalready in I is adjacent to at least one of the vertices in I .\nEven though the above algorithm is very simple, it is not well suited for parallel processing, as it\nis serial in nature. For this reason parallel MIS algorithms are usually based on the randomized\nalgorithm originally developed by Luby for computing a coloring of a graph. Using Luby's\nalgorithm, a maximal independent set I of vertices V a graph is computed in an incremental\nfashion as follows. The set I is initially set to be empty, and the set of candidate vertices, C, is\nset to be equal to V. A unique random number is assigned to each vertex in C, and if a vertex\nhas a random number that is smaller than all of the random numbers of the adjacent vertices, it\nis included in I. The set C is updated so that all the vertices that were selected for inclusion in I\nand their adjacent vertices are removed from it. Note that the vertices that are selected for\ninclusion in I are indeed independent (i.e., not directly connected via an edge). This is because,\nif v was inserted in I, then the random number assigned to v is the smallest among the random\nnumbers assigned to its adjacent vertices; thus, no other vertex u adjacent to v will have been\nselected for inclusion. Now the above steps of random number assignment and vertex selection\nare repeated for the vertices left in C, and I is augmented similarly. This incremental\naugmentation of I ends when C becomes empty. On the average, this algorithm converges after\nO (log | V|) such augmentation steps. The execution of the algorithm for a small graph is\nillustrated in Figure 10.16. In the rest of this section we describe a shared-address-space\nparallel formulation of Luby's", "doc_id": "a69052d8-9daf-4c5f-849c-a99ce8025601", "embedding": null, "doc_hash": "b3e9531b5cbac60b0361ca53ff160cbb08d06e7c0488fbb346e334ca5baa8542", "extra_info": null, "node_info": {"start": 1242463, "end": 1246206}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2bc61a05-4028-424a-93f0-4c6b75fdd57b", "3": "f00f2e14-6cb3-420f-961c-2df9e75cb6bc"}}, "__type__": "1"}, "f00f2e14-6cb3-420f-961c-2df9e75cb6bc": {"__data__": {"text": "to its adjacent vertices; thus, no other vertex u adjacent to v will have been\nselected for inclusion. Now the above steps of random number assignment and vertex selection\nare repeated for the vertices left in C, and I is augmented similarly. This incremental\naugmentation of I ends when C becomes empty. On the average, this algorithm converges after\nO (log | V|) such augmentation steps. The execution of the algorithm for a small graph is\nillustrated in Figure 10.16. In the rest of this section we describe a shared-address-space\nparallel formulation of Luby's algorithm. A message-passing adaption of this algorithm is\ndescribed in the message-passing chapter.\nFigure 10.16. The different augmentation steps of Luby's randomized\nmaximal independent set algorithm. The numbers inside each vertex\ncorrespond to the random number assigned to the vertex.\n\nShared-Address-Space Parallel Formulation\nA parallel formulation of Luby's MIS algorithm for a shared-address-space parallel computer is\nas follows. Let I be an array of size | V|. At the termination of the algorithm, I [i] will store one,\nif vertex vi is part of the MIS, or zero otherwise. Initially, all the elements in I are set to zero,\nand during each iteration of Luby's algorithm, some of the entries of that array will be changed\nto one. Let C be an array of size | V|. During the course of the algorithm, C [i] is one if vertex vi\nis part of the candidate set, or zero otherwise. Initially, all the elements in C are set to one.\nFinally, let R be an array of size | V| that stores the random numbers assigned to each vertex.\nDuring each iteration, the set C is logically partitioned among the p processes. Each process\ngenerates a random number for its assigned vertices from C. When all the processes finish\ngenerating these random numbers, they proceed to determine which vertices can be included in\nI. In particular, for each vertex assigned to them, they check to see if the random number\nassigned to it is smaller than the random numbers assigned to all of its adjacent vertices. If it is\ntrue, they set the corresponding entry in I to one. Because R is shared and can be accessed by\nall the processes, determining whether or not a particular vertex can be included in I is quite\nstraightforward.\nArray C can also be updated in a straightforward fashion as follows. Each process, as soon as it\ndetermines that a particular vertex v will be part of I, will set to zero the entries of C\ncorresponding to its adjacent vertices. Note that even though more than one process may be\nsetting to zero the same entry of C (because it may be adjacent to more than one vertex that\nwas inserted in I), such concurrent writes will not affect the correctness of the results, because\nthe value that gets concurrently written is the same.\nThe complexity of each iteration of Luby's algorithm is similar to that of the serial algorithm,\nwith the extra cost of the global synchronization after each random number assignment. The\ndetailed analysis of Luby's algorithm is left as an exercise (Problem 10.16).\n10.7.2 Single-Source Shortest Paths\nIt is easy to modify Dijkstra's single-source shortest paths algorithm so that it finds the shortest\npaths for sparse graphs efficiently. The modified algorithm is known as Johnson's algorithm.\nRecall that Dijkstra's algorithm performs the following two steps in each iteration. First, it\nextracts a vertex u \n (V - VT) such that l[u] = min{ l[v]|v \n (V - VT)} and inserts it into set VT.\nSecond, for each vertex v \n (V - VT), it computes l[v] = min{ l[v], l[u] + w(u, v)}.", "doc_id": "f00f2e14-6cb3-420f-961c-2df9e75cb6bc", "embedding": null, "doc_hash": "0c5d5a925b8d85f323be0b2744ee35e0057e0ec8f91b1e8fc2d3058610f2b8dc", "extra_info": null, "node_info": {"start": 1246163, "end": 1249732}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a69052d8-9daf-4c5f-849c-a99ce8025601", "3": "8243ba08-275d-4a4a-a884-51ac4b5c976d"}}, "__type__": "1"}, "8243ba08-275d-4a4a-a884-51ac4b5c976d": {"__data__": {"text": "single-source shortest paths algorithm so that it finds the shortest\npaths for sparse graphs efficiently. The modified algorithm is known as Johnson's algorithm.\nRecall that Dijkstra's algorithm performs the following two steps in each iteration. First, it\nextracts a vertex u \n (V - VT) such that l[u] = min{ l[v]|v \n (V - VT)} and inserts it into set VT.\nSecond, for each vertex v \n (V - VT), it computes l[v] = min{ l[v], l[u] + w(u, v)}. Note that,\nduring the second step, only the vertices in the adjacency list of vertex u need to be considered.\nSince the graph is sparse, the number of vertices adjacent to vertex u is considerably smaller\nthan Q(n); thus, using the adjacency-list representation improves performance.\nJohnson's algorithm uses a priority queue Q to store the value l[v] for each vertex v \n (V - VT).\nThe priority queue is constructed so that the vertex with the smallest value in l is always at the\nfront of the queue. A common way to implement a priority queue is as a binary min-heap. A\nbinary min-heap allows us to update the value l[v] for each vertex v in time O (log n).\nAlgorithm 10.5  shows Johnson's algorithm. Initially, for each vertex v other than the source, it\ninserts l[v] = \n  in the priority queue. For the source vertex s it inserts l[s] = 0. At each step\nof the algorithm, the vertex u \n (V - VT) with the minimum value in l is removed from the\npriority queue. The adjacency list for u is traversed, and for each edge ( u, v) the distance l[v] to\nvertex v is updated in the heap. Updating vertices in the heap dominates the overall run time of\nthe algorithm. The total number of updates is equal to the number of edges; thus, the overall\ncomplexity of Johnson's algorithm is Q(|E| log n).\nAlgorithm 10.5 Johnson's sequential single-source shortest paths\nalgorithm.\n1.   procedure  JOHNSON_SINGLE_SOURCE_SP( V, E, s) \n2.   begin \n3.      Q := V ; \n4.      for all v \n Q do \n5.          l[v] := \n ; \n6.      l[s] := 0; \n7.      while Q \n 0 do \n8.      begin \n9.         u := extract min ( Q); \n10.        for each v \n Adj [u] do \n11.           if v \n Q and l[u] + w(u, v) < l[v] then \n12.               l[v] := l[u] + w(u, v); \n13.     endwhile  \n14.  end JOHNSON_SINGLE_SOURCE_SP \nParallelization Strategy\nAn efficient parallel formulation of Johnson's algorithm must maintain the priority queue Q\nefficiently. A simple strategy is for a single process, for example, P0, to maintain Q. All other\nprocesses will then compute new values of l[v] for v \n (V - VT), and give them to P0 to update\nthe priority queue. There are two main limitation of this scheme. First, because a single process\nis responsible for maintaining the priority queue, the overall parallel run time is O (|E| log n)\n(there are O (|E|) queue updates and each update takes time O (log n)). This leads to a parallel\nformulation with no asymptotic speedup, since O (|E| log n)", "doc_id": "8243ba08-275d-4a4a-a884-51ac4b5c976d", "embedding": null, "doc_hash": "8a69862bf48f6aaba84f6904747e6c81279231a252ff88cbc74c9d6f8b42a6d6", "extra_info": null, "node_info": {"start": 1249844, "end": 1252729}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f00f2e14-6cb3-420f-961c-2df9e75cb6bc", "3": "8d3047e2-a2ec-4e61-9317-6a311e064ceb"}}, "__type__": "1"}, "8d3047e2-a2ec-4e61-9317-6a311e064ceb": {"__data__": {"text": "is for a single process, for example, P0, to maintain Q. All other\nprocesses will then compute new values of l[v] for v \n (V - VT), and give them to P0 to update\nthe priority queue. There are two main limitation of this scheme. First, because a single process\nis responsible for maintaining the priority queue, the overall parallel run time is O (|E| log n)\n(there are O (|E|) queue updates and each update takes time O (log n)). This leads to a parallel\nformulation with no asymptotic speedup, since O (|E| log n) is the same as the sequential run\ntime. Second, during each iteration, the algorithm updates roughly | E|/|V| vertices. As a result,\nno more than | E|/|V| processes can be kept busy at any given time, which is very small for\nmost of the interesting classes of sparse graphs, and to a large extent, independent of the size\nof the graphs.\nThe first limitation can be alleviated by distributing the maintainance of the priority queue to\nmultiple processes. This is a non-trivial task, and can only be done effectively on architectures\nwith low latency, such as shared-address-space computers. However, even in the best case,\nwhen each priority queue update takes only time O (1), the maximum speedup that can be\nachieved is O (log n), which is quite small. The second limitation can be alleviated by\nrecognizing the fact that depending on the l value of the vertices at the top of the priority\nqueue, more than one vertex can be extracted at the same time. In particular, if v is the vertex\nat the top of the priority queue, all vertices u such that l[u] = l[v] can also be extracted, and\ntheir adjacency lists processed concurrently. This is because the vertices that are at the same\nminimum distance from the source can be processed in any order. Note that in order for this\napproach to work, all the vertices that are at the same minimum distance must be processed in\nlock-step. An additional degree of concurrency can be extracted if we know that the minimum\nweight over all the edges in the graph is m. In that case, all vertices u such that l[u] \n l[v] +\nm can be processed concurrently (and in lock-step). We will refer to these as the safe vertices.\nHowever, this additional concurrency can lead to asymptotically better speedup than O (log n)\nonly if more than one update operation of the priority queue can proceed concurrently,\nsubstantially complicating the parallel algorithm for maintaining the single priority queue.\nOur discussion thus far was focused on developing a parallel formulation of Johnson's algorithm\nthat finds the shortest paths to the vertices in the same order as the serial algorithm, and\nexplores concurrently only safe vertices. However, as we have seen, such an approach leads to\ncomplicated algorithms and limited concurrency. An alternate approach is to develop a parallel\nalgorithm that processes both safe and unsafe  vertices concurrently, as long as these unsafe\nvertices can be reached from the source via a path involving vertices whose shortest paths have\nalready been computed (i.e., their corresponding l-value in the priority queue is not infinite). In\nparticular, in this algorithm, each one of the p processes extracts one of the p top vertices and\nproceeds to update the l values of the vertices adjacent to it. Of course, the problem with this\napproach is that it does not ensure that the l values of the vertices extracted from the priority\nqueue correspond to the cost of the shortest path. For example, consider two vertices v and u\nthat are at the top of the priority queue, with l[v] < l[u]. According to Johnson's algorithm, at\nthe point a vertex is extracted from the priority queue, its l value is the cost of the shortest path\nfrom the source", "doc_id": "8d3047e2-a2ec-4e61-9317-6a311e064ceb", "embedding": null, "doc_hash": "082eaabf35048ae437dc2afe9f0f073f4b4fe452a1e8edc0bd556e2bee94761c", "extra_info": null, "node_info": {"start": 1252672, "end": 1256386}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8243ba08-275d-4a4a-a884-51ac4b5c976d", "3": "690f3c58-ab5c-4201-af74-9c4fbf67803e"}}, "__type__": "1"}, "690f3c58-ab5c-4201-af74-9c4fbf67803e": {"__data__": {"text": "algorithm, each one of the p processes extracts one of the p top vertices and\nproceeds to update the l values of the vertices adjacent to it. Of course, the problem with this\napproach is that it does not ensure that the l values of the vertices extracted from the priority\nqueue correspond to the cost of the shortest path. For example, consider two vertices v and u\nthat are at the top of the priority queue, with l[v] < l[u]. According to Johnson's algorithm, at\nthe point a vertex is extracted from the priority queue, its l value is the cost of the shortest path\nfrom the source to that vertex. Now, if there is an edge connecting v and u, such that l[v] +\nw(v, u) < l[u], then the correct value of the shortest path to u is l[v] + w(v, u), and not l[u].\nHowever, the correctness of the results can be ensured by detecting when we have incorrectly\ncomputed the shortest path to a particular vertex, and inserting it back into the priority queue\nwith the updated l value. We can detect such instances as follows. Consider a vertex v that has\njust been extracted from the queue, and let u be a vertex adjacent to v that has already been\nextracted from the queue. If l[v] + w(v, u) is smaller than l[u], then the shortest path to u has\nbeen incorrectly computed, and u needs to be inserted back into the priority queue with l[u] =\nl[v] + w(v, u).\nTo see how this approach works, consider the example grid graph shown in Figure 10.17. In\nthis example, there are three processes and we want to find the shortest path from vertex a.\nAfter initialization of the priority queue, vertices b and d will be reachable from the source. In\nthe first step, process P0 and P1 extract vertices b and d and proceed to update the l values of\nthe vertices adjacent to b and d. In the second step, processes P0, P1, and P2 extract e, c, and g,\nand proceed to update the l values of the vertices adjacent to them. Note that when processing\nvertex e, process P0 checks to see if l[e] + w(e, d) is smaller or greater than l[d]. In this\nparticular example, l[e] + w(e, d) > l[d], indicating that the previously computed value of the\nshortest path to d does not change when e is considered, and all computations so far are\ncorrect. In the third step, processes P0 and P1 work on h and f, respectively. Now, when process\nP0 compares l[h] + w(h, g) = 5 against the value of l[g] = 10 that was extracted in the previous\niteration, it finds it to be smaller. As a result, it inserts back into the priority queue vertex g\nwith the updated l[g] value. Finally, in the fourth and last step, the remaining two vertices are\nextracted from the priority queue, and all single-source shortest paths have been computed.\nFigure 10.17. An example of the modified Johnson's algorithm for\nprocessing unsafe vertices concurrently.\nThis approach for parallelizing Johnson's algorithm falls into the category of speculative\ndecomposition discussed in Section 3.2.4. Essentially, the algorithm assumes that the l[] values\nof the top p vertices in the priority queue will not change as a result of processing some of these\nvertices, and proceeds to perform the computations required by Johnson's algorithm. However,\nif at some later point it detects that its assumptions were wrong, it goes back and essentially\nrecomputes the shortest paths of the affected vertices.\nIn order for such a speculative decomposition approach to be effective, we must also remove\nthe bottleneck of working with a single priority queue. In the rest of this section we present", "doc_id": "690f3c58-ab5c-4201-af74-9c4fbf67803e", "embedding": null, "doc_hash": "5f2de04ed1a10556355c5a3b37fa94dd302c300941a453b5903bd0347e939c8e", "extra_info": null, "node_info": {"start": 1256331, "end": 1259841}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8d3047e2-a2ec-4e61-9317-6a311e064ceb", "3": "018d0dc1-b2a3-4435-8070-c5660bb58715"}}, "__type__": "1"}, "018d0dc1-b2a3-4435-8070-c5660bb58715": {"__data__": {"text": "in Section 3.2.4. Essentially, the algorithm assumes that the l[] values\nof the top p vertices in the priority queue will not change as a result of processing some of these\nvertices, and proceeds to perform the computations required by Johnson's algorithm. However,\nif at some later point it detects that its assumptions were wrong, it goes back and essentially\nrecomputes the shortest paths of the affected vertices.\nIn order for such a speculative decomposition approach to be effective, we must also remove\nthe bottleneck of working with a single priority queue. In the rest of this section we present a\nmessage-passing algorithm that uses speculative decomposition to extract concurrency and in\nwhich there is no single priority queue. Instead, each process maintains its own priority queue\nfor the vertices that it is assigned to. Problem 10.13 discusses another approach.\nDistributed Memory Formulation\nLet p be the number of processes, and let G = (V, E) be a sparse graph. We partition the set of\nvertices V into p disjoint sets V1, V2, ..., Vp, and assign each set of vertices and its associated\nadjacency lists to one of the p processes. Each process maintains a priority queue for the\nvertices assigned to it, and computes the shortest paths from the source to these vertices. Thus,\nthe priority queue Q is partitioned into p disjoint priority queues Q1, Q2, ..., Qp, each assigned to\na separate process. In addition to the priority queue, each process Pi also maintains an array sp\nsuch that sp[v] stores the cost of the shortest path from the source vertex to v for each vertex v\n Vi. The cost sp [v] is updated to l[v] each time vertex v is extracted from the priority queue.\nInitially, sp[v] = \n  for every vertex v other than the source, and we insert l[s] into the\nappropriate priority queue for the source vertex s. Each process executes Johnson's algorithm\non its local priority queue. At the end of the algorithm, sp[v] stores the length of the shortest\npath from source to vertex v.\nWhen process Pi extracts the vertex u \n Vi with the smallest value l[u] from Qi, the l values of\nvertices assigned to processes other than Pi may need to be updated. Process Pi sends a\nmessage to processes that store vertices adjacent to u, notifying them of the new values. Upon\nreceiving these values, processes update the values of l. For example, assume that there is an\nedge ( u, v) such that u \n Vi and v \n Vj, and that process Pi has just extracted vertex u from its\npriority queue. Process Pi then sends a message to Pj containing the potential new value of l[v],\nwhich is l[u] + w(u, v). Process Pj, upon receiving this message, sets the value of l[v] stored in\nits priority queue to min{ l[v], l[u] + w(u, v)}.\nSince both processes Pi and Pj execute Johnson's algorithm, it is possible that process Pj has\nalready extracted vertex v from its priority queue. This means that process Pj might have\nalready computed the shortest path sp[v] from the source to vertex v. Then there are two\npossible cases: either sp[v] \n l[u] + w(u, v), or sp[v] > l[u] + w(u, v). The first case means\nthat there is a longer path to vertex v passing through vertex u, and the second case means\nthat there is a shorter path to vertex v passing through vertex u. For the first case, process Pj\nneeds to do nothing, since the shortest path to v does not change. For the second case, process\nPj must update the cost of the shortest path to vertex v. This is done by inserting the vertex v\nback into the priority queue with l[v] =", "doc_id": "018d0dc1-b2a3-4435-8070-c5660bb58715", "embedding": null, "doc_hash": "a35e94f80b0832ce9806ce056ac3d1e620b3bedabb2188ac91a36eacab41774a", "extra_info": null, "node_info": {"start": 1259811, "end": 1263328}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "690f3c58-ab5c-4201-af74-9c4fbf67803e", "3": "9828697a-c93b-4564-8a63-391bccb19e6b"}}, "__type__": "1"}, "9828697a-c93b-4564-8a63-391bccb19e6b": {"__data__": {"text": "\n l[u] + w(u, v), or sp[v] > l[u] + w(u, v). The first case means\nthat there is a longer path to vertex v passing through vertex u, and the second case means\nthat there is a shorter path to vertex v passing through vertex u. For the first case, process Pj\nneeds to do nothing, since the shortest path to v does not change. For the second case, process\nPj must update the cost of the shortest path to vertex v. This is done by inserting the vertex v\nback into the priority queue with l[v] = l[u] + w(u, v) and disregarding the value of sp[v].\nSince a vertex v can be reinserted into the priority queue, the algorithm terminates only when\nall the queues become empty.\nInitially, only the priority queue of the process with the source vertex is non-empty. After that,\nthe priority queues of other processes become populated as messages containing new l values\nare created and sent to adjacent processes. When processes receive new l values, they insert\nthem into their priority queues and perform computations. Consider the problem of computing\nthe single-source shortest paths in a grid graph where the source is located at the bottom-left\ncorner. The computations propagate across the grid graph in the form of a wave. A process is\nidle before the wave arrives, and becomes idle again after the wave has passed. This process is\nillustrated in Figure 10.18. At any time during the execution of the algorithm, only the\nprocesses along the wave are busy. The other processes have either finished their computations\nor have not yet started them. The next sections discuss three mappings of grid graphs onto a p-\nprocess mesh.\nFigure 10.18. The wave of activity in the priority queues.\n2-D Block Mapping  One way to map an n x n grid graph onto p processors is to use the 2-D\nblock mapping ( Section 3.4.1 ). Specifically, we can view the p processes as a logical mesh and\nassign a different block of \n  vertices to each process. Figure 10.19  illustrates this\nmapping.\nFigure 10.19. Mapping the grid graph (a) onto a mesh, and (b) by\nusing the 2-D block mapping. In this example, n = 16 and \n . The\nshaded vertices are mapped onto the shaded process.\nAt any time, the number of busy processes is equal to the number of processes intersected by\nthe wave. Since the wave moves diagonally, no more than O (\n ) processes are busy at any\ntime. Let W be the overall work performed by the sequential algorithm. If we assume that, at\nany time, \n  processes are performing computations, and if we ignore the overhead due to\ninter-process communication and extra work, then the maximum speedup and efficiency are as\nfollows:\n\nThe efficiency of this mapping is poor and becomes worse as the number of processes increases.\n2-D Cyclic Mapping  The main limitation of the 2-D block mapping is that each process is\nresponsible for only a small, confined area of the grid. Alternatively, we can make each process\nresponsible for scattered areas of the grid by using the 2-D cyclic mapping ( Section 3.4.1 ). This\nincreases the time during which a process stays busy. In 2-D cyclic mapping, the n x n grid\ngraph is divided into n2/p blocks, each of size \n . Each block is mapped onto the\n process mesh. Figure 10.20  illustrates this mapping. Each process contains a block of\nn2/p vertices. These vertices belong to diagonals of the graph that are \n  vertices apart. Each\nprocess is assigned roughly \n  such diagonals.\nFigure 10.20. Mapping the grid graph (a) onto a mesh, and (b) by\nusing the 2-D cyclic mapping.", "doc_id": "9828697a-c93b-4564-8a63-391bccb19e6b", "embedding": null, "doc_hash": "3ae1680d06c86f20bedaadccfa5b427d20e809864639402c426984b6ebdae090", "extra_info": null, "node_info": {"start": 1263444, "end": 1266935}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "018d0dc1-b2a3-4435-8070-c5660bb58715", "3": "b37c10b6-67b5-42b7-899b-44d1e2b5cca0"}}, "__type__": "1"}, "b37c10b6-67b5-42b7-899b-44d1e2b5cca0": {"__data__": {"text": "during which a process stays busy. In 2-D cyclic mapping, the n x n grid\ngraph is divided into n2/p blocks, each of size \n . Each block is mapped onto the\n process mesh. Figure 10.20  illustrates this mapping. Each process contains a block of\nn2/p vertices. These vertices belong to diagonals of the graph that are \n  vertices apart. Each\nprocess is assigned roughly \n  such diagonals.\nFigure 10.20. Mapping the grid graph (a) onto a mesh, and (b) by\nusing the 2-D cyclic mapping. In this example, n = 16 and \n  = 4. The\nshaded graph vertices are mapped onto the correspondingly shaded\nmesh processes.\nNow each process is responsible for vertices that belong to different parts of the grid graph. As\nthe wave propagates through the graph, the wave intersects some of the vertices on each\nprocess. Thus, processes remain busy for most of the algorithm. The 2-D cyclic mapping,\nthough, incurs a higher communication overhead than does the 2-D block mapping. Since\nadjacent vertices reside on separate processes, every time a process extracts a vertex u from its\npriority queue it must notify other processes of the new value of l[u]. The analysis of this\nmapping is left as an exercise (Problem 10.17).\n1-D Block Mapping  The two mappings discussed so far have limitations. The 2-D block\nmapping fails to keep more than O (\n) processes busy at any time, and the 2-D cyclic\nmapping has high communication overhead. Another mapping treats the p processes as a linear\narray and assigns n/p stripes of the grid graph to each processor by using the 1-D block\nmapping. Figure 10.21  illustrates this mapping.\nFigure 10.21. Mapping the grid graph (a) onto a linear array of\nprocesses (b). In this example, n = 16 and p = 4. The shaded vertices\nare mapped onto the shaded process.\nInitially, the wave intersects only one process. As computation progresses, the wave spills over\nto the second process so that two processes are busy. As the algorithm continues, the wave\nintersects more processes, which become busy. This process continues until all p processes are\nbusy (that is, until they all have been intersected by the wave). After this point, the number of\nbusy processes decreases. Figure 10.22 illustrates the propagation of the wave. If we assume\nthat the wave propagates at a constant rate, then p/2 processes (on the average) are busy.\nIgnoring any overhead, the speedup and efficiency of this mapping are as follows:\nFigure 10.22. The number of busy processes as the computational\nwave propagates across the grid graph.\nThus, the efficiency of this mapping is at most 50 percent. The 1-D block mapping is\nsubstantially better than the 2-D block mapping but cannot use more than O (n) processes.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n10.8 Bibliographic Remarks\nDetailed discussions of graph theory and graph algorithms can be found in numerous texts.\nGibbons [ Gib85 ] provides a good reference to the algorithms presented in this chapter. Aho,\nHopcroft, and Ullman [ AHU74 ], and Cormen, Leiserson, and Rivest [ CLR90 ] provide a detailed\ndescription of various graph algorithms and issues related to their efficient implementation on\nsequential computers.\nThe sequential minimum spanning tree algorithm described in Section 10.2 is due to Prim\n[Pri57]. Bentley [ Ben80 ] and Deo and Yoo [ DY81 ] present parallel formulations of Prim's MST\nalgorithm. Deo and Yoo's algorithm is suited to a shared-address-space computer.", "doc_id": "b37c10b6-67b5-42b7-899b-44d1e2b5cca0", "embedding": null, "doc_hash": "e8a0feb74c66a28d9a85ef887405afd8729ad9e617b98edbdc773bf2c0c226a3", "extra_info": null, "node_info": {"start": 1266935, "end": 1270352}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9828697a-c93b-4564-8a63-391bccb19e6b", "3": "48a6eb14-09a8-4c76-a824-fa0771c18750"}}, "__type__": "1"}, "48a6eb14-09a8-4c76-a824-fa0771c18750": {"__data__": {"text": "the algorithms presented in this chapter. Aho,\nHopcroft, and Ullman [ AHU74 ], and Cormen, Leiserson, and Rivest [ CLR90 ] provide a detailed\ndescription of various graph algorithms and issues related to their efficient implementation on\nsequential computers.\nThe sequential minimum spanning tree algorithm described in Section 10.2 is due to Prim\n[Pri57]. Bentley [ Ben80 ] and Deo and Yoo [ DY81 ] present parallel formulations of Prim's MST\nalgorithm. Deo and Yoo's algorithm is suited to a shared-address-space computer. It finds the\nMST in Q(n1.5) using Q(n0.5) processes. Bentley's algorithm works on a tree-connected systolic\narray and finds the MST in time Q (n log n) using n/log n processes. The hypercube formulation\nof Prim's MST algorithm in Section 10.2  is similar to Bentley's algorithm.\nThe MST of a graph can be also computed by using either Kruskal's [ Kru56 ] or Sollin's [ Sol77 ]\nsequential algorithms. The complexity of Sollin's algorithm (Problem 10.21) is Q(n2 log n).\nSavage and Jaja [ SJ81] have developed a formulation of Sollin's algorithm for the CREW PRAM.\nTheir algorithm uses n2 processes and solves the problem in time Q(log2 n). Chin, Lam, and\nChen [ CLC82 ] have developed a formulation of Sollin's algorithm for a CREW PRAM that uses\n processes and finds the MST in time Q(log2 n). Awerbuch and Shiloach [ AS87 ]\npresent a formulation of Sollin's algorithm for the shuffle-exchange network that uses Q(n2)\nprocesses and runs in time Q(log2 n). Doshi and Varman [ DV87 ] present a Q(n2/p) time\nalgorithm for a p-process ring-connected computer for Sollin's algorithm. Leighton [ Lei83 ] and\nNath, Maheshwari, and Bhatt [ NMB83 ] present parallel formulations of Sollin's algorithm for a\nmesh of trees network. The first algorithm runs in Q(log2 n) and the second algorithm runs in\nQ(log4 n) for an n x n mesh of trees. Huang [ Hua85 ] describes a formulation of Sollin's\nalgorithm that runs in Q(n2/p) on a \n  mesh of trees.\nThe single-source shortest paths algorithm in Section 10.3  was discovered by Dijkstra [ Dij59 ].\nDue to the similarity between Dijkstra's algorithm and Prim's MST algorithm, all the parallel\nformulations of Prim's algorithm discussed in the previous paragraph can also be applied to the\nsingle-source shortest paths problem. Bellman [ Bel58] and Ford [ FR62] independently\ndeveloped a single-source shortest paths algorithm that operates on graphs with negative\nweights but without negative-weight cycles. The Bellman-Ford single-source algorithm has a\nsequential complexity of O (|V||E|). Paige and Kruskal [ PK89] present parallel formulations of\nboth the Dijkstra and Bellman-Ford single-source shortest paths algorithm. Their formulation of\nDijkstra's algorithm runs on an EREW PRAM of Q(n) processes and runs in time Q(n log n). Their\nformulation of Bellman-Ford's algorithm runs in time Q(n|E|/p + n log p) on a p-process EREW\nPRAM where p \n |E|. They also present algorithms for the CRCW PRAM [ PK89].\nSignificant work has been done on the all-pairs shortest paths problem. The source-partitioning\nformulation of Dijkstra's all-pairs shortest paths is discussed by Jenq and Sahni [ JS87]", "doc_id": "48a6eb14-09a8-4c76-a824-fa0771c18750", "embedding": null, "doc_hash": "babbffb1de8c2b3e205650a7dd23a28a268657756d61bd6ef7011794dfed4877", "extra_info": null, "node_info": {"start": 1270301, "end": 1273457}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b37c10b6-67b5-42b7-899b-44d1e2b5cca0", "3": "71b9257c-0170-4539-9f1f-e7097a703a83"}}, "__type__": "1"}, "71b9257c-0170-4539-9f1f-e7097a703a83": {"__data__": {"text": "formulation of\nDijkstra's algorithm runs on an EREW PRAM of Q(n) processes and runs in time Q(n log n). Their\nformulation of Bellman-Ford's algorithm runs in time Q(n|E|/p + n log p) on a p-process EREW\nPRAM where p \n |E|. They also present algorithms for the CRCW PRAM [ PK89].\nSignificant work has been done on the all-pairs shortest paths problem. The source-partitioning\nformulation of Dijkstra's all-pairs shortest paths is discussed by Jenq and Sahni [ JS87] and\nKumar and Singh [ KS91b ]. The source parallel formulation of Dijkstra's all-pairs shortest paths\nalgorithm is discussed by Paige and Kruskal [ PK89] and Kumar and Singh [ KS91b ]. The Floyd's\nall-pairs shortest paths algorithm discussed in Section 10.4.2  is due to Floyd [ Flo62 ]. The 1-D\nand 2-D block mappings (Problem 10.8) are presented by Jenq and Sahni [ JS87], and the\npipelined version of Floyd's algorithm is presented by Bertsekas and Tsitsiklis [ BT89] and\nKumar and Singh [ KS91b ]. Kumar and Singh [ KS91b ] present isoefficiency analysis and\nperformance comparison of different parallel formulations for the all-pairs shortest paths on\nhypercube- and mesh-connected computers. The discussion in Section 10.4.3  is based upon the\nwork of Kumar and Singh [ KS91b ] and of Jenq and Sahni [ JS87]. In particular, Algorithm 10.4\nis adopted from the paper by Jenq and Sahni [ JS87]. Levitt and Kautz [ LK72] present a\nformulation of Floyd's algorithm for two-dimensional cellular arrays that uses n2 processes and\nruns in time Q(n). Deo, Pank, and Lord have developed a parallel formulation of Floyd's\nalgorithm for the CREW PRAM model that has complexity Q(n) on n2 processes. Chandy and\nMisra [ CM82 ] present a distributed all-pairs shortest-path algorithm based on diffusing\ncomputation.\nThe connected-components algorithm discussed in Section 10.6  was discovered by Woo and\nSahni [ WS89 ]. Cormen, Leiserson, and Rivest [ CLR90 ] discusses ways to efficiently implement\ndisjoint-set data structures with ranking and path compression. Several algorithms exist for\ncomputing the connected components; many of them are based on the technique of vertex\ncollapsing, similar to Sollin's algorithm for the minimum spanning tree. Most of the parallel\nformulations of Sollin's algorithm can also find the connected components. Hirschberg [ Hir76]\nand Hirschberg, Chandra, and Sarwate [ HCS79 ] developed formulations of the connected-\ncomponents algorithm based on vertex collapsing. The former has a complexity of Q(log2 n) on\na CREW PRAM with n2 processes, and the latter has similar complexity and uses \nprocesses. Chin, Lam, and Chen [ CLC81 ] made the vertex collapse algorithm more efficient by\nreducing the number of processes to \n  for a CREW PRAM, while keeping the run time\nat Q(log2 n). Nassimi and Sahni [ NS80 ] used the vertex collapsing technique to develop a\nformulation for a mesh-connected computer that finds the connected components in time Q(n)\nby using n2 processes.\nThe single-source shortest paths algorithm for sparse graphs, discussed in Section 10.7.2 , was\ndiscovered by Johnson [ Joh77 ]. Paige and Kruskal [ PK89] discuss the possibility of maintaining\nthe queue Q in parallel. Rao and Kumar [ RK88a ] presented techniques to perform", "doc_id": "71b9257c-0170-4539-9f1f-e7097a703a83", "embedding": null, "doc_hash": "7b6968b52e19e2aeea432c3725490e889c992f19cfbf13242ce5706176600491", "extra_info": null, "node_info": {"start": 1273515, "end": 1276755}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "48a6eb14-09a8-4c76-a824-fa0771c18750", "3": "54743af6-3252-472f-bb4a-dafd97c382dc"}}, "__type__": "1"}, "54743af6-3252-472f-bb4a-dafd97c382dc": {"__data__": {"text": "while keeping the run time\nat Q(log2 n). Nassimi and Sahni [ NS80 ] used the vertex collapsing technique to develop a\nformulation for a mesh-connected computer that finds the connected components in time Q(n)\nby using n2 processes.\nThe single-source shortest paths algorithm for sparse graphs, discussed in Section 10.7.2 , was\ndiscovered by Johnson [ Joh77 ]. Paige and Kruskal [ PK89] discuss the possibility of maintaining\nthe queue Q in parallel. Rao and Kumar [ RK88a ] presented techniques to perform concurrent\ninsertions and deletions in a priority queue. The 2-D block mapping, 2-D block-cyclic mapping,\nand 1-D block mapping formulation of Johnson's algorithm ( Section 10.7.2) are due to Wada\nand Ichiyoshi [ WI89 ]. They also presented theoretical and experimental evaluation of these\nschemes on a mesh-connected parallel computer.\nThe serial maximal independent set algorithm described in Section 10.7.1  was developed by\nLuby [ Lub86 ] and its parallel formulation on shared-address-space architectures was motivated\nby the algorithm described by Karypis and Kumar [ KK99 ]. Jones and Plassman [ JP93] have\ndeveloped an asynchronous variation of Luby's algorithm that is particularly suited for\ndistributed memory parallel computers. In their algorithm, each vertex is assigned a single\nrandom number, and after a communication step, each vertex determines the number of its\nadjacent vertices that have smaller and greater random numbers. At this point each vertex gets\ninto a loop waiting to receive the color values of its adjacent vertices that have smaller random\nnumbers. Once all these colors have been received, the vertex selects a consistent color, and\nsends it to all of its adjacent vertices with greater random numbers. The algorithm terminates\nwhen all vertices have been colored. Note that besides the initial communication step to\ndetermine the number of smaller and greater adjacent vertices, this algorithm proceeds\nasynchronously.\nOther parallel graph algorithms have been proposed. Shiloach and Vishkin [ SV82] presented an\nalgorithm for finding the maximum flow in a directed flow network with n vertices that runs in\ntime O (n2 log n) on an n-process EREW PRAM. Goldberg and Tarjan [ GT88 ] presented a\ndifferent maximum-flow algorithm that runs in time O (n2 log n) on an n-process EREW PRAM\nbut requires less space. Atallah and Kosaraju [ AK84 ] proposed a number of algorithms for a\nmesh-connected parallel computer. The algorithms they considered are for finding the bridges\nand articulation points of an undirected graph, finding the length of the shortest cycle, finding\nan MST, finding the cyclic index, and testing if a graph is bipartite. Tarjan and Vishkin [ TV85]\npresented algorithms for computing the biconnected components of a graph. Their CRCW PRAM\nformulation runs in time Q(log n) by using Q(|E| + |V|) processes, and their CREW PRAM\nformulation runs in time Q(log2 n) by using Q(n2/log2 n) processes.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n10.1 In the parallel formulation of Prim's minimum spanning tree algorithm ( Section\n10.2), the maximum number of processes that can be used efficiently on a hypercube is\nQ(n/log n). By using Q(n/log n) processes the run time is Q(n log n). What is the run time\nif you use Q(n) processes? What is the minimum parallel run time that can be obtained on\na message-passing parallel computer? How does this time compare with the run time\nobtained when you use Q(n/log", "doc_id": "54743af6-3252-472f-bb4a-dafd97c382dc", "embedding": null, "doc_hash": "b90ef179872471bbff6ad2096491763c586d308a807ce40b885a61a817e26dff", "extra_info": null, "node_info": {"start": 1276719, "end": 1280178}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "71b9257c-0170-4539-9f1f-e7097a703a83", "3": "5b0a7cf3-0fc2-43fd-bb9f-4d1c949783b2"}}, "__type__": "1"}, "5b0a7cf3-0fc2-43fd-bb9f-4d1c949783b2": {"__data__": {"text": "LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n10.1 In the parallel formulation of Prim's minimum spanning tree algorithm ( Section\n10.2), the maximum number of processes that can be used efficiently on a hypercube is\nQ(n/log n). By using Q(n/log n) processes the run time is Q(n log n). What is the run time\nif you use Q(n) processes? What is the minimum parallel run time that can be obtained on\na message-passing parallel computer? How does this time compare with the run time\nobtained when you use Q(n/log n) processes?\n10.2 Show how Dijkstra's single-source algorithm and its parallel formulation ( Section\n10.3) need to be modified in order to output the shortest paths instead of the cost.\nAnalyze the run time of your sequential and parallel formulations.\n10.3 Given a graph G = (V, E), the breadth-first ranking of vertices of G are the values\nassigned to the vertices of V in a breadth-first traversal of G from a node v. Show how the\nbreadth-first ranking of vertices of G can be performed on a p-process mesh.\n10.4 Dijkstra's single-source shortest paths algorithm ( Section 10.3 ) requires non-\nnegative edge weights. Show how Dijkstra's algorithm can be modified to work on graphs\nwith negative weights but no negative cycles in time Q(|E||V|). Analyze the performance\nof the parallel formulation of the modified algorithm on a p-process message-passing\narchitecture.\n10.5 Compute the total amount of memory required by the different parallel formulations\nof the all-pairs shortest paths problem described in Section 10.4 .\n10.6 Show that Floyd's algorithm in Section 10.4.2  is correct if we replace line 7 of\nAlgorithm 10.3  by the following line:\n10.7 Compute the parallel run time, speedup, and efficiency of Floyd's all-pairs shortest\npaths algorithm using 2-D block mapping on a p-process mesh with store-and-forward\nrouting and a p-process hypercube and a p-process mesh with cut-through routing.\n10.8 An alternative way of partitioning the matrix D(k) in Floyd's all-pairs shortest paths\nalgorithm is to use the 1-D block mapping ( Section 3.4.1 ). Each of the p processes is\nassigned n/p consecutive columns of the D(k) matrix.\nCompute the parallel run time, speedup, and efficiency of 1-D block mapping on a\nhypercube-connected parallel computer. What are the advantages and disadvantages\nof this partitioning over the 2-D block mapping presented in Section 10.4.2?a.\nCompute the parallel run time, speedup, and efficiency of 1-D block mapping on a p-\nprocess mesh with store-and-forward routing, a p-process mesh with cut-through\nrouting, and a p-process ring.b.\n10.9 Describe and analyze the performance of a parallel formulation of Floyd's algorithm\nthat uses 1-D block mapping and the pipelining technique described in Section 10.4.2 .\n10.10  Compute the exact parallel run time, speedup, and efficiency of Floyd's pipelined\nformulation ( Section 10.4.2 ).\n10.11  Compute the parallel run time, the speedup, and the efficiency of the parallel\nformulation of the connected-component algorithm presented in Section 10.6  for a p-\nprocess mesh with store-and-forward routing and with cut-through routing. Comment on\nthe difference in the performance of the two architectures.\n10.12  The parallel formulation for the connected-component problem presented in Section\n10.6 uses 1-D block mapping to partition the matrix among processes. Consider an\nalternative parallel formulation in which", "doc_id": "5b0a7cf3-0fc2-43fd-bb9f-4d1c949783b2", "embedding": null, "doc_hash": "52b9c1fe8d7c9e8949e85f6fb03026edee8a42c7f2480b8820ea63eab7d7053a", "extra_info": null, "node_info": {"start": 1280195, "end": 1283595}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "54743af6-3252-472f-bb4a-dafd97c382dc", "3": "d882f4dd-9f35-45a5-9185-af523271fa08"}}, "__type__": "1"}, "d882f4dd-9f35-45a5-9185-af523271fa08": {"__data__": {"text": "pipelined\nformulation ( Section 10.4.2 ).\n10.11  Compute the parallel run time, the speedup, and the efficiency of the parallel\nformulation of the connected-component algorithm presented in Section 10.6  for a p-\nprocess mesh with store-and-forward routing and with cut-through routing. Comment on\nthe difference in the performance of the two architectures.\n10.12  The parallel formulation for the connected-component problem presented in Section\n10.6 uses 1-D block mapping to partition the matrix among processes. Consider an\nalternative parallel formulation in which 2-D block mapping is used instead. Describe this\nformulation and analyze its performance and scalability on a hypercube, a mesh with SF-\nrouting, and a mesh with CT-routing. How does this scheme compare with 1-D block\nmapping?\n10.13  Consider the problem of parallelizing Johnson's single-source shortest paths\nalgorithm for sparse graphs ( Section 10.7.2). One way of parallelizing it is to use p1\nprocesses to maintain the priority queue and p2 processes to perform the computations of\nthe new l values. How many processes can be efficiently used to maintain the priority\nqueue (in other words, what is the maximum value for p1)? How many processes can be\nused to update the l values? Is the parallel formulation that is obtained by using the p1 +\np2 processes cost-optimal? Describe an algorithm that uses p1 processes to maintain the\npriority queue.\n10.14  Consider Dijkstra's single-source shortest paths algorithm for sparse graphs\n(Section 10.7). We can parallelize this algorithm on a p-process hypercube by splitting the\nn adjacency lists among the processes horizontally; that is, each process gets n/p lists.\nWhat is the parallel run time of this formulation? Alternatively, we can partition the\nadjacency list vertically among the processes; that is, each process gets a fraction of each\nadjacency list. If an adjacency list contains m elements, then each process contains a\nsublist of m/p elements. The last element in each sublist has a pointer to the element in\nthe next process. What is the parallel run time and speedup of this formulation? What is\nthe maximum number of processes that it can use?\n10.15  Repeat Problem 10.14 for Floyd's all-pairs shortest paths algorithm.\n10.16  Analyze the performance of Luby's shared-address-space algorithm for finding a\nmaximal independent set of vertices on sparse graphs described in Section 10.7.1. What is\nthe parallel run time and speedup of this formulation?\n10.17  Compute the parallel run time, speedup, and efficiency of the 2-D cyclic mapping of\nthe sparse graph single-source shortest paths algorithm ( Section 10.7.2) for a mesh-\nconnected computer. You may ignore the overhead due to extra work, but you should take\ninto account the overhead due to communication.\n10.18  Analyze the performance of the single-source shortest paths algorithm for sparse\ngraphs ( Section 10.7.2 ) when the 2-D block-cyclic mapping is used ( Section 3.4.1 ).\nCompare it with the performance of the 2-D cyclic mapping computed in Problem 10.17.\nAs in Problem 10.17, ignore extra computation but include communication overhead.\n10.19  Consider the 1-D block-cyclic mapping described in Section 3.4.1 . Describe how you\nwill apply this mapping to the single-source shortest paths problem for sparse graphs.\nCompute the parallel run time, speedup, and efficiency of this mapping. In your analysis,\ninclude the communication overhead but not the overhead due to extra work.\n10.20  Of the mapping schemes presented in Section 10.7.2 and in Problems 10.18 and\n10.19, which one", "doc_id": "d882f4dd-9f35-45a5-9185-af523271fa08", "embedding": null, "doc_hash": "b9ca951fcea342541bfbfdcbda2d1dfd1d052abfcbf596175819226bfc302d08", "extra_info": null, "node_info": {"start": 1283515, "end": 1287104}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5b0a7cf3-0fc2-43fd-bb9f-4d1c949783b2", "3": "ff2feb26-984d-4077-9374-e836773b7a3c"}}, "__type__": "1"}, "ff2feb26-984d-4077-9374-e836773b7a3c": {"__data__": {"text": "Problem 10.17.\nAs in Problem 10.17, ignore extra computation but include communication overhead.\n10.19  Consider the 1-D block-cyclic mapping described in Section 3.4.1 . Describe how you\nwill apply this mapping to the single-source shortest paths problem for sparse graphs.\nCompute the parallel run time, speedup, and efficiency of this mapping. In your analysis,\ninclude the communication overhead but not the overhead due to extra work.\n10.20  Of the mapping schemes presented in Section 10.7.2 and in Problems 10.18 and\n10.19, which one has the smallest overhead due to extra computation?\n10.21  Sollin's algorithm ( Section 10.8 ) starts with a forest of n isolated vertices. In each\niteration, the algorithm simultaneously determines, for each tree in the forest, the smallest\nedge joining any vertex in that tree to a vertex in another tree. All such edges are added\nto the forest. Furthermore, two trees are never joined by more than one edge. This process\ncontinues until there is only one tree in the forest \u2013 the minimum spanning tree. Since the\nnumber of trees is reduced by a factor of at least two in each iteration, this algorithm\nrequires at most log n iterations to find the MST. Each iteration requires at most O (n2)\ncomparisons to find the smallest edge incident on each vertex; thus, its sequential\ncomplexity is Q(n2 log n). Develop a parallel formulation of Sollin's algorithm on an n-\nprocess hypercube-connected parallel computer. What is the run time of your formulation?\nIs it cost optimal?\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 11. Search Algorithms for\nDiscrete Optimization Problems\nSearch algorithms can be used to solve discrete optimization problems (DOPs), a class of\ncomputationally expensive problems with significant theoretical and practical interest. Search\nalgorithms solve DOPs by evaluating candidate solutions from a finite or countably infinite set of\npossible solutions to find one that satisfies a problem-specific criterion. DOPs are also referred\nto as combinatorial problems.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n11.1 Definitions and Examples\nA discrete optimization problem  can be expressed as a tuple ( S, f). The set S is a finite or\ncountably infinite set of all solutions that satisfy specified constraints. This set is called the set\nof feasible solutions . The function f is the cost function that maps each element in set S onto\nthe set of real numbers R.\nThe objective of a DOP is to find a feasible solution xopt, such that f (xopt) \n f (x) for all x \n S.\nProblems from various domains can be formulated as DOPs. Some examples are planning and\nscheduling, the optimal layout of VLSI chips, robot motion planning, test-pattern generation for\ndigital circuits, and logistics and control.\nExample 11.1 The 0/1 integer-linear-programming problem\nIn the 0/1 integer-linear-programming problem, we are given an m x n matrix A, an\nm x 1 vector b, and an n x 1 vector c. The objective is to determine an n x 1 vector x\nwhose elements can take on only the value 0 or 1. The vector must satisfy the\nconstraint\nand the function\nmust be minimized. For this problem, the set S is the set of all values of the vector x\nthat satisfy the equation Ax\n \n b. \nExample 11.2 The 8-puzzle problem\nThe 8-puzzle problem consists of a 3 x 3 grid containing eight tiles, numbered one\nthrough eight. One of the grid segments (called the \"blank\") is empty. A tile can be\nmoved into the blank position from a position adjacent to it, thus creating a blank", "doc_id": "ff2feb26-984d-4077-9374-e836773b7a3c", "embedding": null, "doc_hash": "9e97683b14f720ec549a882fd6844b00d3866e4f3c2d1982f27936e87e434aea", "extra_info": null, "node_info": {"start": 1287136, "end": 1290621}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d882f4dd-9f35-45a5-9185-af523271fa08", "3": "1f941c83-e9fa-4225-9f8d-a06cc6b82dce"}}, "__type__": "1"}, "1f941c83-e9fa-4225-9f8d-a06cc6b82dce": {"__data__": {"text": "an n x 1 vector x\nwhose elements can take on only the value 0 or 1. The vector must satisfy the\nconstraint\nand the function\nmust be minimized. For this problem, the set S is the set of all values of the vector x\nthat satisfy the equation Ax\n \n b. \nExample 11.2 The 8-puzzle problem\nThe 8-puzzle problem consists of a 3 x 3 grid containing eight tiles, numbered one\nthrough eight. One of the grid segments (called the \"blank\") is empty. A tile can be\nmoved into the blank position from a position adjacent to it, thus creating a blank in\nthe tile's original position. Depending on the configuration of the grid, up to four\nmoves are possible: up, down, left, and right. The initial and final configurations of the\ntiles are specified. The objective is to determine a shortest sequence of moves that\ntransforms the initial configuration to the final configuration. Figure 11.1  illustrates\nsample initial and final configurations and a sequence of moves leading from the initial\nconfiguration to the final configuration.\nFigure 11.1. An 8-puzzle problem instance: (a) initial\nconfiguration; (b) final configuration; and (c) a sequence of\nmoves leading from the initial to the final configuration.\nThe set S for this problem is the set of all sequences of moves that lead from the initial\nto the final configurations. The cost function f of an element in S is defined as the\nnumber of moves in the sequence. \nIn most problems of practical interest, the solution set S is quite large. Consequently, it is not\nfeasible to exhaustively enumerate the elements in S to determine the optimal element xopt.\nInstead, a DOP can be reformulated as the problem of finding a minimum-cost path in a graph\nfrom a designated initial node to one of several possible goal nodes. Each element x in S can be\nviewed as a path from the initial node to one of the goal nodes. There is a cost associated with\neach edge of the graph, and a cost function f is defined in terms of these edge costs. For many\nproblems, the cost of a path is the sum of the edge costs. Such a graph is called a state space ,\nand the nodes of the graph are called states . A terminal node  is one that has no successors.\nAll other nodes are called nonterminal nodes . The 8-puzzle problem can be naturally\nformulated as a graph search problem. In particular, the initial configuration is the initial node,\nand the final configuration is the goal node. Example 11.3 illustrates the process of\nreformulating the 0/1 integer-linear-programming problem as a graph search problem.\nExample 11.3 The 0/1 integer-linear-programming problem\nrevisited\nConsider an instance of the 0/1 integer-linear-programming problem defined in\nExample 11.1 . Let the values of A, b, and c be given by\nThe constraints corresponding to A, b, and c are as follows:\nand the function f (x) to be minimized is\nEach of the four elements of vector x can take the value 0 or 1. There are 24 = 16\npossible values for x. However, many of these values do not satisfy the problem's\nconstraints.\nThe problem can be reformulated as a graph-search problem. The initial node\nrepresents the state in which none of the elements of vector x have been assigned\nvalues. In this example, we assign values to vector elements in subscript order; that\nis, first x1, then x2, and so on. The initial node generates two nodes corresponding to\nx1 = 0 and x1 = 1. After a variable xi has been assigned a value, it is called a fixed\nvariable . All variables that are not fixed are called free variables .\nAfter instantiating a variable to 0 or 1, it is possible to check whether an instantiation\nof the remaining free variables can lead to a feasible solution. We do this by using the\nfollowing condition:\nEquation", "doc_id": "1f941c83-e9fa-4225-9f8d-a06cc6b82dce", "embedding": null, "doc_hash": "98471443c46b8ee8b251e363d6cd66250b72b28d81a645d276bc0400798eb21d", "extra_info": null, "node_info": {"start": 1290646, "end": 1294355}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ff2feb26-984d-4077-9374-e836773b7a3c", "3": "c1c73548-4c98-4d36-b9ac-7b6ae716b7bb"}}, "__type__": "1"}, "c1c73548-4c98-4d36-b9ac-7b6ae716b7bb": {"__data__": {"text": "have been assigned\nvalues. In this example, we assign values to vector elements in subscript order; that\nis, first x1, then x2, and so on. The initial node generates two nodes corresponding to\nx1 = 0 and x1 = 1. After a variable xi has been assigned a value, it is called a fixed\nvariable . All variables that are not fixed are called free variables .\nAfter instantiating a variable to 0 or 1, it is possible to check whether an instantiation\nof the remaining free variables can lead to a feasible solution. We do this by using the\nfollowing condition:\nEquation 11.1\nThe left side of Equation 11.1  is the maximum value of \n  that can be\nobtained by instantiating the free variables to either 0 or 1. If this value is greater\nthan or equal to bi , for i = 1, 2,..., m, then the node may lead to a feasible solution.\nFor each of the nodes corresponding to x1 = 0 and x1 = 1, the next variable ( x2) is\nselected and assigned a value. The nodes are then checked for feasibility. This process\ncontinues until all the variables have been assigned and the feasible set has been\ngenerated. Figure 11.2  illustrates this process.\nFigure 11.2. The graph corresponding to the 0/1 integer-linear-\nprogramming problem.\nFunction f (x) is evaluated for each of the feasible solutions; the solution with the\nminimum value is the desired solution. Note that it is unnecessary to generate the\nentire feasible set to determine the solution. Several search algorithms can determine\nan optimal solution by searching only a portion of the graph. \nFor some problems, it is possible to estimate the cost to reach the goal state from an\nintermediate state. This cost is called a heuristic estimate . Let h (x) denote the heuristic\nestimate of reaching the goal state from state x and g(x) denote the cost of reaching state x\nfrom initial state s along the current path. The function h is called a heuristic function . If h(x)\nis a lower bound on the cost of reaching the goal state from state x for all x, then h is called\nadmissible. We define function l(x) as the sum h(x) + g(x). If h is admissible, then l(x) is a\nlower bound on the cost of the path to a goal state that can be obtained by extending the\ncurrent path between s and x. In subsequent examples we will see how an admissible heuristic\ncan be used to determine the least-cost sequence of moves from the initial state to a goal state.\nExample 11.4 An admissible heuristic function for the 8-puzzle\nAssume that each position in the 8-puzzle grid is represented as a pair. The pair (1, 1)\nrepresents the top-left grid position and the pair (3, 3) represents the bottom-right\nposition. The distance between positions ( i, j) and ( k, l) is defined as | i - k| + | j - l|.\nThis distance is called the Manhattan distance . The sum of the Manhattan distances\nbetween the initial and final positions of all tiles is an estimate of the number of\nmoves required to transform the current configuration into the final configuration. This\nestimate is called the Manhattan heuristic . Note that if h(x) is the Manhattan\ndistance between configuration x and the final configuration, then h(x) is also a lower\nbound on the number of moves from configuration x to the final configuration. Hence\nthe Manhattan heuristic is admissible. \nOnce a DOP has been formulated as a graph search problem, it can be solved by algorithms\nsuch as branch-and-bound search and heuristic search. These techniques use heuristics and the\nstructure of the search space to solve DOPs without searching the set S exhaustively.\nDOPs belong to the class of NP-hard problems. One may argue that it is pointless to", "doc_id": "c1c73548-4c98-4d36-b9ac-7b6ae716b7bb", "embedding": null, "doc_hash": "e4c244a6f4e8fc0807e513376fa735f1054054c5bcc9ede36478bfcc80a50f4b", "extra_info": null, "node_info": {"start": 1294328, "end": 1297939}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1f941c83-e9fa-4225-9f8d-a06cc6b82dce", "3": "9ee64aa1-85cb-4170-85ae-2c7ea5ac75bd"}}, "__type__": "1"}, "9ee64aa1-85cb-4170-85ae-2c7ea5ac75bd": {"__data__": {"text": "Manhattan\ndistance between configuration x and the final configuration, then h(x) is also a lower\nbound on the number of moves from configuration x to the final configuration. Hence\nthe Manhattan heuristic is admissible. \nOnce a DOP has been formulated as a graph search problem, it can be solved by algorithms\nsuch as branch-and-bound search and heuristic search. These techniques use heuristics and the\nstructure of the search space to solve DOPs without searching the set S exhaustively.\nDOPs belong to the class of NP-hard problems. One may argue that it is pointless to apply\nparallel processing to these problems, since we can never reduce their worst-case run time to a\npolynomial without using exponentially many processors. However, the average-time\ncomplexity of heuristic search algorithms for many problems is polynomial. Furthermore, there\nare heuristic search algorithms that find suboptimal solutions for specific problems in\npolynomial time. In such cases, bigger problem instances can be solved using parallel\ncomputers. Many DOPs (such as robot motion planning, speech understanding, and task\nscheduling) require real-time solutions. For these applications, parallel processing may be the\nonly way to obtain acceptable performance. Other problems, for which optimal solutions are\nhighly desirable, can be solved for moderate-sized instances in a reasonable amount of time by\nusing parallel search techniques (for example, VLSI floor-plan optimization, and computer-\naided design).[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n11.2 Sequential Search Algorithms\nThe most suitable sequential search algorithm to apply to a state space depends on whether the\nspace forms a graph or a tree. In a tree, each new successor leads to an unexplored part of the\nsearch space. An example of this is the 0/1 integer-programming problem. In a graph,\nhowever, a state can be reached along multiple paths. An example of such a problem is the 8-\npuzzle. For such problems, whenever a state is generated, it is necessary to check if the state\nhas already been generated. If this check is not performed, then effectively the search graph is\nunfolded into a tree in which a state is repeated for every path that leads to it ( Figure 11.3).\nFigure 11.3. Two examples of unfolding a graph into a tree.\nFor many problems (for example, the 8-puzzle), unfolding increases the size of the search space\nby a small factor. For some problems, however, unfolded graphs are much larger than the\noriginal graphs. Figure 11.3(b) illustrates a graph whose corresponding tree has an\nexponentially higher number of states. In this section, we present an overview of various\nsequential algorithms used to solve DOPs that are formulated as tree or graph search problems.\n11.2.1 Depth-First Search Algorithms\nDepth-first search  (DFS) algorithms solve DOPs that can be formulated as tree-search\nproblems. DFS begins by expanding the initial node and generating its successors. In each\nsubsequent step, DFS expands one of the most recently generated nodes. If this node has no\nsuccessors (or cannot lead to any solutions), then DFS backtracks and expands a different node.\nIn some DFS algorithms, successors of a node are expanded in an order determined by their\nheuristic values. A major advantage of DFS is that its storage requirement is linear in the depth\nof the state space being searched. The following sections discuss three algorithms based on\ndepth-first search.\nSimple Backtracking\nSimple backtracking  is a depth-first search method that terminates upon finding the first\nsolution. Thus, it is not guaranteed to find a minimum-cost solution. Simple backtracking uses\nno heuristic information to order the successors of an expanded node. A variant, ordered\nbacktracking , does use heuristics", "doc_id": "9ee64aa1-85cb-4170-85ae-2c7ea5ac75bd", "embedding": null, "doc_hash": "34ebb86e95d349bb7dd28b4b2438e1783379212ea9d991a38aa2e81f81f0ba32", "extra_info": null, "node_info": {"start": 1297920, "end": 1301687}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c1c73548-4c98-4d36-b9ac-7b6ae716b7bb", "3": "5eb06312-6bd7-4bce-aeff-5caf7e23669c"}}, "__type__": "1"}, "5eb06312-6bd7-4bce-aeff-5caf7e23669c": {"__data__": {"text": "DFS algorithms, successors of a node are expanded in an order determined by their\nheuristic values. A major advantage of DFS is that its storage requirement is linear in the depth\nof the state space being searched. The following sections discuss three algorithms based on\ndepth-first search.\nSimple Backtracking\nSimple backtracking  is a depth-first search method that terminates upon finding the first\nsolution. Thus, it is not guaranteed to find a minimum-cost solution. Simple backtracking uses\nno heuristic information to order the successors of an expanded node. A variant, ordered\nbacktracking , does use heuristics to order the successors of an expanded node.\nDepth-First Branch-and-Bound\nDepth-first branch-and-bound  (DFBB) exhaustively searches the state space; that is, it\ncontinues to search even after finding a solution path. Whenever it finds a new solution path, it\nupdates the current best solution path. DFBB discards inferior partial solution paths (that is,\npartial solution paths whose extensions are guaranteed to be worse than the current best\nsolution path). Upon termination, the current best solution is a globally optimal solution.\nIterative Deepening A*\nTrees corresponding to DOPs can be very deep. Thus, a DFS algorithm may get stuck searching\na deep part of the search space when a solution exists higher up on another branch. For such\ntrees, we impose a bound on the depth to which the DFS algorithm searches. If the node to be\nexpanded is beyond the depth bound, then the node is not expanded and the algorithm\nbacktracks. If a solution is not found, then the entire state space is searched again using a\nlarger depth bound. This technique is called iterative deepening depth-first search  (ID-\nDFS). Note that this method is guaranteed to find a solution path with the fewest edges.\nHowever, it is not guaranteed to find a least-cost path.\nIterative deepening A*  (IDA*) is a variant of ID-DFS. IDA* uses the l-values of nodes to\nbound depth (recall from Section 11.1 that for node x, l(x) = g(x) + h(x)). IDA* repeatedly\nperforms cost-bounded DFS over the search space. In each iteration, IDA* expands nodes\ndepth-first. If the l-value of the node to be expanded is greater than the cost bound, then IDA*\nbacktracks. If a solution is not found within the current cost bound, then IDA* repeats the entire\ndepth-first search using a higher cost bound. In the first iteration, the cost bound is set to the l-\nvalue of the initial state s. Note that since g(s) is zero, l(s) is equal to h(s). In each subsequent\niteration, the cost bound is increased. The new cost bound is equal to the minimum l-value of\nthe nodes that were generated but could not be expanded in the previous iteration. The\nalgorithm terminates when a goal node is expanded. IDA* is guaranteed to find an optimal\nsolution if the heuristic function is admissible. It may appear that IDA* performs a lot of\nredundant work across iterations. However, for many problems the redundant work performed\nby IDA* is minimal, because most of the work is done deep in the search space.\nExample 11.5 Depth-first search: the 8-puzzle\nFigure 11.4 shows the execution of depth-first search for solving the 8-puzzle\nproblem. The search starts at the initial configuration. Successors of this state are\ngenerated by applying possible moves. During each step of the search algorithm a\nnew state is selected, and its successors are generated. The DFS algorithm expands\nthe deepest node in the tree. In step 1, the initial state A generates states B and C.\nOne of these is selected according to a predetermined criterion. In the example, we\norder", "doc_id": "5eb06312-6bd7-4bce-aeff-5caf7e23669c", "embedding": null, "doc_hash": "f7731b7a8c4549d7527333006b15e17404e8ff9704606036d497004c66f5e473", "extra_info": null, "node_info": {"start": 1301641, "end": 1305266}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9ee64aa1-85cb-4170-85ae-2c7ea5ac75bd", "3": "0dc73dbc-03dd-456f-b5e5-e1870d927417"}}, "__type__": "1"}, "0dc73dbc-03dd-456f-b5e5-e1870d927417": {"__data__": {"text": "the work is done deep in the search space.\nExample 11.5 Depth-first search: the 8-puzzle\nFigure 11.4 shows the execution of depth-first search for solving the 8-puzzle\nproblem. The search starts at the initial configuration. Successors of this state are\ngenerated by applying possible moves. During each step of the search algorithm a\nnew state is selected, and its successors are generated. The DFS algorithm expands\nthe deepest node in the tree. In step 1, the initial state A generates states B and C.\nOne of these is selected according to a predetermined criterion. In the example, we\norder successors by applicable moves as follows: up, down, left, and right. In step 2,\nthe DFS algorithm selects state B and generates states D, E, and F. Note that the state\nD can be discarded, as it is a duplicate of the parent of B. In step 3, state E is\nexpanded to generate states G and H. Again G can be discarded because it is a\nduplicate of B. The search proceeds in this way until the algorithm backtracks or the\nfinal configuration is generated. \nFigure 11.4. States resulting from the first three steps of\ndepth-first search applied to an instance of the 8-puzzle.\nIn each step of the DFS algorithm, untried alternatives must be stored. For example, in the 8-\npuzzle problem, up to three untried alternatives are stored at each step. In general, if m is the\namount of storage required to store a state, and d is the maximum depth, then the total space\nrequirement of the DFS algorithm is O (md). The state-space tree searched by parallel DFS can\nbe efficiently represented as a stack. Since the depth of the stack increases linearly with the\ndepth of the tree, the memory requirements of a stack representation are low.\nThere are two ways of storing untried alternatives using a stack. In the first representation,\nuntried alternates are pushed on the stack at each step. The ancestors of a state are not\nrepresented on the stack. Figure 11.5(b) illustrates this representation for the tree shown in\nFigure 11.5(a) . In the second representation, shown in Figure 11.5(c) , untried alternatives are\nstored along with their parent state. It is necessary to use the second representation if the\nsequence of transformations from the initial state to the goal state is required as a part of the\nsolution. Furthermore, if the state space is a graph in which it is possible to generate an\nancestor state by applying a sequence of transformations to the current state, then it is\ndesirable to use the second representation, because it allows us to check for duplication of\nancestor states and thus remove any cycles from the state-space graph. The second\nrepresentation is useful for problems such as the 8-puzzle. In Example 11.5 , using the second\nrepresentation allows the algorithm to detect that nodes D and G should be discarded.\nFigure 11.5. Representing a DFS tree: (a) the DFS tree; successor\nnodes shown with dashed lines have already been explored; (b) the\nstack storing untried alternatives only; and (c) the stack storing\nuntried alternatives along with their parent. The shaded blocks\nrepresent the parent state and the block to the right represents\nsuccessor states that have not been explored.\n11.2.2 Best-First Search Algorithms\nBest-first search (BFS) algorithms can search both graphs and trees. These algorithms use\nheuristics to direct the search to portions of the search space likely to yield solutions. Smaller\nheuristic values are assigned to more promising nodes. BFS maintains two lists: open and\nclosed . At the beginning, the initial node is placed on the open list. This list is sorted according\nto a heuristic evaluation function that measures how likely each node is to yield a solution. In\neach step of the search, the most promising node from the open list is removed.", "doc_id": "0dc73dbc-03dd-456f-b5e5-e1870d927417", "embedding": null, "doc_hash": "c5d1d5395f6a6385adad3a557bc1d7316a6e0fcd9cd2c3ed19cc1116c26a65f3", "extra_info": null, "node_info": {"start": 1305297, "end": 1309091}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5eb06312-6bd7-4bce-aeff-5caf7e23669c", "3": "bbb43f78-1408-4f83-9507-bd2db8d70495"}}, "__type__": "1"}, "bbb43f78-1408-4f83-9507-bd2db8d70495": {"__data__": {"text": "Best-First Search Algorithms\nBest-first search (BFS) algorithms can search both graphs and trees. These algorithms use\nheuristics to direct the search to portions of the search space likely to yield solutions. Smaller\nheuristic values are assigned to more promising nodes. BFS maintains two lists: open and\nclosed . At the beginning, the initial node is placed on the open list. This list is sorted according\nto a heuristic evaluation function that measures how likely each node is to yield a solution. In\neach step of the search, the most promising node from the open list is removed. If this node is a\ngoal node, then the algorithm terminates. Otherwise, the node is expanded. The expanded node\nis placed on the closed  list. The successors of the newly expanded node are placed on the open\nlist under one of the following circumstances: (1) the successor is not already on the open or\nclosed  lists, and (2) the successor is already on the open or closed  list but has a lower heuristic\nvalue. In the second case, the node with the higher heuristic value is deleted.\nA common BFS technique is the A* algorithm . The A* algorithm uses the lower bound function\nl as a heuristic evaluation function. Recall from Section 11.1  that for each node x , l(x) is the\nsum of g(x) and h(x). Nodes in the open list are ordered according to the value of the l function.\nAt each step, the node with the smallest l-value (that is, the best node) is removed from the\nopen list and expanded. Its successors are inserted into the open list at the proper positions and\nthe node itself is inserted into the closed  list. For an admissible heuristic function, A* finds an\noptimal solution.\nThe main drawback of any BFS algorithm is that its memory requirement is linear in the size of\nthe search space explored. For many problems, the size of the search space is exponential in\nthe depth of the tree expanded. For problems with large search spaces, memory becomes a\nlimitation.\nExample 11.6 Best-first search: the 8-puzzle\nConsider the 8-puzzle problem from Examples 11.2 and 11.4. Figure 11.6  illustrates\nfour steps of best-first search on the 8-puzzle. At each step, a state x with the\nminimum l-value ( l(x) = g(x) + h(x)) is selected for expansion. Ties are broken\narbitrarily. BFS can check for a duplicate nodes, since all previously generated nodes\nare kept on either the open or closed  list. \nFigure 11.6. Applying best-first search to the 8-puzzle: (a)\ninitial configuration; (b) final configuration; and (c) states\nresulting from the first four steps of best-first search. Each\nstate is labeled with its h-value (that is, the Manhattan\ndistance from the state to the final state).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n11.3 Search Overhead Factor\nParallel search algorithms incur overhead from several sources. These include communication\noverhead, idle time due to load imbalance, and contention for shared data structures. Thus, if\nboth the sequential and parallel formulations of an algorithm do the same amount of work, the\nspeedup of parallel search on p processors is less than p. However, the amount of work done by\na parallel formulation is often different from that done by the corresponding sequential\nformulation because they may explore different parts of the search space.\nLet W be the amount of work done by a single processor, and Wp be the total amount of work\ndone by p processors. The search overhead factor  of the parallel system is defined as the\nratio of the work done by the parallel formulation to that done by the sequential formulation, or\nWp/W. Thus, the upper bound on speedup for the parallel system is given by p x(W/Wp).", "doc_id": "bbb43f78-1408-4f83-9507-bd2db8d70495", "embedding": null, "doc_hash": "1ba83d9ad023f81fdfabcaa0dc1cba1fdff7edec901e1b3fe63187472dd4031d", "extra_info": null, "node_info": {"start": 1309101, "end": 1312741}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0dc73dbc-03dd-456f-b5e5-e1870d927417", "3": "59ebc0dc-7177-40a9-908e-8fdd1486de7d"}}, "__type__": "1"}, "59ebc0dc-7177-40a9-908e-8fdd1486de7d": {"__data__": {"text": "on p processors is less than p. However, the amount of work done by\na parallel formulation is often different from that done by the corresponding sequential\nformulation because they may explore different parts of the search space.\nLet W be the amount of work done by a single processor, and Wp be the total amount of work\ndone by p processors. The search overhead factor  of the parallel system is defined as the\nratio of the work done by the parallel formulation to that done by the sequential formulation, or\nWp/W. Thus, the upper bound on speedup for the parallel system is given by p x(W/Wp). The\nactual speedup, however, may be less due to other parallel processing overhead. In most\nparallel search algorithms, the search overhead factor is greater than one. However, in some\ncases, it may be less than one, leading to superlinear speedup. If the search overhead factor is\nless than one on the average, then it indicates that the serial search algorithm is not the fastest\nalgorithm for solving the problem.\nTo simplify our presentation and analysis, we assume that the time to expand each node is the\nsame, and W and Wp are the number of nodes expanded by the serial and the parallel\nformulations, respectively. If the time for each expansion is tc, then the sequential run time is\ngiven by TS = tcW. In the remainder of the chapter, we assume that tc = 1. Hence, the problem\nsize W and the serial run time T S become the same.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n11.4 Parallel Depth-First Search\nWe start our discussion of parallel depth-first search by focusing on simple backtracking.\nParallel formulations of depth-first branch-and-bound and IDA* are similar to those discussed in\nthis section and are addressed in Sections 11.4.6 and 11.4.7 .\nThe critical issue in parallel depth-first search algorithms is the distribution of the search space\namong the processors. Consider the tree shown in Figure 11.7. Note that the left subtree\n(rooted at node A) can be searched in parallel with the right subtree (rooted at node B). By\nstatically assigning a node in the tree to a processor, it is possible to expand the whole subtree\nrooted at that node without communicating with another processor. Thus, it seems that such a\nstatic allocation yields a good parallel search algorithm.\nFigure 11.7. The unstructured nature of tree search and the imbalance\nresulting from static partitioning.\nLet us see what happens if we try to apply this approach to the tree in Figure 11.7 . Assume that\nwe have two processors. The root node is expanded to generate two nodes (A and B), and each\nof these nodes is assigned to one of the processors. Each processor now searches the subtrees\nrooted at its assigned node independently. At this point, the problem with static node\nassignment becomes apparent. The processor exploring the subtree rooted at node A expands\nconsiderably fewer nodes than does the other processor. Due to this imbalance in the workload,\none processor is idle for a significant amount of time, reducing efficiency. Using a larger number\nof processors worsens the imbalance. Consider the partitioning of the tree for four processors.\nNodes A and B are expanded to generate nodes C, D, E, and F. Assume that each of these nodes\nis assigned to one of the four processors. Now the processor searching the subtree rooted at\nnode E does most of the work, and those searching the subtrees rooted at nodes C and D spend\nmost of their time idle. The static partitioning of unstructured trees yields poor performance\nbecause of substantial variation in the size of partitions of the search space rooted at different\nnodes. Furthermore, since the search space is usually generated dynamically, it is difficult to get\na good estimate of the size of the", "doc_id": "59ebc0dc-7177-40a9-908e-8fdd1486de7d", "embedding": null, "doc_hash": "23174a6d5fff36dcdd807c4897f3caa22ccbec074a3605a0f413a279eb079724", "extra_info": null, "node_info": {"start": 1312742, "end": 1316491}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bbb43f78-1408-4f83-9507-bd2db8d70495", "3": "291a51d1-e632-4c42-b3b2-b585dfcfc46f"}}, "__type__": "1"}, "291a51d1-e632-4c42-b3b2-b585dfcfc46f": {"__data__": {"text": "to generate nodes C, D, E, and F. Assume that each of these nodes\nis assigned to one of the four processors. Now the processor searching the subtree rooted at\nnode E does most of the work, and those searching the subtrees rooted at nodes C and D spend\nmost of their time idle. The static partitioning of unstructured trees yields poor performance\nbecause of substantial variation in the size of partitions of the search space rooted at different\nnodes. Furthermore, since the search space is usually generated dynamically, it is difficult to get\na good estimate of the size of the search space beforehand. Therefore, it is necessary to balance\nthe search space among processors dynamically.\nIn dynamic load balancing , when a processor runs out of work, it gets more work from\nanother processor that has work. Consider the two-processor partitioning of the tree in Figure\n11.7(a) . Assume that nodes A and B are assigned to the two processors as we just described. In\nthis case when the processor searching the subtree rooted at node A runs out of work, it\nrequests work from the other processor. Although the dynamic distribution of work results in\ncommunication overhead for work requests and work transfers, it reduces load imbalance\namong processors. This section explores several schemes for dynamically balancing the load\nbetween processors.\nA parallel formulation of DFS based on dynamic load balancing is as follows. Each processor\nperforms DFS on a disjoint part of the search space. When a processor finishes searching its\npart of the search space, it requests an unsearched part from other processors. This takes the\nform of work request and response messages in message passing architectures, and locking and\nextracting work in shared address space machines. Whenever a processor finds a goal node, all\nthe processors terminate. If the search space is finite and the problem has no solutions, then all\nthe processors eventually run out of work, and the algorithm terminates.\nSince each processor searches the state space depth-first, unexplored states can be\nconveniently stored as a stack. Each processor maintains its own local stack on which it\nexecutes DFS. When a processor's local stack is empty, it requests (either via explicit messages\nor by locking) untried alternatives from another processor's stack. In the beginning, the entire\nsearch space is assigned to one processor, and other processors are assigned null search spaces\n(that is, empty stacks). The search space is distributed among the processors as they request\nwork. We refer to the processor that sends work as the donor  processor and to the processor\nthat requests and receives work as the recipient  processor.\nAs illustrated in Figure 11.8, each processor can be in one of two states: active  (that is, it has\nwork) or idle (that is, it is trying to get work). In message passing architectures, an idle\nprocessor selects a donor processor and sends it a work request. If the idle processor receives\nwork (part of the state space to be searched) from the donor processor, it becomes active. If it\nreceives a reject  message (because the donor has no work), it selects another donor and sends\na work request to that donor. This process repeats until the processor gets work or all the\nprocessors become idle. When a processor is idle and it receives a work request, that processor\nreturns a reject  message. The same process can be implemented on shared address space\nmachines by locking another processors' stack, examining it to see if it has work, extracting\nwork, and unlocking the stack.\nFigure 11.8. A generic scheme for dynamic load balancing.\nOn message passing architectures, in the active state, a processor does a fixed amount of work\n(expands a fixed number of nodes) and then checks for pending work requests. When a work\nrequest is received, the processor partitions its work into", "doc_id": "291a51d1-e632-4c42-b3b2-b585dfcfc46f", "embedding": null, "doc_hash": "34b3553546bf2c7ccea0b8f09dffbedaaf0ff8682a01143e676a4d1aeaa6f6c2", "extra_info": null, "node_info": {"start": 1316502, "end": 1320382}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "59ebc0dc-7177-40a9-908e-8fdd1486de7d", "3": "9cfa6308-42c9-4fd6-8a39-f54308fa805e"}}, "__type__": "1"}, "9cfa6308-42c9-4fd6-8a39-f54308fa805e": {"__data__": {"text": "When a processor is idle and it receives a work request, that processor\nreturns a reject  message. The same process can be implemented on shared address space\nmachines by locking another processors' stack, examining it to see if it has work, extracting\nwork, and unlocking the stack.\nFigure 11.8. A generic scheme for dynamic load balancing.\nOn message passing architectures, in the active state, a processor does a fixed amount of work\n(expands a fixed number of nodes) and then checks for pending work requests. When a work\nrequest is received, the processor partitions its work into two parts and sends one part to the\nrequesting processor. When a processor has exhausted its own search space, it becomes idle.\nThis process continues until a solution is found or until the entire space has been searched. If a\nsolution is found, a message is broadcast to all processors to stop searching. A termination\ndetection algorithm is used to detect whether all processors have become idle without finding a\nsolution ( Section 11.4.4).\n11.4.1 Important Parameters of Parallel DFS\nTwo characteristics of parallel DFS are critical to determining its performance. First is the\nmethod for splitting work at a processor, and the second is the scheme to determine the donor\nprocessor when a processor becomes idle.\nWork-Splitting Strategies\nWhen work is transferred, the donor's stack is split into two stacks, one of which is sent to the\nrecipient. In other words, some of the nodes (that is, alternatives) are removed from the\ndonor's stack and added to the recipient's stack. If too little work is sent, the recipient quickly\nbecomes idle; if too much, the donor becomes idle. Ideally, the stack is split into two equal\npieces such that the size of the search space represented by each stack is the same. Such a split\nis called a half-split . It is difficult to get a good estimate of the size of the tree rooted at an\nunexpanded alternative in the stack. However, the alternatives near the bottom of the stack\n(that is, close to the initial node) tend to have bigger trees rooted at them, and alternatives\nnear the top of the stack tend to have small trees rooted at them. To avoid sending very small\namounts of work, nodes beyond a specified stack depth are not given away. This depth is called\nthe cutoff depth .\nSome possible strategies for splitting the search space are (1) send nodes near the bottom of\nthe stack, (2) send nodes near the cutoff depth, and (3) send half the nodes between the\nbottom of the stack and the cutoff depth. The suitability of a splitting strategy depends on the\nnature of the search space. If the search space is uniform, both strategies 1 and 3 work well. If\nthe search space is highly irregular, strategy 3 usually works well. If a strong heuristic is\navailable (to order successors so that goal nodes move to the left of the state-space tree),\nstrategy 2 is likely to perform better, since it tries to distribute those parts of the search space\nlikely to contain a solution. The cost of splitting also becomes important if the stacks are deep.\nFor such stacks, strategy 1 has lower cost than strategies 2 and 3.\nFigure 11.9 shows the partitioning of the DFS tree of Figure 11.5(a)  into two subtrees using\nstrategy 3. Note that the states beyond the cutoff depth are not partitioned. Figure 11.9  also\nshows the representation of the stack corresponding to the two subtrees. The stack\nrepresentation used in the figure stores only the unexplored alternatives.\nFigure 11.9. Splitting the DFS tree in Figure 11.5 . The two subtrees\nalong with their stack representations are shown in (a) and (b).\nLoad-Balancing Schemes\nThis section discusses three dynamic load-balancing schemes: asynchronous round robin, global\nround", "doc_id": "9cfa6308-42c9-4fd6-8a39-f54308fa805e", "embedding": null, "doc_hash": "c79f120fe606fae5b11df89262a6327ef72a0603db1efaf9791859d9efe6f32e", "extra_info": null, "node_info": {"start": 1320373, "end": 1324118}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "291a51d1-e632-4c42-b3b2-b585dfcfc46f", "3": "ed38f2f6-5c10-402c-b262-4898afa7646c"}}, "__type__": "1"}, "ed38f2f6-5c10-402c-b262-4898afa7646c": {"__data__": {"text": "Figure 11.5(a)  into two subtrees using\nstrategy 3. Note that the states beyond the cutoff depth are not partitioned. Figure 11.9  also\nshows the representation of the stack corresponding to the two subtrees. The stack\nrepresentation used in the figure stores only the unexplored alternatives.\nFigure 11.9. Splitting the DFS tree in Figure 11.5 . The two subtrees\nalong with their stack representations are shown in (a) and (b).\nLoad-Balancing Schemes\nThis section discusses three dynamic load-balancing schemes: asynchronous round robin, global\nround robin, and random polling. Each of these schemes can be coded for message passing as\nwell as shared address space machines.\nAsynchronous Round Robin  In asynchronous round robin (ARR), each processor maintains an\nindependent variable, target . Whenever a processor runs out of work, it uses target  as the label\nof a donor processor and attempts to get work from it. The value of target  is incremented\n(modulo p) each time a work request is sent. The initial value of target  at each processor is set\nto ((label + 1) modulo p) where label is the local processor label. Note that work requests are\ngenerated independently by each processor. However, it is possible for two or more processors\nto request work from the same donor at nearly the same time.\nGlobal Round Robin  Global round robin (GRR) uses a single global variable called target . This\nvariable can be stored in a globally accessible space in shared address space machines or at a\ndesignated processor in message passing machines. Whenever a processor needs work, it\nrequests and receives the value of target , either by locking, reading, and unlocking on shared\naddress space machines or by sending a message requesting the designated processor (say P0).\nThe value of target  is incremented (modulo p) before responding to the next request. The\nrecipient processor then attempts to get work from a donor processor whose label is the value\nof target . GRR ensures that successive work requests are distributed evenly over all processors.\nA drawback of this scheme is the contention for access to target .\nRandom Polling  Random polling (RP) is the simplest load-balancing scheme. When a\nprocessor becomes idle, it randomly selects a donor. Each processor is selected as a donor with\nequal probability, ensuring that work requests are evenly distributed.\n11.4.2 A General Framework for Analysis of Parallel DFS\nTo analyze the performance and scalability of parallel DFS algorithms for any load-balancing\nscheme, we must compute the overhead To of the algorithm. Overhead in any load-balancing\nscheme is due to communication (requesting and sending work), idle time (waiting for work),\ntermination detection, and contention for shared resources. If the search overhead factor is\ngreater than one (i.e., if parallel search does more work than serial search), this will add\nanother term to To. In this section we assume that the search overhead factor is one, i.e., the\nserial and parallel versions of the algorithm perform the same amount of computation. We\nanalyze the case in which the search overhead factor is other than one in Section 11.6.1.\nFor the load-balancing schemes discussed in Section 11.4.1 , idle time is subsumed by\ncommunication overhead due to work requests and transfers. When a processor becomes idle, it\nimmediately selects a donor processor and sends it a work request. The total time for which the\nprocessor remains idle is equal to the time for the request to reach the donor and for the reply\nto arrive. At that point, the idle processor either becomes busy or generates another work\nrequest. Therefore, the time spent in communication subsumes the time for which a processor is\nidle. Since communication overhead is the dominant overhead in parallel DFS, we now consider\na method to compute the communication", "doc_id": "ed38f2f6-5c10-402c-b262-4898afa7646c", "embedding": null, "doc_hash": "726c1fc62dba50d855fe98b406cc51b0a89438aa21129b79b2e23655c3cd9a7e", "extra_info": null, "node_info": {"start": 1324139, "end": 1327987}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9cfa6308-42c9-4fd6-8a39-f54308fa805e", "3": "bd4f891f-93ef-45e5-a51f-252737118a3f"}}, "__type__": "1"}, "bd4f891f-93ef-45e5-a51f-252737118a3f": {"__data__": {"text": "time is subsumed by\ncommunication overhead due to work requests and transfers. When a processor becomes idle, it\nimmediately selects a donor processor and sends it a work request. The total time for which the\nprocessor remains idle is equal to the time for the request to reach the donor and for the reply\nto arrive. At that point, the idle processor either becomes busy or generates another work\nrequest. Therefore, the time spent in communication subsumes the time for which a processor is\nidle. Since communication overhead is the dominant overhead in parallel DFS, we now consider\na method to compute the communication overhead for each load-balancing scheme.\nIt is difficult to derive a precise expression for the communication overhead of the load-\nbalancing schemes for DFS because they are dynamic. This section describes a technique that\nprovides an upper bound on this overhead. We make the following assumptions in the analysis.\nThe work at any processor can be partitioned into independent pieces as long as its size\nexceeds a threshold \n.1.\nA reasonable work-splitting mechanism is available. Assume that work w at one processor 2.\n1.\nis partitioned into two parts: yw and (1 - y)w for 0 \n  y \n 1. Then there exists an\narbitrarily small constant a (0 < a \n 0.5), such that yw > aw and (1 - y)w > aw. We call\nsuch a splitting mechanism a-splitting. The constant a sets a lower bound on the load\nimbalance that results from work splitting: both partitions of w have at least aw work.2.\nThe first assumption is satisfied by most depth-first search algorithms. The third work-splitting\nstrategy described in Section 11.4.1  results in a-splitting even for highly irregular search\nspaces.\nIn the load-balancing schemes to be analyzed, the total work is dynamically partitioned among\nthe processors. Processors work on disjoint parts of the search space independently. An idle\nprocessor polls for work. When it finds a donor processor with work, the work is split and a part\nof it is transferred to the idle processor. If the donor has work wi, and it is split into two pieces\nof size wj and wk, then assumption 2 states that there is a constant a such that wj > awi and wk\n> awi. Note that a is less than 0.5. Therefore, after a work transfer, neither processor (donor\nand recipient) has more than (1 - a)wi work. Suppose there are p pieces of work whose sizes\nare w0,w1, ..., wp-1. Assume that the size of the largest piece is w. If all of these pieces are split,\nthe splitting strategy yields 2 p pieces of work whose sizes are given by y0w0,y1w1, ..., yp-1wp-1,\n(1 - y0)w0, (1 - y1)w1, ..., (1 - yp-1)wp-1. Among them, the size of the largest piece is given by\n(1 - a)w.\nAssume that there are p processors and a single piece of work is assigned to each processor. If\nevery processor receives a work request at least once, then each of these p pieces has been split\nat least once. Thus, the maximum work at any of the processors has been reduced by a factor of\n(1 - a).We define V (p) such that, after every V (p) work requests, each processor receives at\nleast one work request. Note that V (p) \n p. In general, V (p) depends on the load-balancing\nalgorithm. Initially, processor P0 has W units of work, and all other processors have no work.\nAfter V (p) requests, the maximum work remaining at any processor is less than (1- a)W; after\n2V (p) requests, the maximum work remaining at any processor is less than (1 -", "doc_id": "bd4f891f-93ef-45e5-a51f-252737118a3f", "embedding": null, "doc_hash": "0635f595a7edb6aa77d14b6e899c9418407030971294d7d0ee7607f7486c7f0e", "extra_info": null, "node_info": {"start": 1327937, "end": 1331363}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ed38f2f6-5c10-402c-b262-4898afa7646c", "3": "2bac844c-4e04-4cf5-a6b2-651cc769ad41"}}, "__type__": "1"}, "2bac844c-4e04-4cf5-a6b2-651cc769ad41": {"__data__": {"text": "any of the processors has been reduced by a factor of\n(1 - a).We define V (p) such that, after every V (p) work requests, each processor receives at\nleast one work request. Note that V (p) \n p. In general, V (p) depends on the load-balancing\nalgorithm. Initially, processor P0 has W units of work, and all other processors have no work.\nAfter V (p) requests, the maximum work remaining at any processor is less than (1- a)W; after\n2V (p) requests, the maximum work remaining at any processor is less than (1 - a)2W.\nSimilarly, after (log 1/(1- a)(W/\n))V (p) requests, the maximum work remaining at any processor\nis below a threshold value \n . Hence, the total number of work requests is O (V (p) log W).\nCommunication overhead is caused by work requests and work transfers. The total number of\nwork transfers cannot exceed the total number of work requests. Therefore, the total number of\nwork requests, weighted by the total communication cost of one work request and a\ncorresponding work transfer, gives an upper bound on the total communication overhead. For\nsimplicity, we assume the amount of data associated with a work request and work transfer is a\nconstant. In general, the size of the stack should grow logarithmically with respect to the size of\nthe search space. The analysis for this case can be done similarly (Problem 11.3).\nIf tcomm is the time required to communicate a piece of work, then the communication overhead\nTo is given by\nEquation 11.2\nThe corresponding efficiency E is given by\n\nIn Section 5.4.2  we showed that the isoefficiency function can be derived by balancing the\nproblem size W and the overhead function To. As shown by Equation 11.2 , To depends on two\nvalues: tcomm and V (p). The value of tcomm is determined by the underlying architecture, and\nthe function V (p) is determined by the load-balancing scheme. In the following subsections, we\nderive V (p) for each scheme introduced in Section 11.4.1 . We subsequently use these values of\nV (p) to derive the scalability of various schemes on message-passing and shared-address-\nspace machines.\nComputation of V(p) for Various Load-Balancing Schemes\nEquation 11.2  shows that V (p) is an important component of the total communication\noverhead. In this section, we compute the value of V (p) for different load-balancing schemes.\nAsynchronous Round Robin  The worst case value of V (p) for ARR occurs when all processors\nissue work requests at the same time to the same processor. This case is illustrated in the\nfollowing scenario. Assume that processor p - 1 had all the work and that the local counters of\nall the other processors (0 to p - 2) were pointing to processor zero. In this case, for processor\np - 1 to receive a work request, one processor must issue p - 1 requests while each of the\nremaining p - 2 processors generates up to p - 2 work requests (to all processors except\nprocessor p - 1 and itself). Thus, V (p) has an upper bound of ( p - 1) + ( p - 2)( p - 2); that is, V\n(p) = O (p2). Note that the actual value of V (p) is between p and p2.\nGlobal Round Robin  In GRR, all processors receive requests in sequence. After p requests,\neach processor has received one request. Therefore, V (p) is p.\nRandom Polling  For RR, the worst-case value of V (p) is unbounded. Hence, we compute the\naverage-case value of V (p).\nConsider a collection of p boxes. In each trial, a box is chosen at random and marked. We are\ninterested in the mean number of trials required to mark all the boxes. In our", "doc_id": "2bac844c-4e04-4cf5-a6b2-651cc769ad41", "embedding": null, "doc_hash": "4dd986a62fa0727f1a5884f3cad29457e8cdacefa5cddd5d534d5c701f4cc043", "extra_info": null, "node_info": {"start": 1331470, "end": 1334965}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bd4f891f-93ef-45e5-a51f-252737118a3f", "3": "c247e2f5-25aa-40af-a916-0a14fe0c753f"}}, "__type__": "1"}, "c247e2f5-25aa-40af-a916-0a14fe0c753f": {"__data__": {"text": "= O (p2). Note that the actual value of V (p) is between p and p2.\nGlobal Round Robin  In GRR, all processors receive requests in sequence. After p requests,\neach processor has received one request. Therefore, V (p) is p.\nRandom Polling  For RR, the worst-case value of V (p) is unbounded. Hence, we compute the\naverage-case value of V (p).\nConsider a collection of p boxes. In each trial, a box is chosen at random and marked. We are\ninterested in the mean number of trials required to mark all the boxes. In our algorithm, each\ntrial corresponds to a processor sending another randomly selected processor a request for\nwork.\nLet F (i, p) represent a state in which i of the p boxes have been marked, and p - i boxes have\nnot been marked. Since the next box to be marked is picked at random, there is i/p probability\nthat it will be a marked box and ( p - i)/p probability that it will be an unmarked box. Hence the\nsystem remains in state F (i, p) with a probability of i/p and transits to state F (i + 1, p) with a\nprobability of ( p - i)/p. Let f  (i, p) denote the average number of trials needed to change from\nstate F (i, p) to F (p, p). Then, V (p) = f (0, p). We have\nHence,\nwhere Hp is a harmonic number. It can be shown that, as p becomes large, Hp \n 1.69 ln p\n(where ln p denotes the natural logarithm of p). Thus, V (p) = O (p log p).\n11.4.3 Analysis of Load-Balancing Schemes\nThis section analyzes the performance of the load-balancing schemes introduced in Section\n11.4.1 . In each case, we assume that work is transferred in fixed-size messages (the effect of\nrelaxing this assumption is explored in Problem 11.3).\nRecall that the cost of communicating an m-word message in the simplified cost model is tcomm\n= ts + twm. Since the message size m is assumed to be a constant, tcomm = O (1) if there is no\ncongestion on the interconnection network. The communication overhead To (Equation 11.2 )\nreduces to\nEquation 11.3\nWe balance this overhead with problem size W for each load-balancing scheme to derive the\nisoefficiency function due to communication.\nAsynchronous Round Robin  As discussed in Section 11.4.2 , V (p) for ARR is O (p2).\nSubstituting into Equation 11.3 , communication overhead To is given by O (p2 log W). Balancing\ncommunication overhead against problem size W, we have\nSubstituting W into the right-hand side of the same equation and simplifying,\nThe double-log term (log log W) is asymptotically smaller than the first term, provided p grows\nno slower than log W, and can be ignored. The isoefficiency function for this scheme is therefore\ngiven by O ( p2 log p).\nGlobal Round Robin  From Section 11.4.2 , V (p) = O (p) for GRR. Substituting into Equation\n11.3, this yields a communication overhead To of O (p log W). Simplifying as for ARR, the\nisoefficiency function for this scheme due to communication overhead is O (p log p).\nIn this scheme, however, the global variable target  is accessed repeatedly, possibly causing\ncontention. The number of times this variable is accessed is equal to the total number of work\nrequests, O (p log W). If the processors are used efficiently, the total execution time is O (W/p).\nAssume that there is no contention for target  while solving a problem of size W on p processors.\nThen, W/p is larger than the total time during which the shared variable is accessed. As", "doc_id": "c247e2f5-25aa-40af-a916-0a14fe0c753f", "embedding": null, "doc_hash": "f12f0265c58e3d7368113007a71fc2bfa4d5673d74099cb67c9c696f1ea414bd", "extra_info": null, "node_info": {"start": 1334964, "end": 1338307}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2bac844c-4e04-4cf5-a6b2-651cc769ad41", "3": "4e4265c5-f394-4a0a-b5bd-e586df4fb49e"}}, "__type__": "1"}, "4e4265c5-f394-4a0a-b5bd-e586df4fb49e": {"__data__": {"text": "as for ARR, the\nisoefficiency function for this scheme due to communication overhead is O (p log p).\nIn this scheme, however, the global variable target  is accessed repeatedly, possibly causing\ncontention. The number of times this variable is accessed is equal to the total number of work\nrequests, O (p log W). If the processors are used efficiently, the total execution time is O (W/p).\nAssume that there is no contention for target  while solving a problem of size W on p processors.\nThen, W/p is larger than the total time during which the shared variable is accessed. As the\nnumber of processors increases, the execution time ( W/p) decreases, but the number of times\nthe shared variable is accessed increases. Thus, there is a crossover point beyond which the\nshared variable becomes a bottleneck, prohibiting further reduction in run time. This bottleneck\ncan be eliminated by increasing W at a rate such that the ratio between W/p and O (p log W)\nremains constant. This requires W to grow with respect to p as follows:\nEquation 11.4\nWe can simplify Equation 11.4  to express W in terms of p. This yields an isoefficiency term of O\n(p2 log p).\nSince the isoefficiency function due to contention asymptotically dominates the isoefficiency\nfunction due to communication, the overall isoefficiency function is given by O  (p2 log p). Note\nthat although it is difficult to estimate the actual overhead due to contention for the shared\nvariable, we are able to determine the resulting isoefficiency function.\nRandom Polling  We saw in Section 11.4.2 that V (p) = O (p log p) for RP. Substituting this\nvalue into Equation 11.3 , the communication overhead To is O (p log p log W). Equating To with\nthe problem size W and simplifying as before, we derive the isoefficiency function due to\ncommunication overhead as O (p log2 p). Since there is no contention in RP, this function also\ngives its overall isoefficiency function.\n11.4.4 Termination Detection\nOne aspect of parallel DFS that has not been addressed thus far is termination detection. In this\nsection, we present two schemes for termination detection that can be used with the load-\nbalancing algorithms discussed in Section 11.4.1.\nDijkstra's Token Termination Detection Algorithm\nConsider a simplified scenario in which once a processor goes idle, it never receives more work.\nVisualize the p processors as being connected in a logical ring (note that a logical ring can be\neasily mapped to underlying physical topologies). Processor P0 initiates a token when it\nbecomes idle. This token is sent to the next processor in the ring, P1. At any stage in the\ncomputation, if a processor receives a token, the token is held at the processor until the\ncomputation assigned to the processor is complete. On completion, the token is passed to the\nnext processor in the ring. If the processor was already idle, the token is passed to the next\nprocessor. Note that if at any time the token is passed to processor Pi , then all processors P0,\n..., Pi -1 have completed their computation. Processor Pp-1 passes its token to processor P0;\nwhen it receives the token, processor P0 knows that all processors have completed their\ncomputation and the algorithm can terminate.\nSuch a simple scheme cannot be applied to the search algorithms described in this chapter,\nbecause after a processor goes idle, it may receive more work from other processors. The token\ntermination detection scheme thus must be modified.\nIn the modified scheme, the processors are also organized into a ring. A processor can be in one\nof two states: black or white . Initially, all processors are in state white . As before, the token\ntravels in the sequence P0, P1, ...,", "doc_id": "4e4265c5-f394-4a0a-b5bd-e586df4fb49e", "embedding": null, "doc_hash": "88758c4828a88e0d7a2c40691c2232b45a4e6efe04f97b1d688c6dc256ac2b47", "extra_info": null, "node_info": {"start": 1338249, "end": 1341942}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c247e2f5-25aa-40af-a916-0a14fe0c753f", "3": "1fc60c7e-a2d0-4db8-9078-945435990f04"}}, "__type__": "1"}, "1fc60c7e-a2d0-4db8-9078-945435990f04": {"__data__": {"text": "it receives the token, processor P0 knows that all processors have completed their\ncomputation and the algorithm can terminate.\nSuch a simple scheme cannot be applied to the search algorithms described in this chapter,\nbecause after a processor goes idle, it may receive more work from other processors. The token\ntermination detection scheme thus must be modified.\nIn the modified scheme, the processors are also organized into a ring. A processor can be in one\nof two states: black or white . Initially, all processors are in state white . As before, the token\ntravels in the sequence P0, P1, ..., Pp-1, P0. If the only work transfers allowed in the system are\nfrom processor Pi to Pj such that i < j, then the simple termination scheme is still adequate.\nHowever, if processor Pj sends work to processor Pi, the token must traverse the ring again. In\nthis case processor Pj is marked black since it causes the token to go around the ring again.\nProcessor P0 must be able to tell by looking at the token it receives whether it should be\npropagated around the ring again. Therefore the token itself is of two types: a white  (or valid)\ntoken, which when received by processor P0 implies termination; and a black (or invalid) token,\nwhich implies that the token must traverse the ring again. The modified termination algorithm\nworks as follows:\nWhen it becomes idle, processor P0 initiates termination detection by making itself white\nand sending a white  token to processor P1.1.\nIf processor Pj sends work to processor Pi and j > i then processor Pj becomes black. 2.\nIf processor Pi has the token and Pi is idle, then it passes the token to Pi +1. If Pi is black,\nthen the color of the token is set to black before it is sent to Pi +1. If Pi is white , the token\nis passed unchanged.3.\nAfter Pi passes the token to Pi+1, Pi becomes white . 4.\nThe algorithm terminates when processor P0 receives a white  token and is itself idle. The\nalgorithm correctly detects termination by accounting for the possibility of a processor receiving\nwork after it has already been accounted for by the token.\nThe run time of this algorithm is O (P) with a small constant. For a small number of processors,\nthis scheme can be used without a significant impact on the overall performance. For a large\nnumber of processors, this algorithm can cause the overall isoefficiency function of the load-\nbalancing scheme to be at least O (p2) (Problem 11.4).\nTree-Based Termination Detection\nTree-based termination detection associates weights with individual work pieces. Initially\nprocessor P0 has all the work and a weight of one is associated with it. When its work is\npartitioned and sent to another processor, processor P0 retains half of the weight and gives half\nof it to the processor receiving the work. If Pi is the recipient processor and wi is the weight at\nprocessor Pi, then after the first work transfer, both w0 and wi are 0.5. Each time the work at a\nprocessor is partitioned, the weight is halved. When a processor completes its computation, it\nreturns its weight to the processor from which it received work. Termination is signaled when\nthe weight w0 at processor P0 becomes one and processor P0 has finished its work.\nExample 11.7 Tree-based termination detection\nFigure 11.10 illustrates tree-based termination detection for four processors. Initially,\nprocessor P0 has all the weight ( w0 = 1), and the weight at the remaining processors\nis 0 (w1 = w2 = w3 = 0). In step 1, processor P0 partitions its work and gives part of it\nto processor P1. After this step, w0 and w1 are 0.5 and w2 and w3 are 0. In step 2,\nprocessor P1 gives half of its work to processor P2. The weights w1 and w2 after this\nwork transfer are 0.25 and", "doc_id": "1fc60c7e-a2d0-4db8-9078-945435990f04", "embedding": null, "doc_hash": "06ee0b0ffd3190bf090f98af7a234265fa27b77ec175b8cb407c52f90ede6cad", "extra_info": null, "node_info": {"start": 1341918, "end": 1345642}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4e4265c5-f394-4a0a-b5bd-e586df4fb49e", "3": "eb1ffb7e-a5b8-4b32-9ab9-ce1d5a533a4b"}}, "__type__": "1"}, "eb1ffb7e-a5b8-4b32-9ab9-ce1d5a533a4b": {"__data__": {"text": "11.7 Tree-based termination detection\nFigure 11.10 illustrates tree-based termination detection for four processors. Initially,\nprocessor P0 has all the weight ( w0 = 1), and the weight at the remaining processors\nis 0 (w1 = w2 = w3 = 0). In step 1, processor P0 partitions its work and gives part of it\nto processor P1. After this step, w0 and w1 are 0.5 and w2 and w3 are 0. In step 2,\nprocessor P1 gives half of its work to processor P2. The weights w1 and w2 after this\nwork transfer are 0.25 and the weights w0 and w3 remain unchanged. In step 3,\nprocessor P3 gets work from processor P1 and the weights of all processors become\n0.25. In step 4, processor P2 completes its work and sends its weight to processor P1.\nThe weight w1 of processor P1 becomes 0.5. As processors complete their work,\nweights are propagated up the tree until the weight w0 at processor P0 becomes 1. At\nthis point, all work has been completed and termination can be signaled. \nFigure 11.10. Tree-based termination detection. Steps 1\u20136\nillustrate the weights at various processors after each work\ntransfer.\nThis termination detection algorithm has a significant drawback. Due to the finite precision of\ncomputers, recursive halving of the weight may make the weight so small that it becomes 0. In\nthis case, weight will be lost and termination will never be signaled. This condition can be\nalleviated by using the inverse of the weights. If processor Pi has weight wi, instead of\nmanipulating the weight itself, it manipulates 1/ wi. The details of this algorithm are considered\nin Problem 11.5.\nThe tree-based termination detection algorithm does not change the overall isoefficiency\nfunction of any of the search schemes we have considered. This follows from the fact that there\nare exactly two weight transfers associated with each work transfer. Therefore, the algorithm\nhas the effect of increasing the communication overhead by a constant factor. In asymptotic\nterms, this change does not alter the isoefficiency function.\n11.4.5 Experimental Results\nIn this section, we demonstrate the validity of scalability analysis for various parallel DFS\nalgorithms. The satisfiability problem tests the validity of boolean formulae. Such problems\narise in areas such as VLSI design and theorem proving. The satisfiability problem  can be\nstated as follows: given a boolean formula containing binary variables in conjunctive normal\nform, determine if it is unsatisfiable. A boolean formula is unsatisfiable if there exists no\nassignment of truth values to variables for which the formula is true.\nThe Davis-Putnam algorithm is a fast and efficient way to solve this problem. The algorithm\nworks by performing a depth-first search of the binary tree formed by true or false assignments\nto the literals in the boolean expression. Let n be the number of literals. Then the maximum\ndepth of the tree cannot exceed n. If, after a partial assignment of values to literals, the formula\nbecomes false, then the algorithm backtracks. The formula is unsatisfiable if depth-first search\nfails to find an assignment to variables for which the formula is true.\nEven if a formula is unsatisfiable, only a small subset of the 2n possible combinations will\nactually be explored. For example, for a 65-variable problem, the total number of possible\ncombinations is 265 (approximately 3.7 x 1019), but only about 107 nodes are actually expanded\nin a specific problem instance. The search tree for this problem is pruned in a highly nonuniform\nfashion and any attempt to partition the tree statically results in an extremely poor load\nbalance.\nTable 11.1. Average speedups for various load-balancing schemes.\u00a0 Number of processors\nScheme 8 16 32 64 128 256 512 1024\nARR 7.506 14.936 29.664", "doc_id": "eb1ffb7e-a5b8-4b32-9ab9-ce1d5a533a4b", "embedding": null, "doc_hash": "be4b6cd6ba2eb8b1000523f5bd71942f7b132856b98402fca3093c1f22582785", "extra_info": null, "node_info": {"start": 1345737, "end": 1349483}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1fc60c7e-a2d0-4db8-9078-945435990f04", "3": "a82f9a55-5b05-4236-9670-551baf777de0"}}, "__type__": "1"}, "a82f9a55-5b05-4236-9670-551baf777de0": {"__data__": {"text": "subset of the 2n possible combinations will\nactually be explored. For example, for a 65-variable problem, the total number of possible\ncombinations is 265 (approximately 3.7 x 1019), but only about 107 nodes are actually expanded\nin a specific problem instance. The search tree for this problem is pruned in a highly nonuniform\nfashion and any attempt to partition the tree statically results in an extremely poor load\nbalance.\nTable 11.1. Average speedups for various load-balancing schemes.\u00a0 Number of processors\nScheme 8 16 32 64 128 256 512 1024\nARR 7.506 14.936 29.664 57.721 103.738 178.92 259.372 284.425\nGRR 7.384 14.734 29.291 57.729 110.754 184.828 155.051 \u00a0\nRP 7.524 15.000 29.814 58.857 114.645 218.255 397.585 660.582\nThe satisfiability problem is used to test the load-balancing schemes on a message passing\nparallel computer for up to 1024 processors. We implemented the Davis-Putnam algorithm, and\nincorporated the load-balancing algorithms discussed in Section 11.4.1. This program was run\non several unsatisfiable formulae. By choosing unsatisfiable instances, we ensured that the\nnumber of nodes expanded by the parallel formulation is the same as the number expanded by\nthe sequential one; any speedup loss was due only to the overhead of load balancing.\nIn the problem instances on which the program was tested, the total number of nodes in the\ntree varied between approximately 105 and 107. The depth of the trees (which is equal to the\nnumber of variables in the formula) varied between 35 and 65. Speedup was calculated with\nrespect to the optimum sequential execution time for the same problem. Average speedup was\ncalculated by taking the ratio of the cumulative time to solve all the problems in parallel using a\ngiven number of processors to the corresponding cumulative sequential time. On a given\nnumber of processors, the speedup and efficiency were largely determined by the tree size\n(which is roughly proportional to the sequential run time). Thus, speedup on similar-sized\nproblems was quite similar.\nTable 11.2. Number of requests generated for GRR and RP.\n\u00a0 Number of processors\nScheme 8 16 32 64 128 256 512 1024\nGRR 260 661 1572 3445 8557 17088 41382 72874\nRP 562 2013 5106 15060 46056 136457 382695 885872\nAll schemes were tested on a sample set of five problem instances. Table 11.1  shows the\naverage speedup obtained by parallel algorithms using different load-balancing techniques.\nFigure 11.11  is a graph of the speedups obtained. Table 11.2  presents the total number of work\nrequests made by RP and GRR for one problem instance. Figure 11.12  shows the corresponding\ngraph and compares the number of messages generated with the expected values O (p log2 p)\nand O (p log p) for RP and GRR, respectively.\nFigure 11.11. Speedups of parallel DFS using ARR, GRR and RP load-\nbalancing schemes.\nFigure 11.12. Number of work requests generated for RP and GRR and\ntheir expected values ( O(p log2 p) and O(p log p) respectively).\n\nThe isoefficiency function of GRR is O (p2 log p) which is much worse than the isoefficiency\nfunction of RP. This is reflected in the performance of our implementation. From Figure 11.11 ,\nwe see that the performance of GRR deteriorates very rapidly for more than 256 processors.\nGood speedups can be obtained for p > 256 only for very large problem instances. Experimental\nresults also show that ARR is more scalable than GRR, but significantly less scalable than RP.\nAlthough the isoefficiency functions of ARR and GRR are both O (p2 log p), ARR performs better\nthan GRR. The reason", "doc_id": "a82f9a55-5b05-4236-9670-551baf777de0", "embedding": null, "doc_hash": "101c415bc828e6d35dd5637944b413bdfc86146be7d9dd479a5b67bc959280fb", "extra_info": null, "node_info": {"start": 1349407, "end": 1352962}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "eb1ffb7e-a5b8-4b32-9ab9-ce1d5a533a4b", "3": "61386123-97f3-4adf-ac07-6239c5911327"}}, "__type__": "1"}, "61386123-97f3-4adf-ac07-6239c5911327": {"__data__": {"text": "function of GRR is O (p2 log p) which is much worse than the isoefficiency\nfunction of RP. This is reflected in the performance of our implementation. From Figure 11.11 ,\nwe see that the performance of GRR deteriorates very rapidly for more than 256 processors.\nGood speedups can be obtained for p > 256 only for very large problem instances. Experimental\nresults also show that ARR is more scalable than GRR, but significantly less scalable than RP.\nAlthough the isoefficiency functions of ARR and GRR are both O (p2 log p), ARR performs better\nthan GRR. The reason for this is that p2 log p is an upper bound, derived using V (p) = O (p2).\nThis value of V (p) is only a loose upper bound for ARR. In contrast, the value of V (p) used for\nGRR ( O (p)) is a tight bound.\nTo determine the accuracy of the isoefficiency functions of various schemes, we experimentally\nverified the isoefficiency curves for the RP technique (the selection of this technique was\narbitrary). We ran 30 different problem instances varying in size from 105 nodes to 107 nodes\non a varying number of processors. Speedup and efficiency were computed for each of these.\nData points with the same efficiency for different problem sizes and number of processors were\nthen grouped. Where identical efficiency points were not available, the problem size was\ncomputed by averaging over points with efficiencies in the neighborhood of the required value.\nThese data are presented in Figure 11.13, which plots the problem size W against p log2 p for\nvalues of efficiency equal to 0.9, 0.85, 0.74, and 0.64. We expect points corresponding to the\nsame efficiency to be collinear. We can see from Figure 11.13 that the points are reasonably\ncollinear, which shows that the experimental isoefficiency function of RP is close to the\ntheoretically derived isoefficiency function.\nFigure 11.13. Experimental isoefficiency curves for RP for different\nefficiencies.\n11.4.6 Parallel Formulations of Depth-First Branch-and-Bound Search\nParallel formulations of depth-first branch-and-bound search (DFBB) are similar to those of\nDFS. The preceding formulations of DFS can be applied to DFBB with one minor modification:\nall processors are kept informed of the current best solution path. The current best solution path\nfor many problems can be represented by a small data structure. For shared address space\ncomputers, this data structure can be stored in a globally accessible memory. Each time a\nprocessor finds a solution, its cost is compared to that of the current best solution path. If the\ncost is lower, then the current best solution path is replaced. On a message-passing computer,\neach processor maintains the current best solution path known to it. Whenever a processor finds\na solution path better than the current best known, it broadcasts its cost to all other processors,\nwhich update (if necessary) their current best solution cost. Since the cost of a solution is\ncaptured by a single number and solutions are found infrequently, the overhead of\ncommunicating this value is fairly small. Note that, if a processor's current best solution path is\nworse than the globally best solution path, the efficiency of the search is affected but not its\ncorrectness. Because of DFBB's low communication overhead, the performance and scalability of\nparallel DFBB is similar to that of parallel DFS discussed earlier.\n11.4.7 Parallel Formulations of IDA*\nSince IDA* explores the search tree iteratively with successively increasing cost bounds, it is\nnatural to conceive a parallel formulation in which separate processors explore separate parts of\nthe search space independently. Processors may be exploring the tree using different cost\nbounds. This approach suffers from two drawbacks.\nIt is unclear how to select a threshold for a particular processor. If the threshold chosen\nfor a", "doc_id": "61386123-97f3-4adf-ac07-6239c5911327", "embedding": null, "doc_hash": "2431924a9d093d37eba73aca560e4eb3b26131b2db9592b60f392c9370ea2e7a", "extra_info": null, "node_info": {"start": 1352978, "end": 1356823}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "a82f9a55-5b05-4236-9670-551baf777de0", "3": "42aec36b-53df-48cd-bfbe-0ac7ec3ad6bf"}}, "__type__": "1"}, "42aec36b-53df-48cd-bfbe-0ac7ec3ad6bf": {"__data__": {"text": "of DFBB's low communication overhead, the performance and scalability of\nparallel DFBB is similar to that of parallel DFS discussed earlier.\n11.4.7 Parallel Formulations of IDA*\nSince IDA* explores the search tree iteratively with successively increasing cost bounds, it is\nnatural to conceive a parallel formulation in which separate processors explore separate parts of\nthe search space independently. Processors may be exploring the tree using different cost\nbounds. This approach suffers from two drawbacks.\nIt is unclear how to select a threshold for a particular processor. If the threshold chosen\nfor a processor happens to be higher than the global minimum threshold, then the\nprocessor will explore portions of the tree that are not explored by sequential IDA*.1.\nThis approach may not find an optimal solution. A solution found by one processor in a\nparticular iteration is not provably optimal until all the other processors have also\nexhausted the search space associated with thresholds lower than the cost of the solution\nfound.2.\nA more effective approach executes each iteration of IDA* by using parallel DFS ( Section 11.4).\nAll processors use the same cost bound; each processor stores the bound locally and performs\nDFS on its own search space. After each iteration of parallel IDA*, a designated processor\ndetermines the cost bound for the next iteration and restarts parallel DFS with the new bound.\nThe search terminates when a processor finds a goal node and informs all the other processors.\nThe performance and scalability of this parallel formulation of IDA* are similar to those of the\nparallel DFS algorithm.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n11.5 Parallel Best-First Search\nRecall from Section 11.2.2  that an important component of best-first search (BFS) algorithms is\nthe open list. It maintains the unexpanded nodes in the search graph, ordered according to\ntheir l-value. In the sequential algorithm, the most promising node from the open list is\nremoved and expanded, and newly generated nodes are added to the open list.\nIn most parallel formulations of BFS, different processors concurrently expand different nodes\nfrom the open list. These formulations differ according to the data structures they use to\nimplement the open list. Given p processors, the simplest strategy assigns each processor to\nwork on one of the current best nodes on the open list. This is called the centralized strategy\nbecause each processor gets work from a single global open list. Since this formulation of\nparallel BFS expands more than one node at a time, it may expand nodes that would not be\nexpanded by a sequential algorithm. Consider the case in which the first node on the open list is\na solution. The parallel formulation still expands the first p nodes on the open list. However,\nsince it always picks the best p nodes, the amount of extra work is limited. Figure 11.14\nillustrates this strategy. There are two problems with this approach:\nThe termination criterion of sequential BFS fails for parallel BFS. Since at any moment, p\nnodes from the open list are being expanded, it is possible that one of the nodes may be a\nsolution that does not correspond to the best goal node (or the path found is not the\nshortest path). This is because the remaining p - 1 nodes may lead to search spaces\ncontaining better goal nodes. Therefore, if the cost of a solution found by a processor is c,\nthen this solution is not guaranteed to correspond to the best goal node until the cost of\nnodes being searched at other processors is known to be at least c. The termination\ncriterion must be modified to ensure that termination occurs only after the best solution\nhas been found.1.\nSince the open list is accessed for each node expansion, it must be easily accessible to all\nprocessors, which can severely limit performance. Even on shared-address-space\narchitectures, contention for the open", "doc_id": "42aec36b-53df-48cd-bfbe-0ac7ec3ad6bf", "embedding": null, "doc_hash": "8b96aa6f277d8d18a3c47936deb15a9efa183f22ea622028884e3ea4d6cbde09", "extra_info": null, "node_info": {"start": 1356772, "end": 1360675}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "61386123-97f3-4adf-ac07-6239c5911327", "3": "ad84d79b-79cd-4d0e-ac03-11e199ebc793"}}, "__type__": "1"}, "ad84d79b-79cd-4d0e-ac03-11e199ebc793": {"__data__": {"text": "may lead to search spaces\ncontaining better goal nodes. Therefore, if the cost of a solution found by a processor is c,\nthen this solution is not guaranteed to correspond to the best goal node until the cost of\nnodes being searched at other processors is known to be at least c. The termination\ncriterion must be modified to ensure that termination occurs only after the best solution\nhas been found.1.\nSince the open list is accessed for each node expansion, it must be easily accessible to all\nprocessors, which can severely limit performance. Even on shared-address-space\narchitectures, contention for the open list limits speedup. Let texp be the average time to\nexpand a single node, and taccess be the average time to access the open list for a single-\nnode expansion. If there are n nodes to be expanded by both the sequential and parallel\nformulations (assuming that they do an equal amount of work), then the sequential run\ntime is given by n(taccess + texp). Assume that it is impossible to parallelize the expansion\nof individual nodes. Then the parallel run time will be at least ntaccess, because the open\nlist must be accessed at least once for each node expanded. Hence, an upper bound on the\nspeedup is ( taccess + texp)/taccess.2.\nFigure 11.14. A general schematic for parallel best-first search using a\ncentralized strategy. The locking operation is used here to serialize\nqueue access by various processors.\nOne way to avoid the contention due to a centralized open list is to let each processor have a\nlocal open list. Initially, the search space is statically divided among the processors by\nexpanding some nodes and distributing them to the local open lists of various processors. All\nthe processors then select and expand nodes simultaneously. Consider a scenario where\nprocessors do not communicate with each other. In this case, some processors might explore\nparts of the search space that would not be explored by the sequential algorithm. This leads to\na high search overhead factor and poor speedup. Consequently, the processors must\ncommunicate among themselves to minimize unnecessary search. The use of a distributed open\nlist trades-off communication and computation: decreasing communication between distributed\nopen lists increases search overhead factor, and decreasing search overhead factor with\nincreased communication increases communication overhead.\nThe best choice of communication strategy for parallel BFS depends on whether the search\nspace is a tree or a graph. Searching a graph incurs the additional overhead of checking for\nduplicate nodes on the closed list. We discuss some communication strategies for tree and\ngraph search separately.\nCommunication Strategies for Parallel Best-First Tree Search\nA communication strategy allows state-space nodes to be exchanged between open lists on\ndifferent processors. The objective of a communication strategy is to ensure that nodes with\ngood l-values are distributed evenly among processors. In this section we discuss three such\nstrategies, as follows.\nIn the random communication strategy , each processor periodically sends some of its\nbest nodes to the open list of a randomly selected processor. This strategy ensures that, if\na processor stores a good part of the search space, the others get part of it. If nodes are\ntransferred frequently, the search overhead factor can be made very small; otherwise it\ncan become quite large. The communication cost determines the best node transfer\nfrequency. If the communication cost is low, it is best to communicate after every node\nexpansion.1.\nIn the ring communication strategy , the processors are mapped in a virtual ring. Each\nprocessor periodically exchanges some of its best nodes with the open lists of its neighbors\nin the ring. This strategy can be implemented on message passing as well as shared\naddress space machines with the processors organized into a logical ring. As before, the\ncost of communication determines the node transfer frequency. Figure 11.15 illustrates the\nring communication strategy.\nFigure 11.15. A", "doc_id": "ad84d79b-79cd-4d0e-ac03-11e199ebc793", "embedding": null, "doc_hash": "0520d1ed5fd044c823534fb679afa8c6abdf1e832d775b797d203328848adea7", "extra_info": null, "node_info": {"start": 1360682, "end": 1364756}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "42aec36b-53df-48cd-bfbe-0ac7ec3ad6bf", "3": "b6e7d5a8-4972-44a0-b8d1-f8d842f58e2c"}}, "__type__": "1"}, "b6e7d5a8-4972-44a0-b8d1-f8d842f58e2c": {"__data__": {"text": "communication cost determines the best node transfer\nfrequency. If the communication cost is low, it is best to communicate after every node\nexpansion.1.\nIn the ring communication strategy , the processors are mapped in a virtual ring. Each\nprocessor periodically exchanges some of its best nodes with the open lists of its neighbors\nin the ring. This strategy can be implemented on message passing as well as shared\naddress space machines with the processors organized into a logical ring. As before, the\ncost of communication determines the node transfer frequency. Figure 11.15 illustrates the\nring communication strategy.\nFigure 11.15. A message-passing implementation of parallel best-\nfirst search using the ring communication strategy.\nUnless the search space is highly uniform, the search overhead factor of this scheme is\nvery high. The reason is that this scheme takes a long time to distribute good nodes from\none processor to all other processors.2.\nIn the blackboard communication strategy , there is a shared blackboard through which\nnodes are switched among processors as follows. After selecting the best node from its\nlocal open list, a processor expands the node only if its l-value is within a tolerable limit of\nthe best node on the blackboard. If the selected node is much better than the best node on\nthe blackboard, the processor sends some of its best nodes to the blackboard before\nexpanding the current node. If the selected node is much worse than the best node on the\nblackboard, the processor retrieves some good nodes from the blackboard and reselects a\nnode for expansion. Figure 11.16 illustrates the blackboard communication strategy. The\nblackboard strategy is suited only to shared-address-space computers, because the value\nof the best node in the blackboard has to be checked after each node expansion.\nFigure 11.16. An implementation of parallel best-first search using3.\nthe blackboard communication strategy.\nCommunication Strategies for Parallel Best-First Graph Search\nWhile searching graphs, an algorithm must check for node replication. This task is distributed\namong processors. One way to check for replication is to map each node to a specific processor.\nSubsequently, whenever a node is generated, it is mapped to the same processor, which checks\nfor replication locally. This technique can be implemented using a hash function that takes a\nnode as input and returns a processor label. When a node is generated, it is sent to the\nprocessor whose label is returned by the hash function for that node. Upon receiving the node, a\nprocessor checks whether it already exists in the local open or closed  lists. If not, the node is\ninserted in the open list. If the node already exists, and if the new node has a better cost\nassociated with it, then the previous version of the node is replaced by the new node on the\nopen list.\nFor a random hash function, the load-balancing property of this distribution strategy is similar\nto the random-distribution technique discussed in the previous section. This result follows from\nthe fact that each processor is equally likely to be assigned a part of the search space that\nwould also be explored by a sequential formulation. This method ensures an even distribution of\nnodes with good heuristic values among all the processors (Problem 11.10). However, hashing\ntechniques degrade performance because each node generation results in communication\n(Problem 11.11).[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n11.6 Speedup Anomalies in Parallel Search Algorithms\nIn parallel search algorithms, speedup can vary greatly from one execution to another because\nthe portions of the search space examined by various processors are determined dynamically\nand can differ for each execution. Consider the case of sequential and parallel DFS performed\non the tree illustrated in Figure 11.17. Figure 11.17(a)  illustrates sequential DFS search. The\norder of node expansions is indicated by node labels. The sequential formulation generates 13\nnodes before reaching the goal node", "doc_id": "b6e7d5a8-4972-44a0-b8d1-f8d842f58e2c", "embedding": null, "doc_hash": "1088cf901a043685bcffc29601ab7e48d8467d1f88b94c5500ab62c4816c4025", "extra_info": null, "node_info": {"start": 1364726, "end": 1368765}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ad84d79b-79cd-4d0e-ac03-11e199ebc793", "3": "6f7b4745-ecae-45ee-af2f-f89005795b53"}}, "__type__": "1"}, "6f7b4745-ecae-45ee-af2f-f89005795b53": {"__data__": {"text": "]\n  \n11.6 Speedup Anomalies in Parallel Search Algorithms\nIn parallel search algorithms, speedup can vary greatly from one execution to another because\nthe portions of the search space examined by various processors are determined dynamically\nand can differ for each execution. Consider the case of sequential and parallel DFS performed\non the tree illustrated in Figure 11.17. Figure 11.17(a)  illustrates sequential DFS search. The\norder of node expansions is indicated by node labels. The sequential formulation generates 13\nnodes before reaching the goal node G.\nFigure 11.17. The difference in number of nodes searched by\nsequential and parallel formulations of DFS. For this example, parallel\nDFS reaches a goal node after searching fewer nodes than sequential\nDFS.\nNow consider the parallel formulation of DFS illustrated for the same tree in Figure 11.17(b)  for\ntwo processors. The nodes expanded by the processors are labeled R and L. The parallel\nformulation reaches the goal node after generating only nine nodes. That is, the parallel\nformulation arrives at the goal node after searching fewer nodes than its sequential counterpart.\nIn this case, the search overhead factor is 9/13 (less than one), and if communication overhead\nis not too large, the speedup will be superlinear.\nFinally, consider the situation in Figure 11.18. The sequential formulation ( Figure 11.18(a) )\ngenerates seven nodes before reaching the goal node, but the parallel formulation generates 12\nnodes. In this case, the search overhead factor is greater than one, resulting in sublinear\nspeedup.\nFigure 11.18. A parallel DFS formulation that searches more nodes\nthan its sequential counterpart.\nIn summary, for some executions, the parallel version finds a solution after generating fewer\nnodes than the sequential version, making it possible to obtain superlinear speedup. For other\nexecutions, the parallel version finds a solution after generating more nodes, resulting in\nsublinear speedup. Executions yielding speedups greater than p by using p processors are\nreferred to as acceleration anomalies . Speedups of less than p using p processors are called\ndeceleration anomalies .\nSpeedup anomalies also manifest themselves in best-first search algorithms. Here, anomalies\nare caused by nodes on the open list that have identical heuristic values but require vastly\ndifferent amounts of search to detect a solution. Assume that two such nodes exist; node A\nleads rapidly to the goal node, and node B leads nowhere after extensive work. In parallel BFS,\nboth nodes are chosen for expansion by different processors. Consider the relative performance\nof parallel and sequential BFS. If the sequential algorithm picks node A to expand, it arrives\nquickly at a goal. However, the parallel algorithm wastes time expanding node B, leading to a\ndeceleration anomaly. In contrast, if the sequential algorithm expands node B, it wastes\nsubstantial time before abandoning it in favor of node A. However, the parallel algorithm does\nnot waste as much time on node B, because node A yields a solution quickly, leading to an\nacceleration anomaly.\n11.6.1 Analysis of Average Speedup in Parallel DFS\nIn isolated executions of parallel search algorithms, the search overhead factor may be equal to\none, less than one, or greater than one. It is interesting to know the average value of the search\noverhead factor. If it is less than one, this implies that the sequential search algorithm is not\noptimal. In this case, the parallel search algorithm running on a sequential processor (by\nemulating a parallel processor by using time-slicing) would expand fewer nodes than the\nsequential algorithm on the average. In this", "doc_id": "6f7b4745-ecae-45ee-af2f-f89005795b53", "embedding": null, "doc_hash": "3a24f3b18b344870e492391477e0f60435c3ac854336f1661109abea94d6cf62", "extra_info": null, "node_info": {"start": 1368833, "end": 1372526}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b6e7d5a8-4972-44a0-b8d1-f8d842f58e2c", "3": "9adde165-deb7-4ab5-a831-5bc53e249169"}}, "__type__": "1"}, "9adde165-deb7-4ab5-a831-5bc53e249169": {"__data__": {"text": "anomaly.\n11.6.1 Analysis of Average Speedup in Parallel DFS\nIn isolated executions of parallel search algorithms, the search overhead factor may be equal to\none, less than one, or greater than one. It is interesting to know the average value of the search\noverhead factor. If it is less than one, this implies that the sequential search algorithm is not\noptimal. In this case, the parallel search algorithm running on a sequential processor (by\nemulating a parallel processor by using time-slicing) would expand fewer nodes than the\nsequential algorithm on the average. In this section, we show that for a certain type of search\nspace, the average value of the search overhead factor in parallel DFS is less than one. Hence, if\nthe communication overhead is not too large, then on the average, parallel DFS will provide\nsuperlinear speedup for this type of search space.\nAssumptions\nWe make the following assumptions for analyzing speedup anomalies:\nThe state-space tree has M leaf nodes. Solutions occur only at leaf nodes. The amount of\ncomputation needed to generate each leaf node is the same. The number of nodes\ngenerated in the tree is proportional to the number of leaf nodes generated. This is a\nreasonable assumption for search trees in which each node has more than one successor\non the average.1.\nBoth sequential and parallel DFS stop after finding one solution.2.\nIn parallel DFS, the state-space tree is equally partitioned among p processors; thus, each\nprocessor gets a subtree with M/p leaf nodes.3.\nThere is at least one solution in the entire tree. (Otherwise, both parallel search and\nsequential search generate the entire tree without finding a solution, resulting in linear\nspeedup.)4.\nThere is no information to order the search of the state-space tree; hence, the density of\nsolutions across the unexplored nodes is independent of the order of the search.5.\nThe solution density r is defined as the probability of the leaf node being a solution. We\nassume a Bernoulli distribution of solutions; that is, the event of a leaf node being a\nsolution is independent of any other leaf node being a solution. We also assume that r \n1.6.\nThe total number of leaf nodes generated by p processors before one of the processors\nfinds a solution is denoted by Wp. The average number of leaf nodes generated by\nsequential DFS before a solution is found is given by W. Both W and Wp are less than or\nequal to M.7.\nAnalysis of the Search Overhead Factor\nConsider the scenario in which the M leaf nodes are statically divided into p regions, each with K\n= M/p leaves. Let the density of solutions among the leaves in the ith region be ri. In the parallel\nalgorithm, each processor Pi searches region i independently until a processor finds a solution.\nIn the sequential algorithm, the regions are searched in random order.\nTheorem 11.6.1  Let r be the solution density in a region; and assume that the\nnumber of leaves K in the region is large. Then, if  r > 0, the mean number of leaves\ngenerated by a single processor searching the region is  1/r.\nProof:  Since we have a Bernoulli distribution, the mean number of trials is given by\nEquation 11.5\n\nFor a fixed value of r and a large value of K, the second term in Equation 11.5\nbecomes small; hence, the mean number of trials is approximately equal to 1/ r. \nSequential DFS selects any one of the p regions with probability 1/ p and searches it to find a\nsolution. Hence, the average number of leaf nodes expanded by sequential DFS is\nThis expression assumes that a solution is always found in the selected region; thus, only one\nregion must be searched. However, the probability of region i not having any solutions is (1", "doc_id": "9adde165-deb7-4ab5-a831-5bc53e249169", "embedding": null, "doc_hash": "8aa8d68ab206a84c84d7c758b849f879cab2a9568fa3445044fca964b4a9f135", "extra_info": null, "node_info": {"start": 1372520, "end": 1376202}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6f7b4745-ecae-45ee-af2f-f89005795b53", "3": "60ced58c-33e8-4cc9-8bbb-6943f9843f58"}}, "__type__": "1"}, "60ced58c-33e8-4cc9-8bbb-6943f9843f58": {"__data__": {"text": "by\nEquation 11.5\n\nFor a fixed value of r and a large value of K, the second term in Equation 11.5\nbecomes small; hence, the mean number of trials is approximately equal to 1/ r. \nSequential DFS selects any one of the p regions with probability 1/ p and searches it to find a\nsolution. Hence, the average number of leaf nodes expanded by sequential DFS is\nThis expression assumes that a solution is always found in the selected region; thus, only one\nregion must be searched. However, the probability of region i not having any solutions is (1 -\nri)K. In this case, another region must be searched. Taking this into account makes the\nexpression for W more precise and increases the average value of W somewhat. The overall\nresults of the analysis will not change.\nIn each step of parallel DFS, one node from each of the p regions is explored simultaneously.\nHence the probability of success in a step of the parallel algorithm is \n . This is\napproximately r1 + r2 + \u00b7\u00b7\u00b7 + rp (neglecting the second-order terms, since each ri are assumed\nto be small). Hence,\nInspecting the above equations, we see that W = 1/HM and Wp = 1/AM, where HM is the\nharmonic mean of r1, r2, ..., rp, and AM is their arithmetic mean. Since the arithmetic mean\n(AM) and the harmonic mean (HM) satisfy the relation AM \n HM, we have W \n Wp. In\nparticular:\nWhen r1 = r2 = \u00b7\u00b7\u00b7 = rp, AM = HM, therefore W \n Wp. When solutions are uniformly\ndistributed, the average search overhead factor for parallel DFS is one.\nWhen each ri is different, AM  > HM, therefore W > Wp. When solution densities in various\nregions are nonuniform, the average search overhead factor for parallel DFS is less than\none, making it possible to obtain superlinear speedups.\nThe assumption that each node can be a solution independent of the other nodes being\nsolutions is false for most practical problems. Still, the preceding analysis suggests that parallel\nDFS obtains higher efficiency than sequential DFS provided that the solutions are not distributed\nuniformly in the search space and that no information about solution density in various regions\nis available. This characteristic applies to a variety of problem spaces searched by simple\nbacktracking. The result that the search overhead factor for parallel DFS is at least one on the\naverage is important, since DFS is currently the best known and most practical sequential\nalgorithm used to solve many important problems.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n11.7 Bibliographic Remarks\nExtensive literature is available on search algorithms for discrete optimization techniques such\nas branch-and-bound and heuristic search [ KK88a , LW66 , Pea84 ]. The relationship between\nbranch-and-bound search, dynamic programming, and heuristic search techniques in artificial\nintelligence is explored by Kumar and Kanal [ KK83, KK88b ]. The average time complexity of\nheuristic search algorithms for many problems is shown to be polynomial by Smith [Smi84 ] and\nWilf [ Wil86 ]. Extensive work has been done on parallel formulations of search algorithms. We\nbriefly outline some of these contributions.\nParallel Depth-First Search Algorithms\nMany parallel algorithms for DFS have been formulated [ AJM88 , FM87 , KK94 , KGR94 , KR87b ,\nMV87 , Ran91 , Rao90 , SK90 , SK89 , Vor87a ]. Load balancing is the central issue in parallel DFS.\nIn this chapter, distribution of work in parallel DFS was done using stack splitting [ KGR94 ,\nKR87b ]. An alternative scheme for work-distribution is node splitting, in which only a", "doc_id": "60ced58c-33e8-4cc9-8bbb-6943f9843f58", "embedding": null, "doc_hash": "9fbad1d83ee530b39d8573c1c57d142fdcbe775332a2907d8257ccc6edc8c2ee", "extra_info": null, "node_info": {"start": 1376244, "end": 1379749}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9adde165-deb7-4ab5-a831-5bc53e249169", "3": "ce824a6f-b860-4546-b49f-c6a1e27109aa"}}, "__type__": "1"}, "ce824a6f-b860-4546-b49f-c6a1e27109aa": {"__data__": {"text": "of search algorithms. We\nbriefly outline some of these contributions.\nParallel Depth-First Search Algorithms\nMany parallel algorithms for DFS have been formulated [ AJM88 , FM87 , KK94 , KGR94 , KR87b ,\nMV87 , Ran91 , Rao90 , SK90 , SK89 , Vor87a ]. Load balancing is the central issue in parallel DFS.\nIn this chapter, distribution of work in parallel DFS was done using stack splitting [ KGR94 ,\nKR87b ]. An alternative scheme for work-distribution is node splitting, in which only a single\nnode is given out [ FK88, FTI90 , Ran91 ]\nThis chapter discussed formulations of state-space search in which a processor requests work\nwhen it goes idle. Such load-balancing schemes are called receiver-initiated  schemes. In\nother load-balancing schemes, a processor that has work gives away part of its work to another\nprocessor (with or without receiving a request). These schemes are called sender-initiated\nschemes.\nSeveral researchers have used receiver-initiated load-balancing schemes in parallel DFS [ FM87,\nKR87b , KGR94 ]. Kumar et al. [KGR94 ] analyze these load-balancing schemes including global\nround robin, random polling, asynchronous round robin, and nearest neighbor. The description\nand analysis of these schemes in Section 11.4  is based on the papers by Kumar et al. [KGR94 ,\nKR87b ].\nParallel DFS using sender-initiated load balancing has been proposed by some researchers\n[FK88, FTI90 , PFK90 , Ran91 , SK89 ]. Furuichi et al. propose the single-level and multilevel\nsender-based schemes [ FTI90 ]. Kimura and Nobuyuki [ KN91 ] presented the scalability analysis\nof these schemes. Ferguson and Korf [ FK88, PFK90 ] present a load-balancing scheme called\ndistributed tree search (DTS).\nOther techniques using randomized allocation have been presented for parallel DFS of state-\nspace trees [ KP92, Ran91 , SK89 , SK90 ]. Issues relating to granularity control in parallel DFS\nhave also been explored [ RK87 , SK89 ].\nSaletore and Kale [SK90] present a formulation of parallel DFS in which nodes are assigned\npriorities and are expanded accordingly. They show that the search overhead factor of this\nprioritized DFS formulation is very close to one, allowing it to yield consistently increasing\nspeedups with an increasing number of processors for sufficiently large problems.\nIn some parallel formulations of depth-first search, the state space is searched independently in\na random order by different processors [ JAM87, JAM88 ]. Challou et al. [CGK93 ] and Ertel\n[Ert92 ] show that such methods are useful for solving robot motion planning and theorem\nproving problems, respectively.\nMost generic DFS formulations apply to depth-first branch-and-bound and IDA*. Some\nresearchers have specifically studied parallel formulations of depth-first branch-and-bound\n[AKR89, AKR90 , EDH80 ]. Many parallel formulations of IDA* have been proposed [ RK87 ,\nRKR87 , KS91a , PKF92 , MD92 ].\nMost of the parallel DFS formulations are suited only for MIMD computers. Due to the nature of\nthe search problem, SIMD computers were considered inherently unsuitable for parallel search.\nHowever, work by Frye and Myczkowski [ FM92], Powley et al. [PKF92 ], and Mahanti and\nDaniels [ MD92 ] showed that parallel depth-first search techniques can be developed even for\nSIMD computers. Karypis and Kumar [", "doc_id": "ce824a6f-b860-4546-b49f-c6a1e27109aa", "embedding": null, "doc_hash": "874248f1dec4bf9c5cb119737aa6daeab8fe3345c63bd253ed1cef2e42684b83", "extra_info": null, "node_info": {"start": 1379793, "end": 1383093}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "60ced58c-33e8-4cc9-8bbb-6943f9843f58", "3": "c8ea66a0-94fc-40d6-b69d-62d27ed1e316"}}, "__type__": "1"}, "c8ea66a0-94fc-40d6-b69d-62d27ed1e316": {"__data__": {"text": "parallel formulations of IDA* have been proposed [ RK87 ,\nRKR87 , KS91a , PKF92 , MD92 ].\nMost of the parallel DFS formulations are suited only for MIMD computers. Due to the nature of\nthe search problem, SIMD computers were considered inherently unsuitable for parallel search.\nHowever, work by Frye and Myczkowski [ FM92], Powley et al. [PKF92 ], and Mahanti and\nDaniels [ MD92 ] showed that parallel depth-first search techniques can be developed even for\nSIMD computers. Karypis and Kumar [ KK94 ] presented parallel DFS schemes for SIMD\ncomputers that are as scalable as the schemes for MIMD computers.\nSeveral researchers have experimentally evaluated parallel DFS. Finkel and Manber [ FM87 ]\npresent performance results for problems such as the traveling salesman problem and the\nknight's tour for the Crystal multicomputer developed at the University of Wisconsin. Monien\nand Vornberger [ MV87] show linear speedups on a network of transputers for a variety of\ncombinatorial problems. Kumar et al. [AKR89 , AKR90 , AKRS91 , KGR94 ] show linear speedups\nfor problems such as the 15-puzzle, tautology verification, and automatic test pattern\ngeneration for various architectures such as a 128-processor BBN Butterfly, a 128-processor\nIntel iPSC, a 1024-processor nCUBE 2, and a 128-processor Symult 2010. Kumar, Grama, and\nRao [ GKR91, KGR94 , KR87b , RK87 ] have investigated the scalability and performance of many\nof these schemes for hypercubes, meshes, and networks of workstations. Experimental results\nin Section 11.4.5 are taken from the paper by Kumar, Grama, and Rao [ KGR94 ].\nParallel formulations of DFBB have also been investigated by several researchers. Many of these\nformulations are based on maintaining a current best solution, which is used as a global bound.\nIt has been shown that the overhead for maintaining the current best solution tends to be a\nsmall fraction of the overhead for dynamic load balancing. Parallel formulations of DFBB have\nbeen shown to yield near linear speedups for many problems and architectures [ ST95, LM97 ,\nEck97 , Eck94 , AKR89 ].\nMany researchers have proposed termination detection algorithms for use in parallel search.\nDijkstra [ DSG83] proposed the ring termination detection algorithm. The termination detection\nalgorithm based on weights, discussed in Section 11.4.4 , is similar to the one proposed by\nRokusawa et al. [RICN88 ]. Dutt and Mahapatra [ DM93 ] discuss the termination detection\nalgorithm based on minimum spanning trees.\nParallel Formulations of Alpha-Beta Search\nAlpha-beta search is essentially a depth-first branch-and-bound search technique that finds an\noptimal solution tree of an AND/OR graph [ KK83 , KK88b ]. Many researchers have developed\nparallel formulations of alpha-beta search [ ABJ82 , Bau78 , FK88, FF82, HB88 , Lin83 , MC82 ,\nMFMV90 , MP85 , PFK90 ]. Some of these methods have shown reasonable speedups on dozens of\nprocessors [ FK88, MFMV90 , PFK90 ].\nThe utility of parallel processing has been demonstrated in the context of a number of games,\nand in particular, chess. Work on large scale parallel a - b search led to the development of\nDeep Thought [ Hsu90] in 1990. This program was capable of playing chess at grandmaster\nlevel. Subsequent advances in the use of dedicated hardware, parallel processing, and\nalgorithms resulted in the development of IBM's Deep Blue [ HCH95, HG97 ] that beat the\nreigning", "doc_id": "c8ea66a0-94fc-40d6-b69d-62d27ed1e316", "embedding": null, "doc_hash": "e61a3910e56277a88942d6e510c8e62b06dac235e7ce72637e03219c923c62c2", "extra_info": null, "node_info": {"start": 1383084, "end": 1386494}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ce824a6f-b860-4546-b49f-c6a1e27109aa", "3": "1b85c16f-ae28-480d-9c1c-17e41c24710e"}}, "__type__": "1"}, "1b85c16f-ae28-480d-9c1c-17e41c24710e": {"__data__": {"text": "of these methods have shown reasonable speedups on dozens of\nprocessors [ FK88, MFMV90 , PFK90 ].\nThe utility of parallel processing has been demonstrated in the context of a number of games,\nand in particular, chess. Work on large scale parallel a - b search led to the development of\nDeep Thought [ Hsu90] in 1990. This program was capable of playing chess at grandmaster\nlevel. Subsequent advances in the use of dedicated hardware, parallel processing, and\nalgorithms resulted in the development of IBM's Deep Blue [ HCH95, HG97 ] that beat the\nreigning world champion Gary Kasparov. Feldmann et al. [FMM94 ] developed a distributed\nchess program that is acknowledged to be one of the best computer chess players based\nentirely on general purpose hardware.\nParallel Best-First Search\nMany researchers have investigated parallel formulations of A* and branch-and-bound\nalgorithms [ KK84 , KRR88 , LK85, MV87 , Qui89 , HD89a , Vor86 , WM84 , Rao90 , GKP92 , AM88 ,\nCJP83 , KB57 , LP92, Rou87 , PC89, PR89, PR90, PRV88 , Ten90 , MRSR92 , Vor87b , Moh83 , MV85 ,\nHD87 ]. All these formulations use different data structures to store the open list. Some\nformulations use the centralized strategy [ Moh83 , HD87 ]; some use distributed strategies such\nas the random communication strategy [Vor87b , Dal87 , KRR88 ], the ring communication\nstrategy [ Vor86 , WM84 ]; and the blackboard communication strategy [ KRR88 ]. Kumar et al.\n[KRR88 ] experimentally evaluated the centralized strategy and some distributed strategies in\nthe context of the traveling salesman problem, the vertex cover problem and the 15-puzzle.\nDutt and Mahapatra [ DM93, MD93 ] have proposed and evaluated a number of other\ncommunication strategies.\nManzini analyzed the hashing technique for distributing nodes in parallel graph search [ MS90 ].\nEvett et al. [EHMN90 ] proposed parallel retracting A* (PRA*), which operates under limited-\nmemory conditions. In this formulation, each node is hashed to a unique processor. If a\nprocessor receives more nodes than it can store locally, it retracts nodes with poorer heuristic\nvalues. These retracted nodes are reexpanded when more promising nodes fail to yield a\nsolution.\nKarp and Zhang [ KZ88] analyze the performance of parallel best-first branch-and-bound (that\nis, A*) by using a random distribution of nodes for a specific model of search trees. Renolet et\nal. [RDK89 ] use Monte Carlo simulations to model the performance of parallel best-first search.\nWah and Yu [ WY85 ] present stochastic models to analyze the performance of parallel\nformulations of depth-first branch-and-bound and best-first branch-and-bound search.\nBixby [ Bix91] presents a parallel branch-and-cut algorithm to solve the symmetric traveling\nsalesman problem. He also presents solutions of the LP relaxations of airline crew-scheduling\nmodels. Miller et al. [Mil91 ] present parallel formulations of the best-first branch-and-bound\ntechnique for solving the asymmetric traveling salesman problem on heterogeneous network\ncomputer architectures. Roucairol [ Rou91] presents parallel best-first branch-and-bound\nformulations for shared-address-space computers and uses them to solve the multiknapsack\nand quadratic-assignment problems.\nSpeedup Anomalies in Parallel Formulations of Search Algorithms\nMany researchers have analyzed speedup anomalies in parallel search algorithms [", "doc_id": "1b85c16f-ae28-480d-9c1c-17e41c24710e", "embedding": null, "doc_hash": "f24e86475d7a16d3e7132de0729b1efa7bf7ab6aa10c3acdb1b1f940abe0d555", "extra_info": null, "node_info": {"start": 1386443, "end": 1389821}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "c8ea66a0-94fc-40d6-b69d-62d27ed1e316", "3": "afd9d681-b877-45f6-acf5-1f7eb85f9d3d"}}, "__type__": "1"}, "afd9d681-b877-45f6-acf5-1f7eb85f9d3d": {"__data__": {"text": "Miller et al. [Mil91 ] present parallel formulations of the best-first branch-and-bound\ntechnique for solving the asymmetric traveling salesman problem on heterogeneous network\ncomputer architectures. Roucairol [ Rou91] presents parallel best-first branch-and-bound\nformulations for shared-address-space computers and uses them to solve the multiknapsack\nand quadratic-assignment problems.\nSpeedup Anomalies in Parallel Formulations of Search Algorithms\nMany researchers have analyzed speedup anomalies in parallel search algorithms [ IYF79, LS84,\nKor81 , LW86 , MVS86 , RKR87 ]. Lai and Sahni [ LS84] present early work quantifying speedup\nanomalies in best-first search. Lai and Sprague [ LS86] present enhancements and extensions to\nthis work. Lai and Sprague [ LS85] also present an analytical model and derive characteristics of\nthe lower-bound function for which anomalies are guaranteed not to occur as the number of\nprocessors is increased. Li and Wah [ LW84 , LW86 ] and Wah et al. [WLY84 ] investigate\ndominance relations and heuristic functions and their effect on detrimental (speedup of <\n1using p processors) and acceleration anomalies. Quinn and Deo [ QD86] derive an upper bound\non the speedup attainable by any parallel formulation of the branch-and-bound algorithm using\nthe best-bound search strategy. Rao and Kumar [ RK88b, RK93 ] analyze the average speedup in\nparallel DFS for two separate models with and without heuristic ordering information. They\nshow that the search overhead factor in these cases is at most one. Section 11.6.1 is based on\nthe results of Rao and Kumar [ RK93 ].\nFinally, many programming environments have been developed for implementing parallel\nsearch. Some examples are DIB [ FM87], Chare-Kernel [ SK89 ], MANIP [ WM84 ], and PICOS\n[RDK89 ].\nRole of Heuristics\nHeuristics form the most important component of search techniques, and parallel formulations\nof search algorithms must be viewed in the context of these heuristics. In BFS techniques,\nheuristics focus search by lowering the effective branching factor. In DFBB methods, heuristics\nprovide better bounds, and thus serve to prune the search space.\nOften, there is a tradeoff between the strength of the heuristic and the effective size of search\nspace. Better heuristics result in smaller search spaces but are also more expensive to compute.\nFor example, an important application of strong heuristics is in the computation of bounds for\nmixed integer programming (MIP). Mixed integer programming has seen significant advances\nover the years [ JNS97]. Whereas 15 years back, MIP problems with 100 integer variables were\nconsidered challenging, today, many problems with up to 1000 integer variables can be solved\non workstation class machines using branch-and-cut methods. The largest known instances of\nTSPs and QAPs have been solved using branch-and-bound with powerful heuristics [ BMCP98,\nMP93 ]. The presence of effective heuristics may prune the search space considerably. For\nexample, when Padberg and Rinaldi introduced the branch-and-cut algorithm in 1987, they\nused it to solve a 532 city TSP, which was the largest TSP solved optimally at that time.\nSubsequent improvements to the method led to the solution of a a 2392 city problem [ PR91].\nMore recently, using cutting planes, problems with over 7000 cities have been solved [ JNS97 ]\non serial machines. However, for many problems of interest, the reduced search space still\nrequires the use of parallelism [ BMCP98, MP93 , Rou87 , MMR95 ]. Use of powerful heuristics\ncombined with effective", "doc_id": "afd9d681-b877-45f6-acf5-1f7eb85f9d3d", "embedding": null, "doc_hash": "22a7baa5cb0c61b4efb860269534fa9d965ebfa6b6aed71d4a18b9e20de04655", "extra_info": null, "node_info": {"start": 1389815, "end": 1393376}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1b85c16f-ae28-480d-9c1c-17e41c24710e", "3": "42fcc3eb-64af-4bfd-a9e5-b7fea6b8a1a6"}}, "__type__": "1"}, "42fcc3eb-64af-4bfd-a9e5-b7fea6b8a1a6": {"__data__": {"text": "introduced the branch-and-cut algorithm in 1987, they\nused it to solve a 532 city TSP, which was the largest TSP solved optimally at that time.\nSubsequent improvements to the method led to the solution of a a 2392 city problem [ PR91].\nMore recently, using cutting planes, problems with over 7000 cities have been solved [ JNS97 ]\non serial machines. However, for many problems of interest, the reduced search space still\nrequires the use of parallelism [ BMCP98, MP93 , Rou87 , MMR95 ]. Use of powerful heuristics\ncombined with effective parallel processing has enabled the solution of extremely large\nproblems [ MP93]. For example, QAP problems from the Nugent and Eschermann test suites with\nup to 4.8 x 1010 nodes (Nugent22 with Gilmore-Lawler bound) were solved on a NEC Cenju-3\nparallel computer in under nine days [ BMCP98 ]. Another dazzling demonstration of this was\npresented by the IBM Deep Blue. Deep blue used a combination of dedicated hardware for\ngenerating and evaluating board positions and parallel search of the game tree using an IBM\nSP2 to beat the current world chess champion, Gary Kasparov [ HCH95, HG97 ].\nHeuristics have several important implications for exploiting parallelism. Strong heuristics\nnarrow the state space and thus reduce the concurrency available in the search space. Use of\npowerful heuristics poses other computational challenges for parallel processing as well. For\nexample, in branch-and-cut methods, a cut generated at a certain state may be required by\nother states. Therefore, in addition to balancing load, the parallel branch-and-cut formulation\nmust also partition cuts among processors so that processors working in certain LP domains\nhave access to the desired cuts [ BCCL95, LM97 , Eck97 ].\nIn addition to inter-node parallelism that has been discussed up to this point, intra-node\nparallelism can become a viable option if the heuristic is computationally expensive. For\nexample, the assignment heuristic of Pekny et al. has been effectively parallelized for solving\nlarge instances of TSPs [ MP93]. If the degree of inter-node parallelism is small, this form of\nparallelism provides a desirable alternative. Another example of this is in the solution of MIP\nproblems using branch-and-cut methods. Branch-and-cut methods rely on LP relaxation for\ngenerating lower bounds of partially instantiated solutions followed by generation of valid\ninequalities [ JNS97]. These LP relaxations constitute a major part of the overall computation\ntime. Many of the industrial codes rely on Simplex to solve the LP problem since it can adapt the\nsolution to added rows and columns. While interior point methods are better suited to\nparallelism, they tend to be less efficient for reoptimizing LP solutions with added rows and\ncolumns (in branch-and-cut methods). LP relaxation using Simplex has been shown to\nparallelize well on small numbers of processors but efforts to scale to larger numbers of\nprocessors have not been successful. LP based branch and bound methods have also been used\nfor solving quadratic assignment problems using iterative solvers such as preconditioned\nConjugate Gradient to approximately compute the interior point directions [ PLRR94]. These\nmethods have been used to compute lower bounds using linear programs with over 150,000\nconstraints and 300,000 variables for solving QAPs. These and other iterative solvers parallelize\nvery effectively to a large number of processors. A general parallel framework for computing\nheuristic solutions to problems is presented by Pramanick and Kuhl [ PK95].\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n11.1 [ KGR94 ] In Section 11.4.1 , we identified access", "doc_id": "42fcc3eb-64af-4bfd-a9e5-b7fea6b8a1a6", "embedding": null, "doc_hash": "9eea9a7ad74f55bab72efb83ef386afda14b8280a382b5cd30374ab1d03a6a7d", "extra_info": null, "node_info": {"start": 1393400, "end": 1397064}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "afd9d681-b877-45f6-acf5-1f7eb85f9d3d", "3": "4107a2ef-27ba-4c9c-b321-d06259a7f1fd"}}, "__type__": "1"}, "4107a2ef-27ba-4c9c-b321-d06259a7f1fd": {"__data__": {"text": "Gradient to approximately compute the interior point directions [ PLRR94]. These\nmethods have been used to compute lower bounds using linear programs with over 150,000\nconstraints and 300,000 variables for solving QAPs. These and other iterative solvers parallelize\nvery effectively to a large number of processors. A general parallel framework for computing\nheuristic solutions to problems is presented by Pramanick and Kuhl [ PK95].\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n11.1 [ KGR94 ] In Section 11.4.1 , we identified access to the global pointer, target , as a\nbottleneck in the GRR load-balancing scheme. Consider a modification of this scheme in\nwhich it is augmented with message combining. This scheme works as follows. All the\nrequests to read the value of the global pointer target  at processor zero are combined at\nintermediate processors. Thus, the total number of requests handled by processor zero is\ngreatly reduced. This technique is essentially a software implementation of the fetch-and-\nadd operation. This scheme is called GRR-M (GRR with message combining).\nAn implementation of this scheme is illustrated in Figure 11.19. Each processor is at a leaf\nnode in a complete (logical) binary tree. Note that such a logical tree can be easily\nmapped on to a physical topology. When a processor wants to atomically read and\nincrement target , it sends a request up the tree toward processor zero. An internal node of\nthe tree holds a request from one of its children for at most time d, then forwards the\nmessage to its parent. If a request comes from the node's other child within time d, the\ntwo requests are combined and sent up as a single request. If i is the total number of\nincrement requests that have been combined, the resulting increment of target  is i.\nFigure 11.19. Message combining and a sample implementation on\nan eight-processor hypercube.\nThe returned value at each processor is equal to what it would have been if all the\nrequests to target  had been serialized. This is done as follows: each combined message is\nstored in a table at each processor until the request is granted. When the value of target  is\nsent back to an internal node, two values are sent down to the left and right children if\nboth requested a value of target . The two values are determined from the entries in the\ntable corresponding to increment requests by the two children. The scheme is illustrated\nby Figure 11.19, in which the original value of target  is x, and processors P0, P2, P4, P6 and\nP7 issue requests. The total requested increment is five. After the messages are combined\nand processed, the value of target  received at these processors is x, x + 1, x + 2, x + 3\nand x + 4, respectively.\nAnalyze the performance and scalability of this scheme for a message passing architecture.\n11.2 [ Lin92 ] Consider another load-balancing strategy. Assume that each processor\nmaintains a variable called counter . Initially, each processor initializes its local copy of\ncounter  to zero. Whenever a processor goes idle, it searches for two processors Pi and Pi +1\nin a logical ring embedded into any architecture, such that the value of counter  at Pi is\ngreater than that at Pi +1. The idle processor then sends a work request to processor Pi +1.\nIf no such pair of processors exists, the request is sent to processor zero. On receiving a\nwork request, a processor increments its local value of counter .\nDevise algorithms to detect the pairs Pi and Pi +1. Analyze the scalability of this load-\nbalancing scheme based on your algorithm to detect the pairs Pi and Pi +1 for a message\npassing architecture.\nHint:  The upper bound on the number of work transfers for this scheme is similar to that\nfor", "doc_id": "4107a2ef-27ba-4c9c-b321-d06259a7f1fd", "embedding": null, "doc_hash": "339eb2dca8dfeee445be2018eee08f59feb4ee091e1ddc25c3d69197aae94e9f", "extra_info": null, "node_info": {"start": 1397065, "end": 1400782}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "42fcc3eb-64af-4bfd-a9e5-b7fea6b8a1a6", "3": "ef8ab771-cab1-4488-99ef-ff6a203071c4"}}, "__type__": "1"}, "ef8ab771-cab1-4488-99ef-ff6a203071c4": {"__data__": {"text": "is\ngreater than that at Pi +1. The idle processor then sends a work request to processor Pi +1.\nIf no such pair of processors exists, the request is sent to processor zero. On receiving a\nwork request, a processor increments its local value of counter .\nDevise algorithms to detect the pairs Pi and Pi +1. Analyze the scalability of this load-\nbalancing scheme based on your algorithm to detect the pairs Pi and Pi +1 for a message\npassing architecture.\nHint:  The upper bound on the number of work transfers for this scheme is similar to that\nfor GRR.\n11.3 In the analysis of various load-balancing schemes presented in Section 11.4.2, we\nassumed that the cost of transferring work is independent of the amount of work\ntransferred. However, there are problems for which the work-transfer cost is a function of\nthe amount of work transferred. Examples of such problems are found in tree-search\napplications for domains in which strong heuristics are available. For such applications, the\nsize of the stack used to represent the search tree can vary significantly with the number\nof nodes in the search tree.\nConsider a case in which the size of the stack for representing a search space of w nodes\nvaries as \n. Assume that the load-balancing scheme used is GRR. Analyze the\nperformance of this scheme for a message passing architecture.\n11.4 Consider Dijkstra's token termination detection scheme described in Section 11.4.4.\nShow that the contribution of termination detection using this scheme to the overall\nisoefficiency function is O (p2). Comment on the value of the constants associated with this\nisoefficiency term.\n11.5 Consider the tree-based termination detection scheme in Section 11.4.4 . In this\nalgorithm, the weights may become very small and may eventually become zero due to\nthe finite precision of computers. In such cases, termination is never signaled. The\nalgorithm can be modified by manipulating the reciprocal of the weight instead of the\nweight itself. Write the modified termination algorithm and show that it is capable of\ndetecting termination correctly.\n11.6 [ DM93] Consider a termination detection algorithm in which a spanning tree of\nminimum diameter is mapped onto the architecture of the given parallel computer. The\ncenter  of such a tree is a vertex with the minimum distance to the vertex farthest from it.\nThe center of a spanning tree is considered to be its root.\nWhile executing parallel search, a processor can be either idle or busy. The termination\ndetection algorithm requires all work transfers in the system to be acknowledged by an ack\nmessage. A processor is busy if it has work, or if it has sent work to another processor and\nthe corresponding ack message has not been received; otherwise the processor is idle.\nProcessors at the leaves of the spanning tree send stop messages to their parent when\nthey become idle. Processors at intermediate levels in the tree pass the stop message on\nto their parents when they have received stop messages from all their children and they\nthemselves become idle. When the root processor receives stop messages from all its\nchildren and becomes idle, termination is signaled.\nSince it is possible for a processor to receive work after it has sent a stop message to its\nparent, a processor signals that it has received work by sending a resume  message to its\nparent. The resume  message moves up the tree until it meets the previously issued stop\nmessage. On meeting the stop message, the resume  message nullifies the stop message.\nAn ack message is then sent to the processor that transferred part of its work.\nShow using examples that this termination detection technique correctly signals\ntermination. Determine the isoefficiency term due to this termination detection scheme for\na spanning tree of depth log p.\n11.7 [ FTI90, KN91 ] Consider the single-level load-balancing scheme which works as\nfollows: a", "doc_id": "ef8ab771-cab1-4488-99ef-ff6a203071c4", "embedding": null, "doc_hash": "43fa550ffbc4e0e1d865724f8b23a9c0fb0607f62dfa41c8830b8ec00589b477", "extra_info": null, "node_info": {"start": 1400781, "end": 1404679}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4107a2ef-27ba-4c9c-b321-d06259a7f1fd", "3": "ad8d73b8-ac7b-424b-a452-e9316bdb9526"}}, "__type__": "1"}, "ad8d73b8-ac7b-424b-a452-e9316bdb9526": {"__data__": {"text": "received work by sending a resume  message to its\nparent. The resume  message moves up the tree until it meets the previously issued stop\nmessage. On meeting the stop message, the resume  message nullifies the stop message.\nAn ack message is then sent to the processor that transferred part of its work.\nShow using examples that this termination detection technique correctly signals\ntermination. Determine the isoefficiency term due to this termination detection scheme for\na spanning tree of depth log p.\n11.7 [ FTI90, KN91 ] Consider the single-level load-balancing scheme which works as\nfollows: a designated processor called manager  generates many subtasks and gives them\none-by-one to the requesting processors on demand. The manager  traverses the search\ntree depth-first to a predetermined cutoff depth and distributes nodes at that depth as\nsubtasks. Increasing the cutoff depth increases the number of subtasks, but makes them\nsmaller. The processors request another subtask from the manager only after finishing the\nprevious one. Hence, if a processor gets subtasks corresponding to large subtrees, it will\nsend fewer requests to the manager . If the cutoff depth is large enough, this scheme\nresults in good load balance among the processors. However, if the cutoff depth is too\nlarge, the subtasks given out to the processors become small and the processors send\nmore frequent requests to the manager . In this case, the manager  becomes a bottleneck.\nHence, this scheme has a poor scalability. Figure 11.20 illustrates the single-level work-\ndistribution scheme.\nFigure 11.20. The single-level work-distribution scheme for tree\nsearch.\nAssume that the cost of communicating a piece of work between any two processors is\nnegligible. Derive analytical expressions for the scalability of the single-level load-\nbalancing scheme.\n11.8 [ FTI90, KN91 ] Consider the multilevel work-distribution scheme that circumvents the\nsubtask generation bottleneck of the single-level scheme through multiple-level subtask\ngeneration. In this scheme, processors are arranged in an m -ary tree of depth d. The task\nof top-level subtask generation is given to the root processor. It divides the task into\nsuper-subtasks and distributes them to its successor processors on demand. These\nprocessors subdivide the super-subtasks into subtasks and distribute them to successor\nprocessors on request. The leaf processors repeatedly request work from their parents as\nsoon as they have finished their previous work. A leaf processor is allocated to another\nsubtask generator when its designated subtask generator runs out of work. For d = 1, the\nmulti- and single-level schemes are identical. Comment on the performance and scalability\nof this scheme.\n11.9 [ FK88] Consider the distributed tree search scheme in which processors are\nallocated to separate parts of the search tree dynamically. Initially, all the processors are\nassigned to the root. When the root node is expanded (by one of the processors assigned\nto it), disjoint subsets of processors at the root are assigned to each successor, in\naccordance with a selected processor-allocation strategy. One possible processor-\nallocation strategy is to divide the processors equally among ancestor nodes. This process\ncontinues until there is only one processor assigned to a node. At this time, the processor\nsearches the tree rooted at the node sequentially. If a processor finishes searching the\nsearch tree rooted at the node, it is reassigned to its parent node. If the parent node has\nother successor nodes still being explored, then this processor is allocated to one of them.\nOtherwise, the processor is assigned to its parent. This process continues until the entire\ntree is searched. Comment on the performance and scalability of this scheme.\n11.10  Consider a parallel formulation of best-first search of a graph that", "doc_id": "ad8d73b8-ac7b-424b-a452-e9316bdb9526", "embedding": null, "doc_hash": "dc08c6c6c5f5ab3fec6426385b532956f9baf7be7a0501162c4eae7774a615f2", "extra_info": null, "node_info": {"start": 1404624, "end": 1408497}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ef8ab771-cab1-4488-99ef-ff6a203071c4", "3": "e74ef894-b85d-4c23-bbdf-f69cf80657c1"}}, "__type__": "1"}, "e74ef894-b85d-4c23-bbdf-f69cf80657c1": {"__data__": {"text": "process\ncontinues until there is only one processor assigned to a node. At this time, the processor\nsearches the tree rooted at the node sequentially. If a processor finishes searching the\nsearch tree rooted at the node, it is reassigned to its parent node. If the parent node has\nother successor nodes still being explored, then this processor is allocated to one of them.\nOtherwise, the processor is assigned to its parent. This process continues until the entire\ntree is searched. Comment on the performance and scalability of this scheme.\n11.10  Consider a parallel formulation of best-first search of a graph that uses a hash\nfunction to distribute nodes to processors ( Section 11.5). The performance of this scheme\nis influenced by two factors: the communication cost and the number of \"good\" nodes\nexpanded (a \"good\" node is one that would also be expanded by the sequential algorithm).\nThese two factors can be analyzed independently of each other.\nAssuming a completely random hash function (one in which each node has a probability of\nbeing hashed to a processor equal to 1/ p), show that the expected number of nodes\nexpanded by this parallel formulation differs from the optimal number by a constant factor\n(that is, independent of p). Assuming that the cost of communicating a node from one\nprocessor to another is O (1), derive the isoefficiency function of this scheme.\n11.11  For the parallel formulation in Problem 11.10, assume that the number of nodes\nexpanded by the sequential and parallel formulations are the same. Analyze the\ncommunication overhead of this formulation for a message passing architecture. Is the\nformulation scalable? If so, what is the isoefficiency function? If not, for what\ninterconnection network would the formulation be scalable?\nHint:  Note that a fully random hash function corresponds to an all-to-all personalized\ncommunication operation, which is bandwidth sensitive. [ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 12. Dynamic Programming\nDynamic programming  (DP) is a commonly used technique for solving a wide variety of\ndiscrete optimization problems such as scheduling, string-editing, packaging, and inventory\nmanagement. More recently, it has found applications in bioinformatics in matching sequences\nof amino-acids and nucleotides (the Smith-Waterman algorithm). DP views a problem as a set\nof interdependent subproblems. It solves subproblems and uses the results to solve larger\nsubproblems until the entire problem is solved. In contrast to divide-and-conquer, where the\nsolution to a problem depends only on the solution to its subproblems, in DP there may be\ninterrelationships across subproblems. In DP, the solution to a subproblem is expressed as a\nfunction of solutions to one or more subproblems at the preceding levels.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n12.1 Overview of Dynamic Programming\nWe start our discussion with a simple DP algorithm for computing shortest paths in a graph.\nExample 12.1 The shortest-path problem\nConsider a DP formulation for the problem of finding a shortest (least-cost) path\nbetween a pair of vertices in an acyclic graph. (Refer to Section 10.1  for an\nintroduction to graph terminology.) An edge connecting node i to node j has cost c(i,\nj). If two vertices i and j are not connected then c(i, j) = \n . The graph contains n\nnodes numbered 0, 1, ..., n - 1, and has an edge from node i to node j only if i < j.\nThe shortest-path problem is to find a least-cost path between nodes 0 and n - 1. Let f\n(x) denote the cost of the least-cost path from node 0 to node x. Thus, f (0) is zero,\nand finding f (n - 1) solves the problem. The DP formulation for", "doc_id": "e74ef894-b85d-4c23-bbdf-f69cf80657c1", "embedding": null, "doc_hash": "088ee2403ec4ed32ba70e93374879470aa9376e9d1a815c2d06b2de189284c14", "extra_info": null, "node_info": {"start": 1408486, "end": 1412132}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ad8d73b8-ac7b-424b-a452-e9316bdb9526", "3": "486ff700-2fc7-479d-92d5-ebf1c40fa3de"}}, "__type__": "1"}, "486ff700-2fc7-479d-92d5-ebf1c40fa3de": {"__data__": {"text": "An edge connecting node i to node j has cost c(i,\nj). If two vertices i and j are not connected then c(i, j) = \n . The graph contains n\nnodes numbered 0, 1, ..., n - 1, and has an edge from node i to node j only if i < j.\nThe shortest-path problem is to find a least-cost path between nodes 0 and n - 1. Let f\n(x) denote the cost of the least-cost path from node 0 to node x. Thus, f (0) is zero,\nand finding f (n - 1) solves the problem. The DP formulation for this problem yields\nthe following recursive equations for f (x):\nEquation 12.1\nAs an instance of this algorithm, consider the five-node acyclic graph shown in Figure\n12.1. The problem is to find f (4). It can be computed given f (3) and f (2). More\nprecisely,\nFigure 12.1. A graph for which the shortest path between\nnodes 0 and 4 is to be computed.\nTherefore, f (2) and f (3) are elements of the set of subproblems on which f  (4)\ndepends. Similarly, f (3) depends on f (1) and f (2),and f (1) and f (2) depend on f (0).\nSince f (0) is known, it is used to solve f (1) and f (2), which are used to solve f (3). \n\nIn general, the solution to a DP problem is expressed as a minimum (or maximum) of possible\nalternate solutions. Each of these alternate solutions is constructed by composing one or more\nsubproblems. If r represents the cost of a solution composed of subproblems x1, x2, ..., xl, then\nr can be written as\nThe function g is called the composition function , and its nature depends on the problem. If\nthe optimal solution to each problem is determined by composing optimal solutions to the\nsubproblems and selecting the minimum (or maximum), the formulation is said to be a DP\nformulation. Figure 12.2 illustrates an instance of composition and minimization of solutions.\nThe solution to problem x8 is the minimum of the three possible solutions having costs r1, r2,\nand r3. The cost of the first solution is determined by composing solutions to subproblems x1\nand x3, the second solution by composing solutions to subproblems x4 and x5, and the third\nsolution by composing solutions to subproblems x2, x6, and x7.\nFigure 12.2. The computation and composition of subproblem\nsolutions to solve problem f (x8).\nDP represents the solution to an optimization problem as a recursive equation whose left side is\nan unknown quantity and whose right side is a minimization (or maximization) expression.\nSuch an equation is called a functional equation  or an optimization equation . In Equation12.1, the composition function g is given by f (j) + c(j, x). This function is additive, since it is\nthe sum of two terms. In a general DP formulation, the cost function need not be additive. A\nfunctional equation that contains a single recursive term (for example, f (j)) yields a monadic\nDP formulation. For an arbitrary DP formulation, the cost function may contain multiple\nrecursive terms. DP formulations whose cost function contains multiple recursive terms are\ncalled polyadic  formulations.\nThe dependencies between subproblems in a DP formulation can be represented by a directed\ngraph. Each node in the graph represents a subproblem. A directed edge from node i to node j\nindicates that the solution to the subproblem represented by node i is used to compute the\nsolution to the subproblem represented by node j. If the graph is acyclic, then the nodes of the\ngraph can be organized into levels such that subproblems at a particular level depend only on\nsubproblems at previous", "doc_id": "486ff700-2fc7-479d-92d5-ebf1c40fa3de", "embedding": null, "doc_hash": "d43be03d686db5420502beba6e5e7762d1a034997cc4453b8264686a0f228f0f", "extra_info": null, "node_info": {"start": 1412293, "end": 1415741}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e74ef894-b85d-4c23-bbdf-f69cf80657c1", "3": "cae98049-27b5-4d02-a06a-60d31f8e8b3e"}}, "__type__": "1"}, "cae98049-27b5-4d02-a06a-60d31f8e8b3e": {"__data__": {"text": "the cost function may contain multiple\nrecursive terms. DP formulations whose cost function contains multiple recursive terms are\ncalled polyadic  formulations.\nThe dependencies between subproblems in a DP formulation can be represented by a directed\ngraph. Each node in the graph represents a subproblem. A directed edge from node i to node j\nindicates that the solution to the subproblem represented by node i is used to compute the\nsolution to the subproblem represented by node j. If the graph is acyclic, then the nodes of the\ngraph can be organized into levels such that subproblems at a particular level depend only on\nsubproblems at previous levels. In this case, the DP formulation can be categorized as follows.\nIf subproblems at all levels depend only on the results at the immediately preceding levels, the\nformulation is called a serial  DP formulation; otherwise, it is called a nonserial  DP formulation.\nBased on the preceding classification criteria, we define four classes of DP formulations: serial\nmonadic , serial polyadic , nonserial monadic , and nonserial polyadic . These classes,\nhowever, are not exhaustive; some DP formulations cannot be classified into any of these\ncategories.\nDue to the wide variety of problems solved using DP, it is difficult to develop generic parallel\nalgorithms for them. However, parallel formulations of the problems in each of the four DP\ncategories have certain similarities. In this chapter, we discuss parallel DP formulations for\nsample problems in each class. These samples suggest parallel algorithms for other problems in\nthe same class. Note, however, that not all DP problems can be parallelized as illustrated in\nthese examples.[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n12.2 Serial Monadic DP Formulations\nWe can solve many problems by using serial monadic DP formulations. This section discusses\nthe shortest-path problem for a multistage graph and the 0/1 knapsack problem. We present\nparallel algorithms for both and point out the specific properties that influence these parallel\nformulations.\n12.2.1 The Shortest-Path Problem\nConsider a weighted multistage graph of r + 1 levels, as shown in Figure 12.3. Each node at\nlevel i is connected to every node at level i + 1. Levels zero and r contain only one node, and\nevery other level contains n nodes. We refer to the node at level zero as the starting node S and\nthe node at level r as the terminating node R. The objective of this problem is to find the\nshortest path from S to R. The ith node at level l in the graph is labeled \n . The cost of an edge\nconnecting \n  to node \n  is labeled \n . The cost of reaching the goal node R from any node \nis represented by \n . If there are n nodes at level l, the vector \n  is referred to\nas Cl. The shortest-path problem reduces to computing C0. Since the graph has only one\nstarting node, \n . The structure of the graph is such that any path from \n  to R includes\na node \n  (0 \n  j \n n - 1). The cost of any such path is the sum of the cost of the edge\nbetween \n  and \n  and the cost of the shortest path between \n  and R (which is given by\n). Thus, \n , the cost of the shortest path between \n  and R, is equal to the minimum cost\nover all paths through each node in level l + 1. Therefore,\nEquation 12.2\nFigure 12.3. An example of a serial monadic DP formulation for finding\nthe shortest path in a graph whose nodes can be organized into levels.\nSince all nodes \n  have only one edge connecting them to the goal node R at level r, the cost\n is equal to \n . Hence,\nEquation 12.3\nBecause Equation 12.2  contains only one recursive term in its right-hand side, it is a monadic\nformulation. Note that the solution to a subproblem requires solutions to subproblems only at\nthe immediately", "doc_id": "cae98049-27b5-4d02-a06a-60d31f8e8b3e", "embedding": null, "doc_hash": "e3839fea2c12b177968da181ce1a51bdc7a82a290fe46c8f109b03f27d7fc0b0", "extra_info": null, "node_info": {"start": 1415554, "end": 1419292}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "486ff700-2fc7-479d-92d5-ebf1c40fa3de", "3": "fb3a9ef7-2d6e-49d9-8f30-579292ad7fe4"}}, "__type__": "1"}, "fb3a9ef7-2d6e-49d9-8f30-579292ad7fe4": {"__data__": {"text": "all paths through each node in level l + 1. Therefore,\nEquation 12.2\nFigure 12.3. An example of a serial monadic DP formulation for finding\nthe shortest path in a graph whose nodes can be organized into levels.\nSince all nodes \n  have only one edge connecting them to the goal node R at level r, the cost\n is equal to \n . Hence,\nEquation 12.3\nBecause Equation 12.2  contains only one recursive term in its right-hand side, it is a monadic\nformulation. Note that the solution to a subproblem requires solutions to subproblems only at\nthe immediately preceding level. Consequently, this is a serial monadic formulation.\nUsing this recursive formulation of the shortest-path problem, the cost of reaching the goal\nnode R from any node at level l (0 < l < r - 1) is\nNow consider the operation of multiplying a matrix with a vector. In the matrix-vector product,\nif the addition operation is replaced by minimization and the multiplication operation is replaced\nby addition, the preceding set of equations is equivalent to\nEquation 12.4\nwhere Cl and Cl+1 are n x 1 vectors representing the cost of reaching the goal node from each\nnode at levels l and l + 1, and Ml,l+1 is an n x n matrix in which entry ( i, j) stores the cost of the\nedge connecting node i at level l to node j at level l + 1. This matrix is\nThe shortest-path problem has thus been reformulated as a sequence of matrix-vector\nmultiplications. On a sequential computer, the DP formulation starts by computing Cr -1 from\nEquation 12.3 , and then computes Cr -k-1 for k = 1, 2, ..., r -2 using Equation 12.4 . Finally, C0 is\ncomputed using Equation 12.2 .\nSince there are n nodes at each level, the cost of computing each vector Cl is Q(n2). The parallel\nalgorithm for this problem can be derived using the parallel algorithms for the matrix-vector\nproduct discussed in Section 8.1. For example, Q(n) processing elements can compute each\nvector Cl in time Q(n) and solve the entire problem in time Q(rn). Recall that r is the number of\nlevels in the graph.\nMany serial monadic DP formulations with dependency graphs identical to the one considered\nhere can be parallelized using a similar parallel algorithm. For certain dependency graphs,\nhowever, this formulation is unsuitable. Consider a graph in which each node at a level can be\nreached from only a small fraction of nodes at the previous level. Then matrix Ml,l+1 contains\nmany elements with value \n. In this case, matrix M is considered to be a sparse matrix for the\nminimization and addition operations. This is because, for all x, x + \n  = \n , and min{ x, \n}\n= x. Therefore, the addition and minimization operations need not be performed for entries\nwhose value is \n . If we use a regular dense matrix-vector multiplication algorithm, the\ncomputational complexity of each matrix-vector multiplication becomes significantly higher than\nthat of the corresponding sparse matrix-vector multiplication. Consequently, we must use a\nsparse matrix-vector multiplication algorithm to compute each vector.\n12.2.2 The 0/1 Knapsack Problem\nA one-dimensional 0/1 knapsack problem is defined as follows. We are given a knapsack of\ncapacity c and a set of n objects numbered 1, 2, ..., n. Each object i has weight wi and profit pi.\nObject profits and weights are integers. Let v = [v1, v2, ..., vn] be a solution vector in which vi\n= 0 if object i is not in the knapsack, and vi = 1 if it is in the knapsack. The goal is to find a\nsubset of objects to put into the knapsack so that\n(that is,", "doc_id": "fb3a9ef7-2d6e-49d9-8f30-579292ad7fe4", "embedding": null, "doc_hash": "a9a8a711f6a4093b00e7e79567fc8bb0b2ab94e00d554229bd61f83c224997e9", "extra_info": null, "node_info": {"start": 1419387, "end": 1422883}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "cae98049-27b5-4d02-a06a-60d31f8e8b3e", "3": "b3f1ae8e-8bb5-4ade-94f0-09ce9cfb2a94"}}, "__type__": "1"}, "b3f1ae8e-8bb5-4ade-94f0-09ce9cfb2a94": {"__data__": {"text": "The 0/1 Knapsack Problem\nA one-dimensional 0/1 knapsack problem is defined as follows. We are given a knapsack of\ncapacity c and a set of n objects numbered 1, 2, ..., n. Each object i has weight wi and profit pi.\nObject profits and weights are integers. Let v = [v1, v2, ..., vn] be a solution vector in which vi\n= 0 if object i is not in the knapsack, and vi = 1 if it is in the knapsack. The goal is to find a\nsubset of objects to put into the knapsack so that\n(that is, the objects fit into the knapsack) and\n\nis maximized (that is, the profit is maximized).\nA straightforward method to solve this problem is to consider all 2n possible subsets of the n\nobjects and choose the one that fits into the knapsack and maximizes the profit. Here we\nprovide a DP formulation that is faster than the simple method when c = O (2n /n). Let F [i, x]\nbe the maximum profit for a knapsack of capacity x using only objects {1, 2, ..., i}. Then F [n,\nc] is the solution to the problem. The DP formulation for this problem is as follows:\nThis recursive equation yields a knapsack of maximum profit. When the current capacity of the\nknapsack is x, the decision to include object i can lead to one of two situations: (i) the object is\nnot included, knapsack capacity remains x , and profit is unchanged; (ii) the object is included,\nknapsack capacity becomes x - wi, and profit increases by pi. The DP algorithm decides whether\nor not to include an object based on which choice leads to maximum profit.\nThe sequential algorithm for this DP formulation maintains a table F of size n x c. The table is\nconstructed in row-major order. The algorithm first determines the maximum profit by using\nonly the first object with knapsacks of different capacities. This corresponds to filling the first\nrow of the table. Filling entries in subsequent rows requires two entries from the previous row:\none from the same column and one from the column offset by the weight of the object. Thus,\nthe computation of an arbitrary entry F [i, j] requires F [i - 1, j] and F [i - 1, j - wi]. This is\nillustrated in Figure 12.4. Computing each entry takes constant time; the sequential run time of\nthis algorithm is Q(nc).\nFigure 12.4. Computing entries of table F for the 0/1 knapsack\nproblem. The computation of entry F[i, j] requires communication with\nprocessing elements containing entries F[i - 1, j] and F[i - 1, j - wi].\n\nThis formulation is a serial monadic formulation. The subproblems F [i, x] are organized into n\nlevels for i = 1, 2, ..., n. Computation of problems in level i depends only on the subproblems at\nlevel i - 1. Hence the formulation is serial. The formulation is monadic because each of the two\nalternate solutions of F [i, x] depends on only one subproblem. Furthermore, dependencies\nbetween levels are sparse because a problem at one level depends only on two subproblems\nfrom previous level.\nConsider a parallel formulation of this algorithm on a CREW PRAM with c processing elements\nlabeled P0 to Pc-1. Processing element Pr -1 computes the rth column of matrix F. When\ncomputing F [j, r] during iteration j, processing element Pr -1 requires the values F [j - 1, r] and\nF [j - 1, r - wj]. Processing element Pr -1 can read any element of matrix F in constant time, so\ncomputing F [j, r] also requires constant time. Therefore, each iteration takes constant time.\nSince there are n iterations, the parallel run time is Q(n). The", "doc_id": "b3f1ae8e-8bb5-4ade-94f0-09ce9cfb2a94", "embedding": null, "doc_hash": "2e0b36056542477049aaff586383528c47d397348137c83b75442a33f6b7dc60", "extra_info": null, "node_info": {"start": 1422960, "end": 1426383}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "fb3a9ef7-2d6e-49d9-8f30-579292ad7fe4", "3": "dd51fa6b-d105-4b97-aab0-1b593b7399e0"}}, "__type__": "1"}, "dd51fa6b-d105-4b97-aab0-1b593b7399e0": {"__data__": {"text": "algorithm on a CREW PRAM with c processing elements\nlabeled P0 to Pc-1. Processing element Pr -1 computes the rth column of matrix F. When\ncomputing F [j, r] during iteration j, processing element Pr -1 requires the values F [j - 1, r] and\nF [j - 1, r - wj]. Processing element Pr -1 can read any element of matrix F in constant time, so\ncomputing F [j, r] also requires constant time. Therefore, each iteration takes constant time.\nSince there are n iterations, the parallel run time is Q(n). The formulation uses c processing\nelements, hence its processor-time product is Q(nc). Therefore, the algorithm is cost-optimal.\nLet us now consider its formulation on a distributed memory machine with c-processing\nelements. Table F is distributed among the processing elements so that each processing\nelement is responsible for one column. This is illustrated in Figure 12.4 . Each processing\nelement locally stores the weights and profits of all objects. In the jth iteration, for computing F\n[j, r] at processing element Pr -1, F [j - 1, r] is available locally but F [j - 1, r - wj] must be\nfetched from another processing element. This corresponds to the circular wj -shift operation\ndescribed in Section 4.6 . The time taken by this circular shift operation on p processing\nelements is bounded by ( ts + twm) log p for a message of size m on a network with adequate\nbandwidth. Since the size of the message is one word and we have p = c, this time is given by\n(ts + tw) log c. If the sum and maximization operations take time tc, then each iteration takes\ntime tc + (ts + tw) log c. Since there are n such iterations, the total time is given by O (n log c).\nThe processor-time product for this formulation is O (nc log c); therefore, the algorithm is not\ncost-optimal.\nLet us see what happens to this formulation as we increase the number of elements per\nprocessor. Using p-processing elements, each processing element computes c/p elements of the\ntable in each iteration. In the jth iteration, processing element P0 computes the values of\nelements F [j, 1], ..., F [j, c/p], processing element P1 computes values of elements F [j, c/p +\n1], ..., F [j, 2c/p], and so on. Computing the value of F [j, k], for any k, requires values F [j - 1,\nk] and F [j - 1, k - wj]. Required values of the F table can be fetched from remote processing\nelements by performing a circular shift. Depending on the values of wj and p, the required\nnonlocal values may be available from one or two processing elements. Note that the total\nnumber of words communicated via these messages is c/p irrespective of whether they come\nfrom one or two processing elements. The time for this operation is at most (2 ts + twc/p)\nassuming that c/p is large and the network has enough bandwidth ( Section 4.6). Since each\nprocessing element computes c/p such elements, the total time for each iteration is tcc/p + 2ts\n+ twc/p. Therefore, the parallel run time of the algorithm for n iterations is n(tcc/p + 2ts +\ntwc/p). In asymptotic terms, this algorithm's parallel run time is O (nc/p). Its processor-time\nproduct is O (nc), which is cost-optimal.\nThere is an upper bound on the efficiency of this formulation because the amount of data that\nneeds to be communicated is of the same order as the amount of computation. This upper\nbound is determined by the values of tw and tc (Problem 12.1).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n12.3 Nonserial Monadic DP Formulations\nThe DP algorithm for determining the longest common", "doc_id": "dd51fa6b-d105-4b97-aab0-1b593b7399e0", "embedding": null, "doc_hash": "0c0a0727b57b54dc7b45255a4333c7cf2ef9911466ea6316734e7a1f83e2d434", "extra_info": null, "node_info": {"start": 1426355, "end": 1429839}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b3f1ae8e-8bb5-4ade-94f0-09ce9cfb2a94", "3": "ba171e4e-91c8-4790-a4de-f55b34980ff6"}}, "__type__": "1"}, "ba171e4e-91c8-4790-a4de-f55b34980ff6": {"__data__": {"text": "+ 2ts +\ntwc/p). In asymptotic terms, this algorithm's parallel run time is O (nc/p). Its processor-time\nproduct is O (nc), which is cost-optimal.\nThere is an upper bound on the efficiency of this formulation because the amount of data that\nneeds to be communicated is of the same order as the amount of computation. This upper\nbound is determined by the values of tw and tc (Problem 12.1).\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n12.3 Nonserial Monadic DP Formulations\nThe DP algorithm for determining the longest common subsequence of two given sequences can\nbe formulated as a nonserial monadic DP formulation.\n12.3.1 The Longest-Common-Subsequence Problem\nGiven a sequence A = <a1, a2, ..., an>, a subsequence of A can be formed by deleting some\nentries from A. For example, < a, b, z> is a subsequence of < c, a, d, b, r, z>, but < a, c, z> and\n<a, d, l> are not. The longest-common-subsequence  (LCS) problem can be stated as\nfollows. Given two sequences A = <a1, a2, ..., an> and B = <b1, b2, ..., bm>, find the longest\nsequence that is a subsequence of both A and B. For example, if A = <c, a, d, b, r, z> and B =\n<a, s, b, z>, the longest common subsequence of A and B is <a, b, z>.\nLet F [i, j] denote the length of the longest common subsequence of the first i elements of A and\nthe first j elements of B. The objective of the LCS problem is to determine F [n, m]. The DP\nformulation for this problem expresses F [i, j] in terms of F [i - 1, j - 1], F [i, j - 1], and F [i - 1,\nj] as follows:\nGiven sequences A and B , consider two pointers pointing to the start of the sequences. If the\nentries pointed to by the two pointers are identical, then they form components of the longest\ncommon subsequence. Therefore, both pointers can be advanced to the next entry of the\nrespective sequences and the length of the longest common subsequence can be incremented\nby one. If the entries are not identical then two situations arise: the longest common\nsubsequence may be obtained from the longest subsequence of A and the sequence obtained by\nadvancing the pointer to the next entry of B; or the longest subsequence may be obtained from\nthe longest subsequence of B and the sequence obtained by advancing the pointer to the next\nentry of A. Since we want to determine the longest subsequence, the maximum of these two\nmust be selected.\nThe sequential implementation of this DP formulation computes the values in table F in row-\nmajor order. Since there is a constant amount of computation at each entry in the table, the\noverall complexity of this algorithm is Q(nm). This DP formulation is nonserial monadic, as\nillustrated in Figure 12.5(a). Treating nodes along a diagonal as belonging to one level, each\nnode depends on two subproblems at the preceding level and one subproblem two levels\nearlier. The formulation is monadic because a solution to any subproblem at a level is a function\nof only one of the solutions at preceding levels. (Note that, for the third case in Equation 12.5,\nboth F [i, j - 1] and F [i - 1, j] are possible solutions to F [i, j], and the optimal solution to F [i,\nj] is the maximum of the two.) Figure 12.5  shows that this problem has a very regular\nstructure.\nFigure 12.5. (a) Computing entries of table F for the longest-common-\nsubsequence problem. Computation proceeds along the", "doc_id": "ba171e4e-91c8-4790-a4de-f55b34980ff6", "embedding": null, "doc_hash": "129a0c5db640fdb3376c2307ca57d2e15e8964bd07f6d5f0e7a68f6da9a48fc4", "extra_info": null, "node_info": {"start": 1429821, "end": 1433131}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "dd51fa6b-d105-4b97-aab0-1b593b7399e0", "3": "2a55fc4b-2a9b-4ba8-9ddc-e57984155527"}}, "__type__": "1"}, "2a55fc4b-2a9b-4ba8-9ddc-e57984155527": {"__data__": {"text": "monadic because a solution to any subproblem at a level is a function\nof only one of the solutions at preceding levels. (Note that, for the third case in Equation 12.5,\nboth F [i, j - 1] and F [i - 1, j] are possible solutions to F [i, j], and the optimal solution to F [i,\nj] is the maximum of the two.) Figure 12.5  shows that this problem has a very regular\nstructure.\nFigure 12.5. (a) Computing entries of table F for the longest-common-\nsubsequence problem. Computation proceeds along the dotted\ndiagonal lines. (b) Mapping elements of the table to processing\nelements.\nExample 12.2 Computing LCS of two amino-acid sequences\nLet us consider the LCS of two amino-acid sequences H E A G A W G H E E  and P A\nW H E A E . For the interested reader, the names of the corresponding amino-acids\nare A: Alanine, E: Glutamic acid, G: Glycine, H: Histidine, P: Proline, and W:\nTryptophan. The table of F entries for these two sequences is shown in Figure 12.6 .\nThe LCS of the two sequences, as determined by tracing back from the maximum\nscore and enumerating all the matches, is A W H E E . \nFigure 12.6. The F table for computing the LCS of sequences H E\nA G A W G H E E and P A W H E A E .\nTo simplify the discussion, we discuss parallel formulation only for the case in which n = m.\nConsider a parallel formulation of this algorithm on a CREW PRAM with n processing elements.\nEach processing element Pi computes the ith column of table F. Table entries are computed in a\ndiagonal sweep from the top-left to the bottom-right corner. Since there are n processing\nelements, and each processing element can access any entry in table F, the elements of each\ndiagonal are computed in constant time (the diagonal can contain at most n elements). Since\nthere are 2 n - 1 such diagonals, the algorithm requires Q(n) iterations. Thus, the parallel run\ntime is Q(n). The algorithm is cost-optimal, since its Q(n2) processor-time product equals the\nsequential complexity.\nThis algorithm can be adapted to run on a logical linear array of n processing elements by\ndistributing table F among different processing elements. Note that this logical topology can be\nmapped to a variety of physical architectures using embedding techniques in Section 2.7.1 .\nProcessing element Pi stores the ( i + 1)th column of the table. Entries in table F are assigned to\nprocessing elements as illustrated in Figure 12.5(b) . When computing the value of F [i, j],\nprocessing element Pj -1 may need either the value of F [i - 1, j - 1] or the value of F [i, j - 1]\nfrom the processing element to its left. It takes time ts + tw to communicate a single word from\na neighboring processing element. To compute each entry in the table, a processing element\nneeds a single value from its immediate neighbor, followed by the actual computation, which\ntakes time tc. Since each processing element computes a single entry on the diagonal, each\niteration takes time ( ts + tw + tc). The algorithm makes (2 n - 1) diagonal sweeps (iterations)\nacross the table; thus, the total parallel run time is\nSince the sequential run time is n2tc, the efficiency of this algorithm is\n\nA careful examination of this expression reveals that it is not possible to obtain efficiencies\nabove a certain threshold. To compute this threshold, assume it is possible to communicate\nvalues between processing elements instantaneously; that is, ts = tw = 0. In this case, the\nefficiency of the parallel algorithm is\nEquation", "doc_id": "2a55fc4b-2a9b-4ba8-9ddc-e57984155527", "embedding": null, "doc_hash": "3ed34c1ec8fe2dc794b20762d830eb8bf1135d5df8357e87cc0ff5f2b8e43a6d", "extra_info": null, "node_info": {"start": 1433154, "end": 1436613}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ba171e4e-91c8-4790-a4de-f55b34980ff6", "3": "f94941f6-02fe-4e7f-96a2-ca9fba1a8258"}}, "__type__": "1"}, "f94941f6-02fe-4e7f-96a2-ca9fba1a8258": {"__data__": {"text": "diagonal, each\niteration takes time ( ts + tw + tc). The algorithm makes (2 n - 1) diagonal sweeps (iterations)\nacross the table; thus, the total parallel run time is\nSince the sequential run time is n2tc, the efficiency of this algorithm is\n\nA careful examination of this expression reveals that it is not possible to obtain efficiencies\nabove a certain threshold. To compute this threshold, assume it is possible to communicate\nvalues between processing elements instantaneously; that is, ts = tw = 0. In this case, the\nefficiency of the parallel algorithm is\nEquation 12.5\nThus, the efficiency is bounded above by 0.5. This upper bound holds even if multiple columns\nare mapped to a processing element. Higher efficiencies are possible using alternate mappings\n(Problem 12.3).\nNote that the basic characteristic that allows efficient parallel formulations of this algorithm is\nthat table F can be partitioned so computing each element requires data only from neighboring\nprocessing elements. In other words, the algorithm exhibits locality of data access.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n12.4 Serial Polyadic DP Formulations\nFloyd's algorithm for determining the shortest paths between all pairs of nodes in a graph can\nbe reformulated as a serial polyadic DP formulation.\n12.4.1 Floyd's All-Pairs Shortest-Paths Algorithm\nConsider a weighted graph G, which consists of a set of nodes V and a set of edges E. An edge\nfrom node i to node j in E has a weight ci, j. Floyd's algorithm determines the cost di, j of the\nshortest path between each pair of nodes ( i, j) in V (Section 10.4.2 ). The cost of a path is the\nsum of the weights of the edges in the path.\nLet \n  be the minimum cost of a path from node i to node j, using only nodes v0, v1, ..., vk-1.\nThe functional equation of the DP formulation for this problem is\nEquation 12.6\nSince \n  is the shortest path from node i to node j using all n nodes, it is also the cost of the\noverall shortest path between nodes i and j . The sequential formulation of this algorithm\nrequires n iterations, and each iteration requires time Q(n2). Thus, the overall run time of the\nsequential algorithm is Q(n3).\nEquation 12.6  is a serial polyadic formulation. Nodes \n  can be partitioned into n levels, one\nfor each value of k. Elements at level k + 1 depend only on elements at level k. Hence, the\nformulation is serial. The formulation is polyadic since one of the solutions to \n  requires a\ncomposition of solutions to two subproblems \n  and \n  from the previous level.\nFurthermore, the dependencies between levels are sparse because the computation of each\nelement in \n  requires only three results from the preceding level (out of n2).\nA simple CREW PRAM formulation of this algorithm uses n2 processing elements. Processing\nelements are organized into a logical two-dimensional array in which processing element Pi,j\ncomputes the value of \n  for k = 1, 2, ..., n. In each iteration k, processing element Pi,j\nrequires the values \n , \n , and \n . Given these values, it computes the value of \n  in\nconstant time. Therefore, the PRAM formulation has a parallel run time of Q(n). This formulation\nis cost-optimal because its processor-time product is the same as the sequential run time of\nQ(n3). This algorithm can be adapted to various practical architectures to yield efficient parallel\nformulations ( Section 10.4.2 ).\nAs with serial monadic formulations, data locality is of prime importance in serial polyadic\nformulations since many such formulations have sparse connectivity between levels.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n", "doc_id": "f94941f6-02fe-4e7f-96a2-ca9fba1a8258", "embedding": null, "doc_hash": "d83ee28dd23f8f2ea92381064bdb688ada520e201112797a8a5063f89d32fbac", "extra_info": null, "node_info": {"start": 1436538, "end": 1440114}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2a55fc4b-2a9b-4ba8-9ddc-e57984155527", "3": "33d11e6f-f405-4ec7-a187-5c636f06fd1c"}}, "__type__": "1"}, "33d11e6f-f405-4ec7-a187-5c636f06fd1c": {"__data__": {"text": "values, it computes the value of \n  in\nconstant time. Therefore, the PRAM formulation has a parallel run time of Q(n). This formulation\nis cost-optimal because its processor-time product is the same as the sequential run time of\nQ(n3). This algorithm can be adapted to various practical architectures to yield efficient parallel\nformulations ( Section 10.4.2 ).\nAs with serial monadic formulations, data locality is of prime importance in serial polyadic\nformulations since many such formulations have sparse connectivity between levels.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n12.5 Nonserial Polyadic DP Formulations\nIn nonserial polyadic DP formulations, in addition to processing subproblems at a level in\nparallel, computation can also be pipelined to increase efficiency. We illustrate this with the\noptimal matrix-parenthesization problem.\n12.5.1 The Optimal Matrix-Parenthesization Problem\nConsider the problem of multiplying n matrices, A1, A2, ..., An, where each Ai is a matrix with ri -\n1 rows and ri columns. The order in which the matrices are multiplied has a significant impact\non the total number of operations required to evaluate the product.\nExample 12.3 Optimal matrix parenthesization\nConsider three matrices A1, A2, and A3 of dimensions 10 x 20, 20 x 30, and 30 x 40,\nrespectively. The product of these matrices can be computed as ( A1 x A2) x A3 or as A1\nx (A2 x A3). In ( A1 x A2) x A3, computing ( A1 x A2) requires 10 x 20 x 30 operations\nand yields a matrix of dimensions 10 x 30. Multiplying this by A3 requires 10 x 30 x 40\nadditional operations. Therefore the total number of operations is 10 x 20 x 30 + 10 x\n30 x 40 = 18000. Similarly, computing A1 x (A2 x A3) requires 20 x 30 x 40 + 10 x 20\nx 40 = 32000 operations. Clearly, the first parenthesization is desirable. \nThe objective of the parenthesization problem is to determine a parenthesization that minimizes\nthe number of operations. Enumerating all possible parenthesizations is not feasible since there\nare exponentially many of them.\nLet C [i, j] be the optimal cost of multiplying the matrices Ai ,..., Aj. This chain of matrices can\nbe expressed as a product of two smaller chains, Ai, Ai +1,..., Ak and Ak+1 ,..., Aj. The chain Ai,\nAi +1 ,..., Ak results in a matrix of dimensions ri -1 x rk, and the chain Ak+1 ,..., Aj results in a\nmatrix of dimensions rk x rj. The cost of multiplying these two matrices is ri -1rk rj. Hence, the\ncost of the parenthesization ( Ai, Ai +1 ,..., Ak)( Ak+1 ,..., Aj) is given by C [i, k] + C [k + 1, j] +\nri -1rk rj. This gives rise to the following recurrence relation for the parenthesization problem:\nEquation 12.7\nGiven Equation 12.7 , the problem reduces to finding the value of C [1, n]. The composition of\ncosts of matrix chains is shown in Figure 12.7 . Equation 12.7  can be solved if we use a bottom-\nup approach for constructing the table C that stores the values C [i, j]. The algorithm fills table\nC in an order corresponding to solving the parenthesization problem on matrix chains of\nincreasing length. Visualize this by thinking of filling in the table diagonally ( Figure 12.8 ).\nEntries in diagonal l corresponds to the cost of multiplying matrix chains of length l + 1. From\nEquation 12.7 , we can see that the value of C [i, j] is computed as min{", "doc_id": "33d11e6f-f405-4ec7-a187-5c636f06fd1c", "embedding": null, "doc_hash": "a6ac3d769741e618f97e836e152fe14d708022ef909e385d3132d89175152d73", "extra_info": null, "node_info": {"start": 1440113, "end": 1443401}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f94941f6-02fe-4e7f-96a2-ca9fba1a8258", "3": "ba3ad208-d7e5-4540-aae0-f5683f114fcd"}}, "__type__": "1"}, "ba3ad208-d7e5-4540-aae0-f5683f114fcd": {"__data__": {"text": "chains is shown in Figure 12.7 . Equation 12.7  can be solved if we use a bottom-\nup approach for constructing the table C that stores the values C [i, j]. The algorithm fills table\nC in an order corresponding to solving the parenthesization problem on matrix chains of\nincreasing length. Visualize this by thinking of filling in the table diagonally ( Figure 12.8 ).\nEntries in diagonal l corresponds to the cost of multiplying matrix chains of length l + 1. From\nEquation 12.7 , we can see that the value of C [i, j] is computed as min{ C [i, k]+C [k +1, j]+ri -\n1rk rj}, where k can take values from i to j -1. Therefore, computing C [i, j] requires that we\nevaluate ( j - i) terms and select their minimum. The computation of each term takes time tc,\nand the computation of C [i, j] takes time ( j - i)tc. Thus, each entry in diagonal l can be\ncomputed in time ltc.\nFigure 12.7. A nonserial polyadic DP formulation for finding an optimal\nmatrix parenthesization for a chain of four matrices. A square node\nrepresents the optimal cost of multiplying a matrix chain. A circle node\nrepresents a possible parenthesization.\nFigure 12.8. The diagonal order of computation for the optimal matrix-\nparenthesization problem.\nIn computing the cost of the optimal parenthesization sequence, the algorithm computes ( n - 1)\nchains of length two. This takes time ( n - 1)tc. Similarly, computing ( n - 2) chains of length\nthree takes time ( n - 2)2 tc. In the final step, the algorithm computes one chain of length n. This\ntakes time ( n - 1)tc. Thus, the sequential run time of this algorithm is\nEquation 12.8\nThe sequential complexity of the algorithm is Q(n3).\nConsider the parallel formulation of this algorithm on a logical ring of n processing elements. In\nstep l, each processing element computes a single element belonging to the lth diagonal.\nProcessing element Pi computes the ( i + 1)th column of Table C. Figure 12.8  illustrates the\npartitioning of the table among different processing elements. After computing the assigned\nvalue of the element in table C, each processing element sends its value to all other processing\nelements using an all-to-all broadcast ( Section 4.2 ). Therefore, the assigned value in the next\niteration can be computed locally. Computing an entry in table C during iteration l takes time ltc\nbecause it corresponds to the cost of multiplying a chain of length l + 1. An all-to-all broadcast\nof a single word on n processing elements takes time ts log n + tw(n - 1) ( Section 4.2 ). The total\ntime required to compute the entries along diagonal l is ltc + ts log n + tw(n - 1). The parallel\nrun time is the sum of the time taken over computation of n - 1 diagonals.\nThe parallel run time of this algorithm is Q(n2). Since the processor-time product is Q(n3), which\nis the same as the sequential complexity, this algorithm is cost-optimal.\nWhen using p processing elements (1 \n  p \n n) organized in a logical ring, if there are n\nnodes in a diagonal, each processing element stores n/p nodes. Each processing element\ncomputes the cost C [i, j] of the entries assigned to it. After computation, an all-to-all broadcast\nsends the solution costs of the subproblems for the most recently computed diagonal to all the\nother processing elements. Because each processing element has complete information about\nsubproblem costs at preceding diagonals, no other communication is required. The time taken\nfor all-to-all broadcast of n/p words is ts log p +", "doc_id": "ba3ad208-d7e5-4540-aae0-f5683f114fcd", "embedding": null, "doc_hash": "8727652e2876633016200990500e7b147f4818eb6f3cef8b43d65c9b96ca978f", "extra_info": null, "node_info": {"start": 1443444, "end": 1446919}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "33d11e6f-f405-4ec7-a187-5c636f06fd1c", "3": "85b733f7-a307-4ebb-859d-24db7a04601a"}}, "__type__": "1"}, "85b733f7-a307-4ebb-859d-24db7a04601a": {"__data__": {"text": "p \n n) organized in a logical ring, if there are n\nnodes in a diagonal, each processing element stores n/p nodes. Each processing element\ncomputes the cost C [i, j] of the entries assigned to it. After computation, an all-to-all broadcast\nsends the solution costs of the subproblems for the most recently computed diagonal to all the\nother processing elements. Because each processing element has complete information about\nsubproblem costs at preceding diagonals, no other communication is required. The time taken\nfor all-to-all broadcast of n/p words is ts log p + twn(p - 1)/ p \n ts log p + twn. The time to\ncompute n/p entries of the table in the lth diagonal is ltcn/p. The parallel run time is\nIn order terms, TP = Q(n3/p) + Q(n2). Here, Q(n3/p) is the computation time, and Q(n2) the\ncommunication time. If n is sufficiently large with respect to p, communication time can be\nmade an arbitrarily small fraction of computation time, yielding linear speedup.\nThis formulation can use at most Q(n) processing elements to accomplish the task in time Q(n2).\nThis time can be improved by pipelining the computation of the cost C [i, j] on n(n + 1)/2\nprocessing elements. Each processing element computes a single entry c(i, j) of matrix C.\nPipelining works due to the nonserial nature of the problem. Computation of an entry on a\ndiagonal t does not depend only on the entries on diagonal t - 1 but also on all the earlier\ndiagonals. Hence work on diagonal t can start even before work on diagonal t - 1 is completed.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n12.6 Summary and Discussion\nThis chapter provides a framework for deriving parallel algorithms that use dynamic\nprogramming. It identifies possible sources of parallelism, and indicates under what conditions\nthey can be utilized effectively.\nBy representing computation as a graph, we identify three sources of parallelism. First, the\ncomputation of the cost of a single subproblem (a node in a level) can be parallelized. For\nexample, for computing the shortest path in the multistage graph shown in Figure 12.3, node\ncomputation can be parallelized because the complexity of node computation is itself Q(n). For\nmany problems, however, node computation complexity is lower, limiting available parallelism.\nSecond, subproblems at each level can be solved in parallel. This provides a viable method for\nextracting parallelism from a large class of problems (including all the problems in this\nchapter).\nThe first two sources of parallelism are available to both serial and nonserial formulations.\nNonserial formulations allow a third source of parallelism: pipelining of computations among\ndifferent levels. Pipelining makes it possible to start solving a problem as soon as the\nsubproblems it depends on are solved. This form of parallelism is used in the parenthesization\nproblem.\nNote that pipelining was also applied to the parallel formulation of Floyd's all-pairs shortest-\npaths algorithm in Section 10.4.2. As discussed in Section 12.4 , this algorithm corresponds to a\nserial DP formulation. The nature of pipelining in this algorithm is different from the one in\nnonserial DP formulation. In the pipelined version of Floyd's algorithm, computation in a stage\nis pipelined with the communication among earlier stages. If communication cost is zero (as in\na PRAM), then Floyd's algorithm does not benefit from pipelining.\nThroughout the chapter, we have seen the importance of data locality. If the solution to a\nproblem requires results from other subproblems, the cost of communicating those results must\nbe", "doc_id": "85b733f7-a307-4ebb-859d-24db7a04601a", "embedding": null, "doc_hash": "38f98c7fe5ffaf6be0bd21c1c6e841fc00b2d555531910f38e972e86e93b49c8", "extra_info": null, "node_info": {"start": 1446882, "end": 1450452}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ba3ad208-d7e5-4540-aae0-f5683f114fcd", "3": "074bfe58-c21c-4f17-88c1-6349f60f03bf"}}, "__type__": "1"}, "074bfe58-c21c-4f17-88c1-6349f60f03bf": {"__data__": {"text": "As discussed in Section 12.4 , this algorithm corresponds to a\nserial DP formulation. The nature of pipelining in this algorithm is different from the one in\nnonserial DP formulation. In the pipelined version of Floyd's algorithm, computation in a stage\nis pipelined with the communication among earlier stages. If communication cost is zero (as in\na PRAM), then Floyd's algorithm does not benefit from pipelining.\nThroughout the chapter, we have seen the importance of data locality. If the solution to a\nproblem requires results from other subproblems, the cost of communicating those results must\nbe less than the cost of solving the problem. In some problems (the 0/1 knapsack problem, for\nexample) the degree of locality is much smaller than in other problems such as the longest-\ncommon-subsequence problem and Floyd's all-pairs shortest-paths algorithm.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n12.7 Bibliographic Remarks\nDynamic programming was originally presented by Bellman [ Bel57 ] for solving multistage\ndecision problems. Various formal models have since been developed for DP [ KH67 , MM73 ,\nKK88b ]. Several textbooks and articles present sequential DP formulations of the longest-\ncommon-subsequence problem, the matrix chain multiplication problem, the 0/1 knapsack\nproblem, and the shortest-path problem [ CLR90 , HS78 , PS82, Bro79 ].\nLi and Wah [ LW85 , WL88 ] show that monadic serial DP formulations can be solved in parallel\non systolic arrays as matrix-vector products. They further present a more concurrent but non-\ncost-optimal formulation by formulating the problem as a matrix-matrix product. Ranka and\nSahni [ RS90b] present a polyadic serial formulation for the string editing problem and derive a\nparallel formulation based on a checkerboard partitioning.\nThe DP formulation of a large class of optimization problems is similar to that of the optimal\nmatrix-parenthesization problem. Some examples of these problems are optimal\ntriangularization of polygons, optimal binary search trees [ CLR90], and CYK parsing [ AU72 ].\nThe serial complexity of the standard DP formulation for all these problems is Q(n3). Several\nparallel formulations have been proposed by Ibarra et al. [IPS91 ] that use Q(n) processing\nelements on a hypercube and that solve the problem in time Q(n2). Guibas, Kung, and\nThompson [ GKT79 ] present a systolic algorithm that uses Q(n2) processing cells and solves the\nproblem in time Q(n). Karypis and Kumar [ KK93 ] analyze three distinct mappings of the systolic\nalgorithm presented by Guibas et al. [GKT79 ] and experimentally evaluate them by using the\nmatrix-multiplication parenthesization problem. They show that a straightforward mapping of\nthis algorithm to a mesh architecture has an upper bound on efficiency of 1/12. They also\npresent a better mapping without this drawback, and show near-linear speedup on a mesh\nembedded into a 256-processor hypercube for the optimal matrix-parenthesization problem.\nMany faster parallel algorithms for solving the parenthesization problem have been proposed,\nbut they are not cost-optimal and are applicable only to theoretical models such as the PRAM.\nFor example, a generalized method for parallelizing such programs is described by Valiant et al.\n[VSBR83] that leads directly to formulations that run in time O (log2 n) on O (n9) processing\nelements. Rytter [ Ryt88 ] uses the parallel pebble game on trees to reduce the number of\nprocessing elements to O (n6/log n) for a CREW PRAM and O (n6) for a hypercube, yet solves\nthis problem in time O (log2 n).", "doc_id": "074bfe58-c21c-4f17-88c1-6349f60f03bf", "embedding": null, "doc_hash": "4135e2265be65d25dfe542b808a5fd916d941c808220fc1ae6db4479adf22fa9", "extra_info": null, "node_info": {"start": 1450420, "end": 1453973}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "85b733f7-a307-4ebb-859d-24db7a04601a", "3": "fc0d252e-78ee-41f4-825e-846d83f7aad1"}}, "__type__": "1"}, "fc0d252e-78ee-41f4-825e-846d83f7aad1": {"__data__": {"text": "are not cost-optimal and are applicable only to theoretical models such as the PRAM.\nFor example, a generalized method for parallelizing such programs is described by Valiant et al.\n[VSBR83] that leads directly to formulations that run in time O (log2 n) on O (n9) processing\nelements. Rytter [ Ryt88 ] uses the parallel pebble game on trees to reduce the number of\nprocessing elements to O (n6/log n) for a CREW PRAM and O (n6) for a hypercube, yet solves\nthis problem in time O (log2 n). Huang et al. [HLV90 ] present a similar algorithm for CREW\nPRAM models that run in time O (\n log n) on O (n3.5 log n) processing elements. DeMello et\nal. [DCG90 ] use vectorized formulations of DP for the Cray to solve optimal control problems.\nAs we have seen, the serial polyadic formulation of the 0/1 knapsack problem is difficult to\nparallelize due to lack of communication locality. Lee et al. [LSS88] use specific characteristics\nof the knapsack problem and derive a divide-and-conquer strategy for parallelizing the DP\nalgorithm for the 0/1 knapsack problem on a MIMD message-passing computer (Problem 12.2).\nLee et al. demonstrate experimentally that it is possible to obtain linear speedup for large\ninstances of the problem on a hypercube.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n12.1 Consider the parallel algorithm for solving the 0/1 knapsack problem in Section\n12.2.2 . Derive the speedup and efficiency for this algorithm. Show that the efficiency of\nthis algorithm cannot be increased beyond a certain value by increasing the problem size\nfor a fixed number of processing elements. What is the upper bound on efficiency for this\nformulation as a function of tw and tc?\n12.2 [ LSS88] In the parallel formulation of the 0/1 knapsack problem presented in\nSection 12.2.2 , the degree of concurrency is proportional to c, the knapsack capacity. Also\nthis algorithm has limited data locality, as the amount of data to be communicated is of\nthe same order of magnitude as the computation at each processing element. Lee et al.\npresent another formulation in which the degree of concurrency is proportional to n, the\nnumber of weights. This formulation also has much more data locality. In this formulation,\nthe set of weights is partitioned among processing elements. Each processing element\ncomputes the maximum profit it can achieve from its local weights for knapsacks of\nvarious sizes up to c. This information is expressed as lists that are merged to yield the\nglobal solution. Compute the parallel run time, speedup, and efficiency of this formulation.\nCompare the performance of this algorithm with that in Section 12.2.2.\n12.3 We noticed that the parallel formulation of the longest-common-subsequence\nproblem has an upper bound of 0.5 on its efficiency. It is possible to use an alternate\nmapping to achieve higher efficiency for this problem. Derive a formulation that does not\nsuffer from this upper bound, and give the run time of this formulation.\nHint:  Consider the blocked-cyclic mapping discussed in Section 3.4.1.\n12.4 [ HS78 ] The traveling salesman problem (TSP) is defined as follows: Given a set of\ncities and the distance between each pair of cities, determine a tour through all cities of\nminimum length. A tour of all cities is a trip visiting each city once and returning to the\nstarting point. Its length is the sum of distances traveled. This problem can be solved\nusing a DP formulation. View the cities as vertices in a graph G(V, E). Let the set of cities V\nbe represented by { v1, v2, ..., vn} and let", "doc_id": "fc0d252e-78ee-41f4-825e-846d83f7aad1", "embedding": null, "doc_hash": "9f9b8b8132c13b18cb4fd8b737de6d141fc6f629d0620a38eb3c126becd0ae99", "extra_info": null, "node_info": {"start": 1454081, "end": 1457616}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "074bfe58-c21c-4f17-88c1-6349f60f03bf", "3": "58630ecd-040a-4d41-adc7-c96f5ed68f2e"}}, "__type__": "1"}, "58630ecd-040a-4d41-adc7-c96f5ed68f2e": {"__data__": {"text": "[ HS78 ] The traveling salesman problem (TSP) is defined as follows: Given a set of\ncities and the distance between each pair of cities, determine a tour through all cities of\nminimum length. A tour of all cities is a trip visiting each city once and returning to the\nstarting point. Its length is the sum of distances traveled. This problem can be solved\nusing a DP formulation. View the cities as vertices in a graph G(V, E). Let the set of cities V\nbe represented by { v1, v2, ..., vn} and let S \n {v2, v3, ..., vn}. Furthermore, let ci,j be the\ndistance between cities i and j. If f (S, k) represents the cost of starting at city v1, passing\nthrough all the cities in set S, and terminating in city k, then the following recursive\nequations can be used to compute f (S, k):\nEquation 12.9\nBased on Equation 12.9 , derive a parallel formulation. Compute the parallel run time and\nthe speedup. Is this parallel formulation cost-optimal?\n12.5 [ HS78 ] Consider the problem of merging two sorted files containing O (n) and O (m)\nrecords. These files can be merged into a sorted file in time O (m + n). Given r such files,\nthe problem of merging them into a single file can be formulated as a sequence of merge\noperations performed on pairs of files. The overall cost of the merge operation is a function\nof the sequence in which they are merged. The optimal merge order can be formulated as\na greedy problem and its parallel formulations derived using principles illustrated in this\nchapter.\nWrite down the recursive equations for this problem. Derive a parallel formulation for\nmerging files using p processing elements. Compute the parallel run time and speedup,\nand determine whether your parallel formulation is cost-optimal.\n12.6 [ HS78] Consider the problem of designing a fault-tolerant circuit containing n\ndevices connected in series, as shown in Figure 12.9(a) . If the probability of failure of each\nof these devices is given by fi, the overall probability of failure of the circuit is given by\n. Here, \n  represents a product of specified terms. The reliability of this circuit\ncan be improved by connecting multiple functional devices in parallel at each stage, as\nshown in Figure 12.9(b) . If stage i in the circuit has ri duplicate functional units, each with\na probability of failure given by fi , then the overall probability of failure of this stage is\nreduced to \n  and the overall probability of failure of the circuit is given by \n. In general, for physical reasons, the probability of failure at a particular level may not be\n, but some function \n i (ri, mi). The objective of the problem is to minimize the overall\nprobability of failure of the circuit, \n .\nFigure 12.9. (a) n devices connected in a series within a circuit.\n(b) Each stage in the circuit now has mi functional units. There are\nn such stages connected in the series.\nConstruction cost adds a new dimension to this problem. If each of the functional units\nused at stage i costs ci then due to cost constraints, the overall cost \n  should be less\nthan a fixed quantity c.\nThe problem can be formally defined as\n\nwhere mi > 0 and 0 < i \n n.\nLet fi (x) represent the reliability of a system with i stages of cost x. The optimal solution is\ngiven by fn (c). The recursive equation for fi (x) is as follows:\nEquation 12.10\nClassify this formulation into one of the four DP categories, and derive a parallel\nformulation for this algorithm. Determine its parallel run time, speedup, and isoefficiency\nfunction.\n12.7 [ CLR90] Consider", "doc_id": "58630ecd-040a-4d41-adc7-c96f5ed68f2e", "embedding": null, "doc_hash": "24079c542eceb696d1d81dd6ded7a99914f80dd258ea0b3b1a1ba2a9fa93273d", "extra_info": null, "node_info": {"start": 1457617, "end": 1461130}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "fc0d252e-78ee-41f4-825e-846d83f7aad1", "3": "080e157d-637b-404b-b3ba-47366fd7685e"}}, "__type__": "1"}, "080e157d-637b-404b-b3ba-47366fd7685e": {"__data__": {"text": "cost \n  should be less\nthan a fixed quantity c.\nThe problem can be formally defined as\n\nwhere mi > 0 and 0 < i \n n.\nLet fi (x) represent the reliability of a system with i stages of cost x. The optimal solution is\ngiven by fn (c). The recursive equation for fi (x) is as follows:\nEquation 12.10\nClassify this formulation into one of the four DP categories, and derive a parallel\nformulation for this algorithm. Determine its parallel run time, speedup, and isoefficiency\nfunction.\n12.7 [ CLR90] Consider the simplified optimal polygon-triangulation problem. This\nproblem can be defined as follows. Given a simple polygon, break the polygon into a set of\ntriangles by connecting nodes of the polygon with chords. This process is illustrated in\nFigure 12.10. The cost of constructing a triangle with nodes vi, vj, and vk is defined by a\nfunction f (vi, vj, vk). For this problem, let the cost be the total length of the edges of the\ntriangle (using Euclidean distance). The optimal polygon-triangulation problem breaks up\na polygon into a set of triangles such that the total length of each triangle (the sum of the\nindividual lengths) is minimized. Give a DP formulation for this problem. Classify it into\none of the four categories and derive a parallel formulation for p processing elements.\nDetermine its parallel run time, speedup, and isoefficiency function.\nFigure 12.10. Two possible triangulations of a regular polygon.\nHint:  This problem is similar to the optimal matrix-parenthesization problem.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nChapter 13. Fast Fourier Transform\nThe discrete Fourier transform (DFT) plays an important role in many scientific and technical\napplications, including time series and waveform analysis, solutions to linear partial differential\nequations, convolution, digital signal processing, and image filtering. The DFT is a linear\ntransformation that maps n regularly sampled points from a cycle of a periodic signal, like a\nsine wave, onto an equal number of points representing the frequency spectrum of the signal.\nIn 1965, Cooley and Tukey devised an algorithm to compute the DFT of an n-point series in Q(n\nlog n) operations. Their new algorithm was a significant improvement over previously known\nmethods for computing the DFT, which required Q(n2) operations. The revolutionary algorithm\nby Cooley and Tukey and its variations are referred to as the fast Fourier transform  (FFT).\nDue to its wide application in scientific and engineering fields, there has been a lot of interest in\nimplementing FFT on parallel computers.\nSeveral different forms of the FFT algorithm exist. This chapter discusses its simplest form, the\none-dimensional, unordered, radix-2 FFT. Parallel formulations of higher-radix and\nmultidimensional FFTs are similar to the simple algorithm discussed in this chapter because the\nunderlying ideas behind all sequential FFT algorithms are the same. An ordered FFT is obtained\nby performing bit reversal ( Section 13.4) on the output sequence of an unordered FFT. Bit\nreversal does not affect the overall complexity of a parallel implementation of FFT.\nIn this chapter we discuss two parallel formulations of the basic algorithm: the binary-\nexchange algorithm  and the transpose algorithm . Depending on the size of the input n, the\nnumber of processes p, and the memory or network bandwidth, one of these may run faster\nthan the other.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n \n13.1 The Serial Algorithm\nConsider a sequence X = <X [0], X [1], ..., X [n - 1]> of length n . The discrete Fourier\ntransform of the sequence X is the sequence Y = < Y", "doc_id": "080e157d-637b-404b-b3ba-47366fd7685e", "embedding": null, "doc_hash": "746a09ef1d172e6a67b9c73c48988173e2ae7b0d6a88f84ecb83b1246d115caf", "extra_info": null, "node_info": {"start": 1461117, "end": 1464708}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "58630ecd-040a-4d41-adc7-c96f5ed68f2e", "3": "e78a2689-7a37-4bda-96ac-21156028839d"}}, "__type__": "1"}, "e78a2689-7a37-4bda-96ac-21156028839d": {"__data__": {"text": "parallel implementation of FFT.\nIn this chapter we discuss two parallel formulations of the basic algorithm: the binary-\nexchange algorithm  and the transpose algorithm . Depending on the size of the input n, the\nnumber of processes p, and the memory or network bandwidth, one of these may run faster\nthan the other.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n \n13.1 The Serial Algorithm\nConsider a sequence X = <X [0], X [1], ..., X [n - 1]> of length n . The discrete Fourier\ntransform of the sequence X is the sequence Y = < Y [0], Y [1], ..., Y [n - 1]>, where\nEquation 13.1\nIn Equation 13.1  , w is the primitive n th root of unity in the complex plane; that is,\n, where e is the base of natural logarithms. More generally, the powers of w in the\nequation can be thought of as elements of the finite commutative ring of integers modulo n .\nThe powers of w used in an FFT computation are also known as twiddle factors  .\nThe computation of each Y [i ] according to Equation 13.1  requires n complex multiplications.\nTherefore, the sequential complexity of computing the entire sequence Y of length n is Q (n 2 ).\nThe fast Fourier transform algorithm described below reduces this complexity to Q (n log n ).\nAssume that n is a power of two. The FFT algorithm is based on the following step that permits\nan n -point DFT computation to be split into two ( n /2)-point DFT computations:\nEquation 13.2\nLet \n ; that is, \n  is the primitive ( n /2)th root of unity. Then, we can rewrite\nEquation 13.2  as follows:\nEquation 13.3\n\nIn Equation 13.3  , each of the two summations on the right-hand side is an ( n /2)-point DFT\ncomputation. If n is a power of two, each of these DFT computations can be divided similarly\ninto smaller computations in a recursive manner. This leads to the recursive FFT algorithm\ngiven in Algorithm 13.1  . This FFT algorithm is called the radix-2 algorithm because at each\nlevel of recursion, the input sequence is split into two equal halves.\nAlgorithm 13.1 The recursive, one-dimensional, unordered, radix-2\nFFT algorithm. Here \n .\n1.   procedure  R_FFT(X, Y, n, w) \n2.   if (n = 1) then Y[0] := X[0] else \n3.   begin \n4.      R_FFT(< X[0], X[2], ..., X[n - 2]>, < Q[0], Q[1], ..., Q[n/2]>, n/2, w2); \n5.      R_FFT(< X[1], X[3], ..., X[n - 1]>, < T[0], T[1], ..., T[n/2]>, n/2, w2); \n6.      for i := 0 to n - 1 do \n7.          Y[i] :=Q[i mod (n/2)] + wi T[i mod (n/2)]; \n8.   end R_FFT \nFigure 13.1  illustrates how the recursive algorithm works on an 8-point sequence. As the figure\nshows, the first set of computations corresponding to line 7 of Algorithm 13.1  takes place at the\ndeepest level of recursion. At this level, the elements of the sequence whose indices differ by n\n/2 are used in the computation. In each subsequent level, the difference between the indices of\nthe elements used together in a computation decreases by a factor of two. The figure also\nshows the powers of w used in", "doc_id": "e78a2689-7a37-4bda-96ac-21156028839d", "embedding": null, "doc_hash": "57e866f2b73ee5f2dab2a6b936262fa03fe05b4f0280ffd6f4d48daa965aa532", "extra_info": null, "node_info": {"start": 1464706, "end": 1467618}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "080e157d-637b-404b-b3ba-47366fd7685e", "3": "9c31644c-c95c-4af8-bba8-eab570ba39ac"}}, "__type__": "1"}, "9c31644c-c95c-4af8-bba8-eab570ba39ac": {"__data__": {"text": "\n8.   end R_FFT \nFigure 13.1  illustrates how the recursive algorithm works on an 8-point sequence. As the figure\nshows, the first set of computations corresponding to line 7 of Algorithm 13.1  takes place at the\ndeepest level of recursion. At this level, the elements of the sequence whose indices differ by n\n/2 are used in the computation. In each subsequent level, the difference between the indices of\nthe elements used together in a computation decreases by a factor of two. The figure also\nshows the powers of w used in each computation.\nFigure 13.1. A recursive 8-point unordered FFT computation.\nThe size of the input sequence over which an FFT is computed recursively decreases by a factor\nof two at each level of recursion (lines 4 and 5 of Algorithm 13.1  ). Hence, the maximum\nnumber of levels of recursion is log n for an initial sequence of length n . At the m th level of\nrecursion, 2m FFTs of size n /2m each are computed. Thus, the total number of arithmetic\noperations (line 7) at each level is Q (n ) and the overall sequential complexity of Algorithm\n13.1 is Q (n log n ).\nThe serial FFT algorithm can also be cast in an iterative form. The parallel implementations of\nthe iterative form are easier to illustrate. Therefore, before describing parallel FFT algorithms,\nwe give the iterative form of the serial algorithm. An iterative FFT algorithm is derived by\ncasting each level of recursion, starting with the deepest level, as an iteration. Algorithm 13.2\ngives the classic iterative Cooley-Tukey algorithm for an n -point, one-dimensional, unordered,\nradix-2 FFT. The program performs log n iterations of the outer loop starting on line 5. The\nvalue of the loop index m in the iterative version of the algorithm corresponds to the (log n - m\n)th level of recursion in the recursive version (Figure 13.1  ). Just as in each level of recursion,\neach iteration performs n complex multiplications and additions.\nAlgorithm 13.2  has two main loops. The outer loop starting at line 5 is executed log n times for\nan n -point FFT, and the inner loop starting at line 8 is executed n times during each iteration of\nthe outer loop. All operations of the inner loop are constant-time arithmetic operations. Thus,\nthe sequential time complexity of the algorithm is Q (n l og n ). In every iteration of the outer\nloop, the sequence R is updated using the elements that were stored in the sequence S during\nthe previous iteration. For the first iteration, the input sequence X serves as the initial sequence\nR . The updated sequence X from the final iteration is the desired Fourier transform and is\ncopied to the output sequence Y .\nAlgorithm 13.2 The Cooley-Tukey algorithm for one-dimensional,\nunordered, radix-2 FFT. Here \n.\n1.   procedure  ITERATIVE_FFT( X, Y, n) \n2.   begin \n3.      r := log n; \n4.      for i := 0 to n - 1 do R[i] := X[i]; \n5.      for m := 0 to r - 1 do        /* Outer loop */ \n6.          begin \n7.             for i := 0 to n - 1 do S[i]:= R[i]; \n8.             for i := 0 to n - 1 do /* Inner loop */ \n9.                 begin \n     /* Let ( b0b1 \u00b7\u00b7\u00b7 br -1) be the binary representation of i */ \n10.       ", "doc_id": "9c31644c-c95c-4af8-bba8-eab570ba39ac", "embedding": null, "doc_hash": "38bcb579c81307b9ce27ac5c19d69c39a861e842edcdea6b7041273dd2102de8", "extra_info": null, "node_info": {"start": 1467606, "end": 1470753}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e78a2689-7a37-4bda-96ac-21156028839d", "3": "1d6fe272-839a-4911-987c-591eb5115285"}}, "__type__": "1"}, "1d6fe272-839a-4911-987c-591eb5115285": {"__data__": {"text": "to r - 1 do        /* Outer loop */ \n6.          begin \n7.             for i := 0 to n - 1 do S[i]:= R[i]; \n8.             for i := 0 to n - 1 do /* Inner loop */ \n9.                 begin \n     /* Let ( b0b1 \u00b7\u00b7\u00b7 br -1) be the binary representation of i */ \n10.                   j := (b0...bm-10bm+1\u00b7\u00b7\u00b7br -1); \n11.                   k := (b0...bm-11bm+1\u00b7\u00b7\u00b7br -1); \n12.                   \n13.            endfor;   /* Inner loop */ \n14.     endfor;          /* Outer loop */ \n15.     for i := 0 to n - 1 do Y[i] := R[i]; \n16.  end ITERATIVE_FFT \nLine 12 in Algorithm 13.2  performs a crucial step in the FFT algorithm. This step updates R [i ]\nby using S [j ] and S [k ]. The indices j and k are derived from the index i as follows. Assume\nthat n = 2r . Since0 \n  i < n , the binary representation of i contains r bits. Let ( b 0 b 1 \u00b7\u00b7\u00b7br -1 )\nbe the binary representation of index i . In the m th iteration of the outer loop (0 \n  m < r ),\nindex j is derived by forcing the m th most significant bit of i (that is, bm ) to zero. Index k is\nderived by forcing bm to 1. Thus, the binary representations of j and k differ only in their m th\nmost significant bits. In the binary representation of i , bm is either 0 or 1. Hence, of the two\nindices j and k , one is the same as index i , depending on whether bm = 0 or bm = 1. In the m\nth iteration of the outer loop, for each i between 0 and n - 1, R [i ] is generated by executing\nline 12 of Algorithm 13.2  on S [i ] and on another element of S whose index differs from i only\nin the m th most significant bit. Figure 13.2  shows the pattern in which these elements are\npaired for the case in which n = 16.\nFigure 13.2. The pattern of combination of elements of the input and\nthe intermediate sequences during a 16-point unordered FFT\ncomputation.\n[ Team LiB ]\n \n\n[ Team LiB ]\n  \n13.2 The Binary-Exchange Algorithm\nThis section discusses the binary-exchange algorithm  for performing FFT on a parallel\ncomputer. First, a decomposition is induced by partitioning the input or the output vector.\nTherefore, each task starts with one element of the input vector and computes the\ncorresponding element of the output. If each task is assigned the same label as the index of its\ninput or output element, then in each of the log n iterations of the algorithm, exchange of data\ntakes place between pairs of tasks with labels differing in one bit position.\n13.2.1 A Full Bandwidth Network\nIn this subsection, we describe the implementation of the binary-exchange algorithm on a\nparallel computer on which a bisection width ( Section 2.4.4) of Q(p) is available to p parallel\nprocesses. Since the pattern of interaction among the tasks of parallel FFT matches that of a\nhypercube network, we describe the algorithm assuming such an interconnection network.\nHowever, the performance and scalability analysis would be valid for any parallel computer with\nan overall", "doc_id": "1d6fe272-839a-4911-987c-591eb5115285", "embedding": null, "doc_hash": "8316b660cafbdbd13ec3f390a0d46158970ab7aac9a7dc21a96957b92e0faf7f", "extra_info": null, "node_info": {"start": 1471044, "end": 1473946}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9c31644c-c95c-4af8-bba8-eab570ba39ac", "3": "33a0f625-afad-4be8-bfbc-ce8d58403e3f"}}, "__type__": "1"}, "33a0f625-afad-4be8-bfbc-ce8d58403e3f": {"__data__": {"text": "of data\ntakes place between pairs of tasks with labels differing in one bit position.\n13.2.1 A Full Bandwidth Network\nIn this subsection, we describe the implementation of the binary-exchange algorithm on a\nparallel computer on which a bisection width ( Section 2.4.4) of Q(p) is available to p parallel\nprocesses. Since the pattern of interaction among the tasks of parallel FFT matches that of a\nhypercube network, we describe the algorithm assuming such an interconnection network.\nHowever, the performance and scalability analysis would be valid for any parallel computer with\nan overall simultaneous data-transfer capacity of O (p).\nOne Task Per Process\nWe first consider a simple mapping in which one task is assigned to each process. Figure 13.3 illustrates the interaction pattern induced by this mapping of the binary-exchange algorithm for\nn = 16. As the figure shows, process i (0 \n  i < n) initially stores X [i] and finally generates Y\n[i]. In each of the log n iterations of the outer loop, process P i updates the value of R[i] by\nexecuting line 12 of Algorithm 13.2 . All n updates are performed in parallel.\nFigure 13.3. A 16-point unordered FFT on 16 processes. P i denotes the\nprocess labeled i.\nTo perform the updates, process P i requires an element of S from a process whose label differs\nfrom i at only one bit. Recall that in a hypercube, a node is connected to all those nodes whose\nlabels differ from its own at only one bit position. Thus, the parallel FFT computation maps\nnaturally onto a hypercube with a one-to-one mapping of processes to nodes. In the first\niteration of the outer loop, the labels of each pair of communicating processes differ only at\ntheir most significant bits. For instance, processes P 0 to P 7 communicate with P 8 to P 15,\nrespectively. Similarly, in the second iteration, the labels of processes communicating with each\nother differ at the second most significant bit, and so on.\nIn each of the log n iterations of this algorithm, every process performs one complex\nmultiplication and addition, and exchanges one complex number with another process. Thus,\nthere is a constant amount of work per iteration. Hence, it takes time Q(log n) to execute the\nalgorithm in parallel by using a hypercube with n nodes. This hypercube formulation of FFT is\ncost-optimal because its process-time product is Q(n log n), the same as the complexity of a\nserial n-point FFT.\nMultiple Tasks Per Process\nWe now consider a mapping in which the n tasks of an n-point FFT are mapped onto p\nprocesses, where n > p. For the sake of simplicity, let us assume that both n and p are powers\nof two, i.e., n = 2r and p = 2d. As Figure 13.4 shows, we partition the sequences into blocks of\nn/p contiguous elements and assign one block to each process.\nFigure 13.4. A 16-point FFT on four processes. P i denotes the process\nlabeled i. In general, the number of processes is p = 2d and the length\nof the input sequence is n = 2r.\nAn interesting property of the mapping shown in Figure 13.4  is that, if ( b0b1 \u00b7\u00b7\u00b7 br -1) is the\nbinary representation of any i, such that 0 \n  i < n, then R[i] and S[i] are mapped onto the\nprocess labeled ( b0\u00b7\u00b7\u00b7bd-1). That is, the d most significant bits of the index of any element of the\nsequence are the binary representation of the label of the process that the element belongs to.\nThis property of the mapping plays an important role in determining the amount of\ncommunication performed during the parallel execution of the FFT algorithm.\nFigure 13.4 shows that elements with indices differing at their d (= 2) most significant", "doc_id": "33a0f625-afad-4be8-bfbc-ce8d58403e3f", "embedding": null, "doc_hash": "d6f558718f4c42537ae06e0381d94d97dd774118ebe76af0e3f17f3673124205", "extra_info": null, "node_info": {"start": 1473587, "end": 1477177}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1d6fe272-839a-4911-987c-591eb5115285", "3": "3b3c19bf-26f1-4254-beb3-451061b2a996"}}, "__type__": "1"}, "3b3c19bf-26f1-4254-beb3-451061b2a996": {"__data__": {"text": "\u00b7\u00b7\u00b7 br -1) is the\nbinary representation of any i, such that 0 \n  i < n, then R[i] and S[i] are mapped onto the\nprocess labeled ( b0\u00b7\u00b7\u00b7bd-1). That is, the d most significant bits of the index of any element of the\nsequence are the binary representation of the label of the process that the element belongs to.\nThis property of the mapping plays an important role in determining the amount of\ncommunication performed during the parallel execution of the FFT algorithm.\nFigure 13.4 shows that elements with indices differing at their d (= 2) most significant bits are\nmapped onto different processes. However, all elements with indices having the same d most\nsignificant bits are mapped onto the same process. Recall from the previous section that an n-\npoint FFT requires r = log n iterations of the outer loop. In the mth iteration of the loop,\nelements with indices differing in the mth most significant bit are combined. As a result,\nelements combined during the first d iterations belong to different processes, and pairs of\nelements combined during the last ( r - d) iterations belong to the same processes. Hence, this\nparallel FFT algorithm requires interprocess interaction only during the first d = log p of the log\nn iterations. There is no interaction during the last r - d iterations. Furthermore, in the i th of\nthe first d iterations, all the elements that a process requires come from exactly one other\nprocess \u2013 the one whose label is different at the i th most significant bit.\nEach interaction operation exchanges n/p words of data. Therefore, the time spent in\ncommunication in the entire algorithm is ts log p + tw(n/p) log p. A process updates n/p\nelements of R during each of the log n iterations. If a complex multiplication and addition pair\ntakes time tc, then the parallel run time of the binary-exchange algorithm for n-point FFT on a\np-node hypercube network is\nEquation 13.4\nThe process-time product is tcn log n + ts p log p + twn log p. For the parallel system to be cost-\noptimal, this product should be O (n log n) \u2013 the sequential time complexity of the FFT\nalgorithm. This is true for p \n n.\nThe expressions for speedup and efficiency are given by the following equations:\nEquation 13.5\nScalability Analysis\nFrom Section 13.1 , we know that the problem size W for an n-point FFT is\nEquation 13.6\nSince an n-point FFT can utilize a maximum of n processes with the mapping of Figure 13.3 , n\n p or n log n \n p log p to keep p processes busy. Thus, the isoefficiency function of this\nparallel FFT algorithm is W(p log p) due to concurrency. We now derive the isoefficiency function\nfor the binary exchange algorithm due to the different communication-related terms. We can\nrewrite Equation 13.5  as\nIn order to maintain a fixed efficiency E , the expression ( ts p log p)/(tcn log n) + (twlog p)/(tc\nlog n) should be equal to a constant 1/ K, where K = E/(1 - E). We have defined the constant K\nin this manner to keep the terminology consistent with Chapter 5 . As proposed in Section 5.4.2 ,\nwe use an approximation to obtain closed expressions for the isoefficiency function. We first\ndetermine the rate of growth of the problem size with respect to p that would keep the terms\ndue to ts constant. To do this, we assume tw = 0. Now the condition for maintaining constant\nefficiency E is as follows:\nEquation 13.7\nEquation 13.7  gives the isoefficiency function due to the overhead resulting from interaction\nlatency or the message startup time.\nSimilarly, we derive the isoefficiency function due to the overhead resulting from tw. We assume\nthat ts = 0; hence, a fixed efficiency", "doc_id": "3b3c19bf-26f1-4254-beb3-451061b2a996", "embedding": null, "doc_hash": "a6feeaba919d1438c459699acb03e055427f0d9ed0cb49467b6545bbf66d2842", "extra_info": null, "node_info": {"start": 1477223, "end": 1480835}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "33a0f625-afad-4be8-bfbc-ce8d58403e3f", "3": "ae2ad7f5-2068-4364-ab54-0618e47818fe"}}, "__type__": "1"}, "ae2ad7f5-2068-4364-ab54-0618e47818fe": {"__data__": {"text": "approximation to obtain closed expressions for the isoefficiency function. We first\ndetermine the rate of growth of the problem size with respect to p that would keep the terms\ndue to ts constant. To do this, we assume tw = 0. Now the condition for maintaining constant\nefficiency E is as follows:\nEquation 13.7\nEquation 13.7  gives the isoefficiency function due to the overhead resulting from interaction\nlatency or the message startup time.\nSimilarly, we derive the isoefficiency function due to the overhead resulting from tw. We assume\nthat ts = 0; hence, a fixed efficiency E requires that the following relation be maintained:\nEquation 13.8\nIf the term Ktw/tc is less than one, then the rate of growth of the problem size required by\nEquation 13.8  is less than Q(p log p). In this case, Equation 13.7  determines the overall\nisoefficiency function of this parallel system. However, if Ktw/tc exceeds one, then Equation 13.8\ndetermines the overall isoefficiency function, which is now greater than the isoefficiency function\nof Q(p log p) given by Equation 13.7 .\nFor this algorithm, the asymptotic isoefficiency function depends on the relative values of K, tw,\nand tc. Here, K is an increasing function of the efficiency E to be maintained, tw depends on the\ncommunication bandwidth of the parallel computer, and tc depends on the speed of the\ncomputation speed of the processors. The FFT algorithm is unique in that the order of the\nisoefficiency function depends on the desired efficiency and hardware-dependent parameters. In\nfact, the efficiency corresponding to Kt w/tc = 1(i.e.,1/(1 - E) = tc/tw, or E = tc/(tc + tw)) acts as\na kind of threshold. For a fixed tc and tw, efficiencies up to the threshold can be obtained easily.\nFor E \n tc/(tc + tw), the asymptotic isoefficiency function is Q(p log p). Efficiencies much higher\nthan the threshold tc/(tc + t w) can be obtained only if the problem size is extremely large. The\nreason is that for these efficiencies, the asymptotic isoefficiency function is Q\n . The\nfollowing examples illustrate the effect of the value of Ktw/tc on the isoefficiency function.\nExample 13.1 Threshold effect in the binary-exchange\nalgorithm\nConsider a hypothetical hypercube for which the relative values of the hardware\nparameters are given by tc = 2, tw = 4, and ts = 25. With these values, the threshold\nefficiency tc/(tc + tw) is 0.33.\nNow we study the isoefficiency functions of the binary-exchange algorithm on a\nhypercube for maintaining efficiencies below and above the threshold. The\nisoefficiency function of this algorithm due to concurrency is p log p. From Equations\n13.7 and 13.8, the isoefficiency functions due to the ts and tw terms in the overhead\nfunction are K (ts/tc) p log p and \n  log p, respectively. To maintain a\ngiven efficiency E (that is, for a given K), the overall isoefficiency function is given by:\nFigure 13.5  shows the isoefficiency curves given by this function for E = 0.20, 0.25,\n0.30, 0.35, 0.40, and 0.45. Notice that the various isoefficiency curves are regularly\nspaced for efficiencies up to the threshold. However, the problem sizes required to\nmaintain efficiencies above the threshold are much larger. The asymptotic isoefficiency\nfunctions for E = 0.20, 0.25, and 0.30 are Q(p log p). The isoefficiency function for E =\n0.40 is Q(p1.33 log p), and that for E = 0.45 is Q(p1.64 log p).\nFigure 13.5. Isoefficiency functions of the", "doc_id": "ae2ad7f5-2068-4364-ab54-0618e47818fe", "embedding": null, "doc_hash": "eb81b92d955256d643c6c89fed11c6f4dae440fa943a5c0817ecac1ebd719119", "extra_info": null, "node_info": {"start": 1480807, "end": 1484229}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3b3c19bf-26f1-4254-beb3-451061b2a996", "3": "63284a19-1d31-407d-b0ce-7b1b415a32c6"}}, "__type__": "1"}, "63284a19-1d31-407d-b0ce-7b1b415a32c6": {"__data__": {"text": "0.35, 0.40, and 0.45. Notice that the various isoefficiency curves are regularly\nspaced for efficiencies up to the threshold. However, the problem sizes required to\nmaintain efficiencies above the threshold are much larger. The asymptotic isoefficiency\nfunctions for E = 0.20, 0.25, and 0.30 are Q(p log p). The isoefficiency function for E =\n0.40 is Q(p1.33 log p), and that for E = 0.45 is Q(p1.64 log p).\nFigure 13.5. Isoefficiency functions of the binary-exchange\nalgorithm on a hypercube with tc = 2, tw = 4, and ts = 25 for\nvarious values of E.\nFigure 13.6  shows the efficiency curve of n-point FFTs on a 256-node hypercube with\nthe same hardware parameters. The efficiency E is computed by using Equation 13.5\nfor various values of n, when p is equal to 256. The figure shows that the efficiency\ninitially increases rapidly with the problem size, but the efficiency curve flattens out\nbeyond the threshold. \nFigure 13.6. The efficiency of the binary-exchange algorithm as\na function of n on a 256-node hypercube with tc = 2, tw = 4,\nand ts = 25.\nExample 13.1  shows that there is a limit on the efficiency that can be obtained for reasonable\nproblem sizes, and that the limit is determined by the ratio between the CPU speed and the\ncommunication bandwidth of the parallel computer being used. This limit can be raised by\nincreasing the bandwidth of the communication channels. However, making the CPUs faster\nwithout increasing the communication bandwidth lowers the limit. Hence, the binary-exchange\nalgorithm performs poorly on a parallel computer whose communication and computation\nspeeds are not balanced. If the hardware is balanced with respect to its communication and\ncomputation speeds, then the binary-exchange algorithm is fairly scalable, and reasonable\nefficiencies can be maintained while increasing the problem size at the rate of Q(p log p).\n13.2.2 Limited Bandwidth Network\nNow we consider the implementation of the binary-exchange algorithm on a parallel computer\nwhose cross-section bandwidth is less than Q(p). We choose a mesh interconnection network to\nillustrate the algorithm and its performance characteristics. Assume that n tasks are mapped\nonto p processes running on a mesh with \n rows and \n  columns, and that \n  is a power of\ntwo. Let n = 2r and p = 2d. Also assume that the processes are labeled in a row-major fashion\nand that the data are distributed in the same manner as for the hypercube; that is, an element\nwith index ( b0b1\u00b7\u00b7\u00b7br -1) is mapped onto the process labeled ( b0 \u00b7\u00b7\u00b7 bd-1).\nAs in case of the hypercube, communication takes place only during the first log p iterations\nbetween processes whose labels differ at one bit. However, unlike the hypercube, the\ncommunicating processes are not directly linked in a mesh. Consequently, messages travel over\nmultiple links and there is overlap among messages sharing the same links. Figure 13.7  shows\nthe messages sent and received by processes 0 and 37 during an FFT computation on a 64-node\nmesh. As the figure shows, process 0 communicates with processes 1, 2, 4, 8, 16, and 32. Note\nthat all these processes lie in the same row or column of the mesh as that of process 0.\nProcesses 1, 2, and 4 lie in the same row as process 0 at distances of 1, 2, and 4 links,\nrespectively. Processes 8, 16, and 32 lie in the same column, again at distances of 1, 2, and 4\nlinks. More precisely, in log \n of the log p steps that require communication, the\ncommunicating processes are in the same row, and in the remaining log \n  steps, they are in\nthe same column. The number of messages that share at", "doc_id": "63284a19-1d31-407d-b0ce-7b1b415a32c6", "embedding": null, "doc_hash": "b21ef1a3cd9643a8d4df275e485fbba7981f17585d35fb96b0c6eca2197ce032", "extra_info": null, "node_info": {"start": 1484339, "end": 1487929}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ae2ad7f5-2068-4364-ab54-0618e47818fe", "3": "3978f398-50cd-491f-9cba-6b15b8a8f65b"}}, "__type__": "1"}, "3978f398-50cd-491f-9cba-6b15b8a8f65b": {"__data__": {"text": "4, 8, 16, and 32. Note\nthat all these processes lie in the same row or column of the mesh as that of process 0.\nProcesses 1, 2, and 4 lie in the same row as process 0 at distances of 1, 2, and 4 links,\nrespectively. Processes 8, 16, and 32 lie in the same column, again at distances of 1, 2, and 4\nlinks. More precisely, in log \n of the log p steps that require communication, the\ncommunicating processes are in the same row, and in the remaining log \n  steps, they are in\nthe same column. The number of messages that share at least one link is equal to the number\nof links that each message traverses (Problem 13.9) because, during a given FFT iteration, all\npairs of nodes exchange messages that traverse the same number of links.\nFigure 13.7. Data communication during an FFT computation on a\nlogical square mesh of 64 processes. The figure shows all the\nprocesses with which the processes labeled 0 and 37 exchange data.\nThe distance between the communicating processes in a row or a column grows from one link to\n links, doubling in each of the log \n  iterations. This is true for any process in the mesh,\nsuch as process 37 shown in Figure 13.7 . Thus, the total time spent in performing rowwise\ncommunication is \n . An equal amount of time is spent in columnwise\ncommunication. Recall that we assumed that a complex multiplication and addition pair takes\ntime tc. Since a process performs n/p such calculations in each of the log n iterations, the\noverall parallel run time is given by the following equation:\nEquation 13.9\nThe speedup and efficiency are given by the following equations:\nEquation 13.10\nThe process-time product of this parallel system is \n . The process-\ntime product should be O (n log n) for cost-optimality, which is obtained when \n ,\nor p = O (log2 n). Since the communication term due to ts in Equation 13.9  is the same as for\nthe hypercube, the corresponding isoefficiency function is again Q(p log p) as given by Equation\n13.7. By performing isoefficiency analysis along the same lines as in Section 13.2.1 , we can\nshow that the isoefficiency function due to the tw term is \n  (Problem\n13.4). Given this isoefficiency function, the problem size must grow exponentially with the\nnumber of processes to maintain constant efficiency. Hence, the binary-exchange FFT algorithm\nis not very scalable on a mesh.\nThe communication overhead of the binary-exchange algorithm on a mesh cannot be reduced\nby using a different mapping of the sequences onto the processes. In any mapping, there is at\nleast one iteration in which pairs of processes that communicate with each other are at least\n links apart (Problem 13.2). The algorithm inherently requires Q(p) bisection bandwidth\non a p-node ensemble, and on an architecture like a 2-D mesh with Q\n  bisection bandwidth,\nthe communication time cannot be asymptotically better than \n  as discussed\nabove.\n13.2.3 Extra Computations in Parallel FFT\nSo far, we have described a parallel formulation of the FFT algorithm on a hypercube and a\nmesh, and have discussed its performance and scalability in the presence of communication\noverhead on both architectures. In this section, we discuss another source of overhead that can\nbe present in a parallel FFT implementation.\nRecall from Algorithm 13.2  that the computation step of line 12 multiplies a power of w (a\ntwiddle factor) with an element of S. For an n-point FFT, line 12 executes n log n times in the\nsequential algorithm. However, only n distinct powers of w (that is, w0, w1, w2, ..., wn-1) are used\nin the entire algorithm. So some of the twiddle", "doc_id": "3978f398-50cd-491f-9cba-6b15b8a8f65b", "embedding": null, "doc_hash": "36fe684cf0dfb4537a2e9c5301afd6255fbb41352897c688c9eb34ab3c6bffc2", "extra_info": null, "node_info": {"start": 1487887, "end": 1491463}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "63284a19-1d31-407d-b0ce-7b1b415a32c6", "3": "7968ab6c-6f53-4414-b6ec-729069db691d"}}, "__type__": "1"}, "7968ab6c-6f53-4414-b6ec-729069db691d": {"__data__": {"text": "scalability in the presence of communication\noverhead on both architectures. In this section, we discuss another source of overhead that can\nbe present in a parallel FFT implementation.\nRecall from Algorithm 13.2  that the computation step of line 12 multiplies a power of w (a\ntwiddle factor) with an element of S. For an n-point FFT, line 12 executes n log n times in the\nsequential algorithm. However, only n distinct powers of w (that is, w0, w1, w2, ..., wn-1) are used\nin the entire algorithm. So some of the twiddle factors are used repeatedly. In a serial\nimplementation, it is useful to precompute and store all n twiddle factors before starting the\nmain algorithm. That way, the computation of twiddle factors requires only Q(n) complex\noperations rather than the Q(n log n) operations needed to compute all twiddle factors in each\niteration of line 12.\nIn a parallel implementation, the total work required to compute the twiddle factors cannot be\nreduced to Q(n). The reason is that, even if a certain twiddle factor is used more than once, it\nmight be used on different processes at different times. If FFTs of the same size are computed\non the same number of processes, every process needs the same set of twiddle factors for each\ncomputation. In this case, the twiddle factors can be precomputed and stored, and the cost of\ntheir computation can be amortized over the execution of all instances of FFTs of the same size.\nHowever, if we consider only one instance of FFT, then twiddle factor computation gives rise to\nadditional overhead in a parallel implementation, because it performs more overall operations\nthan the sequential implementation.\nAs an example, consider the various powers of w used in the three iterations of an 8-point FFT.\nIn the mth iteration of the loop starting on line 5 of the algorithm, wl is computed for all i (0 \ni < n), such that l is the integer obtained by reversing the order of the m + 1 most significant\nbits of i and then padding them by log n - (m + 1) zeros to the right (refer to Figure 13.1  and\nAlgorithm 13.2  to see how l is derived). Table 13.1  shows the binary representation of the\npowers of w required for all values of i and m for an 8-point FFT.\nIf eight processes are used, then each process computes and uses one column of Table 13.1 .\nProcess 0 computes just one twiddle factor for all its iterations, but some processes (in this\ncase, all other processes 2\u20137) compute a new twiddle factor in each of the three iterations. If p\n= n/2 = 4, then each process computes two consecutive columns of the table. In this case, the\nlast process computes the twiddle factors in the last two columns of the table. Hence, the last\nprocess computes a total of four different powers \u2013 one each for m = 0 (100) and m = 1 (110),\nand two for m = 2 (011 and 111). Although different processes may compute a different\nnumber of twiddle factors, the total overhead due to the extra work is proportional to p times\nthe maximum number of twiddle factors that any single process computes. Let h(n, p) be the\nmaximum number of twiddle factors that any of the p processes computes during an n-point\nFFT. Table 13.2 shows the values of h(8, p) for p = 1, 2, 4, and 8. The table also shows the\nmaximum number of new twiddle factors that any single process computes in each iteration.\nTable 13.1. The binary representation of the various powers of w\ncalculated in different iterations of an 8-point FFT (also see Figure\n13.1). The value of m refers to the iteration number of the outer loop,\nand i is the index of the inner loop of Algorithm 13.2 .\n\u00a0", "doc_id": "7968ab6c-6f53-4414-b6ec-729069db691d", "embedding": null, "doc_hash": "5925316470c4820bc6d96d5f7106ed17e674bd1c55da5174b0e09747ca400418", "extra_info": null, "node_info": {"start": 1491453, "end": 1495042}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3978f398-50cd-491f-9cba-6b15b8a8f65b", "3": "d63f215a-75f6-4e75-8dce-faef2aa5b85d"}}, "__type__": "1"}, "d63f215a-75f6-4e75-8dce-faef2aa5b85d": {"__data__": {"text": "number of twiddle factors that any of the p processes computes during an n-point\nFFT. Table 13.2 shows the values of h(8, p) for p = 1, 2, 4, and 8. The table also shows the\nmaximum number of new twiddle factors that any single process computes in each iteration.\nTable 13.1. The binary representation of the various powers of w\ncalculated in different iterations of an 8-point FFT (also see Figure\n13.1). The value of m refers to the iteration number of the outer loop,\nand i is the index of the inner loop of Algorithm 13.2 .\n\u00a0 i\n\u00a0 0 1 2 3 4 5 6 7\nm = 0 000 000 000 000 100 100 100 100\nm = 1 000 000 100 100 010 010 110 110\n\u00a0 i\n\u00a0 0 1 2 3 4 5 6 7\nm = 2 000 100 010 110 001 101 011 111\nTable 13.2. The maximum number of new powers of w used by any\nprocess in each iteration of an 8-point FFT computation.\n\u00a0 p = 1 p = 2 p = 4 p = 8\nm = 0 2 1 1 1\nm = 1 2 2 1 1\nm = 2 4 4 2 1\nTotal = h(8, p) 8 7 4 3\nThe function h is defined by the following recurrence relation (Problem 13.5):\nThe solution to this recurrence relation for p > 1 and n \n p is\nThus, if it takes time \n  to compute one twiddle factor, then at least one process spends time\n log p computing twiddle factors. The total cost of twiddle factor\ncomputation, summed over all processes, is \n  log p. Since even a serial\nimplementation incurs a cost of \n  in computing twiddle factors, the total parallel overhead due\nto extra work (\n ) is given by the following equation:\nThis overhead is independent of the architecture of the parallel computer used for the FFT\ncomputation. The isoefficiency function due to \n  is Q(p log p). Since this term is of the\nsame order as the isoefficiency terms due to message startup time and concurrency, the extra\ncomputations do not affect the overall scalability of parallel FFT.m = 2 000 100 010 110 001 101 011 111\nTable 13.2. The maximum number of new powers of w used by any\nprocess in each iteration of an 8-point FFT computation.\n\u00a0 p = 1 p = 2 p = 4 p = 8\nm = 0 2 1 1 1\nm = 1 2 2 1 1\nm = 2 4 4 2 1\nTotal = h(8, p) 8 7 4 3\nThe function h is defined by the following recurrence relation (Problem 13.5):\nThe solution to this recurrence relation for p > 1 and n \n p is\nThus, if it takes time \n  to compute one twiddle factor, then at least one process spends time\n log p computing twiddle factors. The total cost of twiddle factor\ncomputation, summed over all processes, is \n  log p. Since even a serial\nimplementation incurs a cost of \n  in computing twiddle factors, the total parallel overhead due\nto extra work (\n ) is given by the following equation:\nThis overhead is independent of the architecture of the parallel computer used for the FFT\ncomputation. The isoefficiency function due to \n  is Q(p log p). Since this term is of the\nsame order as the isoefficiency terms due to message startup time and concurrency, the extra\ncomputations do not affect the overall scalability of parallel FFT.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n13.3 The Transpose Algorithm\nThe binary-exchange algorithm yields good performance on parallel computers with sufficiently\nhigh communication bandwidth with respect to the processing speed of the CPUs. Efficiencies\nbelow a certain threshold can be maintained while increasing the problem size at a moderate\nrate with an increasing number of processes. However, this threshold is very low if the\ncommunication bandwidth of the parallel computer is low compared to the speed of", "doc_id": "d63f215a-75f6-4e75-8dce-faef2aa5b85d", "embedding": null, "doc_hash": "21313da02719121afbbd853cb3b30da211f380d8d49bc38e671f611cf137dc38", "extra_info": null, "node_info": {"start": 1495042, "end": 1498441}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7968ab6c-6f53-4414-b6ec-729069db691d", "3": "056924d6-62a6-4400-b933-4fe735623cb5"}}, "__type__": "1"}, "056924d6-62a6-4400-b933-4fe735623cb5": {"__data__": {"text": "time and concurrency, the extra\ncomputations do not affect the overall scalability of parallel FFT.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n13.3 The Transpose Algorithm\nThe binary-exchange algorithm yields good performance on parallel computers with sufficiently\nhigh communication bandwidth with respect to the processing speed of the CPUs. Efficiencies\nbelow a certain threshold can be maintained while increasing the problem size at a moderate\nrate with an increasing number of processes. However, this threshold is very low if the\ncommunication bandwidth of the parallel computer is low compared to the speed of its\nprocessors. In this section, we describe a different parallel formulation of FFT that trades off\nsome efficiency for a more consistent level of parallel performance. This parallel algorithm\ninvolves matrix transposition, and hence is called the transpose algorithm .\nThe performance of the transpose algorithm is worse than that of the binary-exchange\nalgorithm for efficiencies below the threshold. However, it is much easier to obtain efficiencies\nabove the binary-exchange algorithm's threshold using the transpose algorithm. Thus, the\ntranspose algorithm is particularly useful when the ratio of communication bandwidth to CPU\nspeed is low and high efficiencies are desired. On a hypercube or a p-node network with Q(p)\nbisection width, the transpose algorithm has a fixed asymptotic isoefficiency function of Q(p2 log\np). That is, the order of this isoefficiency function is independent of the ratio of the speed of\npoint-to-point communication and the computation.\n13.3.1 Two-Dimensional Transpose Algorithm\nThe simplest transpose algorithm requires a single transpose operation over a two-dimensional\narray; hence, we call this algorithm the two-dimensional transpose algorithm .\nAssume that \n is a power of 2, and that the sequences of size n used in Algorithm 13.2  are\narranged in a \n  two-dimensional square array, as shown in Figure 13.8  for n = 16.\nRecall that computing the FFT of a sequence of n points requires log n iterations of the outer\nloop of Algorithm 13.2 . If the data are arranged as shown in Figure 13.8 , then the FFT\ncomputation in each column can proceed independently for log \n  iterations without any\ncolumn requiring data from any other column. Similarly, in the remaining log \n  iterations,\ncomputation proceeds independently in each row without any row requiring data from any other\nrow. Figure 13.8 shows the pattern of combination of the elements for a 16-point FFT. The\nfigure illustrates that if data of size n are arranged in a \n  array, then an n-point FFT\ncomputation is equivalent to independent \n -point FFT computations in the columns of the\narray, followed by independent \n -point FFT computations in the rows.\nFigure 13.8. The pattern of combination of elements in a 16-point FFT\nwhen the data are arranged in a 4 x 4 two-dimensional square array.\nIf the \n  array of data is transposed after computing the \n -point column FFTs, then the\nremaining part of the problem is to compute the \n -point columnwise FFTs of the transposed\nmatrix. The transpose algorithm uses this property to compute the FFT in parallel by using a\ncolumnwise striped partitioning to distribute the \n  array of data among the processes.\nFor instance, consider the computation of the 16-point FFT shown in Figure 13.9 , where the 4 x\n4 array of data is distributed among four processes such that each process stores one column of\nthe array. In general, the two-dimensional transpose algorithm works in three phases. In the\nfirst phase, a \n -point FFT is computed for each column. In the second phase, the array of data\nis transposed. The third and final phase is identical to the first phase, and involves the\ncomputation of \n-point FFTs for each column of the transposed array.", "doc_id": "056924d6-62a6-4400-b933-4fe735623cb5", "embedding": null, "doc_hash": "e8ad3ab81d82a971dc5c3c2c8b2140c74aacddc88c48f45e437bf91d59435c7b", "extra_info": null, "node_info": {"start": 1498359, "end": 1502169}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d63f215a-75f6-4e75-8dce-faef2aa5b85d", "3": "ebaaee77-9fbb-4978-87f7-cf823c83c9b8"}}, "__type__": "1"}, "ebaaee77-9fbb-4978-87f7-cf823c83c9b8": {"__data__": {"text": "distribute the \n  array of data among the processes.\nFor instance, consider the computation of the 16-point FFT shown in Figure 13.9 , where the 4 x\n4 array of data is distributed among four processes such that each process stores one column of\nthe array. In general, the two-dimensional transpose algorithm works in three phases. In the\nfirst phase, a \n -point FFT is computed for each column. In the second phase, the array of data\nis transposed. The third and final phase is identical to the first phase, and involves the\ncomputation of \n-point FFTs for each column of the transposed array. Figure 13.9  shows that\nthe first and third phases of the algorithm do not require any interprocess communication. In\nboth these phases, all \n points for each columnwise FFT computation are available on the\nsame process. Only the second phase requires communication for transposing the \nmatrix.\nFigure 13.9. The two-dimensional transpose algorithm for a 16-point\nFFT on four processes.\nIn the transpose algorithm shown in Figure 13.9 , one column of the data array is assigned to\none process. Before analyzing the transpose algorithm further, consider the more general case\nin which p processes are used and 1 \n  p \n \n. The \n  array of data is striped into\nblocks, and one block of \n  rows is assigned to each process. In the first and third phases of\nthe algorithm, each process computes \n  FFTs of size \n  each. The second phase\ntransposes the \n  matrix, which is distributed among p processes with a one-\ndimensional partitioning. Recall from Section 4.5  that such a transpose requires an all-to-all\npersonalized communication.\nNow we derive an expression for the parallel run time of the two-dimensional transpose\nalgorithm. The only inter-process interaction in this algorithm occurs when the \n  array\nof data partitioned along columns or rows and mapped onto p processes is transposed.\nReplacing the message size m by n/p \u2013 the amount of data owned by each process \u2013 in the\nexpression for all-to-all personalized communication in Table 4.1  yields ts (p - 1) + twn/p as the\ntime spent in the second phase of the algorithm. The first and third phases each take time\n. Thus, the parallel run time of the transpose algorithm on a hypercube\nor any Q(p) bisection width network is given by the following equation:\nEquation 13.11\nThe expressions for speedup and efficiency are as follows:\nEquation 13.12\nThe process-time product of this parallel system is tcn log n + ts p2 + twn. This parallel system is\ncost-optimal if n log n = W(p2 log p).\nNote that the term associated with tw in the expression for efficiency in Equation 13.12  is\nindependent of the number of processes. The degree of concurrency of this algorithm requires\nthat \n  because at most \n  processes can be used to partition the \n  array of\ndata in a striped manner. As a result, n = W(p2), or n log n = W(p2 log p). Thus, the problem\nsize must increase at least as fast as Q(p2 log p) with respect to the number of processes to use\nall of them efficiently. The overall isoefficiency function of the two-dimensional transpose\nalgorithm is Q(p2 log p) on a hypercube or another interconnection network with bisection width\nQ(p). This isoefficiency function is independent of the ratio of tw for point-to-point\ncommunication and tc. On a network whose cross-section bandwidth b is less than Q(p) for p\nnodes, the tw term must be multiplied by an appropriate expression of Q(p/b) in order to derive\nTP, S, E, and the isoefficiency function (Problem 13.6).\nComparison with the Binary-Exchange Algorithm\nA comparison of Equations 13.4 and 13.11  shows that the transpose algorithm has a much\nhigher overhead than", "doc_id": "ebaaee77-9fbb-4978-87f7-cf823c83c9b8", "embedding": null, "doc_hash": "3372cd14d40ebf9fd5f300c312f4c3062312ca7cb274dbd62c636c151947d82f", "extra_info": null, "node_info": {"start": 1502195, "end": 1505864}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "056924d6-62a6-4400-b933-4fe735623cb5", "3": "60102272-c71e-4199-aea1-0e27c36ab3f3"}}, "__type__": "1"}, "60102272-c71e-4199-aea1-0e27c36ab3f3": {"__data__": {"text": "on a hypercube or another interconnection network with bisection width\nQ(p). This isoefficiency function is independent of the ratio of tw for point-to-point\ncommunication and tc. On a network whose cross-section bandwidth b is less than Q(p) for p\nnodes, the tw term must be multiplied by an appropriate expression of Q(p/b) in order to derive\nTP, S, E, and the isoefficiency function (Problem 13.6).\nComparison with the Binary-Exchange Algorithm\nA comparison of Equations 13.4 and 13.11  shows that the transpose algorithm has a much\nhigher overhead than the binary-exchange algorithm due to the message startup time ts, but\nhas a lower overhead due to per-word transfer time tw. As a result, either of the two algorithms\nmay be faster depending on the relative values of ts and tw. If the latency ts is very low, then\nthe transpose algorithm may be the algorithm of choice. On the other hand, the binary-\nexchange algorithm may perform better on a parallel computer with a high communication\nbandwidth but a significant startup time.\nRecall from Section 13.2.1 that an overall isoefficiency function of Q(p log p) can be realized by\nusing the binary-exchange algorithm if the efficiency is such that Ktw/tc \n 1, where K = E /(1 -\nE). If the desired efficiency is such that Ktw/tc = 2, then the overall isoefficiency function of both\nthe binary-exchange and the two-dimensional transpose algorithms is Q(p2 log p). When Ktw/tc\n> 2, the two-dimensional transpose algorithm is more scalable than the binary-exchange\nalgorithm; hence, the former should be the algorithm of choice, provided that n \n p2. Note,\nhowever, that the transpose algorithm yields a performance benefit over the binary-exchange\nalgorithm only if the target architecture has a cross-section bandwidth of Q(p) for p nodes\n(Problem 13.6).\n13.3.2 The Generalized Transpose Algorithm\nIn the two-dimensional transpose algorithm, the input of size n is arranged in a \n  two-\ndimensional array that is partitioned along one dimension on p processes. These processes,\nirrespective of the underlying architecture of the parallel computer, can be regarded as\narranged in a logical one-dimensional linear array. As an extension of this scheme, consider the\nn data points to be arranged in an n1/3 x n1/3 x n1/3 three-dimensional array mapped onto a\nlogical \n two-dimensional mesh of processes. Figure 13.10  illustrates this mapping. To\nsimplify the algorithm description, we label the three axes of the three-dimensional array of\ndata as x, y, and z. In this mapping, the x-y plane of the array is checkerboarded into\n parts. As the figure shows, each process stores \n  columns of\ndata, and the length of each column (along the z -axis) is n1/3. Thus, each process has\n elements of data.\nFigure 13.10. Data distribution in the three-dimensional transpose\nalgorithm for an n-point FFT on p processes \n .\nRecall from Section 13.3.1  that the FFT of a two-dimensionally arranged input of size \ncan be computed by first computing the \n -point one-dimensional FFTs of all the columns of\nthe data and then computing the \n -point one-dimensional FFTs of all the rows. If the data are\narranged in an n1/3 x n1/3 x n1/3 three-dimensional array, the entire n-point FFT can be\ncomputed similarly. In this case, n1/3-point FFTs are computed over the elements of the\ncolumns of the array in all three dimensions, choosing one dimension at a time. We call this\nalgorithm the three-dimensional transpose algorithm . This algorithm can be divided into\nthe following five phases:\nIn the first phase,", "doc_id": "60102272-c71e-4199-aea1-0e27c36ab3f3", "embedding": null, "doc_hash": "c847f8f2f112bd1af7fbb5afc0cce63b5485f37da5e4084d459faabd467ac79f", "extra_info": null, "node_info": {"start": 1505885, "end": 1509428}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "ebaaee77-9fbb-4978-87f7-cf823c83c9b8", "3": "d01a482b-133d-4e6c-bbf6-71a18168969c"}}, "__type__": "1"}, "d01a482b-133d-4e6c-bbf6-71a18168969c": {"__data__": {"text": "of all the columns of\nthe data and then computing the \n -point one-dimensional FFTs of all the rows. If the data are\narranged in an n1/3 x n1/3 x n1/3 three-dimensional array, the entire n-point FFT can be\ncomputed similarly. In this case, n1/3-point FFTs are computed over the elements of the\ncolumns of the array in all three dimensions, choosing one dimension at a time. We call this\nalgorithm the three-dimensional transpose algorithm . This algorithm can be divided into\nthe following five phases:\nIn the first phase, n1/3-point FFTs are computed on all the rows along the z-axis. 1.\n2.\n1.\nIn the second phase, all the n1/3 cross-sections of size n1/3 x n1/3 along the y-z plane are\ntransposed.2.\nIn the third phase, n1/3-point FFTs are computed on all the rows of the modified array\nalong the z-axis.3.\nIn the fourth phase, each of the n1/3 x n1/3 cross-sections along the x-z plane is\ntransposed.4.\nIn the fifth and final phase, n1/3-point FFTs of all the rows along the z-axis are computed\nagain.5.\nFor the data distribution shown in Figure 13.10 , in the first, third, and fifth phases of the\nalgorithm, all processes perform \n  FFT computations, each of size n1/3.\nSince all the data for performing these computations are locally available on each process, no\ninterprocess communication is involved in these three odd-numbered phases. The time spent by\na process in each of these phases is \n . Thus, the total\ntime that a process spends in computation is tc(n/p) log n.\nFigure 13.11  illustrates the second and fourth phases of the three-dimensional transpose\nalgorithm. As Figure 13.11(a)  shows, the second phase of the algorithm requires transposing\nsquare cross-sections of size n1/3 x n1/3 along the y-z plane. Each column of \n  processes\nperforms the transposition of \n  such cross-sections. This transposition involves all-to-\nall personalized communications among groups of \n  processes with individual messages of\nsize n/p3/2. If a p-node network with bisection width Q(p) is used, this phase takes time\n. The fourth phase, shown in Figure 13.11(b) , is similar. Here each row of\n processes performs the transpose of \n  cross-sections along the x-z plane. Again,\neach cross-section consists of n1/3 x n1/3 data elements. The communication time of this phase\nis the same as that of the second phase. The total parallel run time of the three-dimensional\ntranspose algorithm for an n-point FFT is\nEquation 13.13\nFigure 13.11. The communication (transposition) phases in the three-\ndimensional transpose algorithm for an n-point FFT on p processes.\nHaving studied the two- and three-dimensional transpose algorithms, we can derive a more\ngeneral q-dimensional transpose algorithm similarly. Let the n-point input be arranged in a\nlogical q-dimensional array of size n1/q x n1/q x\u00b7\u00b7\u00b7xn1/q (a total of q terms). Now the entire n-\npoint FFT computation can be viewed as q subcomputations. Each of the q subcomputations\nalong a different dimension consists of n(q-1)/q FFTs over n1/q data points. We map the array of\ndata onto a logical ( q - 1)-dimensional array of p processes, where p \n n(q-1)/q, and p = 2(q-1)s\nfor some integer s. The FFT of the entire data is now computed in (2 q - 1) phases (recall that\nthere are three phases in the two-dimensional transpose algorithm and five phases in", "doc_id": "d01a482b-133d-4e6c-bbf6-71a18168969c", "embedding": null, "doc_hash": "03267c4280436bf72f915d4621b2bfdfeabb66420153d77203287a0925443196", "extra_info": null, "node_info": {"start": 1509464, "end": 1512771}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "60102272-c71e-4199-aea1-0e27c36ab3f3", "3": "cfff0532-1c8e-4178-8963-035fd8cb465f"}}, "__type__": "1"}, "cfff0532-1c8e-4178-8963-035fd8cb465f": {"__data__": {"text": "entire n-\npoint FFT computation can be viewed as q subcomputations. Each of the q subcomputations\nalong a different dimension consists of n(q-1)/q FFTs over n1/q data points. We map the array of\ndata onto a logical ( q - 1)-dimensional array of p processes, where p \n n(q-1)/q, and p = 2(q-1)s\nfor some integer s. The FFT of the entire data is now computed in (2 q - 1) phases (recall that\nthere are three phases in the two-dimensional transpose algorithm and five phases in the three-\ndimensional transpose algorithm). In the q odd-numbered phases, each process performs n(q-\n1)/q/p of the required n1/q-point FFTs. The total computation time for each process over all q\ncomputation phases is the product of q (the number of computation phases), n(q-1)/q/p (the\nnumber of n1/q-point FFTs computed by each process in each computation phase), and tcn1/q\nlog(n1/q) (the time to compute a single n1/q-point FFT). Multiplying these terms gives a total\ncomputation time of tc(n/p) log n.\nIn each of the ( q - 1) even-numbered phases, sub-arrays of size n1/q x n1/q are transposed on\nrows of the q-dimensional logical array of processes. Each such row contains p1/(q-1) processes.\nOne such transpose is performed along every dimension of the ( q - 1)-dimensional process\narray in each of the ( q - 1) communication phases. The time spent in communication in each\ntransposition is ts(p1/(q-1) - 1) + twn/p. Thus, the total parallel run time of the q-dimensional\ntranspose algorithm for an n-point FFT on a p-node network with bisection width Q(p) is\nEquation 13.14\nEquation 13.14  can be verified by replacing q with 2 and 3, and comparing the result with\nEquations 13.11  and 13.13 , respectively.\nA comparison of Equations 13.11 , 13.13 , 13.14 , and 13.4 shows an interesting trend. As the\ndimension q of the transpose algorithm increases, the communication overhead due to tw\nincreases, but that due to ts decreases. The binary-exchange algorithm and the two-dimensional\ntranspose algorithms can be regarded as two extremes. The former minimizes the overhead due\nto ts but has the largest overhead due to tw. The latter minimizes the overhead due to tw but\nhas the largest overhead due to ts . The variations of the transpose algorithm for 2 < q < log p\nlie between these two extremes. For a given parallel computer, the specific values of tc, ts , and\ntw determine which of these algorithms has the optimal parallel run time (Problem 13.8).\nNote that, from a practical point of view, only the binary-exchange algorithm and the two- and\nthree-dimensional transpose algorithms are feasible. Higher-dimensional transpose algorithms\nare very complicated to code. Moreover, restrictions on n and p limit their applicability. These\nrestrictions for a q-dimensional transpose algorithm are that n must be a power of two that is a\nmultiple of q, and that p must be a power of 2 that is a multiple of ( q - 1). In other words, n =\n2qr, and p = 2(q-1)s, where q, r, and s are integers.\nExample 13.2 A comparison of binary-exchange, 2-D transpose,\nand 3-D transpose algorithms\nThis example shows that either the binary-exchange algorithm or any of the transpose\nalgorithms may be the algorithm of choice for a given parallel computer, depending\non the size of the FFT. Consider a 64-node version of the", "doc_id": "cfff0532-1c8e-4178-8963-035fd8cb465f", "embedding": null, "doc_hash": "c22e9bbbf60da2a65f80f9e3de7369669785e66c4b8ddd6728fd15c6fbb52f9c", "extra_info": null, "node_info": {"start": 1512819, "end": 1516108}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d01a482b-133d-4e6c-bbf6-71a18168969c", "3": "acb6f6d4-af6e-4e55-a556-cc2717d06dde"}}, "__type__": "1"}, "acb6f6d4-af6e-4e55-a556-cc2717d06dde": {"__data__": {"text": "a power of two that is a\nmultiple of q, and that p must be a power of 2 that is a multiple of ( q - 1). In other words, n =\n2qr, and p = 2(q-1)s, where q, r, and s are integers.\nExample 13.2 A comparison of binary-exchange, 2-D transpose,\nand 3-D transpose algorithms\nThis example shows that either the binary-exchange algorithm or any of the transpose\nalgorithms may be the algorithm of choice for a given parallel computer, depending\non the size of the FFT. Consider a 64-node version of the hypercube described in\nExample 13.1 with tc = 2, ts = 25, and tw = 4. Figure 13.12  shows speedups attained\nby the binary-exchange algorithm, the 2-D transpose algorithm, and the 3-D\ntranspose algorithm for different problem sizes. The speedups are based on the\nparallel run times given by Equations 13.4, 13.11 , and 13.13 , respectively. The figure\nshows that for different ranges of n, a different algorithm provides the highest\nspeedup for an n-point FFT. For the given values of the hardware parameters, the\nbinary-exchange algorithm is best suited for very low granularity FFT computations,\nthe 2-D transpose algorithm is best for very high granularity computations, and the 3-\nD transpose algorithm's speedup is the maximum for intermediate granularities. \nFigure 13.12. A comparison of the speedups obtained by the\nbinary-exchange, 2-D transpose, and 3-D transpose algorithms\non a 64-node hypercube with tc = 2, tw = 4, and ts = 25.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \n13.4 Bibliographic Remarks\nDue to the important role of Fourier transform in scientific and technical computations, there\nhas been great interest in implementing FFT on parallel computers and on studying its\nperformance. Swarztrauber [ Swa87] describes many implementations of the FFT algorithm on\nvector and parallel computers. Cvetanovic [ Cve87 ] and Norton and Silberger [ NS87 ] give a\ncomprehensive performance analysis of the FFT algorithm on pseudo-shared-memory\narchitectures such as the IBM RP-3. They consider various partitionings of data among memory\nblocks and, in each case, obtain expressions for communication overhead and speedup in terms\nof problem size, number of processes, memory latency, CPU speed, and speed of\ncommunication. Aggarwal, Chandra, and Snir [ACS89c] analyze the performance of FFT and\nother algorithms on LPRAM \u2013 a new model for parallel computation. This model differs from the\nstandard PRAM model in that remote accesses are more expensive than local accesses in an\nLPRAM. Parallel FFT algorithms and their implementation and experimental evaluation on\nvarious architectures have been pursued by many other researchers [ AGGM90, Bai90 , BCJ90 ,\nBKH89 , DT89 , GK93b , JKFM89 , KA88 , Loa92 ].\nThe basic FFT algorithm whose parallel formulations are discussed in this chapter is called the\nunordered FFT because the elements of the output sequence are stored in bit-reversed index\norder. In other words, the frequency spectrum of the input signal is obtained by reordering the\nelements of the output sequence Y produced by Algorithm 13.2 in such a way that for all i, Y [i]\nis replaced by Y [j], where j is obtained by reversing the bits in the binary representation of i.\nThis is a permutation operation ( Section 4.6 ) and is known as bit reversal . Norton and\nSilberger [ NS87 ] show that an ordered transform can be obtained with at most 2 d + 1\ncommunication steps, where d = log p. Since the unordered FFT computation requires only d\ncommunication", "doc_id": "acb6f6d4-af6e-4e55-a556-cc2717d06dde", "embedding": null, "doc_hash": "a4f9894b9b06d8ae113c3455c0a16a2531a1be69ab158d77ad6821d8dacbe6af", "extra_info": null, "node_info": {"start": 1516096, "end": 1519555}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "cfff0532-1c8e-4178-8963-035fd8cb465f", "3": "7f4e2d94-bad2-44e8-a5ab-a126cc399ce9"}}, "__type__": "1"}, "7f4e2d94-bad2-44e8-a5ab-a126cc399ce9": {"__data__": {"text": "In other words, the frequency spectrum of the input signal is obtained by reordering the\nelements of the output sequence Y produced by Algorithm 13.2 in such a way that for all i, Y [i]\nis replaced by Y [j], where j is obtained by reversing the bits in the binary representation of i.\nThis is a permutation operation ( Section 4.6 ) and is known as bit reversal . Norton and\nSilberger [ NS87 ] show that an ordered transform can be obtained with at most 2 d + 1\ncommunication steps, where d = log p. Since the unordered FFT computation requires only d\ncommunication steps, the total communication overhead in the case of ordered FFT is roughly\ndouble of that for unordered FFT. Clearly, an unordered transform is preferred where\napplicable. The output sequence need not be ordered when the transform is used as a part of a\nlarger computation and as such remains invisible to the user [ Swa87]. In many practical\napplications of FFT, such as convolution and solution of the discrete Poisson equation, bit\nreversal can be avoided [ Loa92]. If required, bit reversal can be performed by using an\nalgorithm described by Van Loan [ Loa92 ] for a distributed-memory parallel computer. The\nasymptotic communication complexity of this algorithm is the same as that of the binary-\nexchange algorithm on a hypercube.\nSeveral variations of the simple FFT algorithm presented here have been suggested in the\nliterature. Gupta and Kumar [ GK93b] show that the total communication overhead for mesh\nand hypercube architectures is the same for the one- and two-dimensional FFTs. Certain\nschemes for computing the DFT have been suggested that involve fewer arithmetic operations\non a serial computer than the simple Cooley-Tukey FFT algorithm requires [ Nus82, RB76 ,\nWin77 ]. Notable among these are computing one-dimensional FFTs with radix greater than two\nand computing multidimensional FFTs by transforming them into a set of one-dimensional FFTs\nby using the polynomial transform method. A radix- q FFT is computed by splitting the input\nsequence of size n into q sequences of size n/q each, computing the q smaller FFTs, and then\ncombining the result. For example, in a radix-4 FFT, each step computes four outputs from four\ninputs, and the total number of iterations is log 4 n rather than log 2 n. The input length should,\nof course, be a power of four. Despite the reduction in the number of iterations, the aggregate\ncommunication time for a radix- q FFT remains the same as that for radix-2. For example, for a\nradix-4 algorithm on a hypercube, each communication step now involves four processes\ndistributed in two dimensions rather than two processes in one dimension. In contrast, the\nnumber of multiplications in a radix-4 FFT is 25% fewer than in a radix-2 FFT [ Nus82]. This\nnumber can be marginally improved by using higher radices, but the amount of communication\nremains unchanged.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nProblems\n13.1 Let the serial run time of an n-point FFT computation be tcn log n. Consider its\nimplementation on an architecture on which the parallel run time is ( tcn log n)/p + (twn log\np)/p. Assume that tc = 1and tw = 0.2.\nWrite expressions for the speedup and efficiency.1.\nWhat is the isoefficiency function if an efficiency of 0.6 is desired?2.\nHow will the isoefficiency function change (if at all) if an efficiency of 0.4 is desired?3.\nRepeat parts 1 and 2 for the case in which tw = 1 and everything else is the same. 4.\n13.2 [ Tho83 ] Show that, while performing FFT on a square mesh of p processes by using\nany mapping of data onto the processes, there", "doc_id": "7f4e2d94-bad2-44e8-a5ab-a126cc399ce9", "embedding": null, "doc_hash": "7bca2c060666f41bf1519e375a2e1a21905ec4ad9cf30ee2c71cc2da2def00a6", "extra_info": null, "node_info": {"start": 1519497, "end": 1523081}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "acb6f6d4-af6e-4e55-a556-cc2717d06dde", "3": "1b22b670-8831-430b-bbe8-49f9a8a106d1"}}, "__type__": "1"}, "1b22b670-8831-430b-bbe8-49f9a8a106d1": {"__data__": {"text": "log n)/p + (twn log\np)/p. Assume that tc = 1and tw = 0.2.\nWrite expressions for the speedup and efficiency.1.\nWhat is the isoefficiency function if an efficiency of 0.6 is desired?2.\nHow will the isoefficiency function change (if at all) if an efficiency of 0.4 is desired?3.\nRepeat parts 1 and 2 for the case in which tw = 1 and everything else is the same. 4.\n13.2 [ Tho83 ] Show that, while performing FFT on a square mesh of p processes by using\nany mapping of data onto the processes, there is at least one iteration in which the pairs of\nprocesses that need to communicate are at least \n  links apart.\n13.3 Describe the communication pattern of the binary-exchange algorithm on a linear\narray of p processes. What are the parallel run time, speedup, efficiency, and isoefficiency\nfunction of the binary-exchange algorithm on a linear array?\n13.4 Show that, if ts = 0, the isoefficiency function of the binary-exchange algorithm on a\nmesh is given by \nHint:  Use Equation 13.10 .\n13.5 Prove that the maximum number of twiddle factors computed by any process in the\nparallel implementation of an n-point FFT using p processes is given by the recurrence\nrelation given in Section 13.2.3 .\n13.6 Derive expressions for the parallel run time, speedup, and efficiency of the two-\ndimensional transpose algorithm described in Section 13.3.1  for an n-point FFT on a p-\nnode two-dimensional mesh and a linear array of p nodes.\n13.7 Ignoring ts, by what factor should the communication bandwidth of a p-node mesh\nbe increased so that it yields the same performance on the two-dimensional transpose\nalgorithm for an n-point FFT on a p-node hypercube?\n13.8 You are given the following sets of communication-related constants for a hypercube\nnetwork: (i) ts = 250, tw = 1, (ii) ts = 50, tw = 1, (iii) ts = 10, tw =1, (iv) ts = 2, tw =1,\nand (v) ts = 0, tw = 1.\nGiven a choice among the binary-exchange algorithm and the two-, three-, four-, and\nfive-dimensional transpose algorithms, which one would you use for n = 215 and p =\n212 for each of the preceding sets of values of ts and tw?1.\nRepeat part 1 for (a) n = 212, p = 26, and (b) n = 220, p = 212. 2.\n13.9 [ GK93b] Consider computing an n-point FFT on a \n  mesh. If the channel\n2.\nbandwidth grows at a rate of Q(px) (x > 0) with the number of nodes p in the mesh, show\nthat the isoefficiency function due to communication overhead is Q(p0.5-x22(tw/tc)p0.5-x) and\nthat due to concurrency is Q(p1+x log p). Also show that the best possible isoefficiency for\nFFT on a mesh is Q(p1.5 log p), even if the channel bandwidth increases arbitrarily with the\nnumber of nodes in the network.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nAppendix A. Complexity of Functions and\nOrder Analysis\nOrder analysis and the asymptotic complexity of functions are used extensively in this book to\nanalyze the performance of algorithms.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nA.1 Complexity of Functions\nWhen analyzing parallel algorithms in this book, we use the following three types of functions:\nExponential functions:  A function f from reals to reals is called an exponential  function\nin x if it can be expressed in the form f (x) = ax for x, a \n \n (the set of real", "doc_id": "1b22b670-8831-430b-bbe8-49f9a8a106d1", "embedding": null, "doc_hash": "d43baaa6dbef0e5861cf62100635ee0466f2c49a6984b32c570d73c14f9749bb", "extra_info": null, "node_info": {"start": 1523139, "end": 1526320}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7f4e2d94-bad2-44e8-a5ab-a126cc399ce9", "3": "351805d9-2e03-4a58-9755-da2ad73f11f1"}}, "__type__": "1"}, "351805d9-2e03-4a58-9755-da2ad73f11f1": {"__data__": {"text": "A. Complexity of Functions and\nOrder Analysis\nOrder analysis and the asymptotic complexity of functions are used extensively in this book to\nanalyze the performance of algorithms.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nA.1 Complexity of Functions\nWhen analyzing parallel algorithms in this book, we use the following three types of functions:\nExponential functions:  A function f from reals to reals is called an exponential  function\nin x if it can be expressed in the form f (x) = ax for x, a \n \n (the set of real numbers) and\na > 1. Examples of exponential functions are 2x, 1.5x +2, and 31.5x.1.\nPolynomial functions:  A function f from reals to reals is called a polynomial  function of\ndegree  b in x if it can be expressed in the form f (x) = xb for x, b \n \n and b > 0. A linear\nfunction is a polynomial function of degree one and a quadratic  function is a polynomial\nfunction of degree two. Examples of polynomial functions are 2, 5 x, and 5.5 x2.3.\nA function f that is a sum of two polynomial functions g and h is also a polynomial function\nwhose degree is equal to the maximum of the degrees of g and h. For example, 2 x + x2 is\na polynomial function of degree two.2.\nLogarithmic functions:  A function f from reals to reals that can be expressed in the form\nf (x) = log b x for b \n \n and b > 1 is logarithmic  in x. In this expression, b is called the\nbase of the logarithm. Examples of logarithmic functions are log 1.5 x and log 2 x. Unless\nstated otherwise, all logarithms in this book are of base two. We use log x to denote log 2x,\nand log2 x to denote (log 2 x)2.3.\nMost functions in this book can be expressed as sums of two or more functions. A function f is\nsaid to dominate  a function g if f (x) grows at a faster rate than g(x). Thus, function f\ndominates function g if and only if f (x)/g(x) is a monotonically increasing function in x. In other\nwords, f dominates g if and only if for any constant c > 0, there exists a value x0 such that f (x)\n> cg(x) for x > x0. An exponential function dominates a polynomial function and a polynomial\nfunction dominates a logarithmic function. The relation dominates  is transitive. If function f\ndominates function g, and function g dominates function h, then function f also dominates\nfunction h. Thus, an exponential function also dominates a logarithmic function.\n[ Team LiB ]\n  \n\n[ Team LiB ]\n  \nA.2 Order Analysis of Functions\nIn the analysis of algorithms, it is often cumbersome or impossible to derive exact expressions\nfor parameters such as run time, speedup, and efficiency. In many cases, an approximation of\nthe exact expression is adequate. The approximation may indeed be more illustrative of the\nbehavior of the function because it focuses on the critical factors influencing the parameter.\nExample A.1 Distances traveled by three cars\nConsider three cars A, B, and C. Assume that we start monitoring the cars at time t =\n0. At t = 0, car A is moving at a velocity of 1000 feet per second and maintains a\nconstant velocity. At t = 0, car B 's velocity is 100 feet per second and it is\naccelerating at a rate of 20 feet per second per second. Car C starts from a standstill\nat t = 0 and accelerates at a rate of 25 feet per second per second. Let DA(t), DB (t),\nand DC (t) represent the distances traveled in t seconds by cars", "doc_id": "351805d9-2e03-4a58-9755-da2ad73f11f1", "embedding": null, "doc_hash": "5ed7d30dac224f1a2ae4af6e9f84ef7e2b876e7ef418963613507b63cf5bf28a", "extra_info": null, "node_info": {"start": 1526307, "end": 1529607}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1b22b670-8831-430b-bbe8-49f9a8a106d1", "3": "e3be30e8-b347-4c37-bc51-2a377a769ee4"}}, "__type__": "1"}, "e3be30e8-b347-4c37-bc51-2a377a769ee4": {"__data__": {"text": "Distances traveled by three cars\nConsider three cars A, B, and C. Assume that we start monitoring the cars at time t =\n0. At t = 0, car A is moving at a velocity of 1000 feet per second and maintains a\nconstant velocity. At t = 0, car B 's velocity is 100 feet per second and it is\naccelerating at a rate of 20 feet per second per second. Car C starts from a standstill\nat t = 0 and accelerates at a rate of 25 feet per second per second. Let DA(t), DB (t),\nand DC (t) represent the distances traveled in t seconds by cars A, B, and C. From\nelementary physics, we know that\nNow, we compare the cars according to the distance they travel in a given time. For t\n> 45 seconds, car B outperforms car A. Similarly, for t > 20 seconds, car C\noutperforms car B, and for t > 40 seconds, car C outperforms car A. Furthermore, DC\n(t) < 1.25 DB (t) and DB (t) < DC (t) for t > 20, which implies that after a certain\ntime, the difference in the performance of cars B and C is bounded by the other scaled\nby a constant multiplicative factor. All these facts can be captured by the order\nanalysis of the expressions. \nThe Q Notation:  From Example A.1 , DC (t) < 1.25 DB(t) and DB(t) < DC (t) for t > 20; that is,\nthe difference in the performance of cars B and C after t = 0 is bounded by the other scaled by a\nconstant multiplicative factor. Such an equivalence in performance is often significant when\nanalyzing performance. The Q notation captures the relationship between these two functions.\nThe functions DC(t) and DB(t) can be expressed by using the Q notation as DC(t) = Q(DB(t)) and\nDB(t) = Q(DC(t)). Furthermore, both functions are equal to Q(t2).\nFormally, the Q notation is defined as follows: given a function g(x), f(x) = Q(g(x)) if and only if\nfor any constants c1, c2 > 0, there exists an x0 \n 0 such that c1g(x) \n f (x) \n c2 g(x) for all x\n x0.\nThe O Notation:  Often, we would like to bound the growth of a particular parameter by a\nsimpler function. From Example A.1  we have seen that for t > 45, DB(t) is always greater than\nDA (t). This relation between DA(t) and DB(t) is expressed using the O (big-oh) notation as DA(t)\n= O (DB(t)).\nFormally, the O notation is defined as follows: given a function g(x), f (x) = O (g(x)) if and only\nif for any constant c > 0, their exists an x0 \n 0 such that f (x) \n cg(x) for all x \n x0. From\nthis definition we deduce that DA(t) = O(t2) and DB(t) = O(t2). Furthermore, DA(t) = O (t) also\nsatisfies the conditions of the O notation.\nThe W Notation:  The O notation sets an upper bound on the rate of growth of a function. The W\nnotation is the converse of the O notation; that is, it sets a lower bound on the rate of growth of\na function. From Example A.1 , DA(t) < DC (t) for t > 40. This relationship can be expressed\nusing the W notation as DC (t) = W(DA(t)).\nFormally, given a function g(x), f (x) = W(g(x)) if and only if for any constant c > 0, there exists\nan x0 \n 0 such that f (x) \n cg(x) for all x \n x0.\nProperties of Functions Expressed in Order Notation\nThe order notations for expressions have a number of properties that are useful", "doc_id": "e3be30e8-b347-4c37-bc51-2a377a769ee4", "embedding": null, "doc_hash": "618bd2ed9269dbd97e9920d0ffa9cd2c775e25d5d649a694a192e7ffe5ad0d49", "extra_info": null, "node_info": {"start": 1529611, "end": 1532703}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "351805d9-2e03-4a58-9755-da2ad73f11f1", "3": "f774a08c-4ae5-47fe-9c1a-727fabcfc736"}}, "__type__": "1"}, "f774a08c-4ae5-47fe-9c1a-727fabcfc736": {"__data__": {"text": "it sets a lower bound on the rate of growth of\na function. From Example A.1 , DA(t) < DC (t) for t > 40. This relationship can be expressed\nusing the W notation as DC (t) = W(DA(t)).\nFormally, given a function g(x), f (x) = W(g(x)) if and only if for any constant c > 0, there exists\nan x0 \n 0 such that f (x) \n cg(x) for all x \n x0.\nProperties of Functions Expressed in Order Notation\nThe order notations for expressions have a number of properties that are useful when analyzing\nthe performance of algorithms. Some of the important properties are as follows:\nxa = O(xb) if and only if a \n b. 1.\nloga (x) = Q(logb(x)) for all a and b. 2.\nax = O (bx) if and only if a \n b. 3.\nFor any constant c, c = O (1). 4.\nIf f = O (g) then f + g = O (g). 5.\nIf f = Q(g) then f + g = Q(g) = Q(f). 6.\nf = O (g) if and only if g = W(f). 7.\nf = Q(g) if and only if f = W(g) and f = O(g). 8.\n[ Team LiB ]\n  \n\n[ Team LiB ]\nBibliography\n[ABJ82] S. G. Akl, D. T. Bernard, and R. J. Jordan. Design and implementation of a parallel tree\nsearch algorithm . IEEE Transactions on Pattern Analysis and Machine Intelligence , PAMI-\n4:192\u2013203, 1982.\n[ACM91] ACM. Resources in Parallel and Concurrent Systems . ACM Press, New York, NY, 1991.\n[ACS89a] A. Aggarwal, A. K. Chandra, and M. Snir. A model for hierarchical memory . Technical\nReport RC 15118 (No. 67337), IBM T. J. Watson Research Center, Yorktown Heights, NY, 1989.\n[ACS89b] A. Aggarwal, A. K. Chandra, and M. Snir. On communication latency in PRAM\ncomputations . Technical Report RC 14973 (No. 66882), IBM T. J. Watson Research Center,\nYorktown Heights, NY, 1989.\n[ACS89c] A. Aggarwal, A. K. Chandra, and M. Snir. Communication complexity of PRAMs .\nTechnical Report RC 14998 (64644), IBM T. J. Watson Research Center, Yorktown Heights, NY,\n1989.\n[ADJ+91] A. Agarwal, G. D'Souza, K. Johnson, D. Kranz, J. Kubiatowicz, K. Kurihara, B.-H. Lim,\nG. Maa, D. Nussbaum, M. Parkin, and D. Yeung. The MIT alewife machine : A large-scale\ndistributed-memory multiprocessor . In Proceedings of Workshop on Scalable Shared Memory\nMultiprocessors . Kluwer Academic, 1991.\n[AFKW90] I. Angus, G. C. Fox, J. Kim, and D. W. Walker. Solving Problems on Concurrent\nProcessors: Software for Concurrent Processors: Volume II . Prentice-Hall, Englewood Cliffs, NJ,\n1990.\n[AG94] G. S. Almasi and A. Gottlieb. Highly Parallel Computing . Benjamin/Cummings, Redwood\nCity, CA, 1994. (Second Edition).\n[Aga89] A. Agarwal. Performance tradeoffs in multithreaded processors . Technical Report 89-\n566, Massachusetts Institute of Technology, Microsystems Program Office, Cambridge, MA,\n1989.\n[Aga91] A. Agarwal. Performance tradeoffs in multithreaded processors . Technical report\nMIT/LCS/TR 501; VLSI memo", "doc_id": "f774a08c-4ae5-47fe-9c1a-727fabcfc736", "embedding": null, "doc_hash": "3f8de638611eff261443e34baeeabafe812fe299ea1e68362ad9fb122194fab3", "extra_info": null, "node_info": {"start": 1532746, "end": 1535454}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "e3be30e8-b347-4c37-bc51-2a377a769ee4", "3": "53133d62-848d-4b88-9429-c37d77543eea"}}, "__type__": "1"}, "53133d62-848d-4b88-9429-c37d77543eea": {"__data__": {"text": "Englewood Cliffs, NJ,\n1990.\n[AG94] G. S. Almasi and A. Gottlieb. Highly Parallel Computing . Benjamin/Cummings, Redwood\nCity, CA, 1994. (Second Edition).\n[Aga89] A. Agarwal. Performance tradeoffs in multithreaded processors . Technical Report 89-\n566, Massachusetts Institute of Technology, Microsystems Program Office, Cambridge, MA,\n1989.\n[Aga91] A. Agarwal. Performance tradeoffs in multithreaded processors . Technical report\nMIT/LCS/TR 501; VLSI memo no. 89-566, Laboratory for Computer Science, Massachusetts\nInstitute of Technology, Cambridge, MA, 1991.\n[AGGM90] A. Averbuch, E. Gabber, B. Gordissky, and Y. Medan. A parallel FFT on an MIMD\nmachine . Parallel Computing , 15:61\u201374, 1990.\n[Agh86] G. Agha. Actors: A Model of Concurrent Computation in Distributed Systems . MIT Press,\nCambridge, MA, 1986.\n[AHMP87] H. Alt, T. Hagerup, K. Mehlhorn, and F. P. Preparata. Deterministic simulation of\nidealized parallel computers on more realistic ones . SIAM Journal of Computing ,\n16(5):808\u2013835, October 1987.\n[AHU74] A. V. Aho, J. E. Hopcroft, and J. D. Ullman. The Design and Analysis of Computer\nAlgorithms . Addison-Wesley, Reading, MA, 1974.\n[AJM88] D. P. Agrawal, V. K. Janakiram, and R. Mehrotra. A randomized parallel branch-and-\nbound algorithm . In Proceedings of the 1988 International Conference on Parallel Processing ,\n1988.\n[AK84] M. J. Atallah and S. R. Kosaraju. Graph problems on a mesh-connected processor array .\nJournal of ACM , 31(3):649\u2013667, July 1984.\n[Akl85] S. G. Akl. Parallel Sorting Algorithms . Academic Press, San Diego, CA, 1985.\n[Akl89] S. G. Akl. The Design and Analysis of Parallel Algorithms . Prentice-Hall, Englewood\nCliffs, NJ, 1989.\n[Akl97] S. G. Akl. Parallel Computation Models and Methods . Prentice-Hall, Englewood Cliffs, NJ,\n1997.\n[AKR89] S. Arvindam, V. Kumar, and V. N. Rao. Floorplan optimization on multiprocessors . In\nProceedings of the 1989 International Conference on Computer Design , 1989. Also published as\nTechnical Report ACT-OODS-241-89, Microelectronics and Computer Corporation, Austin, TX.\n[AKR90] S. Arvindam, V. Kumar, and V. N. Rao. Efficient parallel algorithms for search\nproblems: Applications in VLSI CAD . In Proceedings of the Third Symposium on the Frontiers of\nMassively Parallel Computation , 1990.\n[AKRS91] S. Arvindam, V. Kumar, V. N. Rao, and V. Singh. Automatic test pattern generation\non multiprocessors . Parallel Computing , 17, number 12:1323\u20131342, December 1991.\n[AKS83] M. Ajtai, J. Komlos, and E. Szemeredi. An O (n log n) sorting network . In Proceedings\nof the 15th Annual ACM Symposium on Theory of Computing , 1\u20139, 1983.\n[AL93] S. G. Akl and K. A. Lyons. Parallel Computational Geometry . Prentice-Hall, Englewood\nCliffs, NJ, 1993.\n[AM88] T. S. Abdelrahman and T. N. Mudge. Parallel", "doc_id": "53133d62-848d-4b88-9429-c37d77543eea", "embedding": null, "doc_hash": "c4cf18befc529d7b57d98434b7f6a9d3f83663ed8454a0f600226cf1be7b8497", "extra_info": null, "node_info": {"start": 1535428, "end": 1538203}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f774a08c-4ae5-47fe-9c1a-727fabcfc736", "3": "0a96f709-b94e-41d7-a687-c9ea5482513c"}}, "__type__": "1"}, "0a96f709-b94e-41d7-a687-c9ea5482513c": {"__data__": {"text": "Computing , 17, number 12:1323\u20131342, December 1991.\n[AKS83] M. Ajtai, J. Komlos, and E. Szemeredi. An O (n log n) sorting network . In Proceedings\nof the 15th Annual ACM Symposium on Theory of Computing , 1\u20139, 1983.\n[AL93] S. G. Akl and K. A. Lyons. Parallel Computational Geometry . Prentice-Hall, Englewood\nCliffs, NJ, 1993.\n[AM88] T. S. Abdelrahman and T. N. Mudge. Parallel branch-and-bound algorithms on\nhypercube multiprocessors . In Proceedings of the Third Conference on Hypercubes, Concurrent\nComputers, and Applications , 1492\u20131499, New York, NY, 1988. ACM Press.\n[Amd67] G. M. Amdahl. Validity of the single processor approach to achieving large scale\ncomputing capabilities . In AFIPS Conference Proceedings , 483\u2013485, 1967.\n[And91] G. R. Andrews. Concurrent Programming: Principles and Practice . Benjamin/Cummings,\nRedwood City, CA, 1991.\n[AOB93] B. Abali, F. Ozguner, and A. Bataineh. Balanced parallel sort on hypercube\nmultiprocessors . IEEE Transactions on Parallel and Distributed Systems , 4(5):572\u2013581, May\n1993.\n[AS87] B. Awerbuch and Y. Shiloach. New connectivity and MSF algorithms for shuffle-exchange\nnetwork and PRAM . IEEE Transactions on Computers , C\u2013 36(10):1258\u20131263, October 1987.\n[AU72] A. V. Aho and J. D. Ullman. The Theory of Parsing, Translation and Compiling: Volume\n1, Parsing . Prentice-Hall, Englewood Cliffs, NJ, 1972.\n[B+97] L. S. Blackford et al. ScaLAPACK Users' Guide . SIAM, 1997.\n[BA82] M. Ben-Ari. Principles of Concurrent Programming . Prentice-Hall, Englewood Cliffs, NJ,\n1982.\n[Bab88] R. G. Babb. Programming Parallel Processors . Addison-Wesley, Reading, MA, 1988.\n[Bai90] D. H. Bailey. FFTs in external or hierarchical memory . Journal of Supercomputing ,\n4:23\u201335, 1990.\n[Bar68] G. H. Barnes. The ILLIAC IV computer . IEEE Transactions on Computers , C-\n17(8):746\u2013757, 1968.\n[Bat68] K. E. Batcher. Sorting networks and their applications . In Proceedings of the 1968\nSpring Joint Computer Conference  , 307\u2013314, 1968.\n[Bat76] K. E. Batcher. The Flip network in STARAN . In Proceedings of International Conference\non Parallel Processing , 65\u201371, 1976.\n[Bat80] K. E. Batcher. Design of a massively parallel processor . IEEE Transactions on\nComputers , 836\u2013840, September 1980.\n[Bau78] G. M. Baudet. The Design and Analysis of Algorithms for Asynchronous Multiprocessors .\nPh.D. Thesis, Carnegie-Mellon University, Pittsburgh, PA, 1978.\n[BB90] K. P. Belkhale and P. Banerjee. Approximate algorithms for the partitionable\nindependent task scheduling problem . In Proceedings of the 1990 International Conference on\nParallel Processing , I72\u2013I75, 1990.\n[BBN89] BBN Advanced Computers Inc. TC-2000 Technical Product Summary . Cambridge, MA.\n1989.\n[BCCL95] R. E. Bixby, W. Cook, A. Cox, and E. K. Lee. Parallel mixed integer programming .\nTechnical Report CRPC TR 95554, Center for Research", "doc_id": "0a96f709-b94e-41d7-a687-c9ea5482513c", "embedding": null, "doc_hash": "61dd21206d050782367c8abb7f1350c314f50ddf182be3c55091a8b3930f27ee", "extra_info": null, "node_info": {"start": 1538289, "end": 1541127}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "53133d62-848d-4b88-9429-c37d77543eea", "3": "492f8551-600e-4b52-a503-23313c4aebdd"}}, "__type__": "1"}, "492f8551-600e-4b52-a503-23313c4aebdd": {"__data__": {"text": "Pittsburgh, PA, 1978.\n[BB90] K. P. Belkhale and P. Banerjee. Approximate algorithms for the partitionable\nindependent task scheduling problem . In Proceedings of the 1990 International Conference on\nParallel Processing , I72\u2013I75, 1990.\n[BBN89] BBN Advanced Computers Inc. TC-2000 Technical Product Summary . Cambridge, MA.\n1989.\n[BCCL95] R. E. Bixby, W. Cook, A. Cox, and E. K. Lee. Parallel mixed integer programming .\nTechnical Report CRPC TR 95554, Center for Research on Parallel Computation, Research\nMonograph, 1995.\n[BCJ90] E. C. Bronson, T. L. Casavant, and L. H. Jamieson. Experimental application-driven\narchitecture analysis of an SIMD/MIMD parallel processing system . IEEE Transactions on Parallel\nand Distributed Systems , 1(2):195\u2013205, 1990.\n[BDHM84] D. Bitton, D. J. DeWitt, D. K. Hsiao, and M. J. Menon. A taxonomy of parallel sorting .\nComputing Surveys , 16(3):287\u2013318, September 1984.\n[Bel57] R. Bellman. Dynamic Programming . Princeton University Press, Princeton, NJ, 1957.\n[Bel58] R. Bellman. On a routing problem . Quarterly of Applied Mathematics , 16(1):87\u201390,\n1958.\n[Ben80] J. L. Bentley. A parallel algorithm for constructing minimum spanning trees . Journal of\nthe ACM , 27(1):51\u201359, March 1980.\n[Ber84] S. Berkowitz. On computing the determinant in small parallel time using a small\nnumber of processors . Information Processing Letters , 18(3):147\u2013150, March 1984.\n[Ber89] J. Berntsen. Communication efficient matrix multiplication on hypercubes . Parallel\nComputing , 12:335\u2013342, 1989.\n[BH82] A. Borodin and J. E. Hopcroft. Routing merging and sorting on parallel models of\ncomputation . In Proceedings of the 14th Annual ACM Symposium on Theory of Computing ,\n338\u2013344, May 1982.\n[Bix91] R. E. Bixby. Two applications of linear programming . In Proceedings of the Workshop on\nParallel Computing of Discrete Optimization Problems , 1991.\n[BJK+95] R. Blumofe, C. Joerg, B. Kuszmaul, C. Leiserson, K. Randall, and Y. Zhou. Cilk: An\nefficient multithreaded runtime system . In Proceedings of the 5th Symposium on Principles and\nPractice of Parallel Programming , 1995.\n[BKH89] S. Bershader, T. Kraay, and J. Holland. The giant-Fourier-transform . In Proceedings of\nthe Fourth Conference on Hypercubes, Concurrent Computers, and Applications: Volume I ,\n387\u2013389, 1989.\n[Ble90] G. E. Blelloch. Vector Models for Data-Parallel Computing . MIT Press, Cambridge, MA,\n1990.\n[BMCP98] A. Brungger, A. Marzetta, J. Clausen, and M. Perregaard. Solving large-scale qap\nproblems in parallel with the search library zram . Journal of Parallel and Distributed Computing ,\n50:157\u2013169, 1998.\n[BNK92] A. Bar-Noy and S. Kipnis. Designing broadcasting algorithms in the postal model for\nmessage-passing systems . In Proceedings of 4th ACM Symposium on Parallel Algorithms and\nArchitectures , 13\u201322, 1992.\n[BOS+91] D. P. Bertsekas, C. Ozveren, G. D. Stamoulis, P.", "doc_id": "492f8551-600e-4b52-a503-23313c4aebdd", "embedding": null, "doc_hash": "ef6ef39b7a0606f81a3de78d062805636986c30fbffb6826c16eee300547a2b6", "extra_info": null, "node_info": {"start": 1541039, "end": 1543910}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0a96f709-b94e-41d7-a687-c9ea5482513c", "3": "1a198fcf-7dd6-4ee1-b771-3898d8b9021a"}}, "__type__": "1"}, "1a198fcf-7dd6-4ee1-b771-3898d8b9021a": {"__data__": {"text": "and M. Perregaard. Solving large-scale qap\nproblems in parallel with the search library zram . Journal of Parallel and Distributed Computing ,\n50:157\u2013169, 1998.\n[BNK92] A. Bar-Noy and S. Kipnis. Designing broadcasting algorithms in the postal model for\nmessage-passing systems . In Proceedings of 4th ACM Symposium on Parallel Algorithms and\nArchitectures , 13\u201322, 1992.\n[BOS+91] D. P. Bertsekas, C. Ozveren, G. D. Stamoulis, P. Tseng, and J. N. Tsitsiklis. Optimal\ncommunication algorithms for hypercubes . Journal of Parallel and Distributed Computing ,\n11:263\u2013275, 1991.\n[BR90] R. Boppana and C. S. Raghavendra. On optimal and practical routing methods for a\nmassive data movement operation on hypercubes . Technical report, University of Southern\nCalifornia, Los Angeles, CA, 1990.\n[Bra97] R. Bramley. Technology news & reviews: Chemkin software; OpenMP Fortran Standard;\nODE toolbox for Matlab; Java products; Scientific WorkPlace 3.0 . IEEE Computational Science\nand Engineering , 4(4):75\u201378, October/December 1997.\n[Bro79] K. Brown. Dynamic programming in computer science . Technical Report CMU-CS-79-\n106, Carnegie Mellon University, Pittsburgh, PA, 1979.\n[BS78] G. M. Baudet and D. Stevenson. Optimal sorting algorithms for parallel computers . IEEE\nTransactions on Computers , C\u201327(1):84\u201387, January 1978.\n[BT89] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical\nMethods . Prentice-Hall, NJ, 1989.\n[BT97] D. P. Bertsekas and J. N. Tsitsiklis. Parallel and Distributed Computation: Numerical\nMethods . Athena Scientific, 1997.\n[But97] D. R. Butenhof. Programming with POSIX Threads . Addison-Wesley, Reading, MA,\n1997.\n[Buy99] R. Buyya, editor. High Performance Cluster Computing: Architectures and Systems .\nPrentice Hall, 1999.\n[BW89] M. L. Barton and G. R. Withers. Computing performance as a function of the speed,\nquantity, and the cost of processors . In Supercomputing '89 Proceedings , 759\u2013764, 1989.\n[BW97] J. Beveridge and R. Wiener. Multithreading Applications in Win32: the Complete Guide\nto Threads . Addison-Wesley Developers Press, Reading, MA, 1997.\n[C+95] J. Choi et al. A proposal for a set of Parallel Basic Linear Algebra Subprograms .\nTechnical Report CS-95-292, Computer Science Department, University of Tennessee, 1995.\n[CAHH91] N. P. Chrisopchoides, M. Aboelaze, E. N. Houstis, and C. E. Houstis. The\nparallelization of some level 2 and 3 BLAS operations on distributed-memory machines . In\nProceedings of the First International Conference of the Austrian Center of Parallel Computation .\nSpringer-Verlag Series Lecture Notes in Computer Science, 1991.\n[Can69] L. E. Cannon. A cellular computer to implement the Kalman Filter Algorithm . Ph.D.\nThesis, Montana State University, Bozman, MT, 1969.\n[Car89] G. F. Carey, editor. Parallel Supercomputing: Methods, Algorithms and Applications .\nWiley, New York, NY, 1989.\n[CD87] S. Chandran and L. S. Davis. An approach to parallel vision algorithms . In R. Porth,\neditor, Parallel Processing . SIAM, Philadelphia, PA,", "doc_id": "1a198fcf-7dd6-4ee1-b771-3898d8b9021a", "embedding": null, "doc_hash": "3bce43d689f7058d17e03a317748c51787c3f86e7b0bfa9131c381bfd3e925b8", "extra_info": null, "node_info": {"start": 1543949, "end": 1546983}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "492f8551-600e-4b52-a503-23313c4aebdd", "3": "1e49beb2-333b-4906-835b-446f18db5808"}}, "__type__": "1"}, "1e49beb2-333b-4906-835b-446f18db5808": {"__data__": {"text": "Series Lecture Notes in Computer Science, 1991.\n[Can69] L. E. Cannon. A cellular computer to implement the Kalman Filter Algorithm . Ph.D.\nThesis, Montana State University, Bozman, MT, 1969.\n[Car89] G. F. Carey, editor. Parallel Supercomputing: Methods, Algorithms and Applications .\nWiley, New York, NY, 1989.\n[CD87] S. Chandran and L. S. Davis. An approach to parallel vision algorithms . In R. Porth,\neditor, Parallel Processing . SIAM, Philadelphia, PA, 1987.\n[CDK+00] R. Chandra, L. Dagum, D. Kohr, D. Maydan, J. McDonald, and R. M. (editors). Parallel\nProgramming in OpenMP . Morgan Kaufmann Publishers, 2000.\n[CG87] E. Chu and A. George. Gaussian elimination with partial pivoting and load balancing on\na multiprocessor . Parallel Computing , 5:65\u201374, 1987.\n[CGK93] D. Challou, M. Gini, and V. Kumar. Parallel search algorithms for robot motion\nplanning . In Proceedings of the IEEE Conference on Robotics and Automation , 46\u201351, 1993.\n[CGL92] D. E. Culler, M. Gunter, and J. C. Lee. Analysis of multithreaded microprocessors under\nmultiprogramming . Report UCB/CSD 92/687, University of California, Berkeley, Computer\nScience Division, Berkeley, CA, May 1992.\n[Cha79] A. K. Chandra. Maximal parallelism in matrix multiplication . Technical Report RC-6193,\nIBM T. J. Watson Research Center, Yorktown Heights, NY, 1979.\n[Cha87] R. Chamberlain. An alternate view of LU factorization on a hypercube multiprocessor .\nIn M. T. Heath, editor, Hypercube Multiprocessors 1987 , 569\u2013575. SIAM, Philadelphia, PA,\n1987.\n[CJP83] H. Crowder, E. L. Johnson, and M. Padberg. Solving large-scale zero-one linear\nprogramming problem . Operations Research , 2:803\u2013834, 1983.\n[CKP+93a] D. Culler, R. Karp, D. Patterson, A. Sahay, K. Schauser, E. Santos, R. Subramonian,\nand T. von Eicken. LogP: Towards a realistic model of parallel computation . In Proceedings of\nthe Fourth ACM SIGPLAN Symposium on Principles and Practices of Parallel Programming , 1\u201312,\n1993.\n[CKP+93b] D. E. Culler, R. Karp, D. A. Patterson, et al. Logp: Towards a realistic model of\nparallel computation . In Principles and Practices of Parallel Programming , May 1993.\n[CL93] B. Codenotti and M. Leoncini. Introduction to Parallel Processing . Addison-Wesley, 1993.\n[CLC81] F. Y. Chin, J. Lam, and I. Chen. Optimal parallel algorithms for the connected\ncomponent problem . In Proceedings of the 1981 International Conference on Parallel Processing ,\n170\u2013175, 1981.\n[CLC82] F. Y. Chin, J. Lam, and I. Chen. Efficient parallel algorithms for some graph problems .\nCommunications of the ACM , 25(9):659\u2013665, September 1982.\n[CLR90] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms . MIT Press,\nMcGraw-Hill, New York, NY, 1990.\n[CM82] K. M. Chandy and J. Misra. Distributed computation on graphs: Shortest path\nalgorithms . Communications of the", "doc_id": "1e49beb2-333b-4906-835b-446f18db5808", "embedding": null, "doc_hash": "3f7eb8d15d769cd29ec713c96d0267cfc28e3b2d540f8b786a3b8b16e428eee4", "extra_info": null, "node_info": {"start": 1546960, "end": 1549790}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1a198fcf-7dd6-4ee1-b771-3898d8b9021a", "3": "bad2f220-7e63-4c79-971d-d93654aaa53c"}}, "__type__": "1"}, "bad2f220-7e63-4c79-971d-d93654aaa53c": {"__data__": {"text": "1981.\n[CLC82] F. Y. Chin, J. Lam, and I. Chen. Efficient parallel algorithms for some graph problems .\nCommunications of the ACM , 25(9):659\u2013665, September 1982.\n[CLR90] T. H. Cormen, C. E. Leiserson, and R. L. Rivest. Introduction to Algorithms . MIT Press,\nMcGraw-Hill, New York, NY, 1990.\n[CM82] K. M. Chandy and J. Misra. Distributed computation on graphs: Shortest path\nalgorithms . Communications of the ACM , 25(11):833\u2013837, November 1982.\n[CM98] B. Chapman and P. Mehrotra. OpenMP and HPF: Integrating two paradigms . Lecture\nNotes in Computer Science , 1470, 1998.\n[Col88] R. Cole. Parallel merge sort . SIAM Journal on Computing , 17(4):770\u2013785, August 1988.\n[Col89] M. Cole. Algorithmic Skeletons: Structured Management of Parallel Computation . MIT\nPress, Cambridge, MA, 1989.\n[Con89] T. Conlon. Programming in PARLOG . Addison-Wesley, Reading, MA, 1989.\n[CR89] E. A. Carmona and M. D. Rice. A model of parallel performance . Technical Report AFWL-\nTR-89-01, Air Force Weapons Laboratory, 1989.\n[CR91] E. A. Carmona and M. D. Rice. Modeling the serial and parallel fractions of a parallel\nalgorithm . Journal of Parallel and Distributed Computing , 1991.\n[CS88] B. V. Cherkassky and R. Smith. Efficient mapping and implementations of matrix\nalgorithms on a hypercube . Journal of Supercomputing , 2:7\u201327, 1988.\n[CSG98] D. E. Culler, J. P. Singh, and A. Gupta. Parallel Computer Architecture: A\nHardware/Software Approach . Morgan Kaufmann, 1998.\n[CT92] K. M. Chandy and S. Taylor. An Introduction to Parallel Programming . Jones and\nBartlett, Austin, TX, 1992.\n[CV91] B. S. Chlebus and I. Vrto. Parallel quicksort . Journal of Parallel and Distributed\nProcessing , 1991.\n[Cve87] Z. Cvetanovic. Performance analysis of the FFT algorithm on a shared-memory parallel\narchitecture . IBM Journal of Research and Development , 31(4):435\u2013451, 1987.\n[CWP98] A. Cohen, M. Woodring, and R. Petrusha. Win32 Multithreaded Programming . O'Reilly\n& Associates, 1998.\n[D+92] W. J. Dally et al. The message-driven processor . IEEE Micro , 12(2):23\u201339, 1992.\n[Dal87] W. J. Dally. A VLSI Architecture for Concurrent Data Structures . Kluwer Academic\nPublishers, Boston, MA, 1987.\n[Dal90a] W. J. Dally. Analysis of k-ary n-cube interconnection networks . IEEE Transactions on\nComputers , 39(6), June 1990.\n[Dal90b] W. J. Dally. Network and processor architecture for message-driven computers . In R.\nSauya and G. Birtwistle, editors, VLSI and Parallel Computation . Morgan Kaufmann, San Mateo,\nCA, 1990.\n[Dav86] G. J. Davis. Column LU factorization with pivoting on a hypercube multiprocessor .\nSIAM Journal on Algebraic and Discrete Methods , 7:538\u2013550, 1986. Also available as Technical\nReport ORNL-6219, Oak Ridge National Laboratory, Oak Ridge, TN, 1985.\n[DCG90] J. D. DeMello, J. L. Calvet, and J. M.", "doc_id": "bad2f220-7e63-4c79-971d-d93654aaa53c", "embedding": null, "doc_hash": "cbc9a704d1e0611910ab8dc40668921082b95d2e82990130365357e8eaa45a76", "extra_info": null, "node_info": {"start": 1549835, "end": 1552633}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1e49beb2-333b-4906-835b-446f18db5808", "3": "1ec05fb4-76e3-423e-ac96-a7d17cb6a723"}}, "__type__": "1"}, "1ec05fb4-76e3-423e-ac96-a7d17cb6a723": {"__data__": {"text": ". In R.\nSauya and G. Birtwistle, editors, VLSI and Parallel Computation . Morgan Kaufmann, San Mateo,\nCA, 1990.\n[Dav86] G. J. Davis. Column LU factorization with pivoting on a hypercube multiprocessor .\nSIAM Journal on Algebraic and Discrete Methods , 7:538\u2013550, 1986. Also available as Technical\nReport ORNL-6219, Oak Ridge National Laboratory, Oak Ridge, TN, 1985.\n[DCG90] J. D. DeMello, J. L. Calvet, and J. M. Garcia. Vectorization and multitasking of dynamic\nprogramming in control: experiments on a CRAY-2 . Parallel Computing , 13:261\u2013269, 1990.\n[DDSV99] J. Dongarra, I. S. Duff, D. Sorensen, and H. V. Vorst. Numerical Linear Algebra for\nHigh Performance Computers (Software, Environments, Tools) . SIAM, 1999.\n[DeC89] A. L. DeCegama. The Technology of Parallel Processing: Parallel Processing\nArchitectures and VLSI Hardware: Volume 1 . Prentice-Hall, Englewood Cliffs, NJ, 1989.\n[DEH89] P. M. Dew, R. A. Earnshaw, and T. R. Heywood. Parallel Processing for Computer\nVision and Display . Addison-Wesley, Reading, MA, 1989.\n[Dem82] J. Deminet. Experiences with multiprocessor algorithms . IEEE Transactions on\nComputers , C-31(4):278\u2013288, 1982.\n[DFHM82] D. J. DeWitt, D. B. Friedland, D. K. Hsiao, and M. J. Menon. A taxonomy of parallel\nsorting algorithms . Technical Report TR-482, Computer Sciences Department, University of\nWisconsin, Madison, WI, 1982.\n[DFRC96] F. Dehne, A. Fabri, and A. Rau-Chaplin. Scalable parallel computational geometry for\ncoarse grained multicomputers . International Journal on Computational Geometry ,\n6(3):379\u2013400, 1996.\n[DHvdV93] J. W. Demmel, M. T. Heath, and H. A. van der Vorst. Parallel numerical linear\nalgebra . Acta Numerica , 111\u2013197, 1993.\n[Dij59] E. W. Dijkstra. A note on two problems in connection with graphs . Numerische\nMathematik , 1:269\u2013271, 1959.\n[DM93] S. Dutt and N. R. Mahapatra. Parallel A* algorithms and their performance on\nhypercube multiprocessors . In Proceedings of the Seventh International Parallel Processing\nSymposium , 797\u2013803, 1993.\n[DM98] L. Dagum and R. Menon. OpenMP: An industry-standard API for shared-memory\nprogramming . IEEE Computational Science and Engineering , 5(1):46\u201355, January/March 1998.\n[DNS81] E. Dekel, D. Nassimi, and S. Sahni. Parallel matrix and graph algorithms . SIAM\nJournal on Computing , 10:657\u2013673, 1981.\n[Dra96] D. G. Drake. Introduction to Java threads . JavaWorld: IDG's magazine for the Java\ncommunity , 1(2), April 1996.\n[DRGNP] F. Darema-Rogers, D. George, V. Norton, and G. Pfister. VM parallel environment . In\nProceedings of the IBM Kingston Parallel Processing Symposium .\n[DS86] W. J. Dally and C. L. Seitz. The torus routing chip . Journal of Distributed Computing ,\n1(3):187\u2013196, 1986.\n[DS87] W. J. Dally and C. L. Seitz. Deadlock-free message routing in", "doc_id": "1ec05fb4-76e3-423e-ac96-a7d17cb6a723", "embedding": null, "doc_hash": "86f3e5428c54d15c2518559095be901c2a138dfd6c3c798a5b6e24bd9be47220", "extra_info": null, "node_info": {"start": 1552631, "end": 1555404}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "bad2f220-7e63-4c79-971d-d93654aaa53c", "3": "3030d109-7725-4fbf-9737-c34cd67a2b34"}}, "__type__": "1"}, "3030d109-7725-4fbf-9737-c34cd67a2b34": {"__data__": {"text": "threads . JavaWorld: IDG's magazine for the Java\ncommunity , 1(2), April 1996.\n[DRGNP] F. Darema-Rogers, D. George, V. Norton, and G. Pfister. VM parallel environment . In\nProceedings of the IBM Kingston Parallel Processing Symposium .\n[DS86] W. J. Dally and C. L. Seitz. The torus routing chip . Journal of Distributed Computing ,\n1(3):187\u2013196, 1986.\n[DS87] W. J. Dally and C. L. Seitz. Deadlock-free message routing in multiprocessor\ninterconnection networks . IEEE Transactions on Computers , C-36(5):547\u2013 553, 1987.\n[DSG83] E. W. Dijkstra, W. H. Seijen, and A. J. M. V. Gasteren. Derivation of a termination\ndetection algorithm for a distributed computation . Information Processing Letters ,\n16(5):217\u2013219, 1983.\n[DT89] L. Desbat and D. Trystram. Implementing the discrete Fourier transform on a hypercube\nvector-parallel computer . In Proceedings of the Fourth Conference on Hypercubes, Concurrent\nComputers, and Applications: Volume I , 407\u2013 410, 1989.\n[DV87] K. A. Doshi and P. J. Varman. Optimal graph algorithms on a fixed-size linear array .\nIEEE Transactions on Computers , C\u201336(4):460\u2013470, April 1987.\n[dV89] E. F. V.de Velde. Multicomputer matrix computations: Theory and practice . In\nProceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications ,\n1303\u20131308, 1989.\n[DY81] N. Deo and Y. B. Yoo. Parallel algorithms for the minimum spanning tree problem . In\nProceedings of the 1981 International Conference on Parallel Processing , 188\u2013189, 1981.\n[Eck94] J. Eckstein. Parallel branch-and-bound methods for mixed-integer programming on the\ncm-5. SIAM Journal on Optimization , 4(4):794\u2013814, 1994.\n[Eck97] J. Eckstein. Distributed versus centralized storage and control for parallel branch and\nbound: Mixed integer programming on the cm-5 . Computational Optimization and Applications ,\n7(2):199\u2013220, 1997.\n[Ede89] A. Edelman. Optimal matrix transposition and bit-reversal on hypercubes: Node\naddress\u2013memory address exchanges. Technical report, Thinking Machines Corporation,\nCambridge, MA, 1989.\n[EDH80] O. I. El-Dessouki and W. H. Huen. Distributed enumeration on network computers .\nIEEE Transactions on Computers , C-29:818\u2013825, September 1980.\n[EHHR88] S. C. Eisenstat, M. T. Heath, C. S. Henkel, and C. H. Romine. Modified cyclic\nalgorithms for solving triangular systems on distributed-memory multiprocessors . SIAM Journal\non Scientific and Statistical Computing , 9(3):589\u2013600, 1988.\n[EHMN90] M. Evett, J. Hendler, A. Mahanti, and D. Nau. PRA*: A memory-limited heuristic\nsearch procedure for the connection machine . In Proceedings of the Third Symposium on the\nFrontiers of Massively Parallel Computation , 145\u2013 149, 1990.\n[Ekl72] J. O. Eklundh. A fast computer method for matrix transposing . IEEE Transactions on\nComputers , 21(7):801\u2013803, 1972.\n[Ert92] W. Ertel. OR\u2014parallel theorem proving with random competition . In A. Voronokov,\neditor, LPAR '92: Logic Programming", "doc_id": "3030d109-7725-4fbf-9737-c34cd67a2b34", "embedding": null, "doc_hash": "75e11898657fd839856e26894f9bdf8224bd9f991e02883d0c582e082f895bbc", "extra_info": null, "node_info": {"start": 1555400, "end": 1558325}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1ec05fb4-76e3-423e-ac96-a7d17cb6a723", "3": "1733a2d8-ef91-49df-acdf-4839e87167f4"}}, "__type__": "1"}, "1733a2d8-ef91-49df-acdf-4839e87167f4": {"__data__": {"text": "A. Mahanti, and D. Nau. PRA*: A memory-limited heuristic\nsearch procedure for the connection machine . In Proceedings of the Third Symposium on the\nFrontiers of Massively Parallel Computation , 145\u2013 149, 1990.\n[Ekl72] J. O. Eklundh. A fast computer method for matrix transposing . IEEE Transactions on\nComputers , 21(7):801\u2013803, 1972.\n[Ert92] W. Ertel. OR\u2014parallel theorem proving with random competition . In A. Voronokov,\neditor, LPAR '92: Logic Programming and Automated Reasoning , 226\u2013237. Springer-Verlag,\nNew York, NY, 1992.\n[EZL89] D. L. Eager, J. Zahorjan, and E. D. Lazowska. Speedup versus efficiency in parallel\nsystems . IEEE Transactions on Computers , 38(3):408\u2013423, 1989.\n[Fen81] T. Y. Feng. A survey of interconnection networks . IEEE Computer , 12\u201327, December\n1981.\n[FF82] R. A. Finkel and J. P. Fishburn. Parallelism in alpha-beta search . Artificial Intelligence ,\n19:89\u2013106, 1982.\n[FF86] G. C. Fox and W. Furmanski. Optimal communication algorithms on hypercube .\nTechnical Report CCCP-314, California Institute of Technology, Pasadena, CA, 1986.\n[FJDS96] L. Fosdick, E. Jessup, G. Domik, and C. Schauble. Introduction to High-Performance\nScientific Computing . MIT Press, 1996.\n[FJL+88] G. C. Fox, M. Johnson, G. Lyzenga, S. W. Otto, J. Salmon, and D. Walker. Solving\nProblems on Concurrent Processors: Volume 1 . Prentice-Hall, Englewood Cliffs, NJ, 1988.\n[FK88] C. Ferguson and R. Korf. Distributed tree search and its application to alpha-beta\npruning . In Proceedings of the 1988 National Conference on Artificial Intelligence , 1988.\n[FK89] H. P. Flatt and K. Kennedy. Performance of parallel processors . Parallel Computing ,\n12:1\u201320, 1989.\n[FKO86] E. Felten, S. Karlin, and S. W.Otto. Sorting on a hypercube . Caltech/JPL , 1986. Hm\n244.\n[Fla90] H. P. Flatt. Further applications of the overhead model for parallel systems . Technical\nReport G320-3540, IBM Corporation, Palo Alto Scientific Center, Palo Alto, CA, 1990.\n[Flo62] R. W. Floyd. Algorithm 97: Shortest path . Communications of the ACM , 5(6):345, June\n1962.\n[Fly72] M. J. Flynn. Some computer organizations and their effectiveness . IEEE Transactions on\nComputers , C-21(9):948\u2013960, 1972.\n[Fly95] M. J. Flynn. Computer Architecture: Pipelined and Parallel Processor Design . Jones and\nBartlett, 1995.\n[FM70] W. D. Frazer and A. C. McKellar. Samplesort: A sampling approach to minimal storage\ntree sorting . Journal of the ACM , 17(3):496\u2013507, July 1970.\n[FM87] R. A. Finkel and U. Manber. DIB\u2014a distributed implementation of backtracking . ACM\nTransactions on Programming Languages and Systems , 9(2):235\u2013256, April 1987.\n[FM92] R. Frye and J. Myczkowski. Load balancing algorithms on the connection machine and\ntheir use in Monte-Carlo methods . In Proceedings of the Unstructured Scientific Computation on\nMultiprocessors Conference , 1992.\n[FMM94] R. Feldmann, P.", "doc_id": "1733a2d8-ef91-49df-acdf-4839e87167f4", "embedding": null, "doc_hash": "4c1c9457144a636deea3f8b97aaedb39df4fe64fe5dad713100817afa9f3fa68", "extra_info": null, "node_info": {"start": 1558288, "end": 1561146}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3030d109-7725-4fbf-9737-c34cd67a2b34", "3": "eb3c3009-e404-4f59-8621-91b642414133"}}, "__type__": "1"}, "eb3c3009-e404-4f59-8621-91b642414133": {"__data__": {"text": "of the ACM , 17(3):496\u2013507, July 1970.\n[FM87] R. A. Finkel and U. Manber. DIB\u2014a distributed implementation of backtracking . ACM\nTransactions on Programming Languages and Systems , 9(2):235\u2013256, April 1987.\n[FM92] R. Frye and J. Myczkowski. Load balancing algorithms on the connection machine and\ntheir use in Monte-Carlo methods . In Proceedings of the Unstructured Scientific Computation on\nMultiprocessors Conference , 1992.\n[FMM94] R. Feldmann, P. Mysliwietz, and B. Monien. Studying overheads in massively parallel\nmin/max-tree evaluation . In Proc. of the 6th ACM Symposium on Parallel Algorithms and\nArchitectures , 94\u2013103, 1994.\n[FOH87] G. C. Fox, S. W. Otto, and A. J. G. Hey. Matrix algorithms on a hypercube I: Matrix\nmultiplication . Parallel Computing , 4:17\u201331, 1987.\n[Fos95] I. Foster. Designing and Building Parallel Programs: Concepts and Tools for Parallel\nSoftware Engineering . Addison-Wesley, 1995.\n[Fou94] T. J. Fountain. Parallel Computing: Principles and Practice . Cambridge University Press,\n1994.\n[FR62] L. R. Ford and R. L. Rivest. Flows in Networks . Princeton University Press, Princeton, NJ,\n1962.\n[Fra93] M. Franklin. The multiscalar architecture. Technical Report CS-TR-1993-1196,\nUniversity of Wisconsin, 1993.\n[FTI90] M. Furuichi, K. Taki, and N. Ichiyoshi. A multi-level load balancing scheme for OR-\nparallel exhaustive search programs on the Multi-PSI . In Proceedings of the Second ACM\nSIGPLAN Symposium on Principles and Practice of Parallel Programming , 50\u201359, 1990.\n[FW78] S. Fortune and J. Wyllie. Parallelism in random access machines . In Proceedings of ACM\nSymposium on Theory of Computing , 114\u2013118, 1978.\n[Gal95] B. O. Gallmeister. Posix. 4 : Programming for the Real World . O'Reilly & Associates,\n1995.\n[GBD+94] G. A. Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. PVM:\nParallel Virtual Machine . MIT Press, Cambridge, MA, 1994.\n[Gei85] G. A. Geist. Efficient parallel LU factorization with pivoting on a hypercube\nmultiprocessor . Technical Report ORNL-6211, Oak Ridge National Laboratory, Oak Ridge, TN,\n1985.\n[GGK+83] A. Gottlieb, R. Grishman, C. P. Kruskal, K. P. McAuliffe, L. Rudolph, and M. Snir. The\nNYU Ultracomputer\u2014designing a MIMD, shared memory parallel computer . IEEE Transactions\non Computers , C\u201332(2):175\u2013189, February 1983.\n[GGK93] A. Y. Grama, A. Gupta, and V. Kumar. Isoefficiency: Measuring the scalability of\nparallel algorithms and architectures . IEEE Parallel and Distributed Technology , 1(3):12\u201321,\nAugust 1993.\n[GH85] G. A. Geist and M. T. Heath. Parallel Cholesky factorization on a hypercube\nmultiprocessor . Technical Report ORNL-6190, Oak Ridge National Laboratory, Oak Ridge, TN,\n1985.\n[GH86] G. A. Geist and M. T. Heath. Matrix factorization on a hypercube multiprocessor . In M.\nT. Heath,", "doc_id": "eb3c3009-e404-4f59-8621-91b642414133", "embedding": null, "doc_hash": "e9fdc61d75b06e4a035668895015e296943675a08fe72e3a6d0d644a900f375f", "extra_info": null, "node_info": {"start": 1561152, "end": 1563952}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "1733a2d8-ef91-49df-acdf-4839e87167f4", "3": "282b8c5c-4319-492e-9fea-05e9474bd324"}}, "__type__": "1"}, "282b8c5c-4319-492e-9fea-05e9474bd324": {"__data__": {"text": "V. Kumar. Isoefficiency: Measuring the scalability of\nparallel algorithms and architectures . IEEE Parallel and Distributed Technology , 1(3):12\u201321,\nAugust 1993.\n[GH85] G. A. Geist and M. T. Heath. Parallel Cholesky factorization on a hypercube\nmultiprocessor . Technical Report ORNL-6190, Oak Ridge National Laboratory, Oak Ridge, TN,\n1985.\n[GH86] G. A. Geist and M. T. Heath. Matrix factorization on a hypercube multiprocessor . In M.\nT. Heath, editor, Hypercube Multiprocessors 1986 , 161\u2013180. SIAM, Philadelphia, PA, 1986.\n[GH01] S. Goedecker and A. Hoisie. Performance Optimization of Numerically Intensive Codes .\nSIAM, 2001.\n[Gib85] A. Gibbons. Algorithmic Graph Theory . Cambridge University Press, Cambridge, 1985.\n[Gib89] P. B. Gibbons. A more practical PRAM model . In Proceedings of the 1989 ACM\nSymposium on Parallel Algorithms and Architectures , 158\u2013168, 1989.\n[GK91] A. Gupta and V. Kumar. The scalability of matrix multiplication algorithms on parallel\ncomputers . Technical Report TR 91-54, Department of Computer Science, University of\nMinnesota, Minneapolis, MN, 1991. A short version appears in Proceedings of 1993 International\nConference on Parallel Processing , pages III-115\u2013III-119, 1993.\n[GK93a] A. Gupta and V. Kumar. Performance properties of large scale parallel systems . Journal\nof Parallel and Distributed Computing , 19:234\u2013244, 1993. Also available as Technical Report TR\n92-32, Department of Computer Science, University of Minnesota, Minneapolis, MN.\n[GK93b] A. Gupta and V. Kumar. The scalability of FFT on parallel computers . IEEE Transactions\non Parallel and Distributed Systems , 4(8):922\u2013932, August 1993. A detailed version is available\nas Technical Report TR 90-53, Department of Computer Science, University of Minnesota,\nMinneapolis, MN.\n[GKP92] A. Grama, V. Kumar, and P. M. Pardalos. Parallel processing of discrete optimization\nproblems . In Encyclopaedia of Microcomputers . Marcel Dekker Inc., New York, 1992.\n[GKR91] A. Y. Grama, V. Kumar, and V. N. Rao. Experimental evaluation of load balancing\ntechniques for the hypercube . In Proceedings of the Parallel Computing '91 Conference ,\n497\u2013514, 1991.\n[GKRS96] A. Grama, V. Kumar, S. Ranka, and V. Singh. A3: A simple and asymptotically\naccurate model for parallel computation . In Proceedings of the Sixth Symposium on Frontiers of\nMassively Parallel Computing , Annapolis, MD, 1996.\n[GKS92] A. Gupta, V. Kumar, and A. H. Sameh. Performance and scalability of preconditioned\nconjugate gradient methods on parallel computers . Technical Report TR 92-64, Department of\nComputer Science, University of Minnesota, Minneapolis, MN, 1992. A short version appears in\nProceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing , pages\n664\u2013674, 1993.\n[GKT79] L. J. Guibas, H. T. Kung, and C. D. Thompson. Direct VLSI Implementation of\nCombinatorial Algorithms . In Proceedings of Conference on Very Large Scale Integration,\nCalifornia Institute of Technology , 509\u2013525, 1979.\n[GL96a] G. H. Golub and C. V. Loan. Matrix Computations . The Johns Hopkins University Press,\nBaltimore, MD, 1996.\n[GL96b] W. D. Gropp and", "doc_id": "282b8c5c-4319-492e-9fea-05e9474bd324", "embedding": null, "doc_hash": "a8b6b47e5c2e82775159d529aa82f3df85afc458660bbbbbd1c119b206c20996", "extra_info": null, "node_info": {"start": 1563957, "end": 1567095}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "eb3c3009-e404-4f59-8621-91b642414133", "3": "b9f27d7a-3d04-411e-ae44-0ac9df35f190"}}, "__type__": "1"}, "b9f27d7a-3d04-411e-ae44-0ac9df35f190": {"__data__": {"text": "Sixth SIAM Conference on Parallel Processing for Scientific Computing , pages\n664\u2013674, 1993.\n[GKT79] L. J. Guibas, H. T. Kung, and C. D. Thompson. Direct VLSI Implementation of\nCombinatorial Algorithms . In Proceedings of Conference on Very Large Scale Integration,\nCalifornia Institute of Technology , 509\u2013525, 1979.\n[GL96a] G. H. Golub and C. V. Loan. Matrix Computations . The Johns Hopkins University Press,\nBaltimore, MD, 1996.\n[GL96b] W. D. Gropp and E. Lusk. User's Guide for mpich , a Portable Implementation of MPI .\nMathematics and Computer Science Division, Argonne National Laboratory. ANL-96/6. 1996.\n[GLDS96] W. Gropp, E. Lusk, N. Doss, and A. Skjellum. A high-performance, portable\nimplementation of the MPI message passing interface standard . Parallel Computing ,\n22(6):789\u2013828, September 1996.\n[GLS99] W. Gropp, E. Lusk, and A. Skjellum. Using MPI . MIT Press, 1999. 2nd Edition.\n[GMB88] J. L. Gustafson, G. R. Montry, and R. E. Benner. Development of parallel methods for\na 1024-processor hypercube . SIAM Journal on Scientific and Statistical Computing ,\n9(4):609\u2013638, 1988.\n[GO93] G. H. Golub and J. M. Ortega. Scientific Computing: An Introduction with Parallel\nComputing . Academic Press, 1993.\n[GPS90] K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. Parallel algorithms for dense linear\nalgebra computations  . SIAM Review , 32(1):54\u2013135, March 1990. Also appears in K. A. Gallivan\net al. Parallel Algorithms for Matrix Computations . SIAM, Philadelphia, PA, 1990.\n[GR88] G. A. Geist and C. H. Romine. LU factorization algorithms on distributed-memory\nmultiprocessor architectures . SIAM Journal on Scientific and Statistical Computing ,\n9(4):639\u2013649, 1988. Also available as Technical Report ORNL/TM-10383, Oak Ridge National\nLaboratory, Oak Ridge, TN, 1987.\n[GR90] A. Gibbons and W. Rytter. Efficient Parallel Algorithms . Cambridge University Press,\nCambridge, UK, 1990.\n[Gre91] S. Green. Parallel Processing for Computer Graphics . MIT Press, Cambridge, MA, 1991.\n[GSNL98] W. Gropp, M. Snir, W. Nitzberg, and E. Lusk. MPI: The Complete Reference . MIT\nPress, 1998.\n[GT88] A. V. Goldberg and R. E. Tarjan. A new approach to the maximum-flow problem . Journal\nof the ACM , 35(4):921\u2013940, October 1988.\n[Gup87] A. Gupta. Parallelism in Production Systems . Morgan Kaufmann, Los Altos, CA, 1987.\n[Gus88] J. L. Gustafson. Reevaluating Amdahl's law . Communications of the ACM ,\n31(5):532\u2013533, 1988.\n[Gus92] J. L. Gustafson. The consequences of fixed time performance measurement . In\nProceedings of the 25th Hawaii International Conference on System Sciences: Volume III ,\n113\u2013124, 1992.\n[HB84] K. Hwang and F. A. Briggs. Computer Architecture and Parallel Processing . McGraw-Hill,\nNew York, NY, 1984.\n[HB88] M. M. Huntbach and F. W. Burton. Alpha-beta search on virtual tree machines .\nInformation Science , 44:3\u201317,", "doc_id": "b9f27d7a-3d04-411e-ae44-0ac9df35f190", "embedding": null, "doc_hash": "1bf5ec2d677da8dc9317d6f224d50b0ca2e4f5edbc9c7a296aad19c3536554b4", "extra_info": null, "node_info": {"start": 1567090, "end": 1569932}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "282b8c5c-4319-492e-9fea-05e9474bd324", "3": "f754821e-66ef-457d-90c6-88e38e6360b2"}}, "__type__": "1"}, "f754821e-66ef-457d-90c6-88e38e6360b2": {"__data__": {"text": "1988.\n[Gus92] J. L. Gustafson. The consequences of fixed time performance measurement . In\nProceedings of the 25th Hawaii International Conference on System Sciences: Volume III ,\n113\u2013124, 1992.\n[HB84] K. Hwang and F. A. Briggs. Computer Architecture and Parallel Processing . McGraw-Hill,\nNew York, NY, 1984.\n[HB88] M. M. Huntbach and F. W. Burton. Alpha-beta search on virtual tree machines .\nInformation Science , 44:3\u201317, 1988.\n[HCH95] F.-H. Hsu, M. S. Campbell, and A. J. Hoane. Deep Blue system overview . In\nProceedings of the 1995 International Conference on Supercomputing, Barcelona, Spain ,\n240\u2013244, 1995.\n[HCS79] D. S. Hirschberg, A. K. Chandra, and D. V. Sarwate. Computing connected components\non parallel computers . Communications of the ACM , 22(8):461\u2013 464, August 1979.\n[HD87] S.-R. Huang and L. S. Davis. A tight upper bound for the speedup of parallel best-first\nbranch-and-bound algorithms . Technical report, Center for Automation Research, University of\nMaryland, College Park, MD, 1987.\n[HD89a] S. R. Huang and L. S. Davis. Parallel iterative a* search: An admissible distributed\nheuristic search algorithm . In Proceedings of the Eleventh International Joint Conference on\nArtificial Intelligence , 23\u201329, 1989.\n[HD89b] K. Hwang and D. DeGroot. Parallel Processing for Supercomputers and Artificial\nIntelligence . McGraw-Hill, New York, NY, 1989.\n[HDM97] J. Hill, S. Donaldson, and A. McEwan. Installation and user guide for the oxford bsp\ntoolset: User guide for the oxford bsp toolset (v1.3) implementation of bsplib . Technical report,\nOxford University Computing Laboratory, 1997.\n[Hea85] M. T. Heath. Parallel Cholesky factorization in message-passing multiprocessor\nenvironments . Technical Report ORNL-6150, Oak Ridge National Laboratory, Oak Ridge, TN,\n1985.\n[HG97] S. Hamilton and L. Garber. Deep Blue's hardware-software synergy . IEEE Computer ,\n30(10):29\u201335, October 1997.\n[Hil85] W. D. Hillis. The Connection Machine . MIT Press, Cambridge, MA, 1985.\n[Hil90] M. D. Hill. What is scalability?  Computer Architecture News , 18(4), 1990.\n[Hip89] P. G. Hipes. Matrix multiplication on the JPL/Caltech Mark IIIfp hypercube . Technical\nReport C3P 746, Concurrent Computation Program, California Institute of Technology,\nPasadena, CA, 1989.\n[Hir76] D. S. Hirschberg. Parallel algorithms for the transitive closure and connected component\nproblem . In Proceedings of the 8th Annual ACM Symposium on the Theory of Computing ,\n55\u201357, 1976.\n[Hir78] D. S. Hirschberg. Fast parallel sorting algorithms . Communications of the ACM ,\n21(8):657\u2013666, August 1978.\n[HJ87] C.-T. Ho and S. L. Johnsson. Spanning balanced trees in Boolean cubes . Technical\nReport YALEU/DCS/RR-508, Department of Computer Science, Yale University, New Haven, CT,\n1987.\n[HJE91] C.-T. Ho, S. L. Johnsson, and A. Edelman. Matrix multiplication on hypercubes using\nfull bandwidth and constant storage . In Proceedings of the 1991 International Conference on\nParallel Processing , 447\u2013451, 1991.\n[HK96] S. Hambrusch and A.", "doc_id": "f754821e-66ef-457d-90c6-88e38e6360b2", "embedding": null, "doc_hash": "f7c9f77500ff70402ec956292a409b11d821cbc7a9b83901c0fe46501f359974", "extra_info": null, "node_info": {"start": 1569959, "end": 1572977}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b9f27d7a-3d04-411e-ae44-0ac9df35f190", "3": "40b62148-e2d7-4ce5-bd23-f2968b93fd3a"}}, "__type__": "1"}, "40b62148-e2d7-4ce5-bd23-f2968b93fd3a": {"__data__": {"text": "August 1978.\n[HJ87] C.-T. Ho and S. L. Johnsson. Spanning balanced trees in Boolean cubes . Technical\nReport YALEU/DCS/RR-508, Department of Computer Science, Yale University, New Haven, CT,\n1987.\n[HJE91] C.-T. Ho, S. L. Johnsson, and A. Edelman. Matrix multiplication on hypercubes using\nfull bandwidth and constant storage . In Proceedings of the 1991 International Conference on\nParallel Processing , 447\u2013451, 1991.\n[HK96] S. Hambrusch and A. Khokhar. C3: A parallel model for coarse-grained machines .\nJournal of Parallel and Distributed Computing , 32(2):139\u2013154, February 1996.\n[HLM84] R. E. Hiromoto, O. M. Lubeck, and J. Moore. Experiences with the Denelcor HEP .\nParallel Computing , 1(3\u20134):197\u2013206, 1984.\n[HLV90] S. H. S. Huang, H. Liu, and V. Vishwanathan. A sub-linear parallel algorithm for some\ndynamic programming problems . In Proceedings of the 1990 International Conference on\nParallel Processing , III\u2013261\u2013III\u2013264, 1990.\n[HM80] D. K. Hsiao and M. J. Menon. Parallel record-sorting methods for hardware realization .\nOsu-cisrc-tr-80-7, Computer Science Information Department, Ohio State University, Columbus,\nOH, 1980.\n[HMT+96] H. Hum, O. Maquelin, K. Theobald, X. Tian, and G. Gao. A study of the earth-manna\nmultithreaded system . Intl. J. of Par. Prog. , 24:319\u2013347, 1996.\n[HNR90] P. Heidelberger, A. Norton, and J. T. Robinson. Parallel quicksort using fetch-and-add .\nIEEE Transactions on Computers , C-39(1):133\u2013138, January 1990.\n[Hoa62] C. A. R. Hoare. Quicksort. Computer Journal , 5:10\u201315, 1962.\n[HP89] S. W. Hornick and F. P. Preparata. Deterministic PRAM simulation with constant\nredundancy . In Proceedings of the 1989 ACM Symposium on Parallel Algorithms and\nArchitectures , 103\u2013109, 1989.\n[HQ91] P. J. Hatcher and M. J. Quinn. Data Parallel Programming . MIT Press, Cambridge, MA,\n1991.\n[HR88] M. T. Heath and C. H. Romine. Parallel solution of triangular systems on distributed-\nmemory multiprocessors . SIAM Journal on Scientific and Statistical Computing , 9(3):558\u2013588,\n1988.\n[HR91] C.-T. Ho and M. T. Raghunath. Efficient communication primitives on circuit-switched\nhypercubes . In Sixth Distributed Memory Computing Conference Proceedings , 390\u2013397, 1991.\n[HS78] E. Horowitz and S. Sahni. Fundamentals of Computer Algorithms . Computer Science\nPress, Rockville, MD, 1978.\n[HS86] W. D. Hillis and G. L. Steele. Data parallel algorithms . Communications of the ACM ,\n29(12):1170\u20131183, 1986.\n[Hsu90] F.-H. Hsu. Large scale parallelization of alpha-beta search: An algorithmic and\narchitectural study with computer chess . Technical report, Carnegie Mellon University,\nPittsburgh, PA, 1990. Ph.D. Thesis.\n[Hua85] M. A. Huang. Solving some graph problems with optimal or near-optimal speedup on\nmesh-of-trees networks . In Proceedings of the 26th Annual IEEE Symposium on Foundations of\nComputer Science ,", "doc_id": "40b62148-e2d7-4ce5-bd23-f2968b93fd3a", "embedding": null, "doc_hash": "8bcd2b8395a4edd965b7948c7650807afe774363a170b2041cc40b3eee5f83f0", "extra_info": null, "node_info": {"start": 1572959, "end": 1575800}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f754821e-66ef-457d-90c6-88e38e6360b2", "3": "f046e795-dab7-407d-9746-278978aee8f0"}}, "__type__": "1"}, "f046e795-dab7-407d-9746-278978aee8f0": {"__data__": {"text": ". Communications of the ACM ,\n29(12):1170\u20131183, 1986.\n[Hsu90] F.-H. Hsu. Large scale parallelization of alpha-beta search: An algorithmic and\narchitectural study with computer chess . Technical report, Carnegie Mellon University,\nPittsburgh, PA, 1990. Ph.D. Thesis.\n[Hua85] M. A. Huang. Solving some graph problems with optimal or near-optimal speedup on\nmesh-of-trees networks . In Proceedings of the 26th Annual IEEE Symposium on Foundations of\nComputer Science , 232\u2013340, 1985.\n[HX98] K. Hwang and Z. Xu. Scalable Parallel Computing . McGraw-Hill, New York, NY, 1998.\n[Hyd99] P. Hyde. Java Thread Programming . Sams, 1999.\n[IPS91] O. H. Ibarra, T. C. Pong, and S. M. Sohn. Parallel recognition and parsing on the\nhypercube . IEEE Transactions on Computers , 40(6):764\u2013770, June 1991.\n[IYF79] M. Imai, Y. Yoshida, and T. Fukumura. A parallel searching scheme for multiprocessor\nsystems and its application to combinatorial problems . In Proceedings of the International Joint\nConference on Artificial Intelligence , 416\u2013418, 1979.\n[Jaj92] J. Jaja. An Introduction to Parallel Algorithms . Addison-Wesley, Reading, MA, 1992.\n[JAM87] V. K. Janakiram, D. P. Agrawal, and R. Mehrotra. Randomized parallel algorithms for\nProlog programs and backtracking applications . In Proceedings of the 1987 International\nConference on Parallel Processing , 278\u2013281, 1987.\n[JAM88] V. K. Janakiram, D. P. Agrawal, and R. Mehrotra. A randomized parallel backtracking\nalgorithm . IEEE Transactions on Computers , C-37(12), 1988.\n[JGD87] L. H. Jamieson, D. B. Gannon, and R. J. Douglass, editors. The Characteristics of\nParallel Algorithms . MIT Press, Cambridge, MA, 1987.\n[JH88] S. L. Johnsson and C.-T. Ho. Matrix transposition on Boolean n-cube configured\nensemble architectures . SIAM Journal on Matrix Analysis and Applications , 9(3):419\u2013454, July\n1988.\n[JH89] S. L. Johnsson and C.-T. Ho. Optimum broadcasting and personalized communication in\nhypercubes . IEEE Transactions on Computers , 38(9):1249\u2013 1268, September 1989.\n[JH91] S. L. Johnsson and C.-T. Ho. Optimal all-to-all personalized communication with\nminimum span on Boolean cubes . In Sixth Distributed Memory Computing Conference\nProceedings , 299\u2013304, 1991.\n[JKFM89] S. L. Johnsson, R. Krawitz, R. Frye, and D. McDonald. A radix-2 FFT on the connection\nmachine . Technical report, Thinking Machines Corporation, Cambridge, MA, 1989.\n[JNS97] L. Johnson, G. Nemhauser, and M. Savelsbergh. Progress in integer programming: An\nexposition . Technical report, School of Industrial and Systems Engineering, Georgia Institute of\nTechnology, 1997. Available from http://akula.isye.gatech.edu/mwps/mwps.html .\n[Joh77] D. B. Johnson. Efficient algorithms for shortest paths in sparse networks . Journal of the\nACM, 24(1):1\u201313, March 1977.\n[Joh84] S. L. Johnsson. Combining parallel and sequential sorting on a boolean n-cube. In\nProceedings of International Conference on Parallel", "doc_id": "f046e795-dab7-407d-9746-278978aee8f0", "embedding": null, "doc_hash": "0d219011d16ebf9d81930c0a476d9be0f7f8e8acdd40544127f94ed6e7ffb23f", "extra_info": null, "node_info": {"start": 1575781, "end": 1578702}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "40b62148-e2d7-4ce5-bd23-f2968b93fd3a", "3": "69ca6a43-8a48-4b4e-9f7e-78e09c660a74"}}, "__type__": "1"}, "69ca6a43-8a48-4b4e-9f7e-78e09c660a74": {"__data__": {"text": "Progress in integer programming: An\nexposition . Technical report, School of Industrial and Systems Engineering, Georgia Institute of\nTechnology, 1997. Available from http://akula.isye.gatech.edu/mwps/mwps.html .\n[Joh77] D. B. Johnson. Efficient algorithms for shortest paths in sparse networks . Journal of the\nACM, 24(1):1\u201313, March 1977.\n[Joh84] S. L. Johnsson. Combining parallel and sequential sorting on a boolean n-cube. In\nProceedings of International Conference on Parallel Processing , 1984.\n[Joh87] S. L. Johnsson. Communication efficient basic linear algebra computations on\nhypercube architectures . Journal of Parallel and Distributed Computing , 4(2):133\u2013172, April\n1987.\n[Joh90] S. L. Johnsson. Communication in network architectures . In R. Suaya and G. Birtwistle,\neditors, VLSI and Parallel Computation , 223\u2013389. Morgan Kaufmann, San Mateo, CA, 1990.\n[JP93] M. T. Jones and P. E. Plassmann. A parallel graph coloring heuristic . SIAM Journal on\nScientific Computing , 14:654\u2013669, 1993.\n[JS87] J.-F. Jenq and S. Sahni. All pairs shortest paths on a hypercube multiprocessor . In\nProceedings of the 1987 International Conference on Parallel Processing , 713\u2013716, 1987.\n[KA88] R. A. Kamin and G. B. Adams. Fast Fourier transform algorithm design and tradeoffs .\nTechnical Report RIACS TR 88.18, NASA Ames Research Center, Moffet Field, CA, 1988.\n[KA99a] Y.-K. Kwok and I. Ahmad. Benchmarking and comparison of the task graph scheduling\nalgorithms . Journal of Parallel and Distributed Computing , 59:381\u2013422, 1999.\n[KA99b] Y.-K. Kwok and I. Ahmad. Static scheduling algorithms for allocating directed task\ngraphs to multiprocessors . ACM Computing Surveys , 31(4):406\u2013 471, 1999.\n[KB57] T. C. Koopmans and M. J. Beckmann. Assignment problems and the location of economic\nactivities . Econometrica , 25:53\u201376, 1957.\n[Ken90] Kendall Square Research Corporation. KSR-1 Overview . Waltham, MA. 1990.\n[KF90] A. H. Karp and H. P. Flatt. Measuring parallel processor performance . Communications\nof the ACM , 33(5):539\u2013543, 1990.\n[KG94] V. Kumar and A. Gupta. Analyzing scalability of parallel algorithms and architectures .\nJournal of Parallel and Distributed Computing , 22(3):379\u2013391, 1994. Also available as Technical\nReport TR 91-18, Department of Computer Science Department, University of Minnesota,\nMinneapolis, MN.\n[KGK90] V. Kumar, P. S. Gopalakrishnan, and L. N. Kanal, editors. Parallel Algorithms for\nMachine Intelligence and Vision . Springer-Verlag, New York, NY, 1990.\n[KGR94] V. Kumar, A. Grama, and V. N. Rao. Scalable load balancing techniques for parallel\ncomputers. Journal of Parallel and Distributed Computing , 22(1):60\u2013 79, July 1994.\n[KH67] R. M. Karp and M. H. Held. Finite state processes and dynamic programming . SIAM\nJournal of Applied Math , 15:693\u2013718, 1967.\n[KH83] M. Kumar and D. S. Hirschberg. An efficient implementation of Batcher's odd-even\nmerge algorithm and its application in parallel sorting schemes . IEEE Transactions on\nComputers ,", "doc_id": "69ca6a43-8a48-4b4e-9f7e-78e09c660a74", "embedding": null, "doc_hash": "98d77f4e0cad85bba7a53df45518b2ec5fb8bf7534ce50e7f6b8699ca0a8e7d7", "extra_info": null, "node_info": {"start": 1578684, "end": 1581672}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f046e795-dab7-407d-9746-278978aee8f0", "3": "d2d5a33b-ce41-4e72-876d-a0c7f6a53142"}}, "__type__": "1"}, "d2d5a33b-ce41-4e72-876d-a0c7f6a53142": {"__data__": {"text": "and V. N. Rao. Scalable load balancing techniques for parallel\ncomputers. Journal of Parallel and Distributed Computing , 22(1):60\u2013 79, July 1994.\n[KH67] R. M. Karp and M. H. Held. Finite state processes and dynamic programming . SIAM\nJournal of Applied Math , 15:693\u2013718, 1967.\n[KH83] M. Kumar and D. S. Hirschberg. An efficient implementation of Batcher's odd-even\nmerge algorithm and its application in parallel sorting schemes . IEEE Transactions on\nComputers , C\u201332, March 1983.\n[KK79] P. Kermani and L. Kleinrock. Virtual cut-through: A new communication switching\ntechnique. Computer Networks , 3(4):267\u2013286, 1979.\n[KK83] V. Kumar and L. N. Kanal. A general branch-and-bound formulation for understanding\nand synthesizing and/or tree search procedures . Artificial Intelligence , 21:179\u2013198, 1983.\n[KK84] V. Kumar and L. N. Kanal. Parallel branch-and-bound formulations for and/or tree\nsearch. IEEE Transactions on Pattern Analysis and Machine Intelligence , PAMI\u20136:768\u2013778,\n1984.\n[KK88a] L. N. Kanal and V. Kumar. Search in Artificial Intelligence . Springer-Verlag, New York,\nNY, 1988.\n[KK88b] V. Kumar and L. N. Kanal. The CDP: A unifying formulation for heuristic search,\ndynamic programming, and branch-and-bound . In L. N. Kanal and V. Kumar, editors, Search in\nArtificial Intelligence , 1\u201327. Springer-Verlag, New York, NY, 1988.\n[KK93] G. Karypis and V. Kumar. Efficient Parallel Mappings of a Dynamic Programming\nAlgorithm . In Proceedings of 7th International Parallel Processing Symposium , number\n563\u2013568, 1993.\n[KK94] G. Karypis and V. Kumar. Unstructured tree search on simd parallel computers . Journal\nof Parallel and Distributed Computing , 22(3):379\u2013391, September 1994.\n[KK99] G. Karypis and V. Kumar. Parallel multilevel k-way partitioning for irregular graphs .\nSIAM Review , 41(2):278\u2013300, 1999.\n[KKKS94] L. N. Kanal, V. Kumar, H. Kitano, and C. Suttner, editors. Parallel Processing for\nArtificial Intelligence . North-Holland, Amsterdam, The Netherlands, 1994.\n[KN91] K. Kimura and I. Nobuyuki. Probabilistic analysis of the efficiency of the dynamic load\ndistribution . In Sixth Distributed Memory Computing Conference Proceedings , 1991.\n[Knu73] D. E. Knuth. The Art of Computer Programming: Sorting and Searching . Addison-\nWesley, Reading, MA, 1973.\n[Kor81] W. Kornfeld. The use of parallelism to implement a heuristic search . In Proceedings of\nthe International Joint Conference on Artificial Intelligence , 575\u2013580, 1981.\n[Kow88] J. S. Kowalik. Parallel Computation and Computers for Artificial Intelligence . Kluwer\nAcademic Publishers, Boston, MA, 1988.\n[KP92] C. Kaklamanis and G. Persiano. Branch-and-bound and backtrack search on mesh-\nconnected arrays of processors . In Proceedings of Fourth Annual Symposium on Parallel\nAlgorithms and Architectures , 118\u2013126, 1992.\n[KR87a] V. K. P. Kumar and C. S. Raghavendra. Array processor with multiple broadcasting .\nJournal of Parallel and Distributed Computing , 173\u2013190, 1987.\n[KR87b] V. Kumar", "doc_id": "d2d5a33b-ce41-4e72-876d-a0c7f6a53142", "embedding": null, "doc_hash": "8954cdb2d832191307553e9e1e0b3939452ea103031f1f11b76e7356bff6fdfd", "extra_info": null, "node_info": {"start": 1581699, "end": 1584681}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "69ca6a43-8a48-4b4e-9f7e-78e09c660a74", "3": "f852869a-b39f-4b0f-93cf-55fe035bdfc4"}}, "__type__": "1"}, "f852869a-b39f-4b0f-93cf-55fe035bdfc4": {"__data__": {"text": "Computers for Artificial Intelligence . Kluwer\nAcademic Publishers, Boston, MA, 1988.\n[KP92] C. Kaklamanis and G. Persiano. Branch-and-bound and backtrack search on mesh-\nconnected arrays of processors . In Proceedings of Fourth Annual Symposium on Parallel\nAlgorithms and Architectures , 118\u2013126, 1992.\n[KR87a] V. K. P. Kumar and C. S. Raghavendra. Array processor with multiple broadcasting .\nJournal of Parallel and Distributed Computing , 173\u2013190, 1987.\n[KR87b] V. Kumar and V. N. Rao. Parallel depth-first search, part II: Analysis . International\nJournal of Parallel Programming , 16(6):501\u2013519, 1987.\n[KR88] R. M. Karp and V. Ramachandran. A survey of complexity of algorithms for shared-\nmemory machines . Technical Report 408, University of California, Berkeley, 1988.\n[KR89] V. Kumar and V. N. Rao. Load balancing on the hypercube architecture . In Proceedings\nof the Fourth Conference on Hypercubes, Concurrent Computers, and Applications , 603\u2013608,\n1989.\n[KRR88] V. Kumar, K. Ramesh, and V. N. Rao. Parallel best-first search of state-space graphs:\nA summary of results . In Proceedings of the 1988 National Conference on Artificial Intelligence ,\n122\u2013126, 1988.\n[KRS88] C. P. Kruskal, L. Rudolph, and M. Snir. A complexity theory of efficient parallel\nalgorithms . Technical Report RC13572, IBM T. J. Watson Research Center, Yorktown Heights,\nNY, 1988.\n[Kru56] J. B. Kruskal. On the shortest spanning subtree of a graph and the traveling salesman\nproblem . In Proceedings of the AMS , volume 7, 48\u201350, 1956.\n[KS88] J. Kuehn and B. Smith. The horizon supercomputing system: architecture and software .\nIn Proceedings of Supercomputing Conference , 28\u201334, 1988.\n[KS91a] L. V. Kale and V. Saletore. Efficient parallel execution of IDA* on shared and\ndistributed-memory multiprocessors . In Sixth Distributed Memory Computing Conference\nProceedings , 1991.\n[KS91b] V. Kumar and V. Singh. Scalability of parallel algorithms for the all-pairs shortest path\nproblem . Journal of Parallel and Distributed Computing , 13(2):124\u2013138, October 1991. A short\nversion appears in the Proceedings of the International Conference on Parallel Processing , 1990.\n[KSS95] S. Kleiman, D. Shah, and B. Smaalders. Programming with Threads . SunSoft Press,\nMountainview, CA, 1995.\n[KU86] A. R. Karlin and E. Upfal. Parallel hashing \u2013 an efficient implementation of shared\nmemory . In Proceedings of 18th ACM Conference on Theory of Computing , 160\u2013168, 1986.\n[Kun80] J. T. Kung. The structure of parallel algorithms . In M. Yovits, editor, Advances in\nComputing , 73\u201374. Academic Press, San Diego, CA, 1980.\n[Kun86] H. T. Kung. Memory requirements for balanced computer architectures . In Proceedings\nof the 1986 IEEE Symposium on Computer Architecture , 49\u201354, 1986.\n[KV92] K. Kreeger and N. R. Vempaty. Comparison of meshes vs. hypercubes for data\nrearrangement . Technical Report UCF-CS-92-28, Department of Computer Science, University\nof Central Florida, Orlando, FL, 1992.\n[KZ88] R. M. Karp and Y. Zhang. A randomized parallel branch-and-bound", "doc_id": "f852869a-b39f-4b0f-93cf-55fe035bdfc4", "embedding": null, "doc_hash": "45697f764496f3753a3b15971f365b502471ef0c8557692d9146a59341296fef", "extra_info": null, "node_info": {"start": 1584668, "end": 1587706}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "d2d5a33b-ce41-4e72-876d-a0c7f6a53142", "3": "28e149df-fb73-45da-ba6a-72b7b99c5a1d"}}, "__type__": "1"}, "28e149df-fb73-45da-ba6a-72b7b99c5a1d": {"__data__": {"text": "CA, 1980.\n[Kun86] H. T. Kung. Memory requirements for balanced computer architectures . In Proceedings\nof the 1986 IEEE Symposium on Computer Architecture , 49\u201354, 1986.\n[KV92] K. Kreeger and N. R. Vempaty. Comparison of meshes vs. hypercubes for data\nrearrangement . Technical Report UCF-CS-92-28, Department of Computer Science, University\nof Central Florida, Orlando, FL, 1992.\n[KZ88] R. M. Karp and Y. Zhang. A randomized parallel branch-and-bound procedure . In\nProceedings of the ACM Annual Symposium on Theory of Computing , 290\u2013300, 1988.\n[Law75] D. H. Lawrie. Access and alignment of data in an array processor . IEEE Transactions on\nComputers , C-24(1):1145\u20131155, 1975.\n[LB95a] B. Lewis and D. J. Berg. Threads Primer: A Guide to Multithreaded Programming .\nPrentice Hall PTR/Sun Microsystems Press, 1995.\n[LB95b] B.-H. Lim and R. Bianchini. Limits on the performance benefits of multithreading and\nprefetching . Research report RC 20238 (89547) , IBM T. J. Watson Research Center, Yorktown\nHeights, NY, October 1995.\n[LB97] B. Lewis and D. J. Berg. Multithreaded Programming with Pthreads . Prentice Hall\nPTR/Sun Microsystems Press, 1997.\n[LB98] T. G. Lewis and D. Berg. Multithreaded Programming with PThreads . Sun Microsystems\nPress / Prentice Hall, 1998.\n[LC88] G.-J. Li and T. Coleman. A parallel triangular solver for a hypercube multiprocessor .\nSIAM Journal on Scientific and Statistical Computing , 9:485\u2013502, 1988.\n[LC89] G.-J. Li and T. Coleman. A new method for solving triangular systems on distributed\nmemory message passing multiprocessors . SIAM Journal on Scientific and Statistical Computing ,\n10:382\u2013396, 1989.\n[LD90] S. Lakshmivarahan and S. K. Dhall. Analysis and Design of Parallel Algorithms:\nArithmetic and Matrix Problems . McGraw-Hill, New York, NY, 1990.\n[LDP89] M. R. Leuze, L. W. Dowdy, and K. H. Park. Multiprogramming a distributed-memory\nmultiprocessor . Concurrency: Practice and Experience , 1(1):19\u201333, September 1989.\n[Lea99] D. Lea. Concurrent Programming in Java, Second Edition: Design Principles and\nPatterns . Addison-Wesley, 1999.\n[Lei83] F. T. Leighton. Parallel computation using mesh of trees . In Proceedings of International\nWorkshop on Graph-Theoretic Concepts in Computer Science , 1983.\n[Lei85a] F. T. Leighton. Tight bounds on the complexity of parallel sorting . IEEE Transactions on\nComputers , C\u201334(4):344\u2013354, April 1985.\n[Lei85b] C. E. Leiserson. Fat-trees: Universal networks for hardware efficient supercomputing .\nIn Proceedings of the 1985 International Conference on Parallel Processing , 393\u2013402, 1985.\n[Lei92] F. T. Leighton. Introduction to Parallel Algorithms and Architectures . Morgan Kaufmann,\nSan Mateo, CA, 1992.\n[LER92] T. G. Lewis and H. El-Rewini. Introduction to Parallel Computing . Prentice-Hall,\nEnglewood Cliffs, NJ, 1992.\n[Les93] B. P. Lester. The Art of Parallel Programming . Prentice-Hall, Englewood Cliffs, NJ, 1993.\n[Lev87] S. P. Levitan.", "doc_id": "28e149df-fb73-45da-ba6a-72b7b99c5a1d", "embedding": null, "doc_hash": "f809854eb4f9118e18c1599ede9934940523d366afc6dd6458134fe66fbdac54", "extra_info": null, "node_info": {"start": 1587729, "end": 1590662}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "f852869a-b39f-4b0f-93cf-55fe035bdfc4", "3": "5714f914-a773-4296-aa73-ef4081008cb3"}}, "__type__": "1"}, "5714f914-a773-4296-aa73-ef4081008cb3": {"__data__": {"text": "Proceedings of the 1985 International Conference on Parallel Processing , 393\u2013402, 1985.\n[Lei92] F. T. Leighton. Introduction to Parallel Algorithms and Architectures . Morgan Kaufmann,\nSan Mateo, CA, 1992.\n[LER92] T. G. Lewis and H. El-Rewini. Introduction to Parallel Computing . Prentice-Hall,\nEnglewood Cliffs, NJ, 1992.\n[Les93] B. P. Lester. The Art of Parallel Programming . Prentice-Hall, Englewood Cliffs, NJ, 1993.\n[Lev87] S. P. Levitan. Measuring communications structures in parallel architectures and\nalgorithms . In L. H. Jamieson, D. B. Gannon, and R. J. Douglass, editors, The Characteristics of\nParallel Algorithms . MIT Press, Cambridge, MA, 1987.\n[Lew91] D. A. Lewine. Posix Programmer's Guide: Writing Portable Unix Programs with the\nPosix. 1 Standard.  O'Reilly & Associates, 1991.\n[LHZ98] H. Lu, C. Hu, and W. Zwaenepoel. OpenMP on networks of workstations . In SC '98,\nHigh Performance Networking and Computing Conference , Orlando, Florida, 1998.\n[Lil92] D. J. Lilja. Architectural Alternatives for Exploiting Parallelism . IEEE Computer Society\nPress, Los Alamitos, CA, 1992.\n[Lin83] G. Lindstrom. The key node method: A highly parallel alpha-beta algorithm. Technical\nReport 83-101, Computer Science Department, University of Utah, Salt Lake City, UT, 1983.\n[Lin92] Z. Lin. A distributed fair polling scheme applied to or-parallel logic programming .\nInternational Journal of Parallel Programming , 20(4), August 1992.\n[LK72] K. N. Levitt and W. T. Kautz. Cellular arrays for the solution of graph problems .\nCommunications of the ACM , 15(9):789\u2013801, September 1972.\n[LK85] D. B. Leifker and L. N. Kanal. A hybrid SSS*/alpha-beta algorithm for parallel search of\ngame trees. In Proceedings of the International Joint Conference on Artificial Intelligence ,\n1044\u20131046, 1985.\n[LLG+92] D. Lenoski, J. Laudon, K. Gharachorloo, W. D. Weber, A. Gupta, J. L. Hennessy, M.\nHorowitz, and M. Lam. The Stanford dash multiprocessor . IEEE Computer , 63\u201379, March 1992.\n[LM97] E. K. Lee and J. E. Mitchell. Computational experience of an interior-point algorithm in a\nparallel branch-and-cut framework . In Proceedings for SIAM Conference on Parallel Processing\nfor Scientific Computing , 1997.\n[LMR88] F. T. Leighton, B. Maggs, and S. K. Rao. Universal packet routing algorithms . In 29th\nAnnual Symposium on Foundations of Computer Science , 256\u2013271, 1988.\n[Loa92] C. V. Loan. Computational Frameworks for the Fast Fourier Transform . SIAM,\nPhiladelphia, PA, 1992.\n[LP92] Y. Li and P. M. Pardalos. Parallel algorithms for the quadratic assignment problem . In P.\nM. Pardalos, editor, Advances in Optimization and Parallel Computing , 177\u2013189. North-Holland,\nAmsterdam, The Netherlands, 1992.\n[LPP88] F. Luccio, A. Pietracaprina, and G. Pucci. A probabilistic simulation of PRAMs on a\nbounded degree network . Information Processing Letters , 28:141\u2013147, July 1988.\n[LPP89] F. Luccio, A. Pietracaprina, and", "doc_id": "5714f914-a773-4296-aa73-ef4081008cb3", "embedding": null, "doc_hash": "2c5cd17ab88ace533b56cc6950bba982de9bfc3f5bee25492d14d26ac249a5fb", "extra_info": null, "node_info": {"start": 1590666, "end": 1593586}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "28e149df-fb73-45da-ba6a-72b7b99c5a1d", "3": "409d599e-1f2f-4e4d-b2a7-a47f2368d0b3"}}, "__type__": "1"}, "409d599e-1f2f-4e4d-b2a7-a47f2368d0b3": {"__data__": {"text": "Li and P. M. Pardalos. Parallel algorithms for the quadratic assignment problem . In P.\nM. Pardalos, editor, Advances in Optimization and Parallel Computing , 177\u2013189. North-Holland,\nAmsterdam, The Netherlands, 1992.\n[LPP88] F. Luccio, A. Pietracaprina, and G. Pucci. A probabilistic simulation of PRAMs on a\nbounded degree network . Information Processing Letters , 28:141\u2013147, July 1988.\n[LPP89] F. Luccio, A. Pietracaprina, and G. Pucci. A new scheme for deterministic simulation of\nPRAMs in VLSI . SIAM Journal of Computing , 1989.\n[LRZ95] C. Leiserson, K. Randall, and Y. Zhou. Cilk: An efficient multithreaded runtime system .\nIn Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel\nProgramming (PPoPP) , Santa Barbara, CA, 1995.\n[LS84] T. H. Lai and S. Sahni. Anomalies in parallel branch and bound algorithms .\nCommunications of the ACM , 594\u2013602, 1984.\n[LS85] T. H. Lai and A. Sprague. Performance of parallel branch-and-bound algorithms . IEEE\nTransactions on Computers , C-34(10), October 1985.\n[LS86] T. H. Lai and A. Sprague. A note on anomalies in parallel branch-and-bound algorithms\nwith one-to-one bounding functions . Information Processing Letters , 23:119\u2013122, October\n1986.\n[LSS88] J. Lee, E. Shragowitz, and S. Sahni. A hypercube algorithm for the 0/1 knapsack\nproblem . Journal of Parallel and Distributed Computing , (5):438\u2013456, 1988.\n[Lub86] M. Luby. A simple parallel algorithm for the maximal independent set problem . SIAM\nJournal on Computing , 15(4):1036\u20131053, 1986.\n[LW66] E. L. Lawler and D. Woods. Branch-and-bound methods: A survey . Operations\nResearch , 14, 1966.\n[LW84] G.-J. Li and B. W. Wah. Computational efficiency of parallel approximate branch-and-\nbound algorithms . In Proceedings of the 1984 International Conference on Parallel Processing ,\n473\u2013480, 1984.\n[LW85] G.-J. Li and B. W. Wah. Parallel processing of serial dynamic programming problems . In\nProceedings of COMPSAC 85 , 81\u201389, 1985.\n[LW86] G.-J. Li and B. W. Wah. Coping with anomalies in parallel branch-and-bound\nalgorithms . IEEE Transactions on Computers , C-35, June 1986.\n[LW95] D. Lenoski and W. D. Weber. Scalable Shared-Memory Multiprocessing . Morgan\nKaufmann, San Mateo, CA, 1995.\n[LY86] M. Li and Y. Yesha. New lower bounds for parallel computations . In Proceedings of 18th\nACM Conference on Theory of Computing, 177\u2013187, 1986.\n[MC82] T. A. Marsland and M. S. Campbell. Parallel search of strongly ordered game trees .\nComputing Surveys , 14:533\u2013551, 1982.\n[MD92] A. Mahanti and C. Daniels. SIMD parallel heuristic search . Artificial Intelligence , 1992.\n[MD93] N. R. Mahapatra and S. Dutt. Scalable duplicate pruning strategies for parallel A* graph\nsearch. In Proceedings of the Fifth IEEE Symposium on Parallel and Distributed Processing ,\n1993.\n[MdV87] O. A.", "doc_id": "409d599e-1f2f-4e4d-b2a7-a47f2368d0b3", "embedding": null, "doc_hash": "cce7b6bbbab16084102ba5970573882c44920bcdd1ada3ec7ad20adc621e62b2", "extra_info": null, "node_info": {"start": 1593602, "end": 1596418}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "5714f914-a773-4296-aa73-ef4081008cb3", "3": "747f701b-a2f9-416c-85e5-c84bfc22a6f6"}}, "__type__": "1"}, "747f701b-a2f9-416c-85e5-c84bfc22a6f6": {"__data__": {"text": "T. A. Marsland and M. S. Campbell. Parallel search of strongly ordered game trees .\nComputing Surveys , 14:533\u2013551, 1982.\n[MD92] A. Mahanti and C. Daniels. SIMD parallel heuristic search . Artificial Intelligence , 1992.\n[MD93] N. R. Mahapatra and S. Dutt. Scalable duplicate pruning strategies for parallel A* graph\nsearch. In Proceedings of the Fifth IEEE Symposium on Parallel and Distributed Processing ,\n1993.\n[MdV87] O. A. McBryan and E. F. V. de Velde. Hypercube algorithms and implementations .\nSIAM Journal on Scientific and Statistical Computing , 8(2):s227\u2013s287, March 1987.\n[Mes94] Message Passing Interface Forum. MPI: A Message-Passing Interface Standard .\nAvailable at http://www.mpi-forum.org. May 1994.\n[Mes97] Message Passing Interface Forum. MPI-2: Extensions to the Message-Passing Interface .\nAvailable at http://www.mpi-forum.org . July 1997.\n[MFMV90] B. Monien, R. Feldmann, P. Mysliwietz, and O. Vornberger. Parallel game tree search\nby dynamic tree decomposition . In V. Kumar, P. S. Gopalakrishnan, and L. N. Kanal, editors,\nParallel Algorithms for Machine Intelligence and Vision . Springer-Verlag, New York, NY, 1990.\n[MG89] C. U. Martel and D. Q. Gusfield. A fast parallel quicksort algorithm . Information\nProcessing Letters , 30:97\u2013102, 1989.\n[Mil91] D. Miller. Exact distributed algorithms for travelling salesman problem . In Proceedings\nof the Workshop on Parallel Computing of Discrete Optimization Problems , 1991.\n[MK99] J. Magee and J. Kramer. Concurrency: State Models and Java Programs . John Wiley &\nSons, 1999.\n[MKRS88] R. Miller, V. K. P. Kumar, D. I. Reisis, and Q. F. Stout. Meshes with reconfigurable\nbuses. In Proceedings of MIT Conference on Advanced Research in VLSI , 163\u2013178, 1988.\n[MM73] A. Martelli and U. Montanari. From dynamic programming to search algorithms with\nfunctional costs . In Proceedings of the International Joint Conference on Artifi cial Intelligence ,\n345\u2013349, 1973.\n[MM91] P. Messina and A. Murli, editors. Practical Parallel Computing: Status and Prospects .\nWiley, Chichester, UK, 1991.\n[MMR95] B. Mans, T. Mautor, and C. Roucairol. A parallel depth first search branch and bound\nfor the quadratic assignment problem . European Journal of Oper ational Research,\n81(3):617\u2013628, 1995.\n[Mod88] J. J. Modi. Parallel Algorithms and Matrix Computation . Oxford University Press,\nOxford, UK, 1988.\n[Moh83] J. Mohan. Experience with two parallel programs solving the traveling salesman\nproblem . In Proceedings of the 1983 International Conference on Parallel Processing , 191\u2013193,\n1983.\n[Mol86] C. B. Moler. Matrix computation on distributed-memory multiprocessors . In M. T.\nHeath, editor, Hypercube Multiprocessors 1986 , 181\u2013195. SIAM, Philadelphia, PA, 1986.\n[Mol87] C. B. Moler. Another look at Amdahl's law. Technical Report TN-02-0587-0288, Intel\nScientific Computers, 1987.\n[Mol93] D. I. Moldovan. Parallel Processing: From Applications to Systems . Morgan Kaufmann,\nSan Mateo,", "doc_id": "747f701b-a2f9-416c-85e5-c84bfc22a6f6", "embedding": null, "doc_hash": "d21d25e83010d919712d6d37ce07fa0b17d50f255b01ffd2cb0fd625a761c991", "extra_info": null, "node_info": {"start": 1596425, "end": 1599377}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "409d599e-1f2f-4e4d-b2a7-a47f2368d0b3", "3": "8a000c91-cc57-4d04-9553-361d15e61505"}}, "__type__": "1"}, "8a000c91-cc57-4d04-9553-361d15e61505": {"__data__": {"text": ", 191\u2013193,\n1983.\n[Mol86] C. B. Moler. Matrix computation on distributed-memory multiprocessors . In M. T.\nHeath, editor, Hypercube Multiprocessors 1986 , 181\u2013195. SIAM, Philadelphia, PA, 1986.\n[Mol87] C. B. Moler. Another look at Amdahl's law. Technical Report TN-02-0587-0288, Intel\nScientific Computers, 1987.\n[Mol93] D. I. Moldovan. Parallel Processing: From Applications to Systems . Morgan Kaufmann,\nSan Mateo, CA, 1993.\n[MP85] T. A. Marsland and F. Popowich. Parallel game tree search . IEEE Transactions on Pattern\nAnalysis and Machine Intelligence , PAMI-7(4):442\u2013452, July 1985.\n[MP93] D. L. Miller and J. F. Pekny. The role of performance metrics for parallel mathematical\nprogramming algorithms . ORSA Journal on Computing , 5(1), 1993.\n[MR] D. C. Marinescu and J. R. Rice. On high level characterization of parallelism. Technical\nReport CSD-TR-1011, CAPO Report CER-90-32 , Computer Science Department, Purdue\nUniversity, West Lafayette, IN. Also published in Journal of Parallel and Distributed Computing ,\n1993.\n[MRSR92] G. P. McKeown, V. J. Rayward-Smith, and S. A. Rush. Parallel Branch-and-Bound ,\n111\u2013150. Advanced Topics in Computer Science . Blackwell Scientific Publications, Oxford, UK,\n1992.\n[MS88] Y. W. E. Ma and D. G. Shea. Downward scalability of parallel architectures . In\nProceedings of the 1988 International Conference on Supercomputing , 109\u2013120, 1988.\n[MS90] G. Manzini and M. Somalvico. Probabilistic performance analysis of heuristic search\nusing parallel hash tables . In Proceedings of the International Symposium on Artificial\nIntelligence and Mathematics , 1990.\n[MS96] R. Miller and Q. F. Stout. Parallel Algorithms for Regular Architectures . MIT Press,\nCambridge, MA, 1996.\n[MV84] K. Mehlhorn and U. Vishkin. Randomized and deterministic simulations of PRAMs by\nparallel machines with restricted granularity of parallel memories . Acta Informatica ,\n21(4):339\u2013374, November 1984.\n[MV85] B. Monien and O. Vornberger. The ring machine. Technical report, University of\nPaderborn, Germany, 1985. Also in Computers and Artificial Intelligence , 3(1987).\n[MV87] B. Monien and O. Vornberger. Parallel processing of combinatorial search trees . In\nProceedings of International Workshop on Parallel Algorithms and Architectures , 1987.\n[MVS86] B. Monien, O. Vornberger, and E. Spekenmeyer. Superlinear speedup for parallel\nbacktracking . Technical Report 30, University of Paderborn, Germany, 1986.\n[NA91] D. Nussbaum and A. Agarwal. Scalability of parallel machines . Communications of the\nACM, 34(3):57\u201361, 1991.\n[Nat90] L. Natvig. Investigating the practical value of Cole's O (log n) time crew pram merge\nsort algorithm . In 5th International Symposium on Computing and Information Sciences ,\nOctober 1990.\n[NBF96] B. Nichols, B. Buttlar, and J. P. Farrell. Pthreads Programming . O'Reilly & Associates,\nNewton, MA 02164, 1996.\n[nCU90] nCUBE Corporation. nCUBE 6400 Processor Manual . Beaverton, OR, 1990.\n[ND96] S. J. Norton and M. D. DiPasquale. Thread", "doc_id": "8a000c91-cc57-4d04-9553-361d15e61505", "embedding": null, "doc_hash": "8c0fe1d63bb9f56f27af632faf3462fca26598ad7b5845d32842bd3931e8a743", "extra_info": null, "node_info": {"start": 1599380, "end": 1602372}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "747f701b-a2f9-416c-85e5-c84bfc22a6f6", "3": "4e5cbca3-ad32-4ddf-a881-efb68765f161"}}, "__type__": "1"}, "4e5cbca3-ad32-4ddf-a881-efb68765f161": {"__data__": {"text": "Investigating the practical value of Cole's O (log n) time crew pram merge\nsort algorithm . In 5th International Symposium on Computing and Information Sciences ,\nOctober 1990.\n[NBF96] B. Nichols, B. Buttlar, and J. P. Farrell. Pthreads Programming . O'Reilly & Associates,\nNewton, MA 02164, 1996.\n[nCU90] nCUBE Corporation. nCUBE 6400 Processor Manual . Beaverton, OR, 1990.\n[ND96] S. J. Norton and M. D. DiPasquale. Thread time: the multithreaded programming guide .\nHewlett-Packard professional books. Prentice-Hall, Englewood Cliffs, NJ 07632, 1996.\n[Ni91] L. M. Ni. A layered classification of parallel computers . In Proceedings of 1991\nInternational Conference for Young Computer Scientists , 28\u201333, 1991.\n[Nic90] J. R. Nickolls. The design of the MasPar MP-1: A cost-effective massively parallel\ncomputer . In IEEE Digest of Papers\u2014Comcom , 25\u201328. IEEE Computer Society Press, Los\nAlamitos, CA, 1990.\n[NM93] L. M. Ni and McKinley. A survey of wormhole routing techniques in direct connect\nnetworks. IEEE Computer , 26(2), February 1993.\n[NMB83] D. Nath, S. N. Maheshwari, and P. C. P. Bhatt. Efficient VLSI networks for parallel\nprocessing based on orthogonal trees . IEEE Transactions on Computers , C\u201332:21\u201323, June\n1983.\n[NS79] D. Nassimi and S. Sahni. Bitonic sort on a mesh connected parallel computer . IEEE\nTransactions on Computers , C\u201328(1), January 1979.\n[NS80] D. Nassimi and S. Sahni. Finding connected components and connected ones on a\nmesh-connected computer . SIAM Journal of Computing , 9(4):744\u2013757, November 1980.\n[NS87] A. Norton and A. J. Silberger. Parallelization and performance analysis of the Cooley-\nTukey FFT algorithm for shared memory architectures . IEEE Transactions on Computers , C-\n36(5):581\u2013591, 1987.\n[NS93] M. Nigam and S. Sahni. Sorting n numbers on n x n reconfigurable meshes with buses .\nIn 7th International Parallel Processing Symposium , 174\u2013181, 1993.\n[NSF91] Grand Challenges: High Performance Computing and Communications . A Report by the\nCommittee on Physical, Mathematical and Engineering Sciences, NSF/CISE, 1800 G Street NW,\nWashington, DC, 20550, 1991.\n[Nug88] S. F. Nugent. The iPSC/2 direct-connect communications technology . In Proceedings of\nthe Third Conference on Hypercubes, Concurrent Computers, and Applications , 51\u201360, 1988.\n[Nus82] H. J. Nussbaumer. Fast Fourier Transform and Convolution Algorithms . Springer-\nVerlag, New York, NY, 1982.\n[NW88] D. M. Nicol and F. H. Willard. Problem size, parallel architecture, and optimal speedup .\nJournal of Parallel and Distributed Computing , 5:404\u2013420, 1988.\n[GOV99] Funding a Revolution: Government Support for Computing Research . Committee on\nInnovations in Computing and Communications. National Academy Press, 1999.\n[OR88] J. M. Ortega and C. H. Romine. The ijk forms of factorization methods II: Parallel\nsystems . Parallel Computing , 7:149\u2013162, 1988.\n[Ort88] J. M. Ortega. Introduction to Parallel and Vector Solution of Linear Systems . Plenum\nPress, New York, NY, 1988.\n[OS85] D. P.", "doc_id": "4e5cbca3-ad32-4ddf-a881-efb68765f161", "embedding": null, "doc_hash": "5051bfdfc199b7a0da79ff6d3fcd970bb2a85500a24ec85cc53309494b5fa9ce", "extra_info": null, "node_info": {"start": 1602374, "end": 1605384}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "8a000c91-cc57-4d04-9553-361d15e61505", "3": "2f1f09a5-6fda-48a1-b668-4e342160f2c1"}}, "__type__": "1"}, "2f1f09a5-6fda-48a1-b668-4e342160f2c1": {"__data__": {"text": "Distributed Computing , 5:404\u2013420, 1988.\n[GOV99] Funding a Revolution: Government Support for Computing Research . Committee on\nInnovations in Computing and Communications. National Academy Press, 1999.\n[OR88] J. M. Ortega and C. H. Romine. The ijk forms of factorization methods II: Parallel\nsystems . Parallel Computing , 7:149\u2013162, 1988.\n[Ort88] J. M. Ortega. Introduction to Parallel and Vector Solution of Linear Systems . Plenum\nPress, New York, NY, 1988.\n[OS85] D. P. O'Leary and G. W. Stewart. Data-flow algorithms for parallel matrix computations .\nCommunications of the ACM , 28:840\u2013853, 1985.\n[OS86] D. P. O'Leary and G. W. Stewart. Assignment and scheduling in parallel matrix\nfactorization . Linear Algebra and its Applications , 77:275\u2013299, 1986.\n[Pac98] P. Pacheco. Parallel Programming with MPI . Morgan Kaufmann, 1998.\n[PBG+85] G. F. Pfister, W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfelder, K. P.\nMcAuliffe, E. A. Melton, V. A. Norlton, and J. Weiss. The IBM research parallel processor\nprototype (RP3): Introduction and architecture . In Proceedings of 1985 International Conference\non Parallel Processing , 764\u2013 771, 1985.\n[PC89] P. M. Pardalos and J. Crouse. A parallel algorithm for the quadratic assignment problem .\nIn Supercomputing '89 Proceedings , 351\u2013360. ACM Press, New York, NY, 1989.\n[PD89] K. H. Park and L. W. Dowdy. Dynamic partitioning of multiprocessor systems .\nInternational Journal of Parallel Processing , 18(2):91\u2013120, 1989.\n[Pea84] J. Pearl. Heuristics\u2014Intelligent Search Strategies for Computer Problem Solving .\nAddison-Wesley, Reading, MA, 1984.\n[Per87] R. Perrott. Parallel Programming . Addison-Wesley, Reading, MA, 1987.\n[Pfi98] G. F. Pfister. In Search of Clusters . Prentice Hall, Englewood Cliffs, NJ, 1998. 2nd\nEdition.\n[PFK90] C. Powley, C. Ferguson, and R. Korf. Parallel heuristic search: Two approaches . In V.\nKumar, P. S. Gopalakrishnan, and L. N. Kanal, editors, Parallel Algorithms for Machine\nIntelligence and Vision . Springer-Verlag, New York, NY, 1990.\n[PG98] T. Q. Pham and P. K. Garg. Multithreaded Programming with Win32 . Prentice Hall, 1998.\n[PH90] D. A. Patterson and J. L. Hennessy. Computer Architecture: A Quantitative Approach .\nMorgan Kaufmann, San Mateo, CA, 1990.\n[PH96] D. A. Patterson and J. L. Hennessy. Computer Architecture: A Quantitative Approach,\n2nd edition . Morgan Kaufmann, San Mateo, CA, 1996.\n[PK89] R. C. Paige and C. P. Kruskal. Parallel algorithms for shortest path problems . In\nProceedings of 1989 International Conference on Parallel Processing , 14\u2013 19, 1989.\n[PK95] I. Pramanick and J. G. Kuhl. An inherently parallel method for heuristic problem-solving:\nPart I \u2013 general framework . IEEE Transactions on Parallel and Distributed Systems , 6(10),\nOctober 1995.\n[PKF92] C. Powley, R. Korf, and C. Ferguson. IDA* on the connection machine .", "doc_id": "2f1f09a5-6fda-48a1-b668-4e342160f2c1", "embedding": null, "doc_hash": "c2f8272b9e0641a2f487b9d04f6930128b7523dd3f5a404966e5fd98463dfb53", "extra_info": null, "node_info": {"start": 1605339, "end": 1608192}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "4e5cbca3-ad32-4ddf-a881-efb68765f161", "3": "6a60bc5c-2abb-4bce-8371-9fc55cf252be"}}, "__type__": "1"}, "6a60bc5c-2abb-4bce-8371-9fc55cf252be": {"__data__": {"text": "1996.\n[PK89] R. C. Paige and C. P. Kruskal. Parallel algorithms for shortest path problems . In\nProceedings of 1989 International Conference on Parallel Processing , 14\u2013 19, 1989.\n[PK95] I. Pramanick and J. G. Kuhl. An inherently parallel method for heuristic problem-solving:\nPart I \u2013 general framework . IEEE Transactions on Parallel and Distributed Systems , 6(10),\nOctober 1995.\n[PKF92] C. Powley, R. Korf, and C. Ferguson. IDA* on the connection machine . Artificial\nIntelligence , 1992.\n[Pla89] C. C. Plaxton. Load balancing, selection and sorting on the hypercube . In Proceedings of\nthe 1989 ACM Symposium on Parallel Algorithms and Architectures , 64\u201373, 1989.\n[PLRR94] P. M. Pardalos, Y. Li, K. Ramakrishna, and M. Resende. Lower bounds for the\nquadratic assignment problem . Annals of Operations Research , 50:387\u2013411, 1994. Special\nVolume on Applications of Combinatorial Optimization.\n[PR85] V. Pan and J. H. Reif. Efficient parallel solution of linear systems . In 17th Annual ACM\nSymposium on Theory of Computing , 143\u2013152, 1985.\n[PR89] P. M. Pardalos and G. P. Rodgers. Parallel branch-and-bound algorithms for\nunconstrainted quadratic zero-one programming . In R. Sharda et al., editors, Impacts of Recent\nComputer Advances on Operations Research , 131\u2013143. North-Holland, Amsterdam, The\nNetherlands, 1989.\n[PR90] P. M. Pardalos and G. P. Rodgers. Parallel branch-and-bound algorithms for quadratic\nzero-one programming on a hypercube architecture . Annals of Operations Research ,\n22:271\u2013292, 1990.\n[PR91] M. Padberg and G. Rinaldi. A branch-and-cut algorithm for the resolution of large-scale\nsymmetric traveling salesman problems. SIAM Review , 33:60\u2013 100, 1991.\n[Pri57] R. C. Prim. Shortest connection network and some generalizations . Bell Systems\nTechnical Journal , 36:1389\u20131401, 1957.\n[PRV88] G. Plateau, C. Roucairol, and I. Valabregue. Algorithm PR2 for the parallel size\nreduction of the 0/1 multiknapsack problem . In INRIA Rapports de Recherche , number 811,\n1988.\n[PS82] C. H. Papadimitriou and K. Steiglitz. Combinatorial Optimization: Algorithms and\nComplexity . Prentice-Hall, Englewood Cliffs, NJ, 1982.\n[PV80] F. P. Preparata and J. Vuillemin. Area-time optimal VLSI networks for matrix\nmultiplication . In Proceedings of the 14th Princeton Conference on Information Science and\nSystems , 300\u2013309, 1980.\n[PY88] C. H. Papadimitriou and M. Yannakakis. Towards an architecture independent analysis of\nparallel algorithms . In Proceedings of 20th ACM Symposium on Theory of Computing , 510\u2013513,\n1988.\n[QD86] M. J. Quinn and N. Deo. An upper bound for the speedup of parallel branch-and-bound\nalgorithms . BIT, 26(1), March 1986.\n[Qui88] M. J. Quinn. Parallel sorting algorithms for tightly coupled multiprocessors . Parallel\nComputing , 6:349\u2013357, 1988.\n[Qui89] M. J. Quinn. Analysis and implementation of branch-and-bound algorithms on a\nhypercube multicomputer . IEEE", "doc_id": "6a60bc5c-2abb-4bce-8371-9fc55cf252be", "embedding": null, "doc_hash": "064c3d887460b2832601c002c55acbed9e49b9816f1a78f4014097efa654a0c1", "extra_info": null, "node_info": {"start": 1608210, "end": 1611112}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2f1f09a5-6fda-48a1-b668-4e342160f2c1", "3": "3ba03a1c-3fb6-404c-b2a1-5a88a1c5d8cf"}}, "__type__": "1"}, "3ba03a1c-3fb6-404c-b2a1-5a88a1c5d8cf": {"__data__": {"text": "Symposium on Theory of Computing , 510\u2013513,\n1988.\n[QD86] M. J. Quinn and N. Deo. An upper bound for the speedup of parallel branch-and-bound\nalgorithms . BIT, 26(1), March 1986.\n[Qui88] M. J. Quinn. Parallel sorting algorithms for tightly coupled multiprocessors . Parallel\nComputing , 6:349\u2013357, 1988.\n[Qui89] M. J. Quinn. Analysis and implementation of branch-and-bound algorithms on a\nhypercube multicomputer . IEEE Transactions on Computers , 1989.\n[Qui94] M. J. Quinn. Parallel Computing: Theory and Practice . McGraw-Hill, New York, NY,\n1994.\n[Ram97] V. Ramachandran. Qsm: A general purpose shared-memory model for parallel\ncomputation . In Foundations of Software Technology and Theoretical Computer Science , 1\u20135,\n1997.\n[Ran89] A. G. Ranade. Fluent Parallel Computation . Ph.D. Thesis, Department of Computer\nScience, Yale University, New Haven, CT, 1989.\n[Ran91] A. G. Ranade. Optimal speedup for backtrack search on a butterfly network . In\nProceedings of the Third ACM Symposium on Parallel Algorithms and Architectures , 1991.\n[Rao90] V. N. Rao. Parallel Processing of Heuristic Search.  Ph.D. Thesis, University of Texas,\nAustin, TX, 1990.\n[Ras78] L. Raskin. Performance Evaluation of Multiple Processor Systems . Ph.D. Thesis,\nCarnegie-Mellon University, Pittsburgh, PA, 1978.\n[RB76] C. M. Rader and N. M. Brenner. A new principle for Fast fourier transform . IEEE\nTransactions on Acoustics, Speech and Signal Processing , 24:264\u2013265, 1976.\n[RDK89] C. Renolet, M. Diamond, and J. Kimbel. Analytical and heuristic modeling of\ndistributed algorithms . Technical Report E3646, FMC Corporation, Advanced Systems Center,\nMinneapolis, MN, 1989.\n[Rei81] R. Reischuk. Probabilistic algorithms for sorting and selection . SIAM Journal of\nComputing , 396\u2013409, 1981.\n[RF89] D. A. Reed and R. M. Fujimoto. Multicomputer Networks: Message-Based Parallel\nProcessing . MIT Press, Cambridge, MA, 1989.\n[RICN88] K. Rokusawa, N. Ichiyoshi, T. Chikayama, and H. Nakashima. An efficient termination\ndetection and abortion algorithm for distributed processing systems . In Proceedings of 1988\nInternational Conference on Parallel Processing: Vol. I , 18\u201322, 1988.\n[RK87] V. N. Rao and V. Kumar. Parallel depth-first search, part I: Implementation .\nInternational Journal of Parallel Programming , 16(6):479\u2013499, 1987.\n[RK88a] V. N. Rao and V. Kumar. Concurrent access of priority queues . IEEE Transactions on\nComputers , C\u201337 (12), 1988.\n[RK88b] V. N. Rao and V. Kumar. Superlinear speedup in state-space search . In Proceedings of\nthe 1988 Foundation of Software Technology and Theoretical Computer Science , number 338,\n161\u2013174. Springer-Verlag Series Lecture Notes in Computer Science, 1988.\n[RK93] V. N. Rao and V. Kumar. On the efficicency of parallel backtracking . IEEE Transactions\non Parallel and Distributed Systems , 4(4):427\u2013437, April 1993. available as a technical report\nTR 90-55 , Computer Science Department, University of Minnesota.\n[RKR87] V. N. Rao, V. Kumar,", "doc_id": "3ba03a1c-3fb6-404c-b2a1-5a88a1c5d8cf", "embedding": null, "doc_hash": "94d01cd4441b79fa618b6e675a7bd59fe4b43fac69d99cd68317115928384ce2", "extra_info": null, "node_info": {"start": 1611143, "end": 1614116}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "6a60bc5c-2abb-4bce-8371-9fc55cf252be", "3": "2494c10d-01aa-4916-a318-7ff596aa1c12"}}, "__type__": "1"}, "2494c10d-01aa-4916-a318-7ff596aa1c12": {"__data__": {"text": "V. Kumar. Superlinear speedup in state-space search . In Proceedings of\nthe 1988 Foundation of Software Technology and Theoretical Computer Science , number 338,\n161\u2013174. Springer-Verlag Series Lecture Notes in Computer Science, 1988.\n[RK93] V. N. Rao and V. Kumar. On the efficicency of parallel backtracking . IEEE Transactions\non Parallel and Distributed Systems , 4(4):427\u2013437, April 1993. available as a technical report\nTR 90-55 , Computer Science Department, University of Minnesota.\n[RKR87] V. N. Rao, V. Kumar, and K. Ramesh. A parallel implementation of iterative-\ndeepening-A* . In Proceedings of the National Conference on Artificial Intelligence (AAAI-87) ,\n878\u2013882, 1987.\n[RND77] E. M. Reingold, J. Nievergelt, and N. Deo. Combinatorial Algorithms: Theory and\nPractice . Prentice-Hall, Englewood Cliffs, NJ, 1977.\n[RO88] C. H. Romine and J. M. Ortega. Parallel solution of triangular systems of equations .\nParallel Computing , 6:109\u2013114, 1988.\n[Rob75] M. O. Robin. Probabilistic algorithms . In J. Traub, editor, Algorithms and Complexity:\nNew Directions and Recent Results , 21\u201339. Academic Press, San Diego, CA, 1975.\n[Rob90] Y. Robert. The Impact of Vector and Parallel Architectures on Gaussian Elimination .\nJohn Wiley and Sons, New York, NY, 1990.\n[Rom87] C. H. Romine. The parallel solution of triangular systems on a hypercube . In M. T.\nHeath, editor, Hypercube Multiprocessors 1987 , 552\u2013559. SIAM, Philadelphia, PA, 1987.\n[Rou87] C. Roucairol. A parallel branch-and-bound algorithm for the quadratic assignment\nproblem . Discrete Applied Mathematics, 18:211\u2013225, 1987.\n[Rou91] C. Roucairol. Parallel branch-and-bound on shared-memory multiprocessors . In\nProceedings of the Workshop On Parallel Computing of Discrete Optimization Problems , 1991.\n[RRRR96] K. A. Robbins, S. Robbins, K. R. Robbins, and S. Robbins. Practical UNIX\nProgramming: A Guide to Concurrency, Communication, and Multithreading . Prentice Hall,\n1996.\n[RS90a] A. P. R. Alverson, D. Callahan, D. Cummings, B. Koblenz and B. Smith. The tera\ncomputer system . In International Conference on Supercomputing , 1\u20136, 1990.\n[RS90b] S. Ranka and S. Sahni. Hypercube Algorithms for Image Processing and  Pattern\nRecognition. Springer-Verlag, New York, NY, 1990.\n[RV87] J. H. Reif and L. G. Valiant. A logarithmic time sort for linear size networks . Journal of\nthe ACM , 34(1):60\u201376, January 1987.\n[Ryt88] W. Rytter. Efficient parallel computations for dynamic programming . Theoretical\nComputer Science , 59:297\u2013307, 1988.\n[RZ89] M. Reeve and S. E. Zenith, editors. Parallel Processing and Artificial Intelligence . Wiley,\nChichester, UK, 1989.\n[Saa86] Y. Saad. Communication complexity of the Gaussian elimination algorithm on\nmultiprocessors . Linear Algebra and its Applications , 77:315\u2013340, 1986.\n[SB77] H. Sullivan and T. R. Bashkow. A large scale, homogeneous, fully distributed parallel\nmachine . In Proceedings of Fourth Symposium on Computer Architecture ,", "doc_id": "2494c10d-01aa-4916-a318-7ff596aa1c12", "embedding": null, "doc_hash": "9d77189d02a27c450269f6336e7f1b7fbe59ce6bbd00d93889c248fc9445d96f", "extra_info": null, "node_info": {"start": 1614032, "end": 1616986}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "3ba03a1c-3fb6-404c-b2a1-5a88a1c5d8cf", "3": "7b5ca763-96ef-49bc-bf85-f920654dcdcf"}}, "__type__": "1"}, "7b5ca763-96ef-49bc-bf85-f920654dcdcf": {"__data__": {"text": "Science , 59:297\u2013307, 1988.\n[RZ89] M. Reeve and S. E. Zenith, editors. Parallel Processing and Artificial Intelligence . Wiley,\nChichester, UK, 1989.\n[Saa86] Y. Saad. Communication complexity of the Gaussian elimination algorithm on\nmultiprocessors . Linear Algebra and its Applications , 77:315\u2013340, 1986.\n[SB77] H. Sullivan and T. R. Bashkow. A large scale, homogeneous, fully distributed parallel\nmachine . In Proceedings of Fourth Symposium on Computer Architecture , 105\u2013124, March\n1977.\n[SBCV90] R. H. Saavedra-Barrera, D. E. Culler, and T. Von Eiken. Analysis of multithreaded\narchitectures for parallel computing . Report UCB/CSD 90/569, University of California, Berkeley,\nComputer Science Division, Berkeley, CA, April 1990.\n[Sch80] J. T. Schwartz. Ultracomputers . ACM Transactions on Programming Languages and\nSystems , 2:484\u2013521, October 1980.\n[Sed78] R. Sedgewick. Implementing quicksort programs . Communications of the ACM ,\n21(10):847\u2013857, 1978.\n[Sei85] C. L. Seitz. The cosmic cube . Communications of the ACM , 28(1):22\u201333, 1985.\n[Sei89] S. R. Seidel. Circuit-switched vs. store-and-forward solutions to symmetric\ncommunication problems . In Proceedings of the Fourth Conference on Hypercubes, Concurrent\nComputers, and Applications , 253\u2013255, 1989.\n[Sei92] C. L. Seitz. Mosaic C: An experimental fine-grain multicomputer. Technical report,\nCalifornia Institute of Technology, Pasadena, CA, 1992.\n[SG88] S. R. Seidel and W. L. George. Binsorting on hypercube with d-port communication . In\nProceedings of the Third Conference on Hypercube Concurrent Computers , 1455\u20131461, January\n1988.\n[SG91] X.-H. Sun and J. L. Gustafson. Toward a better parallel performance metric . Parallel\nComputing , 17:1093\u20131109, December 1991. Also available as Technical Report IS-5053, UC-\n32, Ames Laboratory, Iowa State University, Ames, IA.\n[Sha85] J. A. Sharp. Data-Flow Computing . Ellis Horwood, Chichester, UK, 1985.\n[She59] D. L. Shell. A high-speed sorting procedure . Communications of the ACM , 2(7):30\u201332,\nJuly 1959.\n[SHG93] J. P. Singh, J. L. Hennessy, and A. Gupta. Scaling parallel programs for\nmultiprocessors: Methodology and examples . IEEE Computer , 26(7):42\u201350, 1993.\n[Sie77] H. J. Siegel. The universality of various types of SIMD machine interconnection\nnetworks. In Proceedings of the 4th Annual Symposium on Computer Architecture  , 23\u201325,\n1977.\n[Sie85] H. J. Siegel. Interconnection Networks for Large-Scale Parallel Processing . D. C. Heath,\nLexington, MA, 1985.\n[SJ81] C. Savage and J. Jaja. Fast, efficient parallel algorithms for some graph problems . SIAM\nJournal of Computing , 10(4):682\u2013690, November 1981.\n[SK89] W. Shu and L. V. Kale. A dynamic scheduling strategy for the chare-kernel system . In\nProceedings of Supercomputing Conference , 389\u2013398, 1989.\n[SK90] V. Saletore and L. V. Kale. Consistent linear speedup to a first solution in parallel state-\nspace search. In", "doc_id": "7b5ca763-96ef-49bc-bf85-f920654dcdcf", "embedding": null, "doc_hash": "d578beee15f2fbe2e5ba147dc3ea0cc9728e8bdc332f18b520f326ece8f60429", "extra_info": null, "node_info": {"start": 1617025, "end": 1619930}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "2494c10d-01aa-4916-a318-7ff596aa1c12", "3": "b8e1839e-61fc-4cc6-8966-671282874ca9"}}, "__type__": "1"}, "b8e1839e-61fc-4cc6-8966-671282874ca9": {"__data__": {"text": "Heath,\nLexington, MA, 1985.\n[SJ81] C. Savage and J. Jaja. Fast, efficient parallel algorithms for some graph problems . SIAM\nJournal of Computing , 10(4):682\u2013690, November 1981.\n[SK89] W. Shu and L. V. Kale. A dynamic scheduling strategy for the chare-kernel system . In\nProceedings of Supercomputing Conference , 389\u2013398, 1989.\n[SK90] V. Saletore and L. V. Kale. Consistent linear speedup to a first solution in parallel state-\nspace search. In Proceedings of the 1990 National Conference on Artificial Intelligence , 227\u2013233,\nAugust 1990.\n[SKAT91a] V. Singh, V. Kumar, G. Agha, and C. Tomlinson. Efficient algorithms for parallel\nsorting on mesh multicomputers . International Journal of Parallel Programming , 20(2):95\u2013131,\n1991.\n[SKAT91b] V. Singh, V. Kumar, G. Agha, and C. Tomlinson. Scalability of parallel sorting on\nmesh multicomputers . International Journal of Parallel Programming , 20(2), 1991.\n[SM86] S. J. Stolfo and D. P. Miranker. The DADO production system machine . Journal of\nParallel and Distributed Computing , 3:269\u2013296, June 1986.\n[Smi84] D. R. Smith. Random trees and the analysis of branch and bound procedures . Journal\nof the ACM , 31(1), 1984.\n[SN90] X.-H. Sun and L. M. Ni. Another view of parallel speedup . In Supercomputing '90\nProceedings , 324\u2013333, 1990.\n[SN93] X.-H. Sun and L. M. Ni. Scalable problems and memory-bounded speedup . Journal of\nParallel and Distributed Computing , 19:27\u201337, September 1993.\n[Sni82] M. Snir. On parallel search . In Proceedings of Principles of Distributed Computing ,\n242\u2013253, 1982.\n[Sni85] M. Snir. On parallel searching . SIAM Journal of Computing , 14(3):688\u2013708, August\n1985.\n[Sny86] L. Snyder. Type architectures, shared-memory and the corollary of modest potential .\nAnnual Review of Computer Science , 1:289\u2013317, 1986.\n[SOHL+96] M. Snir, S. W. Otto, S. Huss-Lederman, D. W. Walker, and J. Dongarra. MPI: The\nComplete Reference . MIT Press, Cambridge, MA, 1996.\n[Sol77] M. Sollin. An algorithm attributed to Sollin . In S. Goodman and S. Hedetniemi, editors,\nIntroduction to The Design and Analysis of Algorithms . McGraw-Hill, Cambridge, MA, 1977.\n[SR91] X.-H. Sun and D. T. Rover. Scalability of parallel algorithm-machine combinations .\nTechnical Report IS-5057, Ames Laboratory, Iowa State University, Ames, IA, 1991. Also\npublished in IEEE Transactions on Parallel and Distributed Systems .\n[SS88] Y. Saad and M. H. Schultz. Topological properties of hypercubes . IEEE Transactions on\nComputers , 37:867\u2013872, 1988.\n[SS89a] Y. Saad and M. H. Schultz. Data communication in hypercubes . Journal of Parallel and\nDistributed Computing , 6:115\u2013135, 1989. Also available as Technical Report YALEU/DCS/RR-\n428 from the Department of Computer Science, Yale University, New Haven, CT.\n[SS89b] Y. Saad and M. H. Schultz. Data communication in parallel architectures . Parallel\nComputing , 11:131\u2013150, 1989.\n[SS90] H. Shi and J. Schaeffer. Parallel sorting by", "doc_id": "b8e1839e-61fc-4cc6-8966-671282874ca9", "embedding": null, "doc_hash": "7d7d2d3809abdffd332a7ed30d14f96ff700a3e0697c9c99a8715e088075d4d1", "extra_info": null, "node_info": {"start": 1619959, "end": 1622886}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "7b5ca763-96ef-49bc-bf85-f920654dcdcf", "3": "9722d968-fbd0-44b9-a02c-655a19a028a0"}}, "__type__": "1"}, "9722d968-fbd0-44b9-a02c-655a19a028a0": {"__data__": {"text": ", 37:867\u2013872, 1988.\n[SS89a] Y. Saad and M. H. Schultz. Data communication in hypercubes . Journal of Parallel and\nDistributed Computing , 6:115\u2013135, 1989. Also available as Technical Report YALEU/DCS/RR-\n428 from the Department of Computer Science, Yale University, New Haven, CT.\n[SS89b] Y. Saad and M. H. Schultz. Data communication in parallel architectures . Parallel\nComputing , 11:131\u2013150, 1989.\n[SS90] H. Shi and J. Schaeffer. Parallel sorting by regular sampling . Journal of Parallel and\nDistributed Computing , 14:361\u2013372, 1990.\n[ST95] B. M. S. Tschvke, R. Lling. Solving the traveling salesman problem with a distributed\nbranch-and-bound algorithm on a 1024 processor network . In Proceedings of the 9th\nInternational Parallel Processing Symposium , 182\u2013 189, Santa Barbara, CA, April 1995.\n[Sto71] H. S. Stone. Parallel processing with the perfect shuffle . IEEE Transactions on\nComputers , C-20(2):153\u2013161, 1971.\n[Sto93] H. S. Stone. High-Performance Computer Architecture: Third Edition . Addison-Wesley,\nReading, MA, 1993.\n[Sun95] SunSoft. Solaris multithreaded programming guide . SunSoft Press, Mountainview, CA,\n1995.\n[Sup91] Supercomputer Systems Division, Intel Corporation. Paragon XP/S Product Overview .\nBeaverton, OR, 1991.\n[SV81] Y. Shiloach and U. Vishkin. Finding the maximum, merging and sorting in a parallel\ncomputation model . Journal of Algorithms , 88\u2013102, 1981.\n[SV82] Y. Shiloach and U. Vishkin. An O (n2 log n) parallel max-flow algorithm . Journal of\nAlgorithms , 3:128\u2013146, 1982.\n[SW87] Q. F. Stout and B. A. Wagar. Passing messages in link-bound hypercubes. In M. T.\nHeath, editor, Hypercube Multiprocessors 1987 , 251\u2013257. SIAM, Philadelphia, PA, 1987.\n[Swa87] P. N. Swarztrauber. Multiprocessor FFTs . Parallel Computing , 5:197\u2013210, 1987.\n[SZ96] X.-H. Sun and J. P. Zhu. Performance considerations of shared virtual memory\nmachines. IEEE Trans. on Parallel and Distributed Systems , 6(11):1185\u2013 1194, 1996.\n[Tab90] D. Tabak. Multiprocessors . Prentice-Hall, Englewood Cliffs, NJ, 1990.\n[Tab91] D. Tabak. Advanced Multiprocessors . McGraw-Hill, New York, NY, 1991.\n[Tak87] R. Take. An optimal routing method of all-to-all communication on hypercube\nnetworks. In 35th Information Processing Society of Japan , 1987.\n[TEL95] D. M. Tullsen, S. Eggers, and H. M. Levy. Simultaneous multithreading: Maximizing on-\nchip parallelism . In Proceedings of the 22nd Annual International Symposium on Computer\nArchitecture , 1995.\n[Ten90] S. Teng. Adaptive parallel algorithms for integral knapsack problems . Journal of Parallel\nand Distributed Computing , 8:400\u2013406, 1990.\n[Thi90] Thinking Machines Corporation. The CM-2 Technical Summary . Cambridge, MA. 1990.\n[Thi91] Thinking Machines Corporation. The CM-5 Technical Summary . Cambridge, MA. 1991.\n[Tho83] C. D. Thompson. Fourier transforms in VLSI . IBM Journal of Research and\nDevelopment , C-32(11):1047\u20131057, 1983.\n[Thr99]", "doc_id": "9722d968-fbd0-44b9-a02c-655a19a028a0", "embedding": null, "doc_hash": "ef5f994ac32a7f7005ae8a13cf971023af80a3868dc6f6a2345d973ac06675f3", "extra_info": null, "node_info": {"start": 1622878, "end": 1625788}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "b8e1839e-61fc-4cc6-8966-671282874ca9", "3": "667906e4-5af2-4607-afde-796423524863"}}, "__type__": "1"}, "667906e4-5af2-4607-afde-796423524863": {"__data__": {"text": "S. Teng. Adaptive parallel algorithms for integral knapsack problems . Journal of Parallel\nand Distributed Computing , 8:400\u2013406, 1990.\n[Thi90] Thinking Machines Corporation. The CM-2 Technical Summary . Cambridge, MA. 1990.\n[Thi91] Thinking Machines Corporation. The CM-5 Technical Summary . Cambridge, MA. 1991.\n[Tho83] C. D. Thompson. Fourier transforms in VLSI . IBM Journal of Research and\nDevelopment , C-32(11):1047\u20131057, 1983.\n[Thr99] J. Throop. Standards: OpenMP: Shared-memory parallelism from the ashes . Computer ,\n32(5):108\u2013109, May 1999.\n[Tic88] W. F. Tichy. Parallel matrix multiplication on the connection machine . Technical Report\nRIACS TR 88.41, Research Institute for Advanced Computer Science, NASA Ames Research\nCenter, Moffet Field, CA, 1988.\n[TK77] C. D. Thompson and H. T. Kung. Sorting on a mesh-connected parallel computer .\nCommunications of the ACM , 21:263\u2013271, 1977.\n[TL90] Z. Tang and G.-J. Li. Optimal granularity of grid iteration problems . In Proceedings of\nthe 1990 International Conference on Parallel Processing , I111\u2013I118, 1990.\n[Tul96] D. M. Tullsen. Simultaneous multithreading . Ph.D. Thesis, University of Washington,\nSeattle, WA, 1996.\n[TV85] R. E. Tarjan and U. Vishkin. An efficient parallel biconnectivity algorithm . SIAM Journal\non Computing , 14(4):862\u2013874, November 1985.\n[TY96] J.-Y. Tsai and P.-C. Yew. The superthreaded architecture: Thread pipelining with run-\ntime data dependence checking and control speculation . In Proceedings of the International\nConference on Parallel Architectures and Compilation Techniques , 35\u201346, 1996.\n[Upf84] E. Upfal. A probabilistic relation between desirable and feasible models of parallel\ncomputation . In Proceedings of the 16th ACM Conference on Theory of Computing , 258\u2013265,\n1984.\n[UW84] E. Upfal and A. Widgerson. How to share memory in a distributed system . In\nProceedings of the 25th Annual Symposium on the Foundation of Computer Science , 171\u2013180,\n1984.\n[Val75] L. G. Valiant. Parallelism in comparison problems . SIAM Journal of Computing ,\n4(3):348\u2013355, September 1975.\n[Val82] L. G. Valiant. A scheme for fast parallel communication . SIAM Journal on Computing ,\n11:350\u2013361, 1982.\n[Val90a] L. G. Valiant. A bridging model for parallel computation . Communications of the ACM ,\n33(8), 1990.\n[Val90b] L. G. Valiant. General purpose parallel architectures . Handbook of Theoretical\nComputer Science , 1990.\n[Vav89] S. A. Vavasis. Gaussian elimination with pivoting is P-complete . SIAM Journal on\nDiscrete Mathematics , 2:413\u2013423, 1989.\n[VB81] L. G. Valiant and G. J. Brebner. Universal schemes for parallel communication . In\nProceedings of the 13th ACM Symposium on Theory of Computation , 263\u2013277, 1981.\n[VC89] F. A. Van-Catledge. Towards a general model for evaluating the relative performance of\ncomputer systems . International Journal of Supercomputer Applications , 3(2):100\u2013108, 1989.\n[Vor86] O. Vornberger. Implementing branch-and-bound in a ring of processors . Technical\nReport 29, University of Paderborn, Germany, 1986.\n[Vor87a] O.", "doc_id": "667906e4-5af2-4607-afde-796423524863", "embedding": null, "doc_hash": "cd81b0cbbe41262e2399b9980491041591a2c689dab765114e5c88d281e366a1", "extra_info": null, "node_info": {"start": 1625793, "end": 1628841}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "9722d968-fbd0-44b9-a02c-655a19a028a0", "3": "0f927881-92f7-43e8-a227-765004823485"}}, "__type__": "1"}, "0f927881-92f7-43e8-a227-765004823485": {"__data__": {"text": "G. Valiant and G. J. Brebner. Universal schemes for parallel communication . In\nProceedings of the 13th ACM Symposium on Theory of Computation , 263\u2013277, 1981.\n[VC89] F. A. Van-Catledge. Towards a general model for evaluating the relative performance of\ncomputer systems . International Journal of Supercomputer Applications , 3(2):100\u2013108, 1989.\n[Vor86] O. Vornberger. Implementing branch-and-bound in a ring of processors . Technical\nReport 29, University of Paderborn, Germany, 1986.\n[Vor87a] O. Vornberger. The personal supercomputer: A network of transputers . In Proceedings\nof the 1987 International Conference on Supercomputing , 1987.\n[Vor87b] O. Vornberger. Load balancing in a network of transputers . In Proceedings of the\nSecond International Workshop on Distributed Parallel Algorithms , 1987.\n[VS86] J. S. Vitter and R. A. Simmons. New classes for parallel complexity: A study of\nunification and other complete problems for P . IEEE Transactions on Computers , May 1986.\n[VSBR83] L. G. Valiant, S. Skyum, S. Berkowitz, and C. Rackoff. Fast parallel computation of\npolynomials using few processors . SIAM Journal of Computing , 12(4):641\u2013644, 1983.\n[WA98] B. Wilkinson and C. M. Allen. Parallel Programming: Techniques and Applications Using\nNetworked Workstations and Parallel Computers . Prentice Hall, 1998.\n[WA99] B. Wilkinson and M. Allen. Parallel Programming . Prentice-Hall, NJ, 1999.\n[Wag87] B. A. Wagar. Hyperquicksort: A fast sorting algorithm for hypercubes . In Proceedings\nof the Second Conference on Hypercube Multiprocessors , 292\u2013 299, 1987.\n[Wal91] Y. Wallach. Parallel Processing and Ada . Prentice-Hall, Englewood Cliffs, NJ, 1991.\n[WB72] W. A. Wulf and C. G. Bell. C.mmp\u2014a multimicroprocessor . In Proceedings of AFIPS\nConference , 765\u2013777, 1972.\n[Wei97] B. Weissman. Active threads manual . Technical Report TR-97-037, International\nComputer Science Institute, Berkeley, CA 94704, 1997.\n[WF80] C. L. Wu and T. Y. Feng. On a class of multistage interconnection networks . IEEE\nTransactions on Computers , 669\u2013702, August 1980.\n[WF84] C. L. Wu and T. Y. Feng. Interconnection Networks for Parallel and Distributed\nProcessing . IEEE Computer Society Press, Washington, DC, 1984.\n[WI89] K. Wada and N. Ichiyoshi. A distributed shortest path algorithm and its mapping on the\nMulti-PSI . In Proceedings of International Conference of Parallel Processing , 1989.\n[Wil86] H. S. Wilf. Algorithms and Complexity . Prentice-Hall, NJ, 1986.\n[Wil95] G. V. Wilson. Practical Parallel Programming . MIT Press, Cambridge, MA, 1995.\n[Wil00] A. Williams. Windows 2000 Systems Programming Black Book . The Coriolis Group,\n2000.\n[Win77] S. Winograd. A new method for computing DFT . In IEEE International Conference on\nAcoustics, Speech and Signal Processing , 366\u2013368, 1977.\n[WL88] B. W. Wah and G.-J. Li. Systolic processing for dynamic programming problems .\nCircuits, Systems, and Signal Processing , 7(2):119\u2013149, 1988.\n[WLY84] B. W. Wah, G.-J. Li, and C. F. Yu. The status of MANIP\u2014a multicomputer architecture\nfor solving combinatorial extremum-search", "doc_id": "0f927881-92f7-43e8-a227-765004823485", "embedding": null, "doc_hash": "4165872fb1ab6545d976d7410bdedc7459b6bc2ee231e36901b329eefd6c576d", "extra_info": null, "node_info": {"start": 1628796, "end": 1631870}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "667906e4-5af2-4607-afde-796423524863", "3": "95e21963-a06a-49ef-9ffb-683fc5e93a0c"}}, "__type__": "1"}, "95e21963-a06a-49ef-9ffb-683fc5e93a0c": {"__data__": {"text": "S. Winograd. A new method for computing DFT . In IEEE International Conference on\nAcoustics, Speech and Signal Processing , 366\u2013368, 1977.\n[WL88] B. W. Wah and G.-J. Li. Systolic processing for dynamic programming problems .\nCircuits, Systems, and Signal Processing , 7(2):119\u2013149, 1988.\n[WLY84] B. W. Wah, G.-J. Li, and C. F. Yu. The status of MANIP\u2014a multicomputer architecture\nfor solving combinatorial extremum-search problems . In Proceedings of 11th Annual\nInternational Symposium on Computer Architecture , 56\u201363, 1984.\n[WM84] B. W. Wah and Y. W. E. Ma. MANIP\u2014a multicomputer architecture for solving\ncombinatorial extremum-search problems . IEEE Transactions on Computers , C\u201333, May 1984.\n[Woo86] J. V. Woods, editor. Fifth Generation Computer Architectures . North-Holland,\nAmsterdam, The Netherlands, 1986.\n[Wor88] P. H. Worley. Information Requirements and the Implications for Parallel Computation .\nPh.D. Thesis, Stanford University, Department of Computer Science, Palo Alto, CA, 1988.\n[Wor90] P. H. Worley. The effect of time constraints on scaled speedup . SIAM Journal on\nScientific and Statistical Computing , 11(5):838\u2013858, 1990.\n[Wor91] P. H. Worley. Limits on parallelism in the numerical solution of linear PDEs . SIAM\nJournal on Scientific and Statistical Computing , 12:1\u201335, January 1991.\n[WS88] Y. Won and S. Sahni. A balanced bin sort for hypercube multiprocessors . Journal of\nSupercomputing , 2:435\u2013448, 1988.\n[WS89] J. Woo and S. Sahni. Hypercube computing: Connected components . Journal of\nSupercomputing , 3:209\u2013234, 1989.\n[WS91] J. Woo and S. Sahni. Computing biconnected components on a hypercube . Journal of\nSupercomputing , June 1991. Also available as Technical Report TR 89-7 from the Department of\nComputer Science, University of Minnesota, Minneapolis, MN.\n[WY85] B. W. Wah and C. F. Yu. Stochastic modeling of branch-and-bound algorithms with\nbest-first search . IEEE Transactions on Software Engineering , SE-11, September 1985.\n[Zho89] X. Zhou. Bridging the gap between Amdahl's law and Sandia laboratory's result .\nCommunications of the ACM , 32(8):1014\u20135, 1989.\n[Zom96] A. Zomaya, editor. Parallel and Distributed Computing Handbook . McGraw-Hill, 1996.\n[ZRV89] J. R. Zorbas, D. J. Reble, and R. E. VanKooten. Measuring the scalability of parallel\ncomputer systems . In Supercomputing '89 Proceedings , 832\u2013841, 1989.[ Team LiB ]\n", "doc_id": "95e21963-a06a-49ef-9ffb-683fc5e93a0c", "embedding": null, "doc_hash": "82ffe52a584e5b44209daadfeeaff57e11c863102b6c7c0477cc1403ec7f8fbb", "extra_info": null, "node_info": {"start": 1631882, "end": 1634260}, "relationships": {"1": "cb84743a-e549-4d56-8967-58adb8bab729", "2": "0f927881-92f7-43e8-a227-765004823485"}}, "__type__": "1"}}}